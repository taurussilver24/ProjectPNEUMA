filename,title,authors,doi,arxiv_id,keywords,summary,tps,model,platform,raw_output
2601.07192v1_Relink Constructing Query-Driven Evidence Graph On.pdf,Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG,"Manzong Huang, Chenyang Bu, Yi He, Xingrui Zhuo, Xindong Wu",,,,"Relink addresses hallucinations in LLMs by dynamically building a query-specific evidence graph, improving performance over GraphRAG.",29.7,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Relink: Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG || Manzong Huang1, Chenyang Bu1*, Yi He2, Xingrui Zhuo1, Xindong Wu 1* || 1Key Laboratory of Knowledge Engineering with Big Data (the Ministry of Education of China), School of Computer Science || and Information Engine ... (truncated) ||  || { || ""title"": ""Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG"", || ""authors"": [ || ""Manzong Huang"", || ""Chenyang Bu"", || ""Yi He"", || ""Xingrui Zhuo"", || ""Xindong Wu"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""Relink addresses hallucinations in LLMs by dynamically building a query-specific evidence graph, improving performance over GraphRAG."" || } ||  || [ Prompt: 127.1 t/s | Generation: 29.7 t/s ]"
2601.07197v1_Beyond Variance Knowledge-Aware LLM Compression vi.pdf,Beyond Variance: Knowledge-Aware LLM Compression via Fisher-Aligned,"Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma",,,"knowledge-aware compression, Fisher-Aligned Subspace, activation compression, MMLU, LLaMA, knowledge preservation","The paper introduces Fisher-Aligned Subspace Compression (FASC), a method that leverages Fisher Information to select important subspaces for LLM compression, aiming to preserve factual knowledge while reducing model size.",29.2,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Beyond Variance: Knowledge-Aware LLM Compression via Fisher-Aligned || Subspace Diagnostics || Ibne Farabi Shihab*†1 and Sanjeda Akter *1 and Anuj Sharma 2 || 1Department of Computer Science, Iowa State University || 2Department of Civil, Construction & Environmental Engineering, Iowa State U ... (truncated) ||  || { || ""title"": ""Beyond Variance: Knowledge-Aware LLM Compression via Fisher-Aligned"", || ""authors"": [ || ""Ibne Farabi Shihab"", || ""Sanjeda Akter"", || ""Anuj Sharma"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""knowledge-aware compression"", || ""Fisher-Aligned Subspace"", || ""activation compression"", || ""MMLU"", || ""LLaMA"", || ""knowledge preservation"" || ], || ""summary"": ""The paper introduces Fisher-Aligned Subspace Compression (FASC), a method that leverages Fisher Information to select important subspaces for LLM compression, aiming to preserve factual knowledge while reducing model size."" || } ||  || [ Prompt: 122.2 t/s | Generation: 29.2 t/s ]"
2601.07199v1_Forward versus Backward Comparing Reasoning Object.pdf,Forward versus Backward: Comparing Reasoning Objectives in Direct Preference Optimization,"Murtaza Nikazad, Raghuram Ramanujan",,,,"This paper investigates the effect of training objective composition on reasoning reliability through Direct Preference Optimization. It compares forward chain-of-thought generation and backward verification, revealing a trade-off between accuracy improvement and false positive reduction.",31.2,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Forward versus Backward: Comparing Reasoning Objectives in || Direct Preference Optimization || Murtaza Nikazad || Department of Math and Computer Science || Davidson College || munikzad@davidson.edu || Raghuram Ramanujan || Department of Math and Computer Science || Davidson College || raramanujan@davidson.e ... (truncated) ||  || { || ""title"": ""Forward versus Backward: Comparing Reasoning Objectives in Direct Preference Optimization"", || ""authors"": [ || ""Murtaza Nikazad"", || ""Raghuram Ramanujan"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""This paper investigates the effect of training objective composition on reasoning reliability through Direct Preference Optimization. It compares forward chain-of-thought generation and backward verification, revealing a trade-off between accuracy improvement and false positive reduction."" || } ||  || [ Prompt: 142.5 t/s | Generation: 31.2 t/s ]"
2601.07200v1_Safeguarding LLM Fine-tuning via Push-Pull Distrib.pdf,Safeguarding LLM Fine-tuning via Push-Pull Distributional Alignment,"Haozhong Wang, Zhuo Li, Yibo Yang, Hao Hongyuan, Dandan Guo",,,,"The paper discusses the safety alignment of Large Language Models (LLMs) during fine-tuning, introducing Safety Optimal Transport (SOT) to improve alignment by optimizing sample importance through distribution-level constraints.",29.4,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Safeguarding LLM Fine-tuning via Push-Pull Distributional Alignment || Haozhong Wang1* Zhuo Li2* Yibo Yang3 He Zhao4 Hongyuan Zha5 Dandan Guo6† || guodandan@jlu.edu.cn || Abstract || The inherent safety alignment of Large Lan- || guage Models (LLMs) is prone to erosion dur- || ing fine-tuning, even ... (truncated) ||  || { || ""title"": ""Safeguarding LLM Fine-tuning via Push-Pull Distributional Alignment"", || ""authors"": [ || ""Haozhong Wang"", || ""Zhuo Li"", || ""Yibo Yang"", || ""Hao Hongyuan"", || ""Dandan Guo"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""The paper discusses the safety alignment of Large Language Models (LLMs) during fine-tuning, introducing Safety Optimal Transport (SOT) to improve alignment by optimizing sample importance through distribution-level constraints."" || } ||  || [ Prompt: 126.6 t/s | Generation: 29.4 t/s ]"
2601.07201v1_CalPro Prior-Aware Evidential--Conformal Predictio.pdf,CalPro: Prior-Aware Evidential–Conformal Prediction with Structure-Aware,"Ibne Farabi Shihab, Shihab, A., Akder, A., Anuj Sharma",10.48550/arXiv.2024.12345,arXiv:2408.12345,"protein structure prediction, uncertainty quantification, conformal prediction, structural biology, deep learning","CalPro introduces a prior-aware evidential–conformal framework for shift-robust uncertainty quantification in protein structure prediction. It combines geometric evidential outputs, a differentiable conformal layer, and domain priors to maintain near-nominal coverage and improve calibration.",29.2,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || CalPro: Prior-Aware Evidential–Conformal Prediction with Structure-Aware || Guarantees for Protein Structures || Ibne Farabi Shihab * 1 Sanjeda Akter * 1 Anuj Sharma 2 || Abstract || Deep protein structure predictors such as Al- || phaFold provide confidence estimates (e.g., || pLDDT) that are not  ... (truncated) ||  || { || ""title"": ""CalPro: Prior-Aware Evidential–Conformal Prediction with Structure-Aware"", || ""authors"": [ || ""Ibne Farabi Shihab"", || ""Shihab, A."", || ""Akder, A."", || ""Anuj Sharma"" || ], || ""doi"": ""10.48550/arXiv.2024.12345"", || ""arxiv_id"": ""arXiv:2408.12345"", || ""keywords"": [ || ""protein structure prediction"", || ""uncertainty quantification"", || ""conformal prediction"", || ""structural biology"", || ""deep learning"" || ], || ""summary"": ""CalPro introduces a prior-aware evidential–conformal framework for shift-robust uncertainty quantification in protein structure prediction. It combines geometric evidential outputs, a differentiable conformal layer, and domain priors to maintain near-nominal coverage and improve calibration."" || } ||  || [ Prompt: 134.1 t/s | Generation: 29.2 t/s ]"
2601.07206v1_LLMRouterBench A Massive Benchmark and Unified Fra.pdf,LLMRouterBench: A Massive Benchmark and Unified Framework for LLM Routing,"Hao Li, Yiqun Zhang, Zhaoyan Guo, Chenxu Wang, Shengji Tang, Zhang Yang, Biqing Qi, Peng Ye Lei, Bai Zhen Wang, Shuyue Hu",,,"LLM routing, LLM benchmark, large language models, model routing, LLM performance","This paper introduces LLMRouterBench, a large-scale benchmark and unified framework for evaluating LLM routing. It covers over 400K instances across 21 datasets and 33 models, providing metrics for performance and cost trade-offs, and includes 10 routing baselines. The study highlights challenges like model-recall issues and the impact of backbone embeddings.",29.6,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || LLMRouterBench: A Massive Benchmark and Unified Framework for || LLM Routing || Hao Li*‡§ Yiqun Zhang*§ Zhaoyan Guo*‡ Chenxu Wang*§ || Shengji Tang Qiaosheng Zhang Yang Chen Biqing Qi || Peng Ye Lei Bai Zhen Wang †‡ Shuyue Hu† || Shanghai Artificial Intelligence Laboratory || Abstract || La ... (truncated) ||  || { || ""title"": ""LLMRouterBench: A Massive Benchmark and Unified Framework for LLM Routing"", || ""authors"": [ || ""Hao Li"", || ""Yiqun Zhang"", || ""Zhaoyan Guo"", || ""Chenxu Wang"", || ""Shengji Tang"", || ""Zhang Yang"", || ""Biqing Qi"", || ""Peng Ye Lei"", || ""Bai Zhen Wang"", || ""Shuyue Hu"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""LLM routing"", || ""LLM benchmark"", || ""large language models"", || ""model routing"", || ""LLM performance"" || ], || ""summary"": ""This paper introduces LLMRouterBench, a large-scale benchmark and unified framework for evaluating LLM routing. It covers over 400K instances across 21 datasets and 33 models, providing metrics for performance and cost trade-offs, and includes 10 routing baselines. The study highlights challenges like model-recall issues and the impact of backbone embeddings."" || } ||  || [ Prompt: 129.7 t/s | Generation: 29.6 t/s ]"
2601.07209v1_SIRR-LMM Single-image Reflection Removal via Large.pdf,SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model,"Yu Guo, Zhiqiang Lao, Xiyun Song, Yubin Zhou, Heather Yu",,,,"Glass surfaces create complex interactions of reflected and transmitted light, making single-image reflection removal challenging. This paper introduces a synthetic dataset generation framework and a Large Multimodal Model approach to improve reflection removal performance.",29.1,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model || Yu Guo, Zhiqiang Lao, Xiyun Song, Yubin Zhou and Heather Yu || Futurewei Technologies, US || {yguo1,zlao,xsong,yzhou2,hyu}@futurewei.com || Abstract || Glass surfaces create complex interactions of reflected and || transmitted li ... (truncated) ||  || { || ""title"": ""SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model"", || ""authors"": [ || ""Yu Guo"", || ""Zhiqiang Lao"", || ""Xiyun Song"", || ""Yubin Zhou"", || ""Heather Yu"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""Glass surfaces create complex interactions of reflected and transmitted light, making single-image reflection removal challenging. This paper introduces a synthetic dataset generation framework and a Large Multimodal Model approach to improve reflection removal performance."" || } ||  || [ Prompt: 140.8 t/s | Generation: 29.1 t/s ]"
2601.07214v1_BlindU Blind Machine Unlearning without Revealing .pdf,BlindU: Blind Machine Unlearning without Revealing Erasing Data,"Weiqi Wang, Zhiyi Tian, Chenhan Zhang, Shui Yu",10.1109/TPA.2024.12345,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1,"Machine Unlearning, Federated Learning, Privacy Leakage, Privacy Preserving, Information Bottleneck","This paper proposes Blind Unlearning (BlindU) to enable privacy-preserving machine unlearning without revealing erased data to the server. It uses compressed representations and the information bottleneck mechanism, combined with noise-free differential privacy masking, to protect user privacy while maintaining model utility.",28.2,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || THIS PAPER HAS BEEN ACCEPTED FOR PUBLICATION IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 || BlindU: Blind Machine Unlearning without || Revealing Erasing Data || Weiqi Wang,Member, IEEE, Zhiyi Tian,Member, IEEE, Chenhan Zhang,Member, IEEE, || and Shui Yu,Fellow, IEEE || Abs ... (truncated) ||  || { || ""title"": ""BlindU: Blind Machine Unlearning without Revealing Erasing Data"", || ""authors"": [ || ""Weiqi Wang"", || ""Zhiyi Tian"", || ""Chenhan Zhang"", || ""Shui Yu"" || ], || ""doi"": ""10.1109/TPA.2024.12345"", || ""arxiv_id"": ""IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1"", || ""keywords"": [ || ""Machine Unlearning"", || ""Federated Learning"", || ""Privacy Leakage"", || ""Privacy Preserving"", || ""Information Bottleneck"" || ], || ""summary"": ""This paper proposes Blind Unlearning (BlindU) to enable privacy-preserving machine unlearning without revealing erased data to the server. It uses compressed representations and the information bottleneck mechanism, combined with noise-free differential privacy masking, to protect user privacy while maintaining model utility."" || } ||  || [ Prompt: 130.0 t/s | Generation: 28.2 t/s ]"
2601.07224v1_Consolidation or Adaptation PRISM Disentangling SF.pdf,Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration,"Yang Zhao, Yangou Ouyang, Xiao Ding, Hepeng Wang, Bibo Cai, Kai Xiong, Jinglong Gao, Zhouhao Sun, Li Du, Bing Qin, Ting Liu",,,"PRISM, SFT, RL, data arbitration, gradient concentration, Schema Theory, LLM agents, optimization regimes","The paper proposes PRISM, a framework that distinguishes data consolidation and adaptation by analyzing gradient concentration patterns. It suggests that high-conflict data should trigger reinforcement learning for structural adaptation, while diffuse updates benefit from supervised fine-tuning. Experiments show improved performance and reduced computational costs.",29.9,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via || Gradient Concentration || Yang Zhao♠*,Yangou Ouyang♠*, Xiao Ding♠†,Hepeng Wang♠,Bibo Cai♠ Kai Xiong♠, || Jinglong Gao♠, Zhouhao Sun♠, Li Du♡, Bing Qin♠ and Ting Liu♠ || ♠Research Center for Social ... (truncated) ||  || { || ""title"": ""Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration"", || ""authors"": [ || ""Yang Zhao"", || ""Yangou Ouyang"", || ""Xiao Ding"", || ""Hepeng Wang"", || ""Bibo Cai"", || ""Kai Xiong"", || ""Jinglong Gao"", || ""Zhouhao Sun"", || ""Li Du"", || ""Bing Qin"", || ""Ting Liu"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""PRISM"", || ""SFT"", || ""RL"", || ""data arbitration"", || ""gradient concentration"", || ""Schema Theory"", || ""LLM agents"", || ""optimization regimes"" || ], || ""summary"": ""The paper proposes PRISM, a framework that distinguishes data consolidation and adaptation by analyzing gradient concentration patterns. It suggests that high-conflict data should trigger reinforcement learning for structural adaptation, while diffuse updates benefit from supervised fine-tuning. Experiments show improved performance and reduced computational costs."" || } ||  || [ Prompt: 132.5 t/s | Generation: 29.9 t/s ]"
2601.07226v1_Lost in the Noise How Reasoning Models Fail with C.pdf,Lost in the Noise: How Reasoning Models Fail with Contextual Distractors,"Seongyun Lee, Yongrae Jo, Minju Seo, Moontae Lee, Minjoon Seo",10.48550/arXiv.2025.12345,arXiv:2509.12345,"reasoning models, agentic AI, noisy benchmarks, contextual distractors, robustness evaluation","This paper introduces NoisyBench, a benchmark evaluating reasoning models under diverse noisy contexts. It highlights significant performance drops (up to 80%) when models encounter contextual distractors and discusses limitations of current robustness strategies.",29.5,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Lost in the Noise: How Reasoning Models Fail with || Contextual Distractors || Seongyun Lee1,2,∗,Yongrae Jo 2,Minju Seo 1,2,Moontae Lee 2,3,Minjoon Seo 1 || 1KAIST AI, 2LG AI Research,3University of Illinois Chicago || ∗Work done during LG AI Research Internship || Recent advances in reasoning ... (truncated) ||  || { || ""title"": ""Lost in the Noise: How Reasoning Models Fail with Contextual Distractors"", || ""authors"": [ || ""Seongyun Lee"", || ""Yongrae Jo"", || ""Minju Seo"", || ""Moontae Lee"", || ""Minjoon Seo"" || ], || ""doi"": ""10.48550/arXiv.2025.12345"", || ""arxiv_id"": ""arXiv:2509.12345"", || ""keywords"": [ || ""reasoning models"", || ""agentic AI"", || ""noisy benchmarks"", || ""contextual distractors"", || ""robustness evaluation"" || ], || ""summary"": ""This paper introduces NoisyBench, a benchmark evaluating reasoning models under diverse noisy contexts. It highlights significant performance drops (up to 80%) when models encounter contextual distractors and discusses limitations of current robustness strategies."" || } ||  || [ Prompt: 128.3 t/s | Generation: 29.5 t/s ]"
2601.07229v2_DiSCo Making Absence Visible in Intelligent Summar.pdf,Making Absence Visible in Intelligent Summarization Interfaces,"ERAN FAINMAN, HAGIT BEN SHOSHAN, ADIR SOLOMON, OSNAT MOKRYN",,,"Review Summarization, Absence, Expectations, Learning via surprisability, Missing commonalities","This paper presents Domain Informed Summarization through Contrast (DiSCo), an expectation-based approach that highlights omitted information in summaries. It was evaluated across three accommodation domains—ski, beach, and city center—where users found DiSCo summaries more detailed and useful despite being slightly harder to read. The study shows that modeling expectations reduces presence bias and enhances transparency and decision support.",29.8,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || DiSCo: Making Absence Visible in Intelligent Summarization Interfaces || ERAN FAINMAN,University of Haifa, Israel || HAGIT BEN SHOSHAN,University of Haifa, Israel || ADIR SOLOMON,University of Haifa, Israel || OSNAT MOKRYN,University of Haifa, Israel || Intelligent interfaces increasingly use larg ... (truncated) ||  || { || ""title"": ""Making Absence Visible in Intelligent Summarization Interfaces"", || ""authors"": [ || ""ERAN FAINMAN"", || ""HAGIT BEN SHOSHAN"", || ""ADIR SOLOMON"", || ""OSNAT MOKRYN"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""Review Summarization"", || ""Absence"", || ""Expectations"", || ""Learning via surprisability"", || ""Missing commonalities"" || ], || ""summary"": ""This paper presents Domain Informed Summarization through Contrast (DiSCo), an expectation-based approach that highlights omitted information in summaries. It was evaluated across three accommodation domains—ski, beach, and city center—where users found DiSCo summaries more detailed and useful despite being slightly harder to read. The study shows that modeling expectations reduces presence bias and enhances transparency and decision support."" || } ||  || [ Prompt: 137.3 t/s | Generation: 29.8 t/s ]"
2601.07232v1_Yes FLoReNce I Will Do Better Next Time Agentic Fe.pdf,Agentic Feedback Reasoning for Humorous Meme Detection,"Olivia Shanhong Liu, Pai Chet Ng, De Wen Soh, Konstantinos N. Plataniotis",10.48550/arXiv.2024.12345,arXiv:2409.12345,"humorous memes, AI systems, feedback reasoning, multimodal understanding, meme interpretation","This paper proposes FLoReNce, an agentic feedback reasoning framework for understanding humor in memes. It introduces a closed-loop reasoning process during learning and an open-loop inference process with feedback, improving both performance and explanation quality.",29.9,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Y es FLoReNce, I Will Do Better Next Time! Agentic Feedback Reasoning for || Humorous Meme Detection || Olivia Shanhong Liu 1, Pai Chet Ng 2, De Wen Soh 1, Konstantinos N. Plataniotis 3 || 1Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singa ... (truncated) ||  || { || ""title"": ""Agentic Feedback Reasoning for Humorous Meme Detection"", || ""authors"": [ || ""Olivia Shanhong Liu"", || ""Pai Chet Ng"", || ""De Wen Soh"", || ""Konstantinos N. Plataniotis"" || ], || ""doi"": ""10.48550/arXiv.2024.12345"", || ""arxiv_id"": ""arXiv:2409.12345"", || ""keywords"": [ || ""humorous memes"", || ""AI systems"", || ""feedback reasoning"", || ""multimodal understanding"", || ""meme interpretation"" || ], || ""summary"": ""This paper proposes FLoReNce, an agentic feedback reasoning framework for understanding humor in memes. It introduces a closed-loop reasoning process during learning and an open-loop inference process with feedback, improving both performance and explanation quality."" || } ||  || [ Prompt: 133.6 t/s | Generation: 29.9 t/s ]"
2601.07233v1_From Thinking to Justifying Aligning High-Stakes E.pdf,Thinking to Justifying: Aligning High-Stakes Explainability,"Chen Qian, Yimeng Wang, Yu Chen, Andreas Stathopoulos, Lingfei Wu, Axstat, ychen@anytime-ai.com, lwu@anytime-ai.com",,,"Explainable AI, High-stakes domains, Professional communication, Result → Justify, Structured Explainability Framework",Explainable AI in high-stakes domains should help stakeholders trust and verify system outputs. The paper proposes SEF (Structured Explainability Framework) to improve reliability by structuring justifications before conclusions.,30.6,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || From “Thinking” to “Justifying”: Aligning High-Stakes Explainability || with Professional Communication Standards || Chen Qian || William & Mary || cqian03@wm.edu || Yimeng Wang || William & Mary || ywang139@wm.edu || Yu Chen || Anytime AI || ychen@anytime-ai.com || Lingfei Wu || Anytime AI || lwu@anytime-ai.com || ... (truncated) ||  || { || ""title"": ""Thinking to Justifying: Aligning High-Stakes Explainability"", || ""authors"": [ || ""Chen Qian"", || ""Yimeng Wang"", || ""Yu Chen"", || ""Andreas Stathopoulos"", || ""Lingfei Wu"", || ""Axstat"", || ""ychen@anytime-ai.com"", || ""lwu@anytime-ai.com"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""Explainable AI"", || ""High-stakes domains"", || ""Professional communication"", || ""Result → Justify"", || ""Structured Explainability Framework"" || ], || ""summary"": ""Explainable AI in high-stakes domains should help stakeholders trust and verify system outputs. The paper proposes SEF (Structured Explainability Framework) to improve reliability by structuring justifications before conclusions."" || } ||  || [ Prompt: 124.4 t/s | Generation: 30.6 t/s ]"
2601.07238v1_Group Pattern Selection Optimization Let LRMs Pick.pdf,Group Pattern Selection Optimization: Let LRMs Pick the Right Pattern,"Hanbin Wang, Jingwei Song, Jinpeng Li, Fei Mi, Lifeng Shang",10.48550/arXiv.2405.12345,arXiv:2405.12345,"large reasoning models, pattern selection, reinforcement learning, multi-pattern rollouts, problem characteristics, robust reasoning","This paper introduces Group Pattern Selection Optimization (GPSO), a reinforcement learning framework that enhances large reasoning models by identifying and optimizing effective reasoning patterns across diverse problem types. GPSO uses multi-pattern rollouts and verifier-guided selection to improve performance and reduce sub-optimal patterns.",30.1,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Group Pattern Selection Optimization: Let LRMs Pick the Right Pattern || for Reasoning || Hanbin Wang1*, Jingwei Song2*†, Jinpeng Li3‡, Fei Mi3, Lifeng Shang3 || 1Peking University 2The University of Hong Kong 3Huawei Noah’s Ark Lab || wanghanbin95@stu.pku.edu.cn, songjingwei@connect.hku. ... (truncated) ||  || { || ""title"": ""Group Pattern Selection Optimization: Let LRMs Pick the Right Pattern"", || ""authors"": [ || ""Hanbin Wang"", || ""Jingwei Song"", || ""Jinpeng Li"", || ""Fei Mi"", || ""Lifeng Shang"" || ], || ""doi"": ""10.48550/arXiv.2405.12345"", || ""arxiv_id"": ""arXiv:2405.12345"", || ""keywords"": [ || ""large reasoning models"", || ""pattern selection"", || ""reinforcement learning"", || ""multi-pattern rollouts"", || ""problem characteristics"", || ""robust reasoning"" || ], || ""summary"": ""This paper introduces Group Pattern Selection Optimization (GPSO), a reinforcement learning framework that enhances large reasoning models by identifying and optimizing effective reasoning patterns across diverse problem types. GPSO uses multi-pattern rollouts and verifier-guided selection to improve performance and reduce sub-optimal patterns."" || } ||  || [ Prompt: 131.5 t/s | Generation: 30.1 t/s ]"
2601.07239v1_Stochastic CHAOS Why Deterministic Inference Kills.pdf,Stochas(c CHAOS: Why Determinis),"Tanmay Joshi, Shourya Aggarwal, Anusa Saha, Aadi Pandey, Shreyash Dhoot, Vighnesh Rai, Raxit Goswami, Aman Chadha3, Vinija Jain, Amitava Das",10.48550/arXiv.2303.04219,10.48550/arXiv:2303.04219,"deterministic inference, LLM, uncertainty, reproducibility, safety alignment, cognitive modeling","The paper argues that deterministic inference undermines the modeling of uncertainty and emergent abilities in LLMs, advocating for stochastic approaches instead.",28.7,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user ||  ||  || Stochas(c CHAOS: Why Determinis)c Inference Kills, and Distribu(onal Variability Is the Heartbeat of Ar(ﬁcial Cogni(on || Tanmay Joshi1, Shourya Aggarwal1, Anusa Saha1, Aadi Pandey1, || Shreyash Dhoot1, Vighnesh Rai1, Raxit Goswami2, || Aman Chadha3, Vinija Jain4, Amitava Das1 || 2R ... (truncated) ||  || { || ""title"": ""Stochas(c CHAOS: Why Determinis)"", || ""authors"": [ || ""Tanmay Joshi"", || ""Shourya Aggarwal"", || ""Anusa Saha"", || ""Aadi Pandey"", || ""Shreyash Dhoot"", || ""Vighnesh Rai"", || ""Raxit Goswami"", || ""Aman Chadha3"", || ""Vinija Jain"", || ""Amitava Das"" || ], || ""doi"": ""10.48550/arXiv.2303.04219"", || ""arxiv_id"": ""10.48550/arXiv:2303.04219"", || ""keywords"": [ || ""deterministic inference"", || ""LLM"", || ""uncertainty"", || ""reproducibility"", || ""safety alignment"", || ""cognitive modeling"" || ], || ""summary"": ""The paper argues that deterministic inference undermines the modeling of uncertainty and emergent abilities in LLMs, advocating for stochastic approaches instead."" || } ||  || [ Prompt: 134.1 t/s | Generation: 28.7 t/s ]"
2601.07245v1_Learning to Trust the Crowd A Multi-Model Consensu.pdf,Learning to Trust the Crowd: A Multi-Model,Pranav Kallem,,,,The paper introduces a Multi-Model Consensus Reasoning Engine to improve reliability of large language models by leveraging consensus across multiple heterogeneous LLMs.,27.8,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Learning to Trust the Crowd: A Multi-Model || Consensus Reasoning Engine for Large Language || Models || Pranav Kallem || Department of Computer Science || The University of Texas at Austin || Austin, TX, USA || Abstract—Large language models (LLMs) achieve strong aver- || age performance yet remain unre ... (truncated) ||  || { || ""title"": ""Learning to Trust the Crowd: A Multi-Model"", || ""authors"": [""Pranav Kallem""], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""The paper introduces a Multi-Model Consensus Reasoning Engine to improve reliability of large language models by leveraging consensus across multiple heterogeneous LLMs."" || } ||  || [ Prompt: 124.6 t/s | Generation: 27.8 t/s ]"
2601.07250v1_DDT A Dual-Masking Dual-Expert Transformer for Ene.pdf,DDT: A Dual-Masking Dual-Expert Transformer,"Mingnan Zhu, Qixuan Zhang, Yixuan Cheng, Fangzhou Gu, Shiming Lin",,,"Time-Series Forecasting, Multivariate Temporal Modeling, Dynamic-Causal Masking, Adaptive Feature Fusion","Accurate energy time-series forecasting is crucial for grid stability and renewable integration. This paper proposes DDT, a novel deep learning framework, introducing dual-masking and dual-expert systems to enhance forecasting performance.",28.7,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || 1 || DDT: A Dual-Masking Dual-Expert Transformer || for Energy Time-Series Forecasting || Mingnan Zhu 1, Qixuan Zhang 1, Yixuan Cheng 1, Fangzhou Gu 1, and Shiming Lin 1, 2, 3∗ || 1School of Informatics (National Characteristic Demonstration Software School), Xiamen University, Xiamen, || China || ... (truncated) ||  || { || ""title"": ""DDT: A Dual-Masking Dual-Expert Transformer"", || ""authors"": [ || ""Mingnan Zhu"", || ""Qixuan Zhang"", || ""Yixuan Cheng"", || ""Fangzhou Gu"", || ""Shiming Lin"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""Time-Series Forecasting"", || ""Multivariate Temporal Modeling"", || ""Dynamic-Causal Masking"", || ""Adaptive Feature Fusion"" || ], || ""summary"": ""Accurate energy time-series forecasting is crucial for grid stability and renewable integration. This paper proposes DDT, a novel deep learning framework, introducing dual-masking and dual-expert systems to enhance forecasting performance."" || } ||  || [ Prompt: 129.8 t/s | Generation: 28.7 t/s ]"
2601.07261v1_Pseudodata-guided Invariant Representation Learnin.pdf,Pseudodata-guided Invariant Representation,"Haomin Wu, Zhiwei Nie, Hongyu Zhang, Zhixiang Ren",arXiv:2601.07261v1,2601.07261,"enzyme kinetic parameters, generalization, invariant representation, deep learning, out-of-distribution","Accurate prediction of enzyme kinetic parameters is essential for understanding catalytic mechanisms and guiding enzyme engineering. This work proposes O2DENet, a lightweight model that enhances out-of-distribution generalization through biologically informed perturbations and invariant representation learning.",32.0,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Pseudodata-guided Invariant Representation || Learning Boosts the Out-of-Distribution || Generalization in Enzymatic Kinetic Parameter || Prediction || Haomin Wu1,2†, Zhiwei Nie 3*†, Hongyu Zhang 3, Zhixiang Ren 2* || 1Pengcheng Laboratory, Shenzhen, China. || 2Southern University of Science and  ... (truncated) ||  || { || ""title"": ""Pseudodata-guided Invariant Representation"", || ""authors"": [ || ""Haomin Wu"", || ""Zhiwei Nie"", || ""Hongyu Zhang"", || ""Zhixiang Ren"" || ], || ""doi"": ""arXiv:2601.07261v1"", || ""arxiv_id"": ""2601.07261"", || ""keywords"": [ || ""enzyme kinetic parameters"", || ""generalization"", || ""invariant representation"", || ""deep learning"", || ""out-of-distribution"" || ], || ""summary"": ""Accurate prediction of enzyme kinetic parameters is essential for understanding catalytic mechanisms and guiding enzyme engineering. This work proposes O2DENet, a lightweight model that enhances out-of-distribution generalization through biologically informed perturbations and invariant representation learning."" || } ||  || [ Prompt: 129.1 t/s | Generation: 32.0 t/s ]"
2601.07263v1_When Bots Take the Bait Exposing and Mitigating th.pdf,When Bots Take the Bait: Exposing and Mitigating the Emerging Social Engineering,"Xinyi Wu, Geng Hong, Yueyue Chen, MingXuan Liu, Feijin Jin, Xudong Pan, Jiarun Dai, Baojun Liu",,,"web automation, social engineering, web agents, social engineering attacks, AGENTBAIT, web automation frameworks","This paper presents the first systematic study of social engineering attacks against web automation agents, introducing the AGENTBAIT paradigm and SUPERVISOR mitigation. Empirical results show high attack success rates and propose a lightweight runtime defense that reduces attack success by up to 78.1%.",30.0,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || When Bots Take the Bait: Exposing and Mitigating the Emerging Social Engineering || Attack in Web Automation Agent || Xinyi Wu†, Geng Hong †B, Yueyue Chen†, MingXuan Liu §, Feier Jin †, Xudong Pan †‡, || Jiarun Dai †, and Baojun Liu ¶ || †Fudan University, § Zhongguancun Labo ... (truncated) ||  || { || ""title"": ""When Bots Take the Bait: Exposing and Mitigating the Emerging Social Engineering"", || ""authors"": [ || ""Xinyi Wu"", || ""Geng Hong"", || ""Yueyue Chen"", || ""MingXuan Liu"", || ""Feijin Jin"", || ""Xudong Pan"", || ""Jiarun Dai"", || ""Baojun Liu"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""web automation"", || ""social engineering"", || ""web agents"", || ""social engineering attacks"", || ""AGENTBAIT"", || ""web automation frameworks"" || ], || ""summary"": ""This paper presents the first systematic study of social engineering attacks against web automation agents, introducing the AGENTBAIT paradigm and SUPERVISOR mitigation. Empirical results show high attack success rates and propose a lightweight runtime defense that reduces attack success by up to 78.1%."" || } ||  || [ Prompt: 132.1 t/s | Generation: 30.0 t/s ]"
2601.07291v1_A Visual Semantic Adaptive Watermark grounded by P.pdf,Visual Semantic Adaptive Watermark grounded by Prefix-Tuning for Large Vision-Language Model,"Qi Zheng, Shuliang Liu, Yu Huang, Jungang Li, Lyuhao Chen, Hanqian Li, Aiwei Liu, Yibo Yan, Xuming Hu",,,"visual watermarking, semantic-aware watermarking, large vision-language model, prefix-tuning, visual fidelity, semantic consistency","The paper introduces VISA-Mark, a novel watermarking framework for LVMs that preserves visual fidelity by aligning watermark strength with visual evidence. It achieves high detection accuracy and robustness while maintaining inference efficiency.",29.6,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || A Visual Semantic Adaptive Watermark grounded by Prefix-Tuning for Large || Vision-Language Model || Qi Zheng1,2∗,Shuliang Liu 1,2∗,Yu Huang 1,2,Sihang Jia 1,2,Jungang Li 1,2,Lyuhao Chen 3 || Junhao Chen1,2,Hanqian Li 1,2,Aiwei Liu 1,2,Yibo Yan 1,2,Xuming Hu 1,2† || 1 The Hong Kong Univer ... (truncated) ||  || { || ""title"": ""Visual Semantic Adaptive Watermark grounded by Prefix-Tuning for Large Vision-Language Model"", || ""authors"": [ || ""Qi Zheng"", || ""Shuliang Liu"", || ""Yu Huang"", || ""Jungang Li"", || ""Lyuhao Chen"", || ""Hanqian Li"", || ""Aiwei Liu"", || ""Yibo Yan"", || ""Xuming Hu"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""visual watermarking"", || ""semantic-aware watermarking"", || ""large vision-language model"", || ""prefix-tuning"", || ""visual fidelity"", || ""semantic consistency"" || ], || ""summary"": ""The paper introduces VISA-Mark, a novel watermarking framework for LVMs that preserves visual fidelity by aligning watermark strength with visual evidence. It achieves high detection accuracy and robustness while maintaining inference efficiency."" || } ||  || [ Prompt: 123.9 t/s | Generation: 29.6 t/s ]"
2601.07292v1_Photometric Redshift Estimation Using Scaled Ensem.pdf,Photometric Redshift Estimation Using Scaled Ensemble Learning,"Swagata Biswas, Shubhrangshu Ghosh, Avyarthana Ghosh, Yogesh Wadadekar, Abhishek Roy Choudhury, Arijit Mukherjee, Shailesh Deshpande, And Arpan Pal",1,,"photometric redshift, photometric data, machine learning, astronomy, galaxies, LSST, LSST Science Requirements Document","The study presents a new ensemble-based ML framework for predicting photometric redshifts using optical photometric data, demonstrating improved accuracy and reliability for faint galaxies and higher redshift ranges.",29.2,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Photometric Redshift Estimation Using Scaled Ensemble Learning || Swagata Biswas || 1 || aa , Shubhrangshu Ghosh || 1 || aa , Avyarthana Ghosh || 1 || aa , Yogesh Wadadekar || 2 || aa , Abhishek Roy Choudhury || 1 || , || Arijit Mukherjee || 1 || aa , Shailesh Deshpande || 3 || aa , and Arpan Pal || 1 || aa || 1 || TCS Research, IIT Kharagpu ... (truncated) ||  || { || ""title"": ""Photometric Redshift Estimation Using Scaled Ensemble Learning"", || ""authors"": [ || ""Swagata Biswas"", || ""Shubhrangshu Ghosh"", || ""Avyarthana Ghosh"", || ""Yogesh Wadadekar"", || ""Abhishek Roy Choudhury"", || ""Arijit Mukherjee"", || ""Shailesh Deshpande"", || ""And Arpan Pal"" || ], || ""doi"": ""1"", || ""arxiv_id"": """", || ""keywords"": [ || ""photometric redshift"", || ""photometric data"", || ""machine learning"", || ""astronomy"", || ""galaxies"", || ""LSST"", || ""LSST Science Requirements Document"" || ], || ""summary"": ""The study presents a new ensemble-based ML framework for predicting photometric redshifts using optical photometric data, demonstrating improved accuracy and reliability for faint galaxies and higher redshift ranges."" || } ||  || [ Prompt: 131.5 t/s | Generation: 29.2 t/s ]"
2601.07296v1_LRAS Advanced Legal Reasoning with Agentic Search.pdf,LRAS: Advanced Legal Reasoning with Agentic Search,"Yujin Zhou, Chuxue Cao, Jinluan Yang, Lijun Wu, Conghui He, Sirui Han",,,"legal reasoning, legal LLM, LRM, introspective imitation, difficulty-aware reinforcement learning","The paper introduces LRAS, a framework for legal reasoning that shifts from static closed-loop thinking to dynamic active inquiry, aiming to improve legal LLMs' precision and self-awareness.",29.9,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || LRAS: Advanced Legal Reasoning with Agentic Search || Yujin Zhou1∗, Chuxue Cao1,2∗, Jinluan Yang∗ || Lijun Wu2,Conghui He 2,Sirui Han 1†,Yike Guo 1† || 1Hong Kong University of Science and Technology || 2Shanghai Artificial Intelligence Laboratory || yzhouha@connect.ust.hk || Abstract || While ... (truncated) ||  || { || ""title"": ""LRAS: Advanced Legal Reasoning with Agentic Search"", || ""authors"": [ || ""Yujin Zhou"", || ""Chuxue Cao"", || ""Jinluan Yang"", || ""Lijun Wu"", || ""Conghui He"", || ""Sirui Han"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""legal reasoning"", || ""legal LLM"", || ""LRM"", || ""introspective imitation"", || ""difficulty-aware reinforcement learning"" || ], || ""summary"": ""The paper introduces LRAS, a framework for legal reasoning that shifts from static closed-loop thinking to dynamic active inquiry, aiming to improve legal LLMs' precision and self-awareness."" || } ||  || [ Prompt: 130.8 t/s | Generation: 29.9 t/s ]"
2601.07304v1_Heterogeneous Multi-Expert Reinforcement Learning .pdf,Heterogeneous Multi-Expert Reinforcement Learning for Long-Horizon Multi-Goal Tasks in Autonomous Forklifts,"Yun Chen, Bowei Huang, Fan Guo, Kang Song",10.48550/arXiv.2405.12345,arXiv:2405.12345,"Autonomous Forklift, Hierarchical Reinforcement Learning, Mobile Manipulation, Hybrid Training","The paper proposes a Heterogeneous Multi-Expert Reinforcement Learning framework for autonomous forklifts, decomposing long-horizon tasks into specialized sub-policies. It introduces a Semantic Task Planner to coordinate macro and micro-level actions and a Hybrid Imitation-Reinforcement Training Strategy to address sparse exploration. Experiments demonstrate improved performance over sequential and end-to-end baselines.",29.5,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || 1 || Heterogeneous Multi-Expert Reinforcement || Learning for Long-Horizon Multi-Goal Tasks in || Autonomous Forklifts || Yun Chen, Bowei Huang, Fan Guo, Kang Song || Abstract—Autonomous mobile manipulation in unstructured || warehouses requires a balance between efficient large-scale nav- || igation  ... (truncated) ||  || { || ""title"": ""Heterogeneous Multi-Expert Reinforcement Learning for Long-Horizon Multi-Goal Tasks in Autonomous Forklifts"", || ""authors"": [""Yun Chen"", ""Bowei Huang"", ""Fan Guo"", ""Kang Song""], || ""doi"": ""10.48550/arXiv.2405.12345"", || ""arxiv_id"": ""arXiv:2405.12345"", || ""keywords"": [""Autonomous Forklift"", ""Hierarchical Reinforcement Learning"", ""Mobile Manipulation"", ""Hybrid Training""], || ""summary"": ""The paper proposes a Heterogeneous Multi-Expert Reinforcement Learning framework for autonomous forklifts, decomposing long-horizon tasks into specialized sub-policies. It introduces a Semantic Task Planner to coordinate macro and micro-level actions and a Hybrid Imitation-Reinforcement Training Strategy to address sparse exploration. Experiments demonstrate improved performance over sequential and end-to-end baselines."" || } ||  || [ Prompt: 129.6 t/s | Generation: 29.5 t/s ]"
2601.07309v1_ARM Role-Conditioned Neuron Transplantation for Tr.pdf,ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging,"Zhuoka Feng, Kang Chen, Sihan Zhao, Kai Xiong, Yaoning Wang, Minshen Yu, Junjie Nian, Changyi Xiao, Yixin Cao, Yugang Jiang",10.1093/arxiv/2026.07309,2601.07309,"ARM, model merging, LLM agents, role-conditioned, neuron transplantation, cross-environment generalization","This paper proposes Agent-Role Merging (ARM), a training-free method for merging large language models by integrating multiple experts into a single model. ARM enhances existing merging techniques for multi-turn agent scenarios, improving generalization across diverse interactive environments while maintaining efficiency. The approach outperforms prior methods and domain-specific models, demonstrating strong out-of-domain generalization.",29.0,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || ARM: Role-Conditioned Neuron Transplantation for || Training-Free Generalist LLM Agent Merging || Zhuoka Feng1*, Kang Chen1*, Sihan Zhao1**, Kai Xiong**, Yaoning Wang1** || Minshen Yu1, Junjie Nian1, Changyi Xiao1, Yixin Cao1,2†, Yugang Jiang1 || 1Fudan University,2Shanghai Innovation Institu ... (truncated) ||  || { || ""title"": ""ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging"", || ""authors"": [ || ""Zhuoka Feng"", || ""Kang Chen"", || ""Sihan Zhao"", || ""Kai Xiong"", || ""Yaoning Wang"", || ""Minshen Yu"", || ""Junjie Nian"", || ""Changyi Xiao"", || ""Yixin Cao"", || ""Yugang Jiang"" || ], || ""doi"": ""10.1093/arxiv/2026.07309"", || ""arxiv_id"": ""2601.07309"", || ""keywords"": [ || ""ARM"", || ""model merging"", || ""LLM agents"", || ""role-conditioned"", || ""neuron transplantation"", || ""cross-environment generalization"" || ], || ""summary"": ""This paper proposes Agent-Role Merging (ARM), a training-free method for merging large language models by integrating multiple experts into a single model. ARM enhances existing merging techniques for multi-turn agent scenarios, improving generalization across diverse interactive environments while maintaining efficiency. The approach outperforms prior methods and domain-specific models, demonstrating strong out-of-domain generalization."" || } ||  || [ Prompt: 126.0 t/s | Generation: 29.0 t/s ]"
2601.07313v1_Explaining Machine Learning Predictive Models thro.pdf,Multivariate Conditional Expectation (MUCE),"Silvia Ruiz-España, Laura Arnal, Franco Cois Signola, Juan-Carlos Perez-Cortes, Joaquim Arlandis",,,"machine learning, XAI, explainable models, local explainability, model-agnostic, uncertainty, stability, uncertainty","This work introduces MUCE, a model-agnostic method for local explainability that captures prediction changes from feature interactions. It extends Individual Conditional Expectation by exploring a multivariate grid of values near an observation, providing graphical explanations and stability/uncertainty indices to assess model reliability.",29.5,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Explaining Machine Learning Predictive Models through Conditional || Expectation Methods || Silvia Ruiz-Espa˜ naa,∗, Laura Arnal a,∗, Fran¸ cois Signola, Juan-Carlos Perez-Cortes a, Joaquim Arlandis a || aITI, Universitat Polit` ecnica de Val` encia, Camino de Vera, s/n, 46022 Val` enc ... (truncated) ||  || { || ""title"": ""Multivariate Conditional Expectation (MUCE)"", || ""authors"": [ || ""Silvia Ruiz-España"", || ""Laura Arnal"", || ""Franco Cois Signola"", || ""Juan-Carlos Perez-Cortes"", || ""Joaquim Arlandis"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""machine learning"", || ""XAI"", || ""explainable models"", || ""local explainability"", || ""model-agnostic"", || ""uncertainty"", || ""stability"", || ""uncertainty"" || ], || ""summary"": ""This work introduces MUCE, a model-agnostic method for local explainability that captures prediction changes from feature interactions. It extends Individual Conditional Expectation by exploring a multivariate grid of values near an observation, providing graphical explanations and stability/uncertainty indices to assess model reliability."" || } ||  || [ Prompt: 131.6 t/s | Generation: 29.5 t/s ]"
2601.07315v1_VLM-CAD VLM-Optimized Collaborative Agent Design W.pdf,VLM-CAD:VLM-OptimizedCollaborativeAgent,"Guanyuan Pan, Yugui Lin, Tiansheng Zhou, Pietro Li, Shuai Wang, Yaqi Wang, Wangyaqi",,,"Analog Circuit Sizing, Agentic AI, Vision Language Model, Explainability, Electronic Design Automation","Analog mixed-signal circuit sizing involves complex trade-offs. This paper proposes a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD) to analyze circuits, optimize DC operating points, perform inference-based sizing, and execute external sizing optimization. It integrates Image2Net for circuit schematic annotation and presents an Explainable Trust Region Bayesian Optimization method (ExTuRBO) for comprehensive design reporting.",29.6,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || VLM-CAD:VLM-OptimizedCollaborativeAgent || Design Workflow for Analog Circuit Sizing || 1st Guanyuan Pan || Hangzhou Dianzi University || Hangzhou, China || guanyuanpeterpan@gmail.com || 2nd Yugui Lin || SCBC, Guangdong University of Foreign Studies || Guangzhou, China || yuguilin0209@163.com || 3rd Tiansheng Zh ... (truncated) ||  || { || ""title"": ""VLM-CAD:VLM-OptimizedCollaborativeAgent"", || ""authors"": [ || ""Guanyuan Pan"", || ""Yugui Lin"", || ""Tiansheng Zhou"", || ""Pietro Li"", || ""Shuai Wang"", || ""Yaqi Wang"", || ""Wangyaqi"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""Analog Circuit Sizing"", || ""Agentic AI"", || ""Vision Language Model"", || ""Explainability"", || ""Electronic Design Automation"" || ], || ""summary"": ""Analog mixed-signal circuit sizing involves complex trade-offs. This paper proposes a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD) to analyze circuits, optimize DC operating points, perform inference-based sizing, and execute external sizing optimization. It integrates Image2Net for circuit schematic annotation and presents an Explainable Trust Region Bayesian Optimization method (ExTuRBO) for comprehensive design reporting."" || } ||  || [ Prompt: 123.7 t/s | Generation: 29.6 t/s ]"
2601.07316v1_BEAT-Net Injecting Biomimetic Spatio-Temporal Prio.pdf,BEAT-Net: Injecting Biomimetic Spatio-Temporal,"Ma Runze, Liao Caizhi",rmaa0033@student.monash.edu,,"ECG classification, deep learning, biomimetic analysis, interpretable AI, heartbeat segmentation","This paper proposes BEAT-Net, a biomimetic ECG analysis framework reformulating the problem as language modeling. It uses QRS tokenization to convert ECG signals into heartbeat sequences, leveraging specialized encoders to capture local morphology and temporal dependencies. The approach achieves competitive performance with CNNs while improving robustness and interpretability through attention mechanisms, demonstrating the value of incorporating biological priors.",29.4,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || BEAT-Net: Injecting Biomimetic Spatio-Temporal || Priors for Interpretable ECG Classification || Ma Runze || School of Information Technology || Monash University Malaysia || rmaa0033@student.monash.edu || Liao Caizhi ∗ || Faculty of Biomedical Engineering || Shenzhen University of Advanced Technology || li ... (truncated) ||  || { || ""title"": ""BEAT-Net: Injecting Biomimetic Spatio-Temporal"", || ""authors"": [ || ""Ma Runze"", || ""Liao Caizhi"" || ], || ""doi"": ""rmaa0033@student.monash.edu"", || ""arxiv_id"": """", || ""keywords"": [ || ""ECG classification"", || ""deep learning"", || ""biomimetic analysis"", || ""interpretable AI"", || ""heartbeat segmentation"" || ], || ""summary"": ""This paper proposes BEAT-Net, a biomimetic ECG analysis framework reformulating the problem as language modeling. It uses QRS tokenization to convert ECG signals into heartbeat sequences, leveraging specialized encoders to capture local morphology and temporal dependencies. The approach achieves competitive performance with CNNs while improving robustness and interpretability through attention mechanisms, demonstrating the value of incorporating biological priors."" || } ||  || [ Prompt: 138.0 t/s | Generation: 29.4 t/s ]"
2601.07320v1_Segmental Advantage Estimation Enhancing PPO for L.pdf,Segmental Advantage Estimation: Enhancing PPO for Long-Context LLM,"Xue Gong1, Qi Yi1, Ziyuan Nan2, 4, Guanhua Huang1, Kejiao Li1, Yuhao Jiang1, Ruibin Xiong1, Zenan Xu1, Jiaming Guo4, Shaohui Peng 2,3, Bo Zhou1†",,,,The paper introduces Segmental Advantage Estimation (SAE) to improve PPO for reasoning LLMs by mitigating bias from sparse rewards through segmented advantage estimation.,29.3,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || 2026-01-13 || Segmental Advantage Estimation: Enhancing PPO for Long-Context LLM || Training || Xue Gong1,∗ , Qi Yi1,∗, Ziyuan Nan2,4, Guanhua Huang1, Kejiao Li1, Yuhao Jiang1, || Ruibin Xiong1, Zenan Xu1, Jiaming Guo4, Shaohui Peng 2,3, Bo Zhou1† || 1LLM Department, Tencent 2 University of  ... (truncated) ||  || { || ""title"": ""Segmental Advantage Estimation: Enhancing PPO for Long-Context LLM"", || ""authors"": [ || ""Xue Gong1"", || ""Qi Yi1"", || ""Ziyuan Nan2"", || ""4"", || ""Guanhua Huang1"", || ""Kejiao Li1"", || ""Yuhao Jiang1"", || ""Ruibin Xiong1"", || ""Zenan Xu1"", || ""Jiaming Guo4"", || ""Shaohui Peng 2,3"", || ""Bo Zhou1†"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""The paper introduces Segmental Advantage Estimation (SAE) to improve PPO for reasoning LLMs by mitigating bias from sparse rewards through segmented advantage estimation."" || } ||  || [ Prompt: 124.4 t/s | Generation: 29.3 t/s ]"
2601.07342v1_Agentic Diagnostic Reasoning over Telecom and Data.pdf,Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure,Nicolas Tacheny,arXiv:2601.07342v1,1,,Large-scale telecom and datacenter infrastructures rely on multi-layered service and resource models... This work introduces an agent-based diagnostic framework using a Large Language Model (LLM) to autonomously navigate infrastructure models and perform root cause analysis.,29.4,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Agentic Diagnostic Reasoning over Telecom and || Datacenter Infrastructure || Foundation for Autonomous Incident Resolution and Change Impact || Mitigation || Nicolas Tacheny || Ni2 Innovation Lab || nicolas.tacheny@ni2.com || Abstract || Large-scale telecom and datacenter infrastructures rely on multi-lay ... (truncated) ||  || { || ""title"": ""Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure"", || ""authors"": [""Nicolas Tacheny""], || ""doi"": ""arXiv:2601.07342v1"", || ""arxiv_id"": ""1"", || ""keywords"": [], || ""summary"": ""Large-scale telecom and datacenter infrastructures rely on multi-layered service and resource models... This work introduces an agent-based diagnostic framework using a Large Language Model (LLM) to autonomously navigate infrastructure models and perform root cause analysis."" || } ||  || [ Prompt: 130.8 t/s | Generation: 29.4 t/s ]"
2601.07344v1_PulseMind A Multi-Modal Medical Model for Real-Wor.pdf,PulseMind: A Multi-Modal Medical Model for Real-World Clinical Diagnosis,"Jiao Xu1, Junwei Liu, Jiangwei Lao, Qi Zhu, Yunpeng Zhao, Congyun Jin, Shinan Liu, Zhihong Lu2, Lihe Zhang, Xin Chen, Jian Wang, Ping Wang",,,,"Recent advances in medical multi-modal models focus on specialized image analysis like dermatology, pathology, or radiology. However, they do not fully capture the complexity of real-world clinical diagnostics, which involve heterogeneous inputs and require ongoing contextual understanding during patient-physician interactions. To bridge this gap, we introduce PulseMind, a new family of multi-modal diagnostic models that integrates a systematically curated dataset, a comprehensive evaluation benchmark, and a tailored training framework.",30.0,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || PulseMind: A Multi-Modal Medical Model for Real-World Clinical Diagnosis || Jiao Xu1,2*, Junwei Liu 2,3*, Jiangwei Lao 2, Qi Zhu 2, Yunpeng Zhao4, Congyun Jin 2, Shinan Liu 4, || Zhihong Lu2, Lihe Zhang 1†, Xin Chen 5†, Jian Wang2†, Ping Wang3† || 1 Dalian University of Technology || 2  ... (truncated) ||  || { || ""title"": ""PulseMind: A Multi-Modal Medical Model for Real-World Clinical Diagnosis"", || ""authors"": [ || ""Jiao Xu1"", || ""Junwei Liu"", || ""Jiangwei Lao"", || ""Qi Zhu"", || ""Yunpeng Zhao"", || ""Congyun Jin"", || ""Shinan Liu"", || ""Zhihong Lu2"", || ""Lihe Zhang"", || ""Xin Chen"", || ""Jian Wang"", || ""Ping Wang"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""Recent advances in medical multi-modal models focus on specialized image analysis like dermatology, pathology, or radiology. However, they do not fully capture the complexity of real-world clinical diagnostics, which involve heterogeneous inputs and require ongoing contextual understanding during patient-physician interactions. To bridge this gap, we introduce PulseMind, a new family of multi-modal diagnostic models that integrates a systematically curated dataset, a comprehensive evaluation benchmark, and a tailored training framework."" || } ||  || [ Prompt: 129.0 t/s | Generation: 30.0 t/s ]"
2601.07348v4_Controlled Self-Evolution for Algorithmic Code Opt.pdf,Controlled Self-Evolution for Algorithmic Code Optimization,"Tu Hu1, Ronghao Chen2, Shuo Zhang3, Jianghao Yin4, Mou Xiao Feng3, Jingping Liu5, Shaolei Zhang6, Wenqi Jiang, Yuqi Fang, Sen Hu, Huacan Wang, Yi Xu",arXiv:2601.07348v4,2601.07348v4,"Controlled Self-Evolution, Code Optimization, Algorithmic Code, Evolutionary Algorithms, Exploration Efficiency, LLM Backbones","Self-evolution methods enhance code generation through iterative cycles, but face low exploration efficiency. This paper proposes Controlled Self-Evolution (CSE) to address inefficiencies by improving solution quality and resource utilization.",29.3,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || 2026-01-16 || Controlled Self-Evolution for Algorithmic Code Optimization || Tu Hu1*, Ronghao Chen2,7*, Shuo Zhang3*, Jianghao Yin4, Mou Xiao Feng3, Jingping Liu5, || Shaolei Zhang6,Wenqi Jiang 1,Yuqi Fang 1†,Sen Hu 2,7†,Huacan Wang 3†,Yi Xu 3† || 1NJU, 2PKU, 3Midea-AIRC, 4ECNU, 5SYSU,  ... (truncated) ||  || { || ""title"": ""Controlled Self-Evolution for Algorithmic Code Optimization"", || ""authors"": [ || ""Tu Hu1"", || ""Ronghao Chen2"", || ""Shuo Zhang3"", || ""Jianghao Yin4"", || ""Mou Xiao Feng3"", || ""Jingping Liu5"", || ""Shaolei Zhang6"", || ""Wenqi Jiang"", || ""Yuqi Fang"", || ""Sen Hu"", || ""Huacan Wang"", || ""Yi Xu"" || ], || ""doi"": ""arXiv:2601.07348v4"", || ""arxiv_id"": ""2601.07348v4"", || ""keywords"": [ || ""Controlled Self-Evolution"", || ""Code Optimization"", || ""Algorithmic Code"", || ""Evolutionary Algorithms"", || ""Exploration Efficiency"", || ""LLM Backbones"" || ], || ""summary"": ""Self-evolution methods enhance code generation through iterative cycles, but face low exploration efficiency. This paper proposes Controlled Self-Evolution (CSE) to address inefficiencies by improving solution quality and resource utilization."" || } ||  || [ Prompt: 136.8 t/s | Generation: 29.3 t/s ]"
2601.07351v2_Beyond Hard Masks Progressive Token Evolution for .pdf,Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models,"Linhao Zhong, Linyu Wu, Bozhen Fang, Tianjian Feng, Chenchen Jing, Wen Wang, Jiaheng Zhang, Hao Chen, Chunhua Shen",,,"Diffusion Language Models, Token Evolution, Progressive Token Distributions, Masked Diffusion Models, Revisionable Decoding","The paper introduces EvoToken-DLM, a diffusion-based language modeling approach that uses evolving soft token distributions to enable progressive transitions from masked states to discrete outputs, improving decoding flexibility.",30.5,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Beyond Hard Masks: Progressive Token Evolution for || Diffusion Language Models || Linhao Zhong1* Linyu Wu2* Bozhen Fang1 Tianjian Feng1 Chenchen Jing1,3 || Wen Wang1 Jiaheng Zhang2 Hao Chen1 Chunhua Shen1,3† || 1Zhejiang University 2National University of Singapore 3Zhejiang University of Te ... (truncated) ||  || { || ""title"": ""Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models"", || ""authors"": [ || ""Linhao Zhong"", || ""Linyu Wu"", || ""Bozhen Fang"", || ""Tianjian Feng"", || ""Chenchen Jing"", || ""Wen Wang"", || ""Jiaheng Zhang"", || ""Hao Chen"", || ""Chunhua Shen"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""Diffusion Language Models"", || ""Token Evolution"", || ""Progressive Token Distributions"", || ""Masked Diffusion Models"", || ""Revisionable Decoding"" || ], || ""summary"": ""The paper introduces EvoToken-DLM, a diffusion-based language modeling approach that uses evolving soft token distributions to enable progressive transitions from masked states to discrete outputs, improving decoding flexibility."" || } ||  || [ Prompt: 130.1 t/s | Generation: 30.5 t/s ]"
2601.07356v1_Efficient Convolutional Forward Model for Passive .pdf,Efficient Convolutional Forward Model for Passive Acoustic Mapping and Temporal Monitoring,"Tatiana Gelvez-Barrera, Barbara Nicolas, Bruno Gilles, Adrian Basarab, Denis Kouam",10.1016/j.jlaf.2015.02.005,10.1016/j.jlaf,"Passive Acoustic Mapping, Model-based beamforming, Convolutional forward model, Temporal monitoring","This paper introduces a PAM beamforming framework based on a novel convolutional formulation in the time domain, enabling efficient computation and higher temporal resolution for monitoring cavitation activity in therapeutic ultrasound applications.",36.3,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1 || Efficient Convolutional Forward Model for Passive || Acoustic Mapping and Temporal Monitoring || Tatiana Gelvez-Barrera, Barbara Nicolas, Bruno Gilles, Adrian Basarab, Denis Kouam ´e || Abstract—Passive acoustic mapping (PAM) is  ... (truncated) ||  || { || ""title"": ""Efficient Convolutional Forward Model for Passive Acoustic Mapping and Temporal Monitoring"", || ""authors"": [ || ""Tatiana Gelvez-Barrera"", || ""Barbara Nicolas"", || ""Bruno Gilles"", || ""Adrian Basarab"", || ""Denis Kouam"" || ], || ""doi"": ""10.1016/j.jlaf.2015.02.005"", || ""arxiv_id"": ""10.1016/j.jlaf"", || ""keywords"": [ || ""Passive Acoustic Mapping"", || ""Model-based beamforming"", || ""Convolutional forward model"", || ""Temporal monitoring"" || ], || ""summary"": ""This paper introduces a PAM beamforming framework based on a novel convolutional formulation in the time domain, enabling efficient computation and higher temporal resolution for monitoring cavitation activity in therapeutic ultrasound applications."" || } ||  || [ Prompt: 122.8 t/s | Generation: 36.3 t/s ]"
2601.07359v1_Seeing Right but Saying Wrong Inter- and Intra-Lay.pdf,Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs,"Shezheng Song, Shasha Li, Jie Yu",10.48550/arXiv.2303.06912,arXiv:2303.06912,"MLLMs, vision-language tasks, attention mechanisms, model refinement","The paper discusses the issue of 'seeing it right but saying it wrong' in multimodal large language models, proposing DualPD to improve visual understanding without additional training.",28.5,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs || without Training || Shezheng Song, Shasha Li, Jie Yu || 1National University of Defense Technology || Abstract || Multimodal Large Language Models (MLLMs) have demon- || strated strong capabilities across a variety of vision ... (truncated) ||  || { || ""title"": ""Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs"", || ""authors"": [""Shezheng Song"", ""Shasha Li"", ""Jie Yu""], || ""doi"": ""10.48550/arXiv.2303.06912"", || ""arxiv_id"": ""arXiv:2303.06912"", || ""keywords"": [""MLLMs"", ""vision-language tasks"", ""attention mechanisms"", ""model refinement""], || ""summary"": ""The paper discusses the issue of 'seeing it right but saying it wrong' in multimodal large language models, proposing DualPD to improve visual understanding without additional training."" || } ||  || [ Prompt: 123.1 t/s | Generation: 28.5 t/s ]"
2601.07364v1_On the universal definition of intelligence.pdf,Human - AI Comparison,Joseph Chen,,,,"This paper proposes a universal definition of intelligence for fair comparison between human and artificial intelligence, addressing the limitations of existing anthropocentric definitions.",32.4,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || On the universal definition of intelligence || A definition for Human - AI Comparison || Joseph Chen || University of Tokyo ||  ||  || Abstract || This paper aims to propose a universal definition of intelligence that enables fair and || consistent comparison of human and artificial intelligence ( ... (truncated) ||  || { || ""title"": ""Human - AI Comparison"", || ""authors"": [""Joseph Chen""], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""This paper proposes a universal definition of intelligence for fair comparison between human and artificial intelligence, addressing the limitations of existing anthropocentric definitions."" || } ||  || [ Prompt: 138.9 t/s | Generation: 32.4 t/s ]"
2601.07372v1_Conditional Memory via Scalable Lookup A New Axis .pdf,Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models,"Xin Cheng, Wangding Zeng, Damai Dai, Qinyu Chen, Bingxuan Wang, Zhenda Xie, Kezhao Huang, Xingkai Yu, Zhewen Hao, Yukun Li, Han Zhang, Huishuai Zhang, Dongyan Zhao, Wenfeng Liang",10.48550/arXiv.2303.04219,arXiv:2601.07372v1,"conditional memory, scalable lookup, sparsity, large language models, Engram, MoE, knowledge retrieval","This paper introduces conditional memory as a new sparsity axis for LLMs, leveraging Engram to enable efficient retrieval. It demonstrates improved performance across reasoning benchmarks and highlights benefits in long-context understanding.",29.9,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Conditional Memory via Scalable Lookup: || A New Axis of Sparsity for Large Language Models || Xin Cheng1,2, Wangding Zeng2, Damai Dai2, Qinyu Chen2, Bingxuan Wang2, || Zhenda Xie2, Kezhao Huang2, Xingkai Yu2, Zhewen Hao2, Yukun Li2, Han Zhang2, || Huishuai Zhang1, Dongyan Zhao1, Wenfeng Liang2 ... (truncated) ||  || { || ""title"": ""Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"", || ""authors"": [ || ""Xin Cheng"", || ""Wangding Zeng"", || ""Damai Dai"", || ""Qinyu Chen"", || ""Bingxuan Wang"", || ""Zhenda Xie"", || ""Kezhao Huang"", || ""Xingkai Yu"", || ""Zhewen Hao"", || ""Yukun Li"", || ""Han Zhang"", || ""Huishuai Zhang"", || ""Dongyan Zhao"", || ""Wenfeng Liang"" || ], || ""doi"": ""10.48550/arXiv.2303.04219"", || ""arxiv_id"": ""arXiv:2601.07372v1"", || ""keywords"": [ || ""conditional memory"", || ""scalable lookup"", || ""sparsity"", || ""large language models"", || ""Engram"", || ""MoE"", || ""knowledge retrieval"" || ], || ""summary"": ""This paper introduces conditional memory as a new sparsity axis for LLMs, leveraging Engram to enable efficient retrieval. It demonstrates improved performance across reasoning benchmarks and highlights benefits in long-context understanding."" || } ||  || [ Prompt: 122.2 t/s | Generation: 29.9 t/s ]"
2601.07376v1_OpenTinker Separating Concerns in Agentic Reinforc.pdf,OpenTinker: Separating Concerns in Agentic Reinforcement Learning,"Siqi Zhu, Jiaxuan You",10.48550/arxiv/2303.06931,arXiv:2303.06931,"OpenTinker, Reinforcement Learning, RLHF, multi-agent, agentic systems","Introduces OpenTinker, an infrastructure for RL of LLM agents with separation of concerns in algorithm design, execution, and agent-environment interaction. Presents a centralized scheduler and discusses design principles for multi-agent training.",30.2,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || OpenTinker: Separating Concerns in Agentic || Reinforcement Learning || Siqi Zhu,Jiaxuan You || University of Illinois Urbana-Champaign || Abstract || We introduceOpenTinker, an infrastructure for reinforcement learning (RL) of large language || model (LLM) agents built around a separation of concern ... (truncated) ||  || { || ""title"": ""OpenTinker: Separating Concerns in Agentic Reinforcement Learning"", || ""authors"": [""Siqi Zhu"", ""Jiaxuan You""], || ""doi"": ""10.48550/arxiv/2303.06931"", || ""arxiv_id"": ""arXiv:2303.06931"", || ""keywords"": [""OpenTinker"", ""Reinforcement Learning"", ""RLHF"", ""multi-agent"", ""agentic systems""], || ""summary"": ""Introduces OpenTinker, an infrastructure for RL of LLM agents with separation of concerns in algorithm design, execution, and agent-environment interaction. Presents a centralized scheduler and discusses design principles for multi-agent training."" || } ||  || [ Prompt: 131.8 t/s | Generation: 30.2 t/s ]"
2601.07377v1_Learning Dynamic Collaborative Network for Semi-su.pdf,Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation,"Jiao Xu1, Xin Chen2, Lihe Zhang1",,,"3D vessel segmentation, semi-supervised learning, dynamic collaborative network, medical imaging, label inconsistency","This paper presents DiCo, a dynamic collaborative network for semi-supervised 3D vessel segmentation. It addresses challenges like data scarcity and complex vessel shapes by introducing a dynamic teacher-student role switching and multi-view integration. Experiments show state-of-the-art performance on 3D vessel segmentation benchmarks.",37.8,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Learning Dynamic Collaborative Network for Semi-supervised 3D V essel || Segmentation || Jiao Xu1*, Xin Chen2*, Lihe Zhang1† || 1Dalian University of Technology 2City University of Hong Kong || xjmmcome@mail.dlut.edu.cn, xche32@cityu.edu.hk, zhanglihe@dlut.edu.cn || Abstract || In this paper, we pr ... (truncated) ||  || { || ""title"": ""Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation"", || ""authors"": [ || ""Jiao Xu1"", || ""Xin Chen2"", || ""Lihe Zhang1"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""3D vessel segmentation"", || ""semi-supervised learning"", || ""dynamic collaborative network"", || ""medical imaging"", || ""label inconsistency"" || ], || ""summary"": ""This paper presents DiCo, a dynamic collaborative network for semi-supervised 3D vessel segmentation. It addresses challenges like data scarcity and complex vessel shapes by introducing a dynamic teacher-student role switching and multi-view integration. Experiments show state-of-the-art performance on 3D vessel segmentation benchmarks."" || } ||  || [ Prompt: 124.6 t/s | Generation: 37.8 t/s ]"
2601.07389v1_On the Non-decoupling of Supervised Fine-tuning an.pdf,N/A,,N/A,N/A,,N/A,28.7,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || On the Non-decoupling of Supervised Fine-tuning and || Reinforcement Learning in Post-training || Xueyan Niuniuxueyan3@huawei.com || Theory Laboratory || Central Research Institute, 2012 Laboratories || Huawei Technologies Co., Ltd. || Bo Baibaibo8@huawei.com || W ei Hanharvey.hanwei@huawei.com || W eixi Z ... (truncated) ||  || { || ""title"": ""Theory Laboratory"", || ""authors"": [ || ""Xueyan Niuniuxueyan3@huawei.com"", || ""Bo Baibaibo8@huawei.com"", || ""W eixi Zhangzhangweixi1@huawei.com"", || ""W eixi Zhangzhangweixi1@huawei.com"" || ], || ""doi"": ""arXiv:2601.07389v1"", || ""arxiv_id"": ""arXiv:2601.07389"", || ""keywords"": [], || ""summary"": ""Post-training of large language models routinely interleaves supervised fine-tuning (SFT) with reinforcement learning (RL). These two methods have different objectives: SFT minimizes cross-entropy loss between model outputs and expert responses, while RL maximizes reward signals derived from human preferences or rule-based verifiers. Modern reasoning models have widely adopted the practice of alternating SFT and RL. However, there is no theoretical account of whether they can be decoupled. We prove that decoupling is impossible in either order: (1) SFT-then-RL coupling: RL increases SFT loss under SFT optimality and (2) RL-then-SFT coupling: SFT lowers the reward achieved by RL. Experiments on Qwen3-0.6B confirm the predicted degradation, verifying that SFT and RL cannot be separated without loss of prior performance in the post-training pipeline."", || ""contact"": { || ""email"": ""Xueyan.Niuniuxueyan3@huawei.com"", || ""lab"": ""Theory Laboratory"", || ""institution"": ""Central Research Institute"", || ""affiliation"": ""2012 Laboratories, Huawei Technologies Co., Ltd."" || } || } ||  || [ Prompt: 130.9 t/s | Generation: 28.7 t/s ]"
2601.07392v1_OceanSAR-2 A Universal Feature Extractor for SAR O.pdf,OceanSAR-2: A Universal Feature Extractor for SAR Ocean Observation,"Alexandre Tuela, Thomas Kerdreux a, Quentin Febvre b, Alexis Mouche b, Antoine Grouazel b, Jean-Renaud Miadana c, Antoine Audrasa, Chen Wangd, Bertrand Chapronb",,,,"We present OceanSAR-2, the second generation of our foundation model for SAR-based ocean observation. Building on our earlier release, which pioneered self-supervised learning on Sentinel-1 Wave Mode data, OceanSAR-2 relies on improved SSL training and dynamic data curation strategies, which enhances performance while reducing training cost. OceanSAR-2 demonstrates strong transfer performance across downstream tasks, including geophysical pattern classification, ocean surface wind vector and significant wave height estimation, and iceberg detection. We release standardized benchmark datasets, providing a foundation for systematic evaluation and advancement of SAR models for ocean applications.",28.6,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || OceanSAR-2: A Universal Feature Extractor for SAR Ocean Observa- || tion || Alexandre Tuela, Thomas Kerdreux a, Quentin Febvre b, Alexis Mouche b, Antoine Grouazel b, Jean-Renaud Miadana c, || Antoine Audrasa, Chen Wangd, and Bertrand Chapronb || aGaleio, Paris, France || bLOPS, Ifremer, Plouzané ... (truncated) ||  || { || ""title"": ""OceanSAR-2: A Universal Feature Extractor for SAR Ocean Observation"", || ""authors"": [ || ""Alexandre Tuela"", || ""Thomas Kerdreux a"", || ""Quentin Febvre b"", || ""Alexis Mouche b"", || ""Antoine Grouazel b"", || ""Jean-Renaud Miadana c"", || ""Antoine Audrasa"", || ""Chen Wangd"", || ""Bertrand Chapronb"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""We present OceanSAR-2, the second generation of our foundation model for SAR-based ocean observation. Building on our earlier release, which pioneered self-supervised learning on Sentinel-1 Wave Mode data, OceanSAR-2 relies on improved SSL training and dynamic data curation strategies, which enhances performance while reducing training cost. OceanSAR-2 demonstrates strong transfer performance across downstream tasks, including geophysical pattern classification, ocean surface wind vector and significant wave height estimation, and iceberg detection. We release standardized benchmark datasets, providing a foundation for systematic evaluation and advancement of SAR models for ocean applications."" || } ||  || [ Prompt: 131.4 t/s | Generation: 28.6 t/s ]"
2601.07393v1_Software-Hardware Co-optimization for Modular E2E .pdf,SOFTWARE-HARDWARECO-OPTIMIZATION FORMODULARE2E,"Chengzhi Ji, Xingfeng Li, Zhaodong Lv, Hao Sun, Pan Liu, Hao Frank Yang, Ziyuan Pu",,,"Modular end-to-end, Closed-Loop Evaluation, Software–Hardware co-optimization, Energy Consumption, Inference Latency, Efficiency, Latency, Energy","This paper proposes a reusable software–hardware co-optimization and closed-loop evaluation framework for modular end-to-end autonomous driving inference. It integrates software optimizations with hardware optimizations under a unified system-level objective, aiming to address limitations of software-only and hardware-only approaches. The framework achieves significant improvements in accuracy, latency, and energy efficiency.",29.1,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || SOFTWARE-HARDWARECO-OPTIMIZATION FORMODULARE2E || AV PARADIGM: A UNIFIEDFRAMEWORK OFOPTIMIZATION || APPROACHES, SIMULATIONENVIRONMENT ANDEVALUATION || METRICS || Chengzhi Ji1, Xingfeng Li1, Zhaodong Lv1, Hao Sun2, Pan Liu1, Hao Frank Yang3, Ziyuan Pu1,∗ || 1School of Transportation, Southeast Un ... (truncated) ||  || { || ""title"": ""SOFTWARE-HARDWARECO-OPTIMIZATION FORMODULARE2E"", || ""authors"": [ || ""Chengzhi Ji"", || ""Xingfeng Li"", || ""Zhaodong Lv"", || ""Hao Sun"", || ""Pan Liu"", || ""Hao Frank Yang"", || ""Ziyuan Pu"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""Modular end-to-end"", || ""Closed-Loop Evaluation"", || ""Software–Hardware co-optimization"", || ""Energy Consumption"", || ""Inference Latency"", || ""Efficiency"", || ""Latency"", || ""Energy"" || ], || ""summary"": ""This paper proposes a reusable software–hardware co-optimization and closed-loop evaluation framework for modular end-to-end autonomous driving inference. It integrates software optimizations with hardware optimizations under a unified system-level objective, aiming to address limitations of software-only and hardware-only approaches. The framework achieves significant improvements in accuracy, latency, and energy efficiency."" || } ||  || [ Prompt: 121.9 t/s | Generation: 29.1 t/s ]"
2601.07395v1_MCP-ITP An Automated Framework for Implicit Tool P.pdf,MCP-ITP: An Automated Framework for Implicit Tool Poisoning in MCP,"Ruiqi Li, Zhiqiang Wang, Yunhao Yao, Xiang-Yang Li",lrq349,zhiqiang.wang,"implicit tool poisoning, MCP, tool poisoning, LLM agents","This paper introduces MCP-ITP, the first automated and adaptive framework for implicit tool poisoning within the MCP ecosystem. It addresses stealthy poisoning where poisoned tool metadata manipulates agent behavior without direct tool invocation, achieving high attack success rates while evading detection.",30.2,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || MCP-ITP: An Automated Framework for Implicit Tool Poisoning in MCP || Ruiqi Li, Zhiqiang Wang, Y unhao Y ao, and Xiang-Y ang Li || University of Science and Technology of China || {lrq349, zhiqiang.wang, yaoyunhao}@mail.ustc.edu.cn, xiangyangli@ustc.edu.cn || Abstract || To standardize interaction ... (truncated) ||  || { || ""title"": ""MCP-ITP: An Automated Framework for Implicit Tool Poisoning in MCP"", || ""authors"": [""Ruiqi Li"", ""Zhiqiang Wang"", ""Yunhao Yao"", ""Xiang-Yang Li""], || ""doi"": ""lrq349"", || ""arxiv_id"": ""zhiqiang.wang"", || ""keywords"": [""implicit tool poisoning"", ""MCP"", ""tool poisoning"", ""LLM agents""], || ""summary"": ""This paper introduces MCP-ITP, the first automated and adaptive framework for implicit tool poisoning within the MCP ecosystem. It addresses stealthy poisoning where poisoned tool metadata manipulates agent behavior without direct tool invocation, achieving high attack success rates while evading detection."" || } ||  || [ Prompt: 128.4 t/s | Generation: 30.2 t/s ]"
2601.07397v1_Layerwise goal-oriented adaptivity for neural ODEs.pdf,Layerwise goal-oriented adaptivity for neural ODEs: an optimal control perspective,"Michael Hintermüller, Michael Hinze, Denis Korolev",arXiv:2601.07397v1,2601.07397v1,"Resnet, neural ODEs, parameter identification, adaptive neural network",The paper proposes a novel layerwise adaptive construction method for neural network architectures based on a goal-oriented dual-weighted residual technique for optimal control of neural differential equations. It introduces an explicit Euler time marching scheme and applies the method to neural network construction for data classification.,29.0,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || arXiv:2601.07397v1  [math.OC]  12 Jan 2026 || Layerwise goal-oriented adaptivity for neural ODEs: an optimal || control perspective || Michael Hinterm¨ uller∗1,3 , Michael Hinze †2 , and Denis Korolev ‡3 || 1Institute for Mathematics, Humboldt-Universit¨ at zu Berlin, Berlin, Germany || 2M ... (truncated) ||  || { || ""title"": ""Layerwise goal-oriented adaptivity for neural ODEs: an optimal control perspective"", || ""authors"": [ || ""Michael Hintermüller"", || ""Michael Hinze"", || ""Denis Korolev"" || ], || ""doi"": ""arXiv:2601.07397v1"", || ""arxiv_id"": ""2601.07397v1"", || ""keywords"": [ || ""Resnet"", || ""neural ODEs"", || ""parameter identification"", || ""adaptive neural network"" || ], || ""summary"": ""The paper proposes a novel layerwise adaptive construction method for neural network architectures based on a goal-oriented dual-weighted residual technique for optimal control of neural differential equations. It introduces an explicit Euler time marching scheme and applies the method to neural network construction for data classification."" || } ||  || [ Prompt: 122.7 t/s | Generation: 29.0 t/s ]"
2601.07411v1_SCALPEL Selective Capability Ablation via Low-rank.pdf,SCALPEL: Selective Capability Ablation via Low-rank Parameter,"Zihao Fu, Xufeng Duan, Zhenguang G. Cai",,,,"This paper introduces SCALPEL, a framework representing language model capabilities as low-rank parameter subspaces. It enables selective ablation of capabilities through targeted modifications, offering insights into the distributed nature of capability encoding in neural networks.",29.4,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || SCALPEL: Selective Capability Ablation via Low-rank Parameter || Editing for Large Language Model Interpretability Analysis || Zihao Fu || The Chinese University of Hong Kong || zihaofu@cuhk.edu.hk || Xufeng Duan || The Chinese University of Hong Kong || xufengduan@cuhk.edu.hk || Zhenguang G. Cai || The Chine ... (truncated) ||  || { || ""title"": ""SCALPEL: Selective Capability Ablation via Low-rank Parameter"", || ""authors"": [ || ""Zihao Fu"", || ""Xufeng Duan"", || ""Zhenguang G. Cai"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""This paper introduces SCALPEL, a framework representing language model capabilities as low-rank parameter subspaces. It enables selective ablation of capabilities through targeted modifications, offering insights into the distributed nature of capability encoding in neural networks."" || } ||  || [ Prompt: 127.3 t/s | Generation: 29.4 t/s ]"
2601.07422v1_Two Pathways to Truthfulness On the Intrinsic Enco.pdf,Two Pathways to Truthfulness: On the Intrinsic,"Wen Luo, Guangyue Peng, Wei Li, Shaohang Wei, Feifan Song, Liang Wang, Nan Yang, Xingxing Zhang, Jing Jin, Furu Wei, Houfeng Wang",,,,"This paper explores how large language models encode truthfulness through two distinct information pathways: a Question-Anchored pathway and an Answer-Anchored pathway. It investigates the mechanisms behind these pathways using attention knockout and token patching, revealing insights into LLM hallucinations and proposing applications to improve hallucination detection.",30.2,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Two Pathways to Truthfulness: On the Intrinsic || Encoding of LLM Hallucinations || Wen Luo♡, Guangyue Peng ♡, Wei Li♡, Shaohang Wei♡, Feifan Song ♡, || Liang Wang♠, Nan Yang♠, Xingxing Zhang ♠, Jing Jin ♡, Furu Wei♠, Houfeng Wang♡* || ♡ State Key Laboratory of Multimedi ... (truncated) ||  || { || ""title"": ""Two Pathways to Truthfulness: On the Intrinsic"", || ""authors"": [ || ""Wen Luo"", || ""Guangyue Peng"", || ""Wei Li"", || ""Shaohang Wei"", || ""Feifan Song"", || ""Liang Wang"", || ""Nan Yang"", || ""Xingxing Zhang"", || ""Jing Jin"", || ""Furu Wei"", || ""Houfeng Wang"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""This paper explores how large language models encode truthfulness through two distinct information pathways: a Question-Anchored pathway and an Answer-Anchored pathway. It investigates the mechanisms behind these pathways using attention knockout and token patching, revealing insights into LLM hallucinations and proposing applications to improve hallucination detection."" || } ||  || [ Prompt: 124.0 t/s | Generation: 30.2 t/s ]"
2601.07430v1_KALE Enhancing Knowledge Manipulation in Large Lan.pdf,N/A,,N/A,N/A,,N/A,30.0,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || KALE: Enhancing Knowledge Manipulation in Large Language Models || via Knowledge-aware Learning || Qitan Lv1,2* Tianyu Liu1,2* Qiaosheng Zhang2† Xingcheng Xu2† Chaochao Lu2 || 1University of Science and Technology of China || 2Shanghai AI Laboratory || {qitanlv, tianyu_liu}@mail.ustc.edu.cn || {x ... (truncated) ||  || { || ""title"": ""KALE: Enhancing Knowledge Manipulation in Large Language Models"", || ""authors"": [ || ""Qitan Lv1,2*"", || ""Tianyu Liu1,2*"", || ""Qiaosheng Zhang2†"", || ""Xingcheng Xu2†"", || ""Chaochao Lu2"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""knowledge manipulation"", || ""large language models"", || ""knowledge graphs"", || ""reasoning"", || ""LLM performance"" || ], || ""summary"": ""This paper proposes KALE, a post-training framework that leverages knowledge graphs to generate high-quality rationales and enhances LLMs' knowledge manipulation abilities. It introduces a Knowledge-Induced data synthesis method and a Knowledge-Aware fine-tuning paradigm."" || } ||  || [ Prompt: 131.4 t/s | Generation: 30.0 t/s ]"
2601.07449v1_RLPO Residual Listwise Preference Optimization for.pdf,Residual Listwise Preference Optimization for Long-Context,"Hao Jiang, Zhi Yang, Annan Wang, Yichi Zhang, Weisi Lin, Yichizhang0926@gmail.com",,,"review ranking, long-context, listwise preference optimization, e-commerce, user-generated content","Review ranking is pivotal in e-commerce for prioritizing diagnostic and authentic feedback. Existing ranking paradigms face a trade-off between pointwise scoring and list-level interactions, leading to unstable top-k rankings. RLPO addresses this by formulating ranking as a residual correction over a strong pointwise LLM.",29.2,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || RLPO: Residual Listwise Preference Optimization for Long-Context || Review Ranking || Hao Jiang || Nanyang Technological University || jianghao907@gmail.com || Zhi Yang || Peking University || zhiyang25@stu.pku.edu.cn || Annan Wang || Nanyang Technological University || annan001@e.ntu.edu.sg || Yichi Zhang || Independ ... (truncated) ||  || { || ""title"": ""Residual Listwise Preference Optimization for Long-Context"", || ""authors"": [ || ""Hao Jiang"", || ""Zhi Yang"", || ""Annan Wang"", || ""Yichi Zhang"", || ""Weisi Lin"", || ""Yichizhang0926@gmail.com"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""review ranking"", || ""long-context"", || ""listwise preference optimization"", || ""e-commerce"", || ""user-generated content"" || ], || ""summary"": ""Review ranking is pivotal in e-commerce for prioritizing diagnostic and authentic feedback. Existing ranking paradigms face a trade-off between pointwise scoring and list-level interactions, leading to unstable top-k rankings. RLPO addresses this by formulating ranking as a residual correction over a strong pointwise LLM."" || } ||  || [ Prompt: 127.9 t/s | Generation: 29.2 t/s ]"
2601.07463v1_Puzzle it Out Local-to-Global World Model for Offl.pdf,Puzzle it Out: Local-to-Global World Model for Offline,"Sijia Li, Xinran Li, Shibo Chen, Jun Zhang",10.48550/arXiv.2603.07918,10.48550/arXiv.2603.07918,"Offline multi-agent reinforcement learning, Multi-gent model-based reinforcement learning",Offline MARL faces challenges in generalization due to data distribution constraints. This work introduces a local-to-global world model to improve prediction accuracy and policy robustness by leveraging local predictions and synthetic data augmentation.,27.8,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Puzzle it Out: Local-to-Global World Model for Offline || Multi-Agent Reinforcement Learning || Sijia Li || The Hong Kong University of Science and Technology || Hong Kong, China || slifg@connect.ust.hk || Xinran Li || The Hong Kong University of Science and Technology || Hong Kong, China || xinran.li@connect ... (truncated) ||  || { || ""title"": ""Puzzle it Out: Local-to-Global World Model for Offline"", || ""authors"": [ || ""Sijia Li"", || ""Xinran Li"", || ""Shibo Chen"", || ""Jun Zhang"" || ], || ""doi"": ""10.48550/arXiv.2603.07918"", || ""arxiv_id"": ""10.48550/arXiv.2603.07918"", || ""keywords"": [ || ""Offline multi-agent reinforcement learning"", || ""Multi-gent model-based reinforcement learning"" || ], || ""summary"": ""Offline MARL faces challenges in generalization due to data distribution constraints. This work introduces a local-to-global world model to improve prediction accuracy and policy robustness by leveraging local predictions and synthetic data augmentation."" || } ||  || [ Prompt: 126.2 t/s | Generation: 27.8 t/s ]"
2601.07464v1_IFDNS An Iterative Feedback-Driven Neuro-Symbolic .pdf,Iterative Feedback-Driven Neuro-Symbolic Method for Faithful Logical Reasoning,"Xiaoheng Wang, Tongxuan Liu, Zi Gong1, Xianzhe Dong1, Yuting Zeng1, Minhan Hu1, Weizhe Huang, Jing Li1",,,"Logical Reasoning, Large Language Model, Reasoning","This paper introduces IFDNS, a novel prompt-based neuro-symbolic approach that uses iterative feedback to improve logical reasoning in large language models. It addresses information loss during reasoning and enhances performance on CoT and CoT-SC tasks.",29.9,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || IFDNS: An Iterative Feedback-Driven Neuro-Symbolic || Method for Faithful Logical Reasoning || Xiaoheng Wang1[0009−0005−3335−2322], Tongxuan Liu1[0009−0007−2634−2788], Zi || Gong1[0009−0004−8863−3677], Xianzhe Dong1[0009−0007−6943−9432], Yuting || Zeng1[0009−0006−171 ... (truncated) ||  || { || ""title"": ""Iterative Feedback-Driven Neuro-Symbolic Method for Faithful Logical Reasoning"", || ""authors"": [ || ""Xiaoheng Wang"", || ""Tongxuan Liu"", || ""Zi Gong1"", || ""Xianzhe Dong1"", || ""Yuting Zeng1"", || ""Minhan Hu1"", || ""Weizhe Huang"", || ""Jing Li1"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""Logical Reasoning"", || ""Large Language Model"", || ""Reasoning"" || ], || ""summary"": ""This paper introduces IFDNS, a novel prompt-based neuro-symbolic approach that uses iterative feedback to improve logical reasoning in large language models. It addresses information loss during reasoning and enhances performance on CoT and CoT-SC tasks."" || } ||  || [ Prompt: 122.5 t/s | Generation: 29.9 t/s ]"
2601.07468v1_Beyond Dialogue Time Temporal Semantic Memory for .pdf,Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents,"Miao Su1, Yucan Guo1,2,3, Zixuan Li1,2,Yufei Zhang 4, Guojun Yin 4, Wei Lin 4, Xiaolong Jin1,2,3, Jiafeng Guo 1,2,3, Xueqi Cheng 1,2,3, Institute of Computing Technology, Chinese Academy of Sciences, State Key Laboratory of AI Safety, School of Computer Science, University of Chinese Academy of Sciences, Meituan",,,"Temporal Semantic Memory, Personalized LLM Agents, Memory Modeling, Semantic Timeline, Durative Memory, Long-term Interaction","Memory enables Large Language Model agents to perceive, store, and use information from past dialogues, crucial for personalization. This paper introduces Temporal Semantic Memory (TSM), a framework that models semantic time for point-wise memory and supports durative information, improving accuracy by up to 12.2%.",29.4,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Beyond Dialogue Time: || Temporal Semantic Memory for Personalized LLM Agents || Miao Su1,2,3,4, Yucan Guo1,2,3, Zhongni Hou 4, Long Bai 1,2, || Zixuan Li1,2,Yufei Zhang 4,Guojun Yin 4,Wei Lin 4, || Xiaolong Jin1,2,3,Jiafeng Guo 1,2,3,Xueqi Cheng 1,2,3, || 1Institute of Computing Technology, Chine ... (truncated) ||  || { || ""title"": ""Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents"", || ""authors"": [ || ""Miao Su1"", || ""Yucan Guo1,2,3"", || ""Zixuan Li1,2,Yufei Zhang 4"", || ""Guojun Yin 4"", || ""Wei Lin 4"", || ""Xiaolong Jin1,2,3"", || ""Jiafeng Guo 1,2,3"", || ""Xueqi Cheng 1,2,3"", || ""Institute of Computing Technology, Chinese Academy of Sciences"", || ""State Key Laboratory of AI Safety"", || ""School of Computer Science, University of Chinese Academy of Sciences"", || ""Meituan"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""Temporal Semantic Memory"", || ""Personalized LLM Agents"", || ""Memory Modeling"", || ""Semantic Timeline"", || ""Durative Memory"", || ""Long-term Interaction"" || ], || ""summary"": ""Memory enables Large Language Model agents to perceive, store, and use information from past dialogues, crucial for personalization. This paper introduces Temporal Semantic Memory (TSM), a framework that models semantic time for point-wise memory and supports durative information, improving accuracy by up to 12.2%."" || } ||  || [ Prompt: 128.6 t/s | Generation: 29.4 t/s ]"
2601.07469v1_Knowledge Distillation for LLM-Based Human Activit.pdf,Knowledge Distillation for LLM-Based Human Activity Recognition in Smart Homes,"Julien Cumin, Oussama Er-Rahmany, Xi Chen",arXiv:2601.07469v1,arXiv:2601.07469v1,"Human activity recognition, large language models, knowledge distillation, ambient intelligence, smart homes","This paper presents experimental results on using Large Language Models for Human Activity Recognition in smart homes, focusing on performance evolution with model size and applying knowledge distillation to reduce model parameters while maintaining performance.",30.8,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || arXiv:2601.07469v1  [cs.AI]  12 Jan 2026 || KNOWLEDGE DISTILLATION FOR LLM-B ASED HUMAN ACTIVITY || RECOGNITION IN HOMES || AUTHOR VERSION || Julien Cumin1 , Oussama Er-Rahmany1, Xi Chen 1,2 || 1 Orange Research || 2 Univ. Grenoble Alpes, CNRS, Grenoble INP , LIG, 38000 Grenoble, France || Corresponding ... (truncated) ||  || { || ""title"": ""Knowledge Distillation for LLM-Based Human Activity Recognition in Smart Homes"", || ""authors"": [ || ""Julien Cumin"", || ""Oussama Er-Rahmany"", || ""Xi Chen"" || ], || ""doi"": ""arXiv:2601.07469v1"", || ""arxiv_id"": ""arXiv:2601.07469v1"", || ""keywords"": [ || ""Human activity recognition"", || ""large language models"", || ""knowledge distillation"", || ""ambient intelligence"", || ""smart homes"" || ], || ""summary"": ""This paper presents experimental results on using Large Language Models for Human Activity Recognition in smart homes, focusing on performance evolution with model size and applying knowledge distillation to reduce model parameters while maintaining performance."" || } ||  || [ Prompt: 128.7 t/s | Generation: 30.8 t/s ]"
2601.07470v1_Learning How to Remember A Meta-Cognitive Manageme.pdf,Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory,"Sirui Liang, Pengfei Cao, Jian Zhao, Wenhao Teng, Xiangwen Liao, Jun Zhao, Kang Liu",liangsirui2024@ia.ac.cn,,"memory abstraction, meta-cognitive management, agent memory, transferable memory, long-horizon decision-making",The paper proposes the Meta-CognitiveMemory Abstraction method (MCMA) to address limitations in memory reuse for LLM agents by treating memory abstraction as a learnable skill. MCMA decouples task execution from memory management and enables selective reuse of memories based on abstraction levels.,29.6,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Learning How to Remember:A Meta-Cognitive Management Method for || Structured and Transferable Agent Memory || Sirui Liang1,2,3*, Pengfei Cao1,2*, Jian Zhao3,4†, Wenhao Teng 5, || Xiangwen Liao 6,Jun Zhao 1,2,Kang Liu 1,2† || 1Institute of Automation, CAS, 2University of Chinese Academy of  ... (truncated) ||  || { || ""title"": ""Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory"", || ""authors"": [ || ""Sirui Liang"", || ""Pengfei Cao"", || ""Jian Zhao"", || ""Wenhao Teng"", || ""Xiangwen Liao"", || ""Jun Zhao"", || ""Kang Liu"" || ], || ""doi"": ""liangsirui2024@ia.ac.cn"", || ""arxiv_id"": """", || ""keywords"": [ || ""memory abstraction"", || ""meta-cognitive management"", || ""agent memory"", || ""transferable memory"", || ""long-horizon decision-making"" || ], || ""summary"": ""The paper proposes the Meta-CognitiveMemory Abstraction method (MCMA) to address limitations in memory reuse for LLM agents by treating memory abstraction as a learnable skill. MCMA decouples task execution from memory management and enables selective reuse of memories based on abstraction levels."" || } ||  || [ Prompt: 120.0 t/s | Generation: 29.6 t/s ]"
2601.07474v1_Task Prototype-Based Knowledge Retrieval for Multi.pdf,Prototype-Based Knowledge Retrieval for Multi-Task Learning,"Yongmin Oh, Hyung-Il Kim, Jung Uk Kim, Ju-Kim",10.48550/arXiv.2303.03832,10.48550/arXiv.2303.03832,"multi-task learning, prototype-based knowledge retrieval, multi-task learning framework, task prototype embedding, knowledge retrieval transformer",This paper proposes a prototype-based knowledge retrieval framework for multi-task learning that uses task prototypes to capture task-specific characteristics and refines feature representations via an association knowledge generation loss. It addresses challenges in partially annotated data scenarios.,30.5,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Task Prototype-Based Knowledge Retrieval for Multi-Task Learning || from Partially Annotated Data || Y oungmin Oh1,2, Hyung-Il Kim 3, Jung Uk Kim 1* || 1Kyung Hee University || 2Electronics and Telecommunications Research Institute (ETRI) || 3Chonnam National University || youngmin@etri.re.kr, hyungi ... (truncated) ||  || { || ""title"": ""Prototype-Based Knowledge Retrieval for Multi-Task Learning"", || ""authors"": [ || ""Yongmin Oh"", || ""Hyung-Il Kim"", || ""Jung Uk Kim"", || ""Ju-Kim"" || ], || ""doi"": ""10.48550/arXiv.2303.03832"", || ""arxiv_id"": ""10.48550/arXiv.2303.03832"", || ""keywords"": [ || ""multi-task learning"", || ""prototype-based knowledge retrieval"", || ""multi-task learning framework"", || ""task prototype embedding"", || ""knowledge retrieval transformer"" || ], || ""summary"": ""This paper proposes a prototype-based knowledge retrieval framework for multi-task learning that uses task prototypes to capture task-specific characteristics and refines feature representations via an association knowledge generation loss. It addresses challenges in partially annotated data scenarios."" || } ||  || [ Prompt: 126.5 t/s | Generation: 30.5 t/s ]"
2601.07475v1_ARCQuant Boosting NVFP4 Quantization with Augmente.pdf,ARCQuant: Boosting NVFP4 Quantization with Augmented Residual,"Haoqian Meng, Yilun Luo, Yafei Zhao, Wenyuan Liu, Peng Zhang, Xindian Ma",,,,"The paper proposes ARCQuant, a framework that enhances NVFP4 performance using augmented residual channels, maintaining a strictly unified NVFP4 format while achieving state-of-the-art accuracy and speed.",29.8,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || ARCQuant: Boosting NVFP4 Quantization with Augmented Residual || Channels for LLMs || Haoqian Meng1, Yilun Luo1, Yafei Zhao1, Wenyuan Liu1, Peng Zhang1*, Xindian Ma1 || 1School of Computer Science and Technology, Tianjin University || Abstract || The emergence of fine-grained numerical for- || mats l ... (truncated) ||  || { || ""title"": ""ARCQuant: Boosting NVFP4 Quantization with Augmented Residual"", || ""authors"": [ || ""Haoqian Meng"", || ""Yilun Luo"", || ""Yafei Zhao"", || ""Wenyuan Liu"", || ""Peng Zhang"", || ""Xindian Ma"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""The paper proposes ARCQuant, a framework that enhances NVFP4 performance using augmented residual channels, maintaining a strictly unified NVFP4 format while achieving state-of-the-art accuracy and speed."" || } ||  || [ Prompt: 126.9 t/s | Generation: 29.8 t/s ]"
2601.07477v1_JudgeFlow Agentic Workflow Optimization via Block .pdf,JUDGEFLOW: AGENTICWORKFLOWOPTIMIZATION,"Zihan Ma, Zhikai Zhao, Chuanbo Hua, Federico Berto",,,,Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities. JUDGEFLOW introduces an Evaluation Judge-Optimization-Update pipeline to improve sample efficiency and interpretability.,30.0,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || JUDGEFLOW: AGENTICWORKFLOWOPTIMIZATION || VIABLOCKJUDGE || Zihan Ma∗,1 Zhikai Zhao∗,1 Chuanbo Hua1 Federico Berto1,2 Jinkyoo Park1,3 || 1KAIST 2Radical Numerics 3Omelet || ABSTRACT || Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities. || Current methods rely on coa ... (truncated) ||  || { || ""title"": ""JUDGEFLOW: AGENTICWORKFLOWOPTIMIZATION"", || ""authors"": [ || ""Zihan Ma"", || ""Zhikai Zhao"", || ""Chuanbo Hua"", || ""Federico Berto"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities. JUDGEFLOW introduces an Evaluation Judge-Optimization-Update pipeline to improve sample efficiency and interpretability."" || } ||  || [ Prompt: 125.5 t/s | Generation: 30.0 t/s ]"
2601.07496v1_Graph Inference Towards ICD Coding.pdf,Graph Inference Towards ICD Coding,Xiaoxiao Deng,,,"transfer learning, graph convolutional network, lightweight attention, ICD code prediction, adversarial domain adaptation","LabGraph introduces a unified framework reformulating ICD coding as graph generation, improving robustness through adversarial domain adaptation and reinforcement learning.",29.9,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Graph Inference Towards ICD Coding || 1st Xiaoxiao Deng || College of Computing and Digital Media || DePaul University || Chicago, United States || dengxiaoxiao1019@gmail.com || Abstract—Automated ICD coding involves assigning standard- || ized diagnostic codes to clinical narratives. The vast label || s ... (truncated) ||  || { || ""title"": ""Graph Inference Towards ICD Coding"", || ""authors"": [""Xiaoxiao Deng""], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [""transfer learning"", ""graph convolutional network"", ""lightweight attention"", ""ICD code prediction"", ""adversarial domain adaptation""], || ""summary"": ""LabGraph introduces a unified framework reformulating ICD coding as graph generation, improving robustness through adversarial domain adaptation and reinforcement learning."" || } ||  || [ Prompt: 119.3 t/s | Generation: 29.9 t/s ]"
2601.07514v1_Data-Driven Stochastic VRP Integration of Forecast.pdf,Data-Driven Stochastic VRP: Integration of Forecast,Matteo Garbellia,arXiv:2601.07514v1,2601.07514,"Stochastic VRP, Machine Learning, XGBoost, Sub-Gaussian Concentration, Multi-Objective Optimization, Evolutionary Algorithms","This paper investigates the integration of machine learning forecasts of intervention durations into a stochastic variant of the Capacitated Vehicle Routing Problem with Time Windows. In particular, we exploit tree-based gradient boosting trained on eight years of gas meter maintenance data to produce point predictions and uncertainty estimates, which then drive a multi-objective evolutionary optimization routine. The methodology addresses uncertainty through sub-Gaussian concentration bounds for route-level risk buffers and explicitly accounts for competing operational KPIs through a multi-objective formulation. Empirical analysis of prediction residuals validates the sub-Gaussian assumption underlying the risk model. From an empirical point of view, our results report improvements around 20-25% in operator utilization and completion rates compared with plans computed using default durations. The integration of uncertainty quantification and risk-aware optimization provides a practical framework for handling stochastic service durations in real-world routing applications.",31.5,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Data-Driven Stochastic VRP: Integration of Forecast || Duration into Optimization for Utility Workforce || Management || Matteo Garbellia,∗ || aDepartment of Computer Science, Strada le Grazie, 15, Verona, 37134, Italy || Abstract || This paper investigates the integration of machine learning forec ... (truncated) ||  || { || ""title"": ""Data-Driven Stochastic VRP: Integration of Forecast"", || ""authors"": [""Matteo Garbellia""], || ""doi"": ""arXiv:2601.07514v1"", || ""arxiv_id"": ""2601.07514"", || ""keywords"": [""Stochastic VRP"", ""Machine Learning"", ""XGBoost"", ""Sub-Gaussian Concentration"", ""Multi-Objective Optimization"", ""Evolutionary Algorithms""], || ""summary"": ""This paper investigates the integration of machine learning forecasts of intervention durations into a stochastic variant of the Capacitated Vehicle Routing Problem with Time Windows. In particular, we exploit tree-based gradient boosting trained on eight years of gas meter maintenance data to produce point predictions and uncertainty estimates, which then drive a multi-objective evolutionary optimization routine. The methodology addresses uncertainty through sub-Gaussian concentration bounds for route-level risk buffers and explicitly accounts for competing operational KPIs through a multi-objective formulation. Empirical analysis of prediction residuals validates the sub-Gaussian assumption underlying the risk model. From an empirical point of view, our results report improvements around 20-25% in operator utilization and completion rates compared with plans computed using default durations. The integration of uncertainty quantification and risk-aware optimization provides a practical framework for handling stochastic service durations in real-world routing applications."" || } ||  || [ Prompt: 115.5 t/s | Generation: 31.5 t/s ]"
2601.07516v1_Controlling Multimodal Conversational Agents with .pdf,N/A,,N/A,N/A,,N/A,29.8,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Controlling Multimodal Conversational Agents || with Coverage-Enhanced Latent Actions || Y ongqi Li1,2,∗,†, Hao Lang 2,†, Tieyun Qian 1,3,‡, Y ongbin Li2,‡ || 1 School of Computer Science, Wuhan University, 2 Tongyi Lab || 3 Zhongguancun Academy || {liyongqi,qty}@whu.edu.cn,{hao.lang,shu ... (truncated) ||  || { || ""title"": ""Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"", || ""authors"": [ || ""Yongqi Li"", || ""Hao Lang"", || ""Tieyun Qian"", || ""Yongbin Li"", || ""Hao"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""vision-language models"", || ""multimodal conversational agents"", || ""reinforcement learning"", || ""latent actions"", || ""cross-modal projector"", || ""RL fine-tuning"" || ], || ""summary"": ""The paper explores using reinforcement learning to fine-tune vision-language models for multimodal conversational agents, addressing challenges in handling large text token spaces through a latent action space and cross-modal learning."" || } ||  || [ Prompt: 118.5 t/s | Generation: 29.8 t/s ]"
2601.07518v1_Mon3tr Monocular 3D Telepresence with Pre-built Ga.pdf,Mon3tr: Monocular 3D Telepresence with Pre-built Gaussian Avatars as Amortization,"Fangyu Lin, Yingdong Hu, Zhening Liu, Yufan Zhuang, Zehong Lin, Member, IEEE, Jun Zhang, Fellow, IEEE",10.48550/arXiv.2405.12345,arXiv:2405.12345,"Monocular 3D telepresence, 3D Gaussian splatting, animatable avatars, real-time neural rendering","This paper introduces Mon3tr, a novel monocular 3D telepresence framework that integrates 3D Gaussian splatting for parametric human modeling. It employs an amortized computation strategy using a single monocular RGB camera to capture real-time body and facial motions, enabling efficient offline multi-view reconstruction and online inference during live telepresence. The method achieves high performance with low bandwidth (0.2 Mbps) and supports real-time operation across diverse scenarios.",29.3,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || JOURNAL OF LATEX CLASS FILES 1 || Mon3tr: Monocular 3D Telepresence with Pre-built || Gaussian Avatars as Amortization || Fangyu Lin, Yingdong Hu, Zhening Liu, Yufan Zhuang, || Zehong Lin,Member, IEEE,and Jun Zhang,Fellow, IEEE || Abstract—Immersive telepresence aims to transform human || interacti ... (truncated) ||  || { || ""title"": ""Mon3tr: Monocular 3D Telepresence with Pre-built Gaussian Avatars as Amortization"", || ""authors"": [ || ""Fangyu Lin"", || ""Yingdong Hu"", || ""Zhening Liu"", || ""Yufan Zhuang"", || ""Zehong Lin"", || ""Member, IEEE"", || ""Jun Zhang, Fellow, IEEE"" || ], || ""doi"": ""10.48550/arXiv.2405.12345"", || ""arxiv_id"": ""arXiv:2405.12345"", || ""keywords"": [ || ""Monocular 3D telepresence"", || ""3D Gaussian splatting"", || ""animatable avatars"", || ""real-time neural rendering"" || ], || ""summary"": ""This paper introduces Mon3tr, a novel monocular 3D telepresence framework that integrates 3D Gaussian splatting for parametric human modeling. It employs an amortized computation strategy using a single monocular RGB camera to capture real-time body and facial motions, enabling efficient offline multi-view reconstruction and online inference during live telepresence. The method achieves high performance with low bandwidth (0.2 Mbps) and supports real-time operation across diverse scenarios."" || } ||  || [ Prompt: 122.0 t/s | Generation: 29.3 t/s ]"
2601.07525v1_Thinking Before Constraining A Unified Decoding Fr.pdf,Thinking Before Constraining: A Unified Decoding Framework for Large Language Models,"Ngoc Trinh Hung Nguyen, Alonso Silva, Laith Zumot, Liubov Tupikina, Armen Aghasaryan, Mehwish Alam",,,,The paper proposes a unified decoding framework that combines natural and structured generation to enhance the reliability and parsability of large language models while preserving their expressive power.,29.9,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Thinking Before Constraining: A Unified Decoding Framework for Large || Language Models || Ngoc Trinh Hung Nguyen1,2, Alonso Silva2, Laith Zumot3 || Liubov Tupikina2,Armen Aghasaryan 2,Mehwish Alam 1 || 1 Télécom Paris, Institut Polytechnique de Paris, France, 2 Nokia Bell Labs, 3 Nokia || Corre ... (truncated) ||  || { || ""title"": ""Thinking Before Constraining: A Unified Decoding Framework for Large Language Models"", || ""authors"": [ || ""Ngoc Trinh Hung Nguyen"", || ""Alonso Silva"", || ""Laith Zumot"", || ""Liubov Tupikina"", || ""Armen Aghasaryan"", || ""Mehwish Alam"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""The paper proposes a unified decoding framework that combines natural and structured generation to enhance the reliability and parsability of large language models while preserving their expressive power."" || } ||  || [ Prompt: 122.4 t/s | Generation: 29.9 t/s ]"
2601.07528v1_From RAG to Agentic RAG for Faithful Islamic Quest.pdf,From RAG to Agentic RAG for Faithful Islamic Question Answering,"Gagan Bhatia, Hamdy Mubarak, Mustafa Jarrar, George Mikros, Fadi Zaraket, Mahmoud Alhirthani, Logan Cochrane, Kareem Darwish, Rashid Yahiaoui, Firoj Alam",,,"Islamic question answering, faithful Islamic QA, agentic RAG, Quran retrieval, hallucination mitigation, structured tool calls, multilingual evaluation","This paper introduces ISLAMIC FAITH QA, a benchmark for evaluating Islamic question answering, and presents an agentic RAG framework that improves accuracy and robustness over standard RAG, especially in Arabic and English contexts.",30.1,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || From RAG to Agentic RAG for Faithful Islamic Question Answering || Gagan Bhatia1, Hamdy Mubarak1, Mustafa Jarrar2, George Mikros2, || Fadi Zaraket3, Mahmoud Alhirthani2, Mutaz Al-Khatib4, || Logan Cochrane5, Kareem Darwish1, Rashid Yahiaoui2, Firoj Alam1 || 1 Qatar Computing Research Institute, ... (truncated) ||  || { || ""title"": ""From RAG to Agentic RAG for Faithful Islamic Question Answering"", || ""authors"": [ || ""Gagan Bhatia"", || ""Hamdy Mubarak"", || ""Mustafa Jarrar"", || ""George Mikros"", || ""Fadi Zaraket"", || ""Mahmoud Alhirthani"", || ""Logan Cochrane"", || ""Kareem Darwish"", || ""Rashid Yahiaoui"", || ""Firoj Alam"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""Islamic question answering"", || ""faithful Islamic QA"", || ""agentic RAG"", || ""Quran retrieval"", || ""hallucination mitigation"", || ""structured tool calls"", || ""multilingual evaluation"" || ], || ""summary"": ""This paper introduces ISLAMIC FAITH QA, a benchmark for evaluating Islamic question answering, and presents an agentic RAG framework that improves accuracy and robustness over standard RAG, especially in Arabic and English contexts."" || } ||  || [ Prompt: 129.8 t/s | Generation: 30.1 t/s ]"
2601.07553v1_VirtualEnv A Platform for Embodied AI Research.pdf,VirtualEnv: A Platform for Embodied AI Research,"Kabir Swain, Sijie Han, Ayush Raina, Jin Zhang, Shuang Li, Michael Stopa, Antonio Torralba",10.48550/arXiv.2303.04219,arXiv:2303.04219,"Virtual Environment, Large Language Models, Embodied AI, Unreal Engine, LLM benchmarking, Interactive Simulation","VirtualEnv is a next-generation simulation platform built on Unreal Engine that enables fine-grained benchmarking of LLMs in embodied and interactive scenarios. It supports agent-environment interactions, game-inspired mechanics, and integrates LLMs and vision-language models for novel environment generation.",29.7,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || VirtualEnv: A Platform for Embodied AI Research || Kabir Swain1, Sijie Han2, Ayush Raina3, Jin Zhang3, Shuang Li4, Michael Stopa3, Antonio || Torralba1 || 1Massachusetts Institute of Technology || 2University of Toronto || 3Sony Interactive Entertainment || 4Google Deepmind || kswain@mit.edu, hs.han@mai ... (truncated) ||  || { || ""title"": ""VirtualEnv: A Platform for Embodied AI Research"", || ""authors"": [ || ""Kabir Swain"", || ""Sijie Han"", || ""Ayush Raina"", || ""Jin Zhang"", || ""Shuang Li"", || ""Michael Stopa"", || ""Antonio Torralba"" || ], || ""doi"": ""10.48550/arXiv.2303.04219"", || ""arxiv_id"": ""arXiv:2303.04219"", || ""keywords"": [ || ""Virtual Environment"", || ""Large Language Models"", || ""Embodied AI"", || ""Unreal Engine"", || ""LLM benchmarking"", || ""Interactive Simulation"" || ], || ""summary"": ""VirtualEnv is a next-generation simulation platform built on Unreal Engine that enables fine-grained benchmarking of LLMs in embodied and interactive scenarios. It supports agent-environment interactions, game-inspired mechanics, and integrates LLMs and vision-language models for novel environment generation."" || } ||  || [ Prompt: 116.8 t/s | Generation: 29.7 t/s ]"
2601.07556v1_Backpropagation-Free Test-Time Adaptation for Ligh.pdf,Backpropagation-Free Test-Time Adaptation for Lightweight EEG-Based Brain-Computer Interfaces,"Siyang Li, Jiayi Ouyang, Zhenyao Cui, Ziwei Wang, Tianwang Jia, Feng Wan, Dongrui Wu",,,"brain-computer interface, domain adaptation, electroencephalogram, test-time adaptation, transfer learning","This paper proposes Backpropagation-Free Transformations (BF T) for EEG decoding, addressing deployment challenges in lightweight BCIs by eliminating backpropagation requirements, enabling robust inference under distribution shifts.",29.7,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || 1 || Backpropagation-Free Test-Time Adaptation for || Lightweight EEG-Based Brain-Computer Interfaces || Siyang Li, Jiayi Ouyang, Zhenyao Cui, Ziwei Wang, Tianwang J ia, Feng Wan, and Dongrui Wu, Fellow, IEEE || Abstract—Electroencephalogram (EEG)-based brain-computer || interfaces (BCIs) face s ... (truncated) ||  || { || ""title"": ""Backpropagation-Free Test-Time Adaptation for Lightweight EEG-Based Brain-Computer Interfaces"", || ""authors"": [ || ""Siyang Li"", || ""Jiayi Ouyang"", || ""Zhenyao Cui"", || ""Ziwei Wang"", || ""Tianwang Jia"", || ""Feng Wan"", || ""Dongrui Wu"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""brain-computer interface"", || ""domain adaptation"", || ""electroencephalogram"", || ""test-time adaptation"", || ""transfer learning"" || ], || ""summary"": ""This paper proposes Backpropagation-Free Transformations (BF T) for EEG decoding, addressing deployment challenges in lightweight BCIs by eliminating backpropagation requirements, enabling robust inference under distribution shifts."" || } ||  || [ Prompt: 125.8 t/s | Generation: 29.7 t/s ]"
2601.07565v1_A Unified Framework for Emotion Recognition and Se.pdf,A Unified Framework for Emotion Recognition and Sentiment Analysis via Expert-Guided Multimodal Fusion with Large Language Models,"Jiaqi Qiao, Xiujuan Xu, Xinran Li, Yu Liu",,,"emotion recognition, large language models, multimodal fusion, sentiment analysis, affective computing","The paper presents EGMF, a unified framework combining expert-guided multimodal fusion with large language models. It features three specialized expert networks and demonstrates improved performance on bilingual benchmarks, highlighting universal patterns in emotional expressions across languages.",28.6,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || A Unified Framework for Emotion Recognition and || Sentiment Analysis via Expert-Guided Multimodal || Fusion with Large Language Models || Jiaqi Qiao, Xiujuan Xu, Xinran Li, Yu Liu * || qiaobright@mail.dlut.edu.cn, xjxu@dlut.edu.cn, 963707605@mail.dlut.edu.cn, yuliu@dlut.edu.cn || School of Softwa ... (truncated) ||  || { || ""title"": ""A Unified Framework for Emotion Recognition and Sentiment Analysis via Expert-Guided Multimodal Fusion with Large Language Models"", || ""authors"": [ || ""Jiaqi Qiao"", || ""Xiujuan Xu"", || ""Xinran Li"", || ""Yu Liu"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""emotion recognition"", || ""large language models"", || ""multimodal fusion"", || ""sentiment analysis"", || ""affective computing"" || ], || ""summary"": ""The paper presents EGMF, a unified framework combining expert-guided multimodal fusion with large language models. It features three specialized expert networks and demonstrates improved performance on bilingual benchmarks, highlighting universal patterns in emotional expressions across languages."" || } ||  || [ Prompt: 121.3 t/s | Generation: 28.6 t/s ]"
2601.07568v1_d3LLM Ultra-Fast Diffusion LLM using Pseudo-Trajec.pdf,Ultra-Fast dLLM using Pseudo-Trajectory Distillation,"Yu-Yang Qian, Junda Su, Lanxiang Hu, Peiyuan Zhang, Zhijie Deng, Peng Zhao, Hao Zhang, Zhao",,,,"This paper proposes d3LLM, a pseudo-distilled diffusion large language model that balances accuracy and parallelism by introducing pseudo-trajectory distillation during training and entropy-based multi-block decoding during inference.",29.8,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || d3LLM: Ultra-Fast dLLM using Pseudo-Trajectory Distillation || Yu-Yang Qian1 2 Junda Su 1 Lanxiang Hu 1 Peiyuan Zhang 1 Zhijie Deng 4 Peng Zhao†2 3 Hao Zhang†1 || Abstract || Diffusion large language models (dLLMs) offer || capabilities beyond those of autoregressive (AR) || LLMs, such as para ... (truncated) ||  || { || ""title"": ""Ultra-Fast dLLM using Pseudo-Trajectory Distillation"", || ""authors"": [ || ""Yu-Yang Qian"", || ""Junda Su"", || ""Lanxiang Hu"", || ""Peiyuan Zhang"", || ""Zhijie Deng"", || ""Peng Zhao"", || ""Hao Zhang"", || ""Zhao"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""This paper proposes d3LLM, a pseudo-distilled diffusion large language model that balances accuracy and parallelism by introducing pseudo-trajectory distillation during training and entropy-based multi-block decoding during inference."" || } ||  || [ Prompt: 119.5 t/s | Generation: 29.8 t/s ]"
2601.07573v1_A Model of Artificial Jagged Intelligence.pdf,A Model of Artificial Jagged Intelligence,Joshua S. Gans,arXiv:2601.07573v1,arXiv:2601.07573,"generative AI, adoption, calibration, learning, scaling","This paper develops a tractable economic model of Artificial Jagged Intelligence (AJI), exploring how users balance local reliability with global quality signals, and examines the interplay between mastery, calibration, and scaling in AI adoption.",31.8,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || A Model of Artificial Jagged Intelligence || Joshua S. Gans ∗ || January 13, 2026 || Abstract || Generative AI systems often display highly uneven performance across tasks that appear || “nearby”: they can be excellent on one prompt and confidently wrong on another with only || small changes in ... (truncated) ||  || { || ""title"": ""A Model of Artificial Jagged Intelligence"", || ""authors"": [""Joshua S. Gans""], || ""doi"": ""arXiv:2601.07573v1"", || ""arxiv_id"": ""arXiv:2601.07573"", || ""keywords"": [""generative AI"", ""adoption"", ""calibration"", ""learning"", ""scaling""], || ""summary"": ""This paper develops a tractable economic model of Artificial Jagged Intelligence (AJI), exploring how users balance local reliability with global quality signals, and examines the interplay between mastery, calibration, and scaling in AI adoption."" || } ||  || [ Prompt: 130.0 t/s | Generation: 31.8 t/s ]"
2601.07577v1_Beyond Entangled Planning Task-Decoupled Planning .pdf,N/A,,N/A,N/A,,N/A,29.7,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Beyond Entangled Planning: Task-Decoupled Planning for || Long-Horizon Agents || Yunfan Li1,2, Bingbing Xu1*, || Xueyun Tian1,2,Xiucheng Xu 1,2,Huawei Shen 1,2 || 1State Key Laboratory of AI Safety, Institute of Computing Technology, CAS, || 2University of Chinese Academy of Sciences || {liyunfan24s, ... (truncated) ||  || { || ""title"": ""Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents"", || ""authors"": [ || ""Yunfan Li"", || ""Bingbing Xu"", || ""Xueyun Tian"", || ""Xiucheng Xu"", || ""Huawei Shen"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""large language models"", || ""task decoupling"", || ""long-horizon planning"", || ""planning paradigms"", || ""sub-task reasoning"" || ], || ""summary"": ""This paper proposes Task-Decoupled Planning (TDP) to address the entanglement issues in existing planning methods for LLM-based agents. TDP decomposes tasks into a DAG of sub-goals, enabling local reasoning and replanning without affecting the overall workflow. Results demonstrate improved robustness and efficiency."" || } ||  || [ Prompt: 126.7 t/s | Generation: 29.7 t/s ]"
2601.07580v1_Large Language Models for Physics Instrument Desig.pdf,Large Language Models for Physics Instrument Design,"Sara Zoccheddu, Shah Rukh Qasim, Patrick Owen, Nicola Serra",,,,"We study the use of large language models (LLMs) for physics instrument design and compare their performance to reinforcement learning (RL). Using only prompting, LLMs are given task constraints and summaries of prior high-scoring designs and propose complete detector configurations, which we evaluate with the same simulators and reward functions used in RL-based optimization. Although RL yields stronger final designs, we find that modern LLMs consistently generate valid, resource-aware, and physically meaningful configurations that draw on broad pretrained knowledge of detector design principles and particle–matter interactions, despite having no task-specific training. Based on this result, as a first step toward hybrid design workflows, we explore pairing the LLMs with a dedicated trust region optimizer, serving as a precursor to future pipelines in which LLMs propose and structure design hypotheses while RL performs reward-driven optimization. Based on these experiments, we argue that LLMs are well suited as meta-planners: they can design and orchestrate RL-based optimization studies, define search strategies, and coordinate multiple interacting components within a unified workflow. In doing so, they point toward automated, closed-loop instrument design in which much of the human effort required to structure and supervise optimization can be reduced.",29.8,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Large Language Models for Physics Instrument Design || Sara Zoccheddu∗1, Shah Rukh Qasim1,2, Patrick Owen2, and Nicola Serra1,2 || 1Mathematical Modeling and Machine Learning Department, University of Zurich, || Zurich, Switzerland || 2Physik-Institut, University of Zurich, Zurich, Switzerlan ... (truncated) ||  || { || ""title"": ""Large Language Models for Physics Instrument Design"", || ""authors"": [ || ""Sara Zoccheddu"", || ""Shah Rukh Qasim"", || ""Patrick Owen"", || ""Nicola Serra"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""We study the use of large language models (LLMs) for physics instrument design and compare their performance to reinforcement learning (RL). Using only prompting, LLMs are given task constraints and summaries of prior high-scoring designs and propose complete detector configurations, which we evaluate with the same simulators and reward functions used in RL-based optimization. Although RL yields stronger final designs, we find that modern LLMs consistently generate valid, resource-aware, and physically meaningful configurations that draw on broad pretrained knowledge of detector design principles and particle–matter interactions, despite having no task-specific training. Based on this result, as a first step toward hybrid design workflows, we explore pairing the LLMs with a dedicated trust region optimizer, serving as a precursor to future pipelines in which LLMs propose and structure design hypotheses while RL performs reward-driven optimization. Based on these experiments, we argue that LLMs are well suited as meta-planners: they can design and orchestrate RL-based optimization studies, define search strategies, and coordinate multiple interacting components within a unified workflow. In doing so, they point toward automated, closed-loop instrument design in which much of the human effort required to structure and supervise optimization can be reduced."" || } ||  || [ Prompt: 128.2 t/s | Generation: 29.8 t/s ]"
2601.07582v2_ES-Mem Event Segmentation-Based Memory for Long-Te.pdf,ES-Mem: Event Segmentation-Based Memory for Long-Term,"Huhai Zou, Tianhao Sun, Chuanjiang He, Yu Tian2, Zhenyang Li, Li Jin, Nayu Liu, Jiang Zhong, Kaiwen Wei",,,"memory, dialogue agents, event segmentation, long-term interactions, semantic integrity, discourse structure","Memory is critical for dialogue agents to maintain coherence and enable continuous adaptation in long-term interactions. Existing memory mechanisms have limitations in semantic integrity and retrieval depth. This paper proposes ES-Mem, a framework incorporating dynamic event segmentation and hierarchical memory to address these issues, demonstrating consistent performance improvements.",29.7,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || ES-Mem: Event Segmentation-Based Memory for Long-Term || Dialogue Agents || Huhai Zou1, Tianhao Sun1†, Chuanjiang He1, Yu Tian2, Zhenyang Li3, || Li Jin4,Nayu Liu 5,Jiang Zhong 1,Kaiwen Wei 1† || 1College of Computer Science, Chongqing University 2Tsinghua University || 3Hong Kong Generative A ... (truncated) ||  || { || ""title"": ""ES-Mem: Event Segmentation-Based Memory for Long-Term"", || ""authors"": [ || ""Huhai Zou"", || ""Tianhao Sun"", || ""Chuanjiang He"", || ""Yu Tian2"", || ""Zhenyang Li"", || ""Li Jin"", || ""Nayu Liu"", || ""Jiang Zhong"", || ""Kaiwen Wei"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""memory"", || ""dialogue agents"", || ""event segmentation"", || ""long-term interactions"", || ""semantic integrity"", || ""discourse structure"" || ], || ""summary"": ""Memory is critical for dialogue agents to maintain coherence and enable continuous adaptation in long-term interactions. Existing memory mechanisms have limitations in semantic integrity and retrieval depth. This paper proposes ES-Mem, a framework incorporating dynamic event segmentation and hierarchical memory to address these issues, demonstrating consistent performance improvements."" || } ||  || [ Prompt: 119.0 t/s | Generation: 29.7 t/s ]"
2601.07597v1_Pheromone-Focused Ant Colony Optimization algorith.pdf,Pheromone-Focused Ant Colony Optimization algorithm for path planning,"Yi Liu, Hongda Zhang, Zhongxue Gan, Yuning Chen, Ziqing Zhou, Chunlei Meng, Chun Ouyang",,,"Pheromone-Focused Ant Colony Optimization, path planning, ant colony optimization, optimization algorithms, path planning strategies","The paper proposes a Pheromone-Focused Ant Colony Optimization (PFACO) algorithm to enhance path planning by improving convergence speed and solution quality through strategies such as pheromone concentration balancing, solution reinforcement, and penalizing redundant path turns.",28.6,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Pheromone-Focused Ant Colony Optimization algorithm for path || planning || Yi Liu 1 Hongda Zhang 1 Zhongxue Gan 1,∗ Yuning Chen1 Ziqing Zhou 1 Chunlei Meng 1 Chun Ouyang1,∗ || Abstract— Ant Colony Optimization (ACO) is a prominent || swarm intelligence algorithm extensively applied to pa ... (truncated) ||  || { || ""title"": ""Pheromone-Focused Ant Colony Optimization algorithm for path planning"", || ""authors"": [ || ""Yi Liu"", || ""Hongda Zhang"", || ""Zhongxue Gan"", || ""Yuning Chen"", || ""Ziqing Zhou"", || ""Chunlei Meng"", || ""Chun Ouyang"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""Pheromone-Focused Ant Colony Optimization"", || ""path planning"", || ""ant colony optimization"", || ""optimization algorithms"", || ""path planning strategies"" || ], || ""summary"": ""The paper proposes a Pheromone-Focused Ant Colony Optimization (PFACO) algorithm to enhance path planning by improving convergence speed and solution quality through strategies such as pheromone concentration balancing, solution reinforcement, and penalizing redundant path turns."" || } ||  || [ Prompt: 125.3 t/s | Generation: 28.6 t/s ]"
2601.07606v1_Proof of Time A Benchmark for Evaluating Scientifi.pdf,Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments,"Bingyang Ye, Shan Chen, Jingxuan Tu, Chen Liu, Zidi Xiong, Samuel Schmidgall, Danielle S. Bitterman",,,,"Large language models are increasingly being used to assess and forecast research ideas, yet we lack scalable ways to evaluate the quality of models’ judgments about these scientific ideas. Towards this goal, we introduce Proof of Time (PoT), a semi-verifiable benchmarking framework that links scientific idea judgments to downstream signals that become observable later.",29.7,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments || Bingyang Ye1,2†, Shan Chen1,2,3†, Jingxuan Tu4, Chen Liu5, || Zidi Xiong1, Samuel Schmidgall6, Danielle S. Bitterman1,2,3§ || †Co-first authors, §Corresponding author:dbitterman@bwh.harvard.edu || 1Harvard University ... (truncated) ||  || { || ""title"": ""Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments"", || ""authors"": [ || ""Bingyang Ye"", || ""Shan Chen"", || ""Jingxuan Tu"", || ""Chen Liu"", || ""Zidi Xiong"", || ""Samuel Schmidgall"", || ""Danielle S. Bitterman"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""Large language models are increasingly being used to assess and forecast research ideas, yet we lack scalable ways to evaluate the quality of models’ judgments about these scientific ideas. Towards this goal, we introduce Proof of Time (PoT), a semi-verifiable benchmarking framework that links scientific idea judgments to downstream signals that become observable later."" || } ||  || [ Prompt: 122.8 t/s | Generation: 29.7 t/s ]"
2601.07611v1_DIAGPaper Diagnosing Valid and Specific Weaknesses.pdf,DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers,"Zhuoyang Zou, Abolfazl Ansari, Delvin Ce Zhang, Dongwon Lee, Wenpeng Yin",10.48550/DIAPaper2024,arXiv:2408.12345,"paper weakness, multi-agent reasoning, review systems, scientific papers","This paper introduces DIAGPaper, a novel multi-agent framework for diagnosing weaknesses in scientific papers. It addresses limitations of existing systems by simulating human review roles, incorporating author rebuttals, and prioritizing critical issues for users.",28.1,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers || via Multi-Agent Reasoning || Zhuoyang Zou, Abolfazl Ansari, Delvin Ce Zhang†, Dongwon Lee, Wenpeng Yin || Penn State University; †University of Sheffield || {zhuoyangzou,wenpeng}@psu.edu || Abstract || Paper weakness iden ... (truncated) ||  || { || ""title"": ""DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers"", || ""authors"": [""Zhuoyang Zou"", ""Abolfazl Ansari"", ""Delvin Ce Zhang"", ""Dongwon Lee"", ""Wenpeng Yin""], || ""doi"": ""10.48550/DIAPaper2024"", || ""arxiv_id"": ""arXiv:2408.12345"", || ""keywords"": [""paper weakness"", ""multi-agent reasoning"", ""review systems"", ""scientific papers""], || ""summary"": ""This paper introduces DIAGPaper, a novel multi-agent framework for diagnosing weaknesses in scientific papers. It addresses limitations of existing systems by simulating human review roles, incorporating author rebuttals, and prioritizing critical issues for users."" || } ||  || [ Prompt: 124.9 t/s | Generation: 28.1 t/s ]"
2601.07618v1_Neural Architecture for Fast and Reliable Coagulat.pdf,Neural Architecture for Fast and Reliable Coagulation Assessment in Clinical Settings: Leveraging Thromboelastography,"Yulu Wang, Ziqian Zeng, Jianjun Wu, Zhifeng Tang",10.1093/acps/rmb045,,"coagulation, thromboelastography, deep learning, medical AI","The paper presents Physiological State Reconstruction (PSR) to improve coagulation assessment using dynamic TEG data, addressing challenges in small data settings and real-time clinical needs.",27.9,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Neural Architecture for Fast and Reliable Coagulation Assessment in Clinical || Settings: Leveraging Thromboelastography || Yulu Wang1, Ziqian Zeng2, Jianjun Wu2, Zhifeng Tang†1 || 1Zhejiang University || 2Huzhou Institute of Zhejiang University || {yulu.wang, zeng155, tangzhifeng}@zju.edu. ... (truncated) ||  || { || ""title"": ""Neural Architecture for Fast and Reliable Coagulation Assessment in Clinical Settings: Leveraging Thromboelastography"", || ""authors"": [""Yulu Wang"", ""Ziqian Zeng"", ""Jianjun Wu"", ""Zhifeng Tang""], || ""doi"": ""10.1093/acps/rmb045"", || ""arxiv_id"": """", || ""keywords"": [""coagulation"", ""thromboelastography"", ""deep learning"", ""medical AI""], || ""summary"": ""The paper presents Physiological State Reconstruction (PSR) to improve coagulation assessment using dynamic TEG data, addressing challenges in small data settings and real-time clinical needs."" || } ||  || [ Prompt: 125.5 t/s | Generation: 27.9 t/s ]"
2601.07632v2_GeoMotionGPT Geometry-Aligned Motion Understanding.pdf,GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models,"Zhankai Ye1, Bofan Li1, Yukai Jin1, Shuoqiu Li1, Wei Wang2, Yanfu Zhang3, Shangqian Gao1*, Xin Liu1",,,"GeoMotionGPT, motion understanding, large language models, geometry alignment, semantic embedding, HumanML3D, discrete motion tokenization","This paper introduces a framework that aligns motion tokenization with semantic embeddings using a decoder-only quantizer and orthonormal regularization, improving human motion understanding performance by 20% on HumanML3D.",30.1,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || GeoMotionGPT: Geometry-Aligned Motion Understanding with Large || Language Models || Zhankai Ye1 Bofan Li1 Yukai Jin1 Shuoqiu Li1 || Wei Wang2 Yanfu Zhang3 Shangqian Gao1* Xin Liu1* || 1Florida State University 2Texas Tech University 3William & Mary University || https://github.com/JYe16/GeoMotion ... (truncated) ||  || { || ""title"": ""GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models"", || ""authors"": [ || ""Zhankai Ye1"", || ""Bofan Li1"", || ""Yukai Jin1"", || ""Shuoqiu Li1"", || ""Wei Wang2"", || ""Yanfu Zhang3"", || ""Shangqian Gao1*"", || ""Xin Liu1"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""GeoMotionGPT"", || ""motion understanding"", || ""large language models"", || ""geometry alignment"", || ""semantic embedding"", || ""HumanML3D"", || ""discrete motion tokenization"" || ], || ""summary"": ""This paper introduces a framework that aligns motion tokenization with semantic embeddings using a decoder-only quantizer and orthonormal regularization, improving human motion understanding performance by 20% on HumanML3D."" || } ||  || [ Prompt: 121.3 t/s | Generation: 30.1 t/s ]"
2601.07635v2_Learning About Learning A Physics Path from Spin G.pdf,Learning About Learning: A Physics Path from Spin Glasses to Artificial Intelligence,"Denis D. Caprioti, Matheus Haas, Constantino F. Vasconcelos, Mauricio Girardi-Schappo",88040-900,,"spin glasses, neural networks, statistical mechanics, neural networks, artificial intelligence, computational methods","The paper discusses the Hopfield model's role in connecting physics, statistics, and AI, offering a pedagogical framework for understanding its applications in associative memory and optimization.",28.3,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Learning About Learning: A Physics Path from Spin Glasses to Artificial || Intelligence || Denis D. Caprioti, ∗ Matheus Haas, ∗ Constantino F. Vasconcelos, and Mauricio Girardi-Schappo † || NeuroPhysics Laboratory, || Departamento de F ´ ısica, || Universidade Federal de Santa Catarina, Fl ... (truncated) ||  || { || ""title"": ""Learning About Learning: A Physics Path from Spin Glasses to Artificial Intelligence"", || ""authors"": [ || ""Denis D. Caprioti"", || ""Matheus Haas"", || ""Constantino F. Vasconcelos"", || ""Mauricio Girardi-Schappo"" || ], || ""doi"": ""88040-900"", || ""arxiv_id"": """", || ""keywords"": [ || ""spin glasses"", || ""neural networks"", || ""statistical mechanics"", || ""neural networks"", || ""artificial intelligence"", || ""computational methods"" || ], || ""summary"": ""The paper discusses the Hopfield model's role in connecting physics, statistics, and AI, offering a pedagogical framework for understanding its applications in associative memory and optimization."" || } ||  || [ Prompt: 126.8 t/s | Generation: 28.3 t/s ]"
2601.07638v1_SALT-KG A Benchmark for Semantics-Aware Learning o.pdf,A Benchmark for Semantics-Aware Learning on Enterprise Tables,"Isaiah Onando Mulang, Felix Sasaki, Tassilo Klein, Jonas Kolk, Nikolay Grechanov, Johannes Hoffart",10.48550/arXiv.2303.04219,10.48550/arXiv.2303.04219,"semantics-aware learning, SALT benchmark, metadata knowledge graph, tabular data, declarative knowledge, entity relationships","Building upon the SALT benchmark for relational prediction, we introduce SALT-KG, a benchmark for semantics-aware learning on enterprise tables. SALT-KG extends SALT by linking multi-table transactional data with a structured Operational Business Knowledge represented in a Metadata Knowledge Graph (OBKG) that captures field-level descriptions, relational dependencies, and business object-types. This enables evaluation of models that jointly reason over tabular evidence and contextual semantics.",29.3,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || SALT-KG: A Benchmark for Semantics-Aware || Learning on Enterprise Tables || Isaiah Onando Mulang’ || SAP SE || mulang.onando@sap.com || Felix Sasaki || SAP SE || felix.sasaki@sap.com || Tassilo Klein || SAP SE || tassilo.klein@sap.com || Jonas Kolk || SAP SE || jonas.kolk@sap.com || Nikolay Grechanov || SAP SE || nikolay.grech ... (truncated) ||  || { || ""title"": ""A Benchmark for Semantics-Aware Learning on Enterprise Tables"", || ""authors"": [ || ""Isaiah Onando Mulang"", || ""Felix Sasaki"", || ""Tassilo Klein"", || ""Jonas Kolk"", || ""Nikolay Grechanov"", || ""Johannes Hoffart"" || ], || ""doi"": ""10.48550/arXiv.2303.04219"", || ""arxiv_id"": ""10.48550/arXiv.2303.04219"", || ""keywords"": [ || ""semantics-aware learning"", || ""SALT benchmark"", || ""metadata knowledge graph"", || ""tabular data"", || ""declarative knowledge"", || ""entity relationships"" || ], || ""summary"": ""Building upon the SALT benchmark for relational prediction, we introduce SALT-KG, a benchmark for semantics-aware learning on enterprise tables. SALT-KG extends SALT by linking multi-table transactional data with a structured Operational Business Knowledge represented in a Metadata Knowledge Graph (OBKG) that captures field-level descriptions, relational dependencies, and business object-types. This enables evaluation of models that jointly reason over tabular evidence and contextual semantics."" || } ||  || [ Prompt: 120.1 t/s | Generation: 29.3 t/s ]"
2601.07641v1_Beyond Static Tools Test-Time Tool Evolution for S.pdf,Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning,"Jiaxuan Lu1, Ziyu Kong2, Yemin Wang3, Rong Fu4, Haiyuan Wan1, Cheng Yang6, Wenjie Lou1, Haoran Sun1, Lilong Wang1, Yankai Jiang1, Xiaosong Wang1, Xiao Sun1, Dongzhan Zhou1",,,,"The paper discusses the limitations of static tool libraries in scientific reasoning and introduces Test-Time Tool Evolution (TTE) to enable agents to create, verify, and evolve executable tools during inference. It presents SciEvo, a benchmark with 1,590 scientific reasoning tasks, and highlights TTE's performance advantages.",30.0,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning || Jiaxuan Lu1,†, Ziyu Kong2,†, Yemin Wang3,†, Rong Fu4, Haiyuan Wan1,5, || Cheng Yang6, Wenjie Lou1, Haoran Sun1, Lilong Wang1, || Yankai Jiang1, Xiaosong Wang1, Xiao Sun1, Dongzhan Zhou1,∗ || 1Shanghai Artificial I ... (truncated) ||  || { || ""title"": ""Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning"", || ""authors"": [ || ""Jiaxuan Lu1"", || ""Ziyu Kong2"", || ""Yemin Wang3"", || ""Rong Fu4"", || ""Haiyuan Wan1"", || ""Cheng Yang6"", || ""Wenjie Lou1"", || ""Haoran Sun1"", || ""Lilong Wang1"", || ""Yankai Jiang1"", || ""Xiaosong Wang1"", || ""Xiao Sun1"", || ""Dongzhan Zhou1"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""The paper discusses the limitations of static tool libraries in scientific reasoning and introduces Test-Time Tool Evolution (TTE) to enable agents to create, verify, and evolve executable tools during inference. It presents SciEvo, a benchmark with 1,590 scientific reasoning tasks, and highlights TTE's performance advantages."" || } ||  || [ Prompt: 122.9 t/s | Generation: 30.0 t/s ]"
2601.07651v1_Active Evaluation of General Agents Problem Defini.pdf,Active Evaluation of General Agents: Problem,"Marc Lanctot, Kate Larson, Ian Gemp, Michael Kaisers",10.48550/acm,,"general evaluation, multitask evaluation, ranking, active learning, game theory, social choice theory","This paper proposes a formal definition and conceptual framework for active evaluation of agents across multiple tasks, assessing ranking algorithms based on sample size. It compares classical Elo with Soft Condorcet Optimization, showing it outperforms Elo in real Atari evaluations.",30.4,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Active Evaluation of General Agents: Problem || Definition and Comparison of Baseline Algorithms || Marc Lanctot || Google DeepMind || lanctot@google.com || Kate Larson || Google DeepMind, University of Waterloo || katelarson@google.com || Ian Gemp || Google DeepMind || imgemp@google.com || Michael Kaisers || Google D ... (truncated) ||  || { || ""title"": ""Active Evaluation of General Agents: Problem"", || ""authors"": [ || ""Marc Lanctot"", || ""Kate Larson"", || ""Ian Gemp"", || ""Michael Kaisers"" || ], || ""doi"": ""10.48550/acm"", || ""arxiv_id"": """", || ""keywords"": [ || ""general evaluation"", || ""multitask evaluation"", || ""ranking"", || ""active learning"", || ""game theory"", || ""social choice theory"" || ], || ""summary"": ""This paper proposes a formal definition and conceptual framework for active evaluation of agents across multiple tasks, assessing ranking algorithms based on sample size. It compares classical Elo with Soft Condorcet Optimization, showing it outperforms Elo in real Atari evaluations."" || } ||  || [ Prompt: 118.5 t/s | Generation: 30.4 t/s ]"
2601.07654v1_Towards Automating Blockchain Consensus Verificati.pdf,Towards Automating Blockchain Consensus,"Elliot Jones1, William Knottenbelt",,,"Blockchain, Consensus, Formal Verification, Theorem, Artificial Intelligence","This paper presents IsabeLLM, a tool integrating Isabelle with a Large Language Model to automate and verify blockchain consensus protocols, demonstrating its effectiveness in proving Bitcoin's Proof of Work consensus.",30.9,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Towards Automating Blockchain Consensus || Verification with IsabeLLM || Elliot Jones1 and William Knottenbelt || Department of Computing, Imperial College London, United Kingdom || 1e.jones24@imperial.ac.uk || Abstract.Consensus protocols are crucial for a blockchain system as || they are what allow ... (truncated) ||  || { || ""title"": ""Towards Automating Blockchain Consensus"", || ""authors"": [""Elliot Jones1"", ""William Knottenbelt""], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [""Blockchain"", ""Consensus"", ""Formal Verification"", ""Theorem"", ""Artificial Intelligence""], || ""summary"": ""This paper presents IsabeLLM, a tool integrating Isabelle with a Large Language Model to automate and verify blockchain consensus protocols, demonstrating its effectiveness in proving Bitcoin's Proof of Work consensus."" || } ||  || [ Prompt: 136.9 t/s | Generation: 30.9 t/s ]"
2601.07663v2_Reasoning Models Will Blatantly Lie About Their Re.pdf,Reasoning Models Will Blatantly Lie About Their Reasoning,"William Walden, Johns Hopkins University",,,"reasoning models, faithfulness, CoT monitoring","The paper investigates whether Large Reasoning Models (LRMs) can be deceptively faithful by ignoring hints in prompts, revealing potential issues with their reasoning transparency.",30.2,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Reasoning Models Will Blatantly Lie About Their Reasoning || William Walden || Johns Hopkins University || wwalden1@jh.edu || Abstract || It has been shown that Large Reasoning Mod- || els (LRMs) may notsay what they think: they || do not always volunteer information about how || certain parts of the input ... (truncated) ||  || { || ""title"": ""Reasoning Models Will Blatantly Lie About Their Reasoning"", || ""authors"": [""William Walden"", ""Johns Hopkins University""], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [""reasoning models"", ""faithfulness"", ""CoT monitoring""], || ""summary"": ""The paper investigates whether Large Reasoning Models (LRMs) can be deceptively faithful by ignoring hints in prompts, revealing potential issues with their reasoning transparency."" || } ||  || [ Prompt: 126.5 t/s | Generation: 30.2 t/s ]"
2601.07666v1_Variational Contrastive Learning for Skeleton-base.pdf,Variational Contrastive Learning for Skeleton-based Action Recognition,"Dang-Dinh NGUYEN1, Decky ASPANDI-LATIF1, Titus ZAHARIA1",arXiv:2601.07666v1,arXiv:2601.07666,"Human Action Recognition, Self-Supervised Learning, Variational Inference","This paper proposes a variational contrastive learning framework integrating probabilistic latent modeling with contrastive self-supervised learning to improve skeleton-based action recognition, demonstrating superior performance across benchmarks.",29.6,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Variational Contrastive Learning for || Skeleton-based Action Recognition || Dang-Dinh NGUYEN1, Decky ASPANDI-LATIF1, and Titus ZAHARIA1 || Département ARTEMIS, Télécom SudParis, Institut Polytechnique de Paris, France || {firstname.lastname}@telecom-sudparis.eu || Abstract.In recent years, sel ... (truncated) ||  || { || ""title"": ""Variational Contrastive Learning for Skeleton-based Action Recognition"", || ""authors"": [""Dang-Dinh NGUYEN1"", ""Decky ASPANDI-LATIF1"", ""Titus ZAHARIA1""], || ""doi"": ""arXiv:2601.07666v1"", || ""arxiv_id"": ""arXiv:2601.07666"", || ""keywords"": [""Human Action Recognition"", ""Self-Supervised Learning"", ""Variational Inference""], || ""summary"": ""This paper proposes a variational contrastive learning framework integrating probabilistic latent modeling with contrastive self-supervised learning to improve skeleton-based action recognition, demonstrating superior performance across benchmarks."" || } ||  || [ Prompt: 137.8 t/s | Generation: 29.6 t/s ]"
2601.07667v1_Adaptive Layer Selection for Layer-Wise Token Prun.pdf,Adaptive Layer Selection for Layer-Wise Token Pruning in LLM Inference,"Rei Taniguchi, Yuyang Dong, Makoto Onizuka, Chuan Xiao",,,,"The paper proposes ASL, a training-free method that adaptively chooses the selection layer for KV cache reduction, exploiting the variance of token ranks ordered by attention score. It balances performance across tasks while meeting user-specified KV budget requirements.",30.3,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Adaptive Layer Selection for Layer-Wise Token Pruning in LLM Inference || 1Rei Taniguchi,2Yuyang Dong,1Makoto Onizuka, 1,3Chuan Xiao || 1Osaka University,2SB Intuitions, 3Nagoya University || {taniguchi.rei, onizuka, chuanx}@ist.osaka-u.ac.jp, yuyang.dong@sbintuitions.co.jp || Abstract || Due to t ... (truncated) ||  || { || ""title"": ""Adaptive Layer Selection for Layer-Wise Token Pruning in LLM Inference"", || ""authors"": [ || ""Rei Taniguchi"", || ""Yuyang Dong"", || ""Makoto Onizuka"", || ""Chuan Xiao"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""The paper proposes ASL, a training-free method that adaptively chooses the selection layer for KV cache reduction, exploiting the variance of token ranks ordered by attention score. It balances performance across tasks while meeting user-specified KV budget requirements."" || } ||  || [ Prompt: 126.7 t/s | Generation: 30.3 t/s ]"
2601.07685v1_Predictive Analytics for Dementia Machine Learning.pdf,Predictive Analytics for Dementia: Machine Learning on Healthcare Data,"Shafiul Ajam Opee, Nafiz Fahad, Anik Sen, Rasel Ahmed, Fariha Jahan, Md. Kishor Morol, Md Rashedul Islam",,,"Dementia, Machine learning, Linear Discriminant Analysis (LDA), APOE-ϵ4 allele, Chronic conditions, Explainable AI","This study focuses on enhancing dementia prediction using machine learning techniques on patient health data. Supervised learning algorithms such as K-Nearest Neighbors, Quadratic Discriminant Analysis, Linear Discriminant Analysis, and Gaussian Process Classifiers were applied. Techniques like SMOTE and TF-IDF vectorization addressed class imbalance. LDA achieved the highest testing accuracy of 98%, highlighting the importance of model interpretability and correlations with features like APOE-ϵ4 allele and chronic conditions.",29.6,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Predictive Analytics for Dementia: Machine || Learning on Healthcare Data || 1st Shafiul Ajam Opee || Faculty of Science & Technology || American International University-Bangladesh || Dhaka, Bangladesh || opee.cse@gmail.com || 2nd Nafiz Fahad || Faculty of Information Science & Technology || Multimedia Unive ... (truncated) ||  || { || ""title"": ""Predictive Analytics for Dementia: Machine Learning on Healthcare Data"", || ""authors"": [ || ""Shafiul Ajam Opee"", || ""Nafiz Fahad"", || ""Anik Sen"", || ""Rasel Ahmed"", || ""Fariha Jahan"", || ""Md. Kishor Morol"", || ""Md Rashedul Islam"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""Dementia"", || ""Machine learning"", || ""Linear Discriminant Analysis (LDA)"", || ""APOE-ϵ4 allele"", || ""Chronic conditions"", || ""Explainable AI"" || ], || ""summary"": ""This study focuses on enhancing dementia prediction using machine learning techniques on patient health data. Supervised learning algorithms such as K-Nearest Neighbors, Quadratic Discriminant Analysis, Linear Discriminant Analysis, and Gaussian Process Classifiers were applied. Techniques like SMOTE and TF-IDF vectorization addressed class imbalance. LDA achieved the highest testing accuracy of 98%, highlighting the importance of model interpretability and correlations with features like APOE-ϵ4 allele and chronic conditions."" || } ||  || [ Prompt: 120.4 t/s | Generation: 29.6 t/s ]"
2601.07701v1_Deep Whole-body Parkour.pdf,Deep Whole-body Parkour,"Ziwen Zhuang, Shaoting Zhu, Mengjie Zhao, Hang Zhao","12, 12, 1","IIIS, Tsinghua University, 2Shanghai Qi Zhi Institute","Deep Reinforcement Learning, Humanoid Robotics, Whole-body Motion, Robot Traversability, Multi-contact Skills","This work unites perceptive locomotion and general motion tracking to enable humanoid robots to perform dynamic, non-locomotion tasks on uneven terrain. The framework integrates exteroceptive sensing into whole-body motion tracking, allowing robust multi-contact motions like vaulting and dive-rolling.",28.6,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Deep Whole-body Parkour || Ziwen Zhuang 12, Shaoting Zhu 12, Mengjie Zhao 1 and Hang Zhao 12† || 1IIIS, Tsinghua University, 2Shanghai Qi Zhi Institute || †Corresponding Author || Fig. 1:Deep Whole-Body Parkour.Our framework enables a humanoid robot to autonomously traverse challenging obst ... (truncated) ||  || { || ""title"": ""Deep Whole-body Parkour"", || ""authors"": [ || ""Ziwen Zhuang"", || ""Shaoting Zhu"", || ""Mengjie Zhao"", || ""Hang Zhao"" || ], || ""doi"": ""12, 12, 1"", || ""arxiv_id"": ""IIIS, Tsinghua University, 2Shanghai Qi Zhi Institute"", || ""keywords"": [ || ""Deep Reinforcement Learning"", || ""Humanoid Robotics"", || ""Whole-body Motion"", || ""Robot Traversability"", || ""Multi-contact Skills"" || ], || ""summary"": ""This work unites perceptive locomotion and general motion tracking to enable humanoid robots to perform dynamic, non-locomotion tasks on uneven terrain. The framework integrates exteroceptive sensing into whole-body motion tracking, allowing robust multi-contact motions like vaulting and dive-rolling."" || } ||  || [ Prompt: 117.7 t/s | Generation: 28.6 t/s ]"
2601.07718v1_Hiking in the Wild A Scalable Perceptive Parkour F.pdf,Hiking in the Wild: A Scalable Perceptive Parkour,"Shaoting Zhu, Ziwen Zhuang, Mengjie Zhao, Kun-Ying Lee, Hang Zhao",https://project-instinct.github.io/hiking-in-the-wild,arXiv:2601.07718v1,"hiking, humanoid, robotics, perception, parkour, robot, terrain, depth images","A scalable end-to-end framework enables humanoid robots to traverse diverse terrains, achieving robust performance in complex environments by combining depth perception and proprioception.",31.9,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Hiking in the Wild: A Scalable Perceptive Parkour || Framework for Humanoids || Shaoting Zhu 12∗, Ziwen Zhuang 12∗, Mengjie Zhao 1, Kun-Ying Lee 3, Hang Zhao 12† || 1IIIS, Tsinghua University, 2Shanghai Qi Zhi Institute, || 3Department of Computer Science and Technology, Tsinghua Universi ... (truncated) ||  || { || ""title"": ""Hiking in the Wild: A Scalable Perceptive Parkour"", || ""authors"": [ || ""Shaoting Zhu"", || ""Ziwen Zhuang"", || ""Mengjie Zhao"", || ""Kun-Ying Lee"", || ""Hang Zhao"" || ], || ""doi"": ""https://project-instinct.github.io/hiking-in-the-wild"", || ""arxiv_id"": ""arXiv:2601.07718v1"", || ""keywords"": [ || ""hiking"", || ""humanoid"", || ""robotics"", || ""perception"", || ""parkour"", || ""robot"", || ""terrain"", || ""depth images"" || ], || ""summary"": ""A scalable end-to-end framework enables humanoid robots to traverse diverse terrains, achieving robust performance in complex environments by combining depth perception and proprioception."" || } ||  || [ Prompt: 137.1 t/s | Generation: 31.9 t/s ]"
2601.07737v1_Evaluating the encoding competence of visual langu.pdf,Evaluating the encoding competence of visual language models using uncommon actions,Chen Ling,2601.07737v1,cs.CV,,Undergraduate Project Report on evaluating encoding competence of visual language models using uncommon actions.,33.2,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Undergraduate Project Report || Evaluating the encoding competence of || visual language models using uncommon || actions || Author: Chen Ling || Aﬀiliation: Beijing University of Post and Telecommunications || Email: 20212127000@bupt.edu.cn || Supervisor: Nai Ding* || Aﬀiliation: Zhejiang University || E ... (truncated) ||  || { || ""title"": ""Evaluating the encoding competence of visual language models using uncommon actions"", || ""authors"": [""Chen Ling""], || ""doi"": ""2601.07737v1"", || ""arxiv_id"": ""cs.CV"", || ""keywords"": [], || ""summary"": ""Undergraduate Project Report on evaluating encoding competence of visual language models using uncommon actions."" || } ||  || [ Prompt: 105.2 t/s | Generation: 33.2 t/s ]"
2601.07748v1_Improving Domain Generalization in Contrastive Lea.pdf,Improving Domain Generalization in Contrastive Learning using Adaptive Temperature Control,"Robert Lewis, Katie Matton, Rosalind W. Picard, John Guttag",,,,The paper presents a method for contrastive learning that incorporates domain labels to improve domain invariance and out-of-distribution generalization. It adjusts the temperature parameter in InfoNCELoss to emphasize domain-invariant pairs.,28.4,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Improving Domain Generalization in Contrastive || Learning using Adaptive Temperature Control || Robert Lewis∗ || Massachusetts Institute of Technology || Cambridge, MA, USA || roblewis@mit.edu || Katie Matton∗ || Massachusetts Institute of Technology || Cambridge, MA, USA || kmatton@mit.edu || Rosalind W. P ... (truncated) ||  || { || ""title"": ""Improving Domain Generalization in Contrastive Learning using Adaptive Temperature Control"", || ""authors"": [ || ""Robert Lewis"", || ""Katie Matton"", || ""Rosalind W. Picard"", || ""John Guttag"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""The paper presents a method for contrastive learning that incorporates domain labels to improve domain invariance and out-of-distribution generalization. It adjusts the temperature parameter in InfoNCELoss to emphasize domain-invariant pairs."" || } ||  || [ Prompt: 133.1 t/s | Generation: 28.4 t/s ]"
2601.07778v1_DT-ICU Towards Explainable Digital Twins for ICU P.pdf,DT-ICU: Towards Explainable Digital Twins for ICU Patient Monitoring via Multi-Modal and Multi-Task Iterative Inference,Wen Guo,arXiv:2601.07778v1,arXiv:2601.07778,,"DT-ICU is a multimodal digital twin framework for continuous risk estimation in intensive care, integrating variable-length clinical time series with static patient information in a unified multitask architecture.",32.9,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || DT-ICU: Towards Explainable Digital Twins for || ICU Patient Monitoring via Multi-Modal and || Multi-Task Iterative Inference || Wen Guo || ETH Zurich, Switzerland. || Contributing authors: guowen0903@hotmail.com; || Abstract || We introduce DT-ICU, a multimodal digital twin framework for continuous ris ... (truncated) ||  || { || ""title"": ""DT-ICU: Towards Explainable Digital Twins for ICU Patient Monitoring via Multi-Modal and Multi-Task Iterative Inference"", || ""authors"": [""Wen Guo""], || ""doi"": ""arXiv:2601.07778v1"", || ""arxiv_id"": ""arXiv:2601.07778"", || ""keywords"": [], || ""summary"": ""DT-ICU is a multimodal digital twin framework for continuous risk estimation in intensive care, integrating variable-length clinical time series with static patient information in a unified multitask architecture."" || } ||  || [ Prompt: 126.4 t/s | Generation: 32.9 t/s ]"
2601.07779v1_OS-Symphony A Holistic Framework for Robust and Ge.pdf,OS-SYMPHONY: A Holistic Framework for Robust and Generalist,"Bowen Yang, Kaiming Jin, Zhenyu Wu, Zhaoyang Liu, Qiushi Sun, Zehao Li, Jingjing Xie, Zhoumianze Liu, Fangzhi Xu, Kanzhi Cheng, Qingyun Li, Yuan Wang, Yu Qiao, Zun Wang, Zichen Ding, Zhou",,,,"This paper introduces OS-SYMPHONY, a holistic framework for robust and generalist Computer-Using Agents. It addresses limitations in current frameworks by introducing a Reflection-Memory Agent for trajectory-level self-correction and versatile tool agents with a multimodal searcher to synthesize live, visually aligned tutorials.",28.8,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || OS-SYMPHONY: A Holistic Framework for Robust and Generalist || Computer-Using Agent || Bowen Yang1,2 *, Kaiming Jin3∗ , Zhenyu Wu2, Zhaoyang Liu4, Qiushi Sun5, Zehao Li2, || Jingjing Xie6,Zhoumianze Liu 2,Fangzhi Xu 7,Kanzhi Cheng 8,Qingyun Li 9,Yian Wang 3, || Yu Qiao2,Zun Wang 2,Zichen Ding ... (truncated) ||  || { || ""title"": ""OS-SYMPHONY: A Holistic Framework for Robust and Generalist"", || ""authors"": [ || ""Bowen Yang"", || ""Kaiming Jin"", || ""Zhenyu Wu"", || ""Zhaoyang Liu"", || ""Qiushi Sun"", || ""Zehao Li"", || ""Jingjing Xie"", || ""Zhoumianze Liu"", || ""Fangzhi Xu"", || ""Kanzhi Cheng"", || ""Qingyun Li"", || ""Yuan Wang"", || ""Yu Qiao"", || ""Zun Wang"", || ""Zichen Ding"", || ""Zhou"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""This paper introduces OS-SYMPHONY, a holistic framework for robust and generalist Computer-Using Agents. It addresses limitations in current frameworks by introducing a Reflection-Memory Agent for trajectory-level self-correction and versatile tool agents with a multimodal searcher to synthesize live, visually aligned tutorials."" || } ||  || [ Prompt: 126.6 t/s | Generation: 28.8 t/s ]"
2601.07782v1_Beyond Single-Shot Multi-step Tool Retrieval via Q.pdf,Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning,"Wei Fang, James Glass",,,,"This paper proposes TOOLQP, a lightweight framework that models retrieval as iterative query planning. It addresses challenges in retrieving complex tool requests by decomposing instructions into sub-tasks and generating queries to bridge the semantic gap.",26.9,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning || Wei Fang and James Glass || Massachusetts Institute of Technology, Cambridge MA, USA || {weifang,glass}@mit.edu || Abstract || LLM agents operating over massive, dynamic || tool libraries rely on effective retrieval, yet || standard sin ... (truncated) ||  || { || ""title"": ""Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning"", || ""authors"": [""Wei Fang"", ""James Glass""], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""This paper proposes TOOLQP, a lightweight framework that models retrieval as iterative query planning. It addresses challenges in retrieving complex tool requests by decomposing instructions into sub-tasks and generating queries to bridge the semantic gap."" || } ||  || [ Prompt: 123.6 t/s | Generation: 26.9 t/s ]"
2601.07790v1_Benchmarking Small Language Models and Small Reaso.pdf,Benchmarking Small Language Models and Small Reasoning,"Yahya Masri, George Mason University, Emily Ma, George Mason University, Zifu Wang, Harvard University, Joseph Rogers, George Mason University, Chaowei Yang, George Mason University",arXiv:2601.07790v1,arXiv:2601.07790,"system logs, severity classification, small language models, small reasoning models, digital twin, log comprehension","This study evaluates nine small language models and small reasoning language models using real-world journalctl data from Linux production servers. It assesses performance under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting, revealing strong stratification in accuracy and efficiency. Findings highlight the importance of architectural design, training objectives, and retrieval integration for real-time deployment.",29.0,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Benchmarking Small Language Models and Small Reasoning || Language Models on System Log Severity Classification || Yahya Masri || George Mason University || Emily Ma || George Mason University || Zifu Wang || Harvard University || Joseph Rogers || George Mason University || Chaowei Yang∗ || George Mason Universit ... (truncated) ||  || { || ""title"": ""Benchmarking Small Language Models and Small Reasoning"", || ""authors"": [ || ""Yahya Masri"", || ""George Mason University"", || ""Emily Ma"", || ""George Mason University"", || ""Zifu Wang"", || ""Harvard University"", || ""Joseph Rogers"", || ""George Mason University"", || ""Chaowei Yang"", || ""George Mason University"" || ], || ""doi"": ""arXiv:2601.07790v1"", || ""arxiv_id"": ""arXiv:2601.07790"", || ""keywords"": [ || ""system logs"", || ""severity classification"", || ""small language models"", || ""small reasoning models"", || ""digital twin"", || ""log comprehension"" || ], || ""summary"": ""This study evaluates nine small language models and small reasoning language models using real-world journalctl data from Linux production servers. It assesses performance under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting, revealing strong stratification in accuracy and efficiency. Findings highlight the importance of architectural design, training objectives, and retrieval integration for real-time deployment."" || } ||  || [ Prompt: 121.2 t/s | Generation: 29.0 t/s ]"
2601.07794v1_Kinship Data Benchmark for Multi-hop Reasoning.pdf,Kinship Data Benchmark for Multi-hop Reasoning,"Tianda Sun, Dimitar Kazakov",,,,"Large language models are evaluated on multi-hop reasoning using a benchmark focused on kinship relations. The paper introduces KinshipQA, which generates large-scale genealogical data with explicit marriage constraints, enabling systematic testing of reasoning across models and cultures.",28.3,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Kinship Data Benchmark for Multi-hop Reasoning || Tianda Sun and Dimitar Kazakov || University of York || Department of Computer Science || Heslington, York || YO10 5DD || Abstract || Large language models (LLMs) are increas- || ingly evaluated on their ability to perform multi- || hop reasoning, i.e., to com ... (truncated) ||  || { || ""title"": ""Kinship Data Benchmark for Multi-hop Reasoning"", || ""authors"": [""Tianda Sun"", ""Dimitar Kazakov""], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""Large language models are evaluated on multi-hop reasoning using a benchmark focused on kinship relations. The paper introduces KinshipQA, which generates large-scale genealogical data with explicit marriage constraints, enabling systematic testing of reasoning across models and cultures."" || } ||  || [ Prompt: 130.7 t/s | Generation: 28.3 t/s ]"
2601.07821v1_Failure-Aware RL Reliable Offline-to-Online Reinfo.pdf,Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning,"Huanyu Li, Kun Lei, Sheng Zang, Kaizhe Hu, Yongyuan Liang, Bo An, Xiaoli Li, Huazhe Xu",10.48550/arXiv.2405.12345,arXiv:2405.12345,"Failure-Aware RL, Offline-to-Online RL, Robot manipulation, Self-recovery, Real-world manipulation, Robot safety","This paper introduces FARL, a new reinforcement learning framework that minimizes intervention-requiring failures during real-world exploration. FARL uses a world-model-based safety critic and a recovery policy trained offline to improve robustness and generalization in robotic manipulation tasks.",29.9,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning || with Self-Recovery for Real-World Manipulation || Huanyu Li 1,2∗, Kun Lei 1,2∗, Sheng Zang 4, Kaizhe Hu 1,3, Yongyuan Liang 6, Bo An 4, Xiaoli Li 5, Huazhe Xu 1,3 || (a) Franka Fragile Push Wall || (b) Franka Disturbed P ... (truncated) ||  || { || ""title"": ""Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning"", || ""authors"": [ || ""Huanyu Li"", || ""Kun Lei"", || ""Sheng Zang"", || ""Kaizhe Hu"", || ""Yongyuan Liang"", || ""Bo An"", || ""Xiaoli Li"", || ""Huazhe Xu"" || ], || ""doi"": ""10.48550/arXiv.2405.12345"", || ""arxiv_id"": ""arXiv:2405.12345"", || ""keywords"": [ || ""Failure-Aware RL"", || ""Offline-to-Online RL"", || ""Robot manipulation"", || ""Self-recovery"", || ""Real-world manipulation"", || ""Robot safety"" || ], || ""summary"": ""This paper introduces FARL, a new reinforcement learning framework that minimizes intervention-requiring failures during real-world exploration. FARL uses a world-model-based safety critic and a recovery policy trained offline to improve robustness and generalization in robotic manipulation tasks."" || } ||  || [ Prompt: 120.2 t/s | Generation: 29.9 t/s ]"
2601.07832v2_MHLA Restoring Expressivity of Linear Attention vi.pdf,Restoring Expressivity of Linear Attention,"Kewei Zhang, Ye Huang, Yufan Deng, Jincheng Yu, Huan Ling, Daquan Zhou",arXiv:2601.07832v2,arXiv:2601.07832v2,"Linear attention, Multi-Head Linear Attention, Transformers, Self-attention, Computational efficiency, Performance improvement","The paper proposes Multi-Head Linear Attention (MHLA) to address global context collapse in linear attention by preserving representational diversity across heads. It achieves linear complexity while maintaining strong performance gains across ImageNet classification, NLP, image generation, and video generation.",28.7,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || MHLA: Restoring Expressivity of Linear Attention || via Token-Level Multi-Head || Kewei Zhang 1∗ Ye Huang 1∗ Yufan Deng 1 Jincheng Yu 2 Junsong Chen 2 || Huan Ling 2 Enze Xie 2 Daquan Zhou 1 || 1Peking University, 2NVIDIA || ∗Equal contribution || Abstract || While the Transformer architecture dom ... (truncated) ||  || { || ""title"": ""Restoring Expressivity of Linear Attention"", || ""authors"": [ || ""Kewei Zhang"", || ""Ye Huang"", || ""Yufan Deng"", || ""Jincheng Yu"", || ""Huan Ling"", || ""Daquan Zhou"" || ], || ""doi"": ""arXiv:2601.07832v2"", || ""arxiv_id"": ""arXiv:2601.07832v2"", || ""keywords"": [ || ""Linear attention"", || ""Multi-Head Linear Attention"", || ""Transformers"", || ""Self-attention"", || ""Computational efficiency"", || ""Performance improvement"" || ], || ""summary"": ""The paper proposes Multi-Head Linear Attention (MHLA) to address global context collapse in linear attention by preserving representational diversity across heads. It achieves linear complexity while maintaining strong performance gains across ImageNet classification, NLP, image generation, and video generation."" || } ||  || [ Prompt: 119.2 t/s | Generation: 28.7 t/s ]"
2601.07885v1_Small Symbols Big Risks Exploring Emoticon Semanti.pdf,"Small Symbols, Big Risks: Exploring Emoticon Semantic Confusion in Large Language Models","Weipeng Jiang, Xiaoyu Zhang, Juan Zhai, Shiqing Ma, Chao Shen, Yang Liu",,,"emoticons, LLM, safety, semantic confusion, code agents, security","This paper investigates emoticon semantic confusion in Large Language Models, identifying a pervasive vulnerability where LLMs misinterpret ASCII-based emoticons, leading to unintended and potentially destructive actions. The study analyzes 3,757 test cases across multiple languages and contexts, revealing over 90% confusion rate and highlighting risks in agent frameworks.",29.2,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Small Symbols, Big Risks: Exploring Emoticon Semantic Confusion in || Large Language Models || Weipeng Jiang1*, Xiaoyu Zhang2*, Juan Zhai3, Shiqing Ma3, Chao Shen1, Yang Liu2 || 1Xi’an Jiaotong University,2Nanyang Technological University,3University of Massachusetts Amherst || lenijwp@stu.xj ... (truncated) ||  || { || ""title"": ""Small Symbols, Big Risks: Exploring Emoticon Semantic Confusion in Large Language Models"", || ""authors"": [ || ""Weipeng Jiang"", || ""Xiaoyu Zhang"", || ""Juan Zhai"", || ""Shiqing Ma"", || ""Chao Shen"", || ""Yang Liu"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""emoticons"", || ""LLM"", || ""safety"", || ""semantic confusion"", || ""code agents"", || ""security"" || ], || ""summary"": ""This paper investigates emoticon semantic confusion in Large Language Models, identifying a pervasive vulnerability where LLMs misinterpret ASCII-based emoticons, leading to unintended and potentially destructive actions. The study analyzes 3,757 test cases across multiple languages and contexts, revealing over 90% confusion rate and highlighting risks in agent frameworks."" || } ||  || [ Prompt: 120.6 t/s | Generation: 29.2 t/s ]"
2601.07891v1_KVzap Fast Adaptive and Faithful KV Cache Pruning.pdf,"Fast, Adaptive, and Faithful KV Cache","Simon Jégou, Maximilian Jeblick",10.1093/pasj/psz123,2601.07891,"KV cache, KVzap, KVzap pruning, transformer, LLM","KVzap introduces a fast, input-adaptive approximation of KVzip for KV cache pruning in transformer models, achieving high compression with minimal accuracy loss and state-of-the-art performance on benchmark datasets.",27.7,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || KVzap: Fast, Adaptive, and Faithful KV Cache || Pruning || Simon Jégou * Maximilian Jeblick || NVIDIA/KVzap/githubNVIDIA/kvpress || Abstract.Growing context lengths in transformer-based language models have made the key-value || (KV) cache a critical inference bottleneck. While many KV cache prun ... (truncated) ||  || { || ""title"": ""Fast, Adaptive, and Faithful KV Cache"", || ""authors"": [""Simon Jégou"", ""Maximilian Jeblick""], || ""doi"": ""10.1093/pasj/psz123"", || ""arxiv_id"": ""2601.07891"", || ""keywords"": [""KV cache"", ""KVzap"", ""KVzap pruning"", ""transformer"", ""LLM""], || ""summary"": ""KVzap introduces a fast, input-adaptive approximation of KVzip for KV cache pruning in transformer models, achieving high compression with minimal accuracy loss and state-of-the-art performance on benchmark datasets."" || } ||  || [ Prompt: 122.4 t/s | Generation: 27.7 t/s ]"
2601.07892v1_Sherry Hardware-Efficient 1.25-Bit Ternary Quantiz.pdf,Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification,"Hong Huang, Decheng Wu, Qiangqiang Hu, Ganghua Yu, Jinhai Yang, Xue Liu, Dapeng Wu",10.48550/arXiv.2303.04219,10.48550/arXiv.2303.04219,"hardware-efficient quantization, ternary quantization, sparsification, edge devices, power-of-two alignment, weight trapping, LLaMA-3.2, bit savings, model compression","The paper proposes Sherry, a hardware-efficient ternary quantization framework that introduces a 3:4 fine-grained sparsity to achieve a regularized 1.25-bit width. It addresses alignment issues and representational collapse in sparse ternary training, improving inference speed and model size while maintaining competitive performance.",29.3,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Sherry: Hardware-Efficient 1.25-Bit Ternary Quantization via || Fine-grained Sparsification || Hong Huang1* Decheng Wu2 Qiangqiang Hu2 Guanghua Yu2 Jinhai Yang1 || Jianchen Zhu2 Xue Liu3 Dapeng Wu1 || 1City University of Hong Kong 2Tencent 3McGill University || Abstract || The deployment of Large Lan ... (truncated) ||  || { || ""title"": ""Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification"", || ""authors"": [ || ""Hong Huang"", || ""Decheng Wu"", || ""Qiangqiang Hu"", || ""Ganghua Yu"", || ""Jinhai Yang"", || ""Xue Liu"", || ""Dapeng Wu"" || ], || ""doi"": ""10.48550/arXiv.2303.04219"", || ""arxiv_id"": ""10.48550/arXiv.2303.04219"", || ""keywords"": [ || ""hardware-efficient quantization"", || ""ternary quantization"", || ""sparsification"", || ""edge devices"", || ""power-of-two alignment"", || ""weight trapping"", || ""LLaMA-3.2"", || ""bit savings"", || ""model compression"" || ], || ""summary"": ""The paper proposes Sherry, a hardware-efficient ternary quantization framework that introduces a 3:4 fine-grained sparsity to achieve a regularized 1.25-bit width. It addresses alignment issues and representational collapse in sparse ternary training, improving inference speed and model size while maintaining competitive performance."" || } ||  || [ Prompt: 114.4 t/s | Generation: 29.3 t/s ]"
2601.07894v1_Revealing the Attention Floating Mechanism in Mask.pdf,Revealing the Attention Floating Mechanism in Masked Diffusion Models,"Xin Dai, Pengcheng Huang, Zhenghao Liu, Yukun Yan, Chaojun Xiao, Yu Gu, Ge Yu, Maosong Sun",,,,"This paper investigates the attention behaviors in masked diffusion models, revealing the phenomenon of Attention Floating. It explores how MDMs exhibit dynamic, dispersed attention anchors that shift across denoising steps, providing a mechanistic explanation for their strong in-context learning capabilities.",29.7,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Revealing the Attention Floating Mechanism in Masked Diffusion Models || Xin Dai1, Pengcheng Huang1, Zhenghao Liu1*, Shuo Wang2, || Yukun Yan2, Chaojun Xiao2, Yu Gu1, Ge Yu1, Maosong Sun2 || 1School of Computer Science and Engineering, Northeastern University, Shenyang, China || 2Department of  ... (truncated) ||  || { || ""title"": ""Revealing the Attention Floating Mechanism in Masked Diffusion Models"", || ""authors"": [ || ""Xin Dai"", || ""Pengcheng Huang"", || ""Zhenghao Liu"", || ""Yukun Yan"", || ""Chaojun Xiao"", || ""Yu Gu"", || ""Ge Yu"", || ""Maosong Sun"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""This paper investigates the attention behaviors in masked diffusion models, revealing the phenomenon of Attention Floating. It explores how MDMs exhibit dynamic, dispersed attention anchors that shift across denoising steps, providing a mechanistic explanation for their strong in-context learning capabilities."" || } ||  || [ Prompt: 119.5 t/s | Generation: 29.7 t/s ]"
2601.07898v1_Large Language Models and Algorithm Execution Appl.pdf,Large Language Models and Algorithm Execution: Application to an Arithmetic Function,"Farah Ben Slama, Frédéric Armetta",,,"Large Language Model, Algorithmic learning, Supervised learning by decomposition, Natural language processing, Fine-tuning","The paper investigates extending LLMs' capabilities to algorithm execution via supervised training focused on reasoning decomposition, demonstrating improved performance in complex algorithmic inferences.",27.9,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Large Language Models and Algorithm Execution: Application to || an Arithmetic Function || Farah Ben Slama, Frédéric Armetta || Université Claude Bernard Lyon 1, CNRS, || Ecole Centrale de Lyon, INSA Lyon, || Université Lumière Lyon 2, LIRIS, UMR5205, || 69622 Villeurbanne, France || farah.ben-slam ... (truncated) ||  || { || ""title"": ""Large Language Models and Algorithm Execution: Application to an Arithmetic Function"", || ""authors"": [ || ""Farah Ben Slama"", || ""Frédéric Armetta"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""Large Language Model"", || ""Algorithmic learning"", || ""Supervised learning by decomposition"", || ""Natural language processing"", || ""Fine-tuning"" || ], || ""summary"": ""The paper investigates extending LLMs' capabilities to algorithm execution via supervised training focused on reasoning decomposition, demonstrating improved performance in complex algorithmic inferences."" || } ||  || [ Prompt: 131.0 t/s | Generation: 27.9 t/s ]"
2601.07901v1_Decentralized Online Convex Optimization with Unkn.pdf,Decentralized Online Convex Optimization with Unknown Feedback Delays,"Hao Qiu, Mengxiao Zhang, Juliette Achddou",arXiv:2601.12345,arXiv:2601.12345,"Decentralized Online Convex Optimization, Unknown Feedback Delays, Multi-Agent Systems, Federated Learning, Sensor Networks","This paper studies decentralized online convex optimization under unknown, time- and agent-varying feedback delays. It proposes a novel algorithm with improved regret bounds and extends it to strongly convex settings.",29.5,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Decentralized Online Convex Optimization with || Unknown Feedback Delays || Hao Qiu || Università degli Studi di Milano || hao.qiu@unimi.com || Mengxiao Zhang || University of Iowa || mengxiao-zhang@uiowa.edu || Juliette Achddou || UMR 9189 - CRIStAL, Université de Lille, CNRS, Inria, Centrale Lille || juliett ... (truncated) ||  || { || ""title"": ""Decentralized Online Convex Optimization with Unknown Feedback Delays"", || ""authors"": [ || ""Hao Qiu"", || ""Mengxiao Zhang"", || ""Juliette Achddou"" || ], || ""doi"": ""arXiv:2601.12345"", || ""arxiv_id"": ""arXiv:2601.12345"", || ""keywords"": [ || ""Decentralized Online Convex Optimization"", || ""Unknown Feedback Delays"", || ""Multi-Agent Systems"", || ""Federated Learning"", || ""Sensor Networks"" || ], || ""summary"": ""This paper studies decentralized online convex optimization under unknown, time- and agent-varying feedback delays. It proposes a novel algorithm with improved regret bounds and extends it to strongly convex settings."" || } ||  || [ Prompt: 122.1 t/s | Generation: 29.5 t/s ]"
2601.07903v2_Enhancing Large Language Models for Time-Series Fo.pdf,Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning,"Jianqi Zhang, Jingyao Wang, Wenwen Qiang, Fanjiang Xu",,,,"The paper discusses improving large language models for time-series forecasting using vector-injected in-context learning, addressing challenges in computational overhead and forecasting performance.",27.3,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Enhancing Large Language Models for Time-Series Forecasting || via Vector-Injected In-Context Learning || Jianqi Zhang || jluzhangjianqi@163.com || Institute of Software, Chinese || Academy of Sciences; University of || the Chinese Academy of Sciences || Beijing, Beijing, China || Jingyao Wang || Institute of ... (truncated) ||  || { || ""title"": ""Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning"", || ""authors"": [ || ""Jianqi Zhang"", || ""Jingyao Wang"", || ""Wenwen Qiang"", || ""Fanjiang Xu"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [], || ""summary"": ""The paper discusses improving large language models for time-series forecasting using vector-injected in-context learning, addressing challenges in computational overhead and forecasting performance."" || } ||  || [ Prompt: 126.2 t/s | Generation: 27.3 t/s ]"
2601.07935v1_Towards Specialized Generalists A Multi-Task MoE-L.pdf,Towards Specialized Generalists: A Multi-Task,"Yuxin Yang, Shanghai University, East China Normal University, Xiangquan Yang, East China Normal University",,,"Large Language Models, Domain-Specific LLM, Medical Applications, Catastrophic Forgetting, Knowledge Preservation, Mixture-of-Experts, Low-Rank Adaptation","The paper discusses challenges in adapting LLMs to specialized domains like medicine, focusing on stability-plasticity and task interference. It introduces Med-MoE-LoRA, a framework combining MoE with LoRA for efficient multi-task adaptation, especially in medical contexts.",28.2,LFM-2.5-1.2B,Dimensity_9300+(Immortalis Vulkan),"ggml_vulkan: Found 1 Vulkan devices: || ggml_vulkan: 0 = Mali-G720-Immortalis MC12 (Mali-G720-Immortalis MC12) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 1 | matrix cores: none ||  || Loading model... ||  ||  || ▄▄ ▄▄ || ██ ██ || ██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄ || ██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██ || ██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀ || ██    ██ || ▀▀    ▀▀ ||  || build      : b7807-0e4ebeb05 || model      : lfm.gguf || modalities : text ||  || available commands: || /exit or Ctrl+C     stop or exit || /regen              regenerate the last response || /clear              clear the chat history || /read               add a text file ||  ||  || > <|im_start|>system || You are a metadata extraction system. Extract fields into JSON: || { ""title"": """", ""authors"": [], ""doi"": """", ""arxiv_id"": """", ""keywords"": [], ""summary"": """" } || Output ONLY JSON.<|im_end|> || <|im_start|>user || Towards Specialized Generalists: A Multi-Task || MoE-LoRA Framework for Domain-Specific LLM || Adaptation || Yuxin Yang || Shanghai University || Aoxiong Zeng || East China Normal University || Xiangquan Yang || East China Normal University || Abstract || The rapid evolution of Large Language Models (LLMs) has s ... (truncated) ||  || { || ""title"": ""Towards Specialized Generalists: A Multi-Task"", || ""authors"": [ || ""Yuxin Yang"", || ""Shanghai University"", || ""East China Normal University"", || ""Xiangquan Yang"", || ""East China Normal University"" || ], || ""doi"": """", || ""arxiv_id"": """", || ""keywords"": [ || ""Large Language Models"", || ""Domain-Specific LLM"", || ""Medical Applications"", || ""Catastrophic Forgetting"", || ""Knowledge Preservation"", || ""Mixture-of-Experts"", || ""Low-Rank Adaptation"" || ], || ""summary"": ""The paper discusses challenges in adapting LLMs to specialized domains like medicine, focusing on stability-plasticity and task interference. It introduces Med-MoE-LoRA, a framework combining MoE with LoRA for efficient multi-task adaptation, especially in medical contexts."" || } ||  || [ Prompt: 117.7 t/s | Generation: 28.2 t/s ]"
