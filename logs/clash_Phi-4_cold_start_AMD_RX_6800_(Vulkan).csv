filename,title,authors,doi,arxiv_id,keywords,summary,tps,total_time,tokens,mode,model,platform
2601.07192v1_Relink Constructing Query-Driven Evidence Graph On.pdf,Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG,"Manzong Huang, Chenyang Bu, Yi He, Xingrui Zhuo, Xindong Wu",,,"GraphRAG, Knowledge Graph, Large Language Models, Retrieval-Augmented Generation, Open-Domain Question Answering, Evidence Graph, Latent Relations, Hallucinations","Graph-based Retrieval-Augmented Generation (GraphRAG) mitigates hallucinations in Large Language Models (LLMs) by grounding them in structured knowledge. However, current GraphRAG methods are limited by a static, pre-constructed Knowledge Graph (KG), which faces challenges such as incomplete reasoning paths and the presence of distractor facts. To address these issues, the paper proposes Relink, a framework that dynamically constructs a query-specific evidence graph. Relink instantiates required facts from a latent relation pool derived from the original text corpus to repair broken paths and employs a query-aware evaluation strategy to select the most useful candidates for answering queries. This approach allows Relink to discard distractor facts and construct precise evidence paths, achieving significant improvements in performance on Open-Domain Question Answering benchmarks.",16.59,18.207,302,cold_start,Phi-4,AMD RX 6800 (Vulkan)
2601.07197v1_Beyond Variance Knowledge-Aware LLM Compression vi.pdf,Beyond Variance: Knowledge-Aware LLM Compression via Fisher-Aligned Subspace Diagnostics,"Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma",,,"Large Language Models, activation compression, Singular Value Decomposition, Fisher Information Matrix, knowledge-intensive benchmarks, MMLU, LAMA, Mistral-7B, Llama-3-8B","This paper introduces Fisher-Aligned Subspace Compression (FASC), a novel framework for compressing Large Language Models (LLMs) that prioritizes factual knowledge preservation. Unlike traditional methods such as Singular Value Decomposition (SVD), which focus on high-variance dimensions, FASC uses the Fisher Information Matrix to identify and preserve dimensions critical for factual knowledge, often found in low-variance but high-gradient-sensitivity subspaces. The paper proposes the Dependence Violation Score (ρ) as a diagnostic metric to quantify activation-gradient coupling, revealing where factual knowledge is stored within transformer architectures. Experiments on models like Mistral-7B and Llama-3-8B show that FASC maintains 6-8% more accuracy on knowledge-intensive benchmarks compared to variance-based methods at a 50% rank reduction. This enables a 7B model to match the factual recall of a 13B uncompressed model, highlighting ρ as a key indicator of stored knowledge.",17.2,19.822,341,cold_start,Phi-4,AMD RX 6800 (Vulkan)
2601.07199v1_Forward versus Backward Comparing Reasoning Object.pdf,Forward versus Backward: Comparing Reasoning Objectives in Direct Preference Optimization,"Murtaza Nikazad, Raghuram Ramanujan",,,"Direct Preference Optimization, reasoning, hallucination, GSM8K, Low-Rank Adaptation","This paper explores the impact of different training objectives on the reasoning reliability of large language models using Direct Preference Optimization. It compares forward chain-of-thought generation, which trains models to produce correct reasoning traces, with backward verification, which trains models to verify and acknowledge errors in candidate solutions. Experiments on the GSM8K dataset reveal a trade-off: forward-only training significantly improves accuracy, while backward-only training reduces false positives but offers minimal accuracy gains. Both methods decrease the model's error acknowledgment rate, indicating increased confidence in outputs. The study concludes that forward and backward reasoning objectives provide distinct and complementary learning signals, enhancing problem-solving and verification calibration, respectively. The research is facilitated by an efficient training and evaluation pipeline using Low-Rank Adaptation.",17.12,14.957,256,cold_start,Phi-4,AMD RX 6800 (Vulkan)
2601.07200v1_Safeguarding LLM Fine-tuning via Push-Pull Distrib.pdf,Safeguarding LLM Fine-tuning via Push-Pull Distributional Alignment,"Haozhong Wang, Zhuo Li, Yibo Yang, He Zhao, Hongyuan Zha, Dandan Guo",,,"Large Language Models, fine-tuning, safety alignment, Optimal Transport, data distribution, model safety","The paper introduces Safety Optimal Transport (SOT), a novel framework designed to enhance the safety of fine-tuning Large Language Models (LLMs). Traditional methods focus on instance-level data selection, which often overlooks the global data distribution and fails to repel harmful patterns. SOT addresses this by employing a dual-reference 'push-pull' weight-learning mechanism, optimizing sample importance by pulling the downstream distribution towards a safe anchor and pushing it away from harmful references. This approach establishes a robust safety boundary, effectively purifying training data. Experiments demonstrate that SOT significantly improves model safety while maintaining competitive performance, offering a superior safety-utility trade-off compared to existing methods.",16.04,16.649,267,cold_start,Phi-4,AMD RX 6800 (Vulkan)
2601.07201v1_CalPro Prior-Aware Evidential--Conformal Predictio.pdf,CalPro: Prior-Aware Evidential–Conformal Prediction with Structure-Aware Guarantees for Protein Structures,"Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma",,,"protein structure prediction, uncertainty quantification, conformal prediction, evidential learning, distribution shift, calibration, deep learning, structure-aware guarantees","CalPro introduces a novel framework for robust uncertainty quantification in protein structure prediction, addressing the limitations of existing confidence estimates like pLDDT. It combines a geometric evidential head, a differentiable conformal layer, and domain priors to provide shift-robust uncertainty quantification. The framework maintains near-nominal coverage and achieves tighter intervals in regions where priors are informative. Empirical results show significant improvements in coverage degradation, calibration error, and downstream tasks like ligand-docking success compared to baseline methods. The approach is also applicable to other structured regression tasks.",15.67,16.206,254,cold_start,Phi-4,AMD RX 6800 (Vulkan)
