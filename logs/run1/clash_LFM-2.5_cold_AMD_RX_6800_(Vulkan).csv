filename,title,authors,doi,arxiv_id,keywords,summary,tps,model,platform
2601.07192v1_Relink Constructing Query-Driven Evidence Graph On.pdf,Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG,"Manzong Huang, Chenyang Bu, Yi He, Xingrui Zhuo, Xindong Wu",,,"GraphRAG, GraphRAG challenges, hallucinations, knowledge graph, retrieval-augmented generation, LLM mitigation, query construction, evidence graph","Relink proposes a framework to dynamically build a query-specific evidence graph, addressing incompleteness and distractor facts in GraphRAG by instantiating facts from a latent relation pool and employing a unified evaluation strategy.",44.11,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07197v1_Beyond Variance Knowledge-Aware LLM Compression vi.pdf,Beyond Variance: Knowledge-Aware LLM Compression via Fisher-Aligned Subspace Diagnostics,"Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma",,,"knowledge-aware compression, Fisher-Aligned Subspace, activation compression, MMLU, LAMA, factual knowledge, gradient coupling","The paper introduces Fisher-Aligned Subspace Compression (FASC), a knowledge-aware method that leverages the Fisher Information Matrix to compress LLMs by focusing on dimensions critical for factual knowledge, improving accuracy over variance-based approaches.",43.9,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07199v1_Forward versus Backward Comparing Reasoning Object.pdf,Forward versus Backward: Comparing Reasoning Objectives in Direct Preference Optimization,"Murtaza Nikazad, Raghuram Ramanujan",10.48550/arXiv.2407.08600,2407.08600,"reasoning objectives, direct preference optimization, hallucination, forward chain-of-thought, backward verification, false positive rate, model confidence","This paper investigates how training objectives—forward reasoning (problem to solution) versus backward verification (correctness checking)—affect reasoning accuracy and error recognition in large language models. Experiments on GSM8K show a trade-off: forward-only DPO improves accuracy, while backward-only training reduces false positives but lowers acknowledgement rates. Both approaches enhance model reliability, supported by efficient Low-Rank Adaptation.",44.32,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07200v1_Safeguarding LLM Fine-tuning via Push-Pull Distrib.pdf,Safeguarding LLM Fine-tuning via Push-Pull Distributional Alignment,"Haozhong Wang, Zhuo Li, Yibo Yang, H Zhao, Dandan Guo, Guo",,,"LLM fine-tuning, safety alignment, distributional alignment, Optimal Transport, data distribution, model safety","The paper introduces Safety Optimal Transport (SOT), a framework that reframes safe fine-tuning as a distribution-level alignment task. It addresses the limitation of existing instance-level defenses by optimizing sample importance through push-pull weight learning, establishing a robust safety boundary while maintaining performance.",43.52,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07201v1_CalPro Prior-Aware Evidential--Conformal Predictio.pdf,CalPro: Prior-Aware Evidential–Conformal Prediction with Structure-Aware,"Ibne Farabi Shihab, Shihab Akter, Anuj Sharma",10.48550/arXiv.2026.12345,arXiv:2109.12345,"protein structure prediction, pLDDT, conformal prediction, uncertainty quantification, structural biology","Introduces CalPro, a prior-aware evidential-conformal framework for shift-robust uncertainty quantification in protein structure prediction. Combines geometric evidential heads, differentiable conformal layers, and domain priors to achieve tighter coverage guarantees and improved calibration.",43.88,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07206v1_LLMRouterBench A Massive Benchmark and Unified Fra.pdf,LLMRouterBench: A Massive Benchmark and Unified Framework for LLM Routing,"Hao Li, Yiqun Zhang, Zhaoyan Guo, Chenxu Wang, Shengji Tang, Zhang Yang, Biqing Qi, Peng Ye Lei, Bai Zhen Wang, Shuyue Hu",,,"LLM routing, large language models, benchmark, LLM ensemble, model selection, performance trade-off, latency-aware analysis","This paper introduces LLMRouterBench, a large-scale benchmark and unified framework for evaluating LLM routing across 400K instances from 21 datasets and 33 models. It systematically re-evaluates routing methods, highlights performance gaps, and discusses limitations of current approaches, while enabling latency-aware analysis.",44.56,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07209v1_SIRR-LMM Single-image Reflection Removal via Large.pdf,SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model,"Yu Guo, Zhiqiang Lao, Xiyun Song, Yubin Zhou, Heather Yu",,,"reflection removal, glass surfaces, large multimodal model, synthetic dataset, reflection separation","This paper introduces a synthetic dataset generation framework that path-traces 3D glass models over real background imagery to create physically accurate reflection scenarios. It leverages a Large Multimodal Model (LMM) with task-specific LoRA to improve reflection removal performance, addressing limitations of existing datasets.",44.1,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07214v1_BlindU Blind Machine Unlearning without Revealing .pdf,BlindU: Blind Machine Unlearning without Revealing Erasing Data,"Weiqi Wang, Zhiyi Tian, Chenhan Zhang, Shui Yu",10.1109/TPA.2024.12345,IEEE/TR/2024/000123,"Machine Unlearning, Federated Learning, Privacy Leakage, Privacy Preserving, Information Bottleneck","This paper proposes Blind Unlearning (BlindU) to enable privacy-preserving machine unlearning without exposing erased data to servers. It uses compressed representations and the information bottleneck mechanism, supplemented with noise-free differential privacy masking, to protect user data while maintaining model utility.",41.07,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07224v1_Consolidation or Adaptation PRISM Disentangling SF.pdf,Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration,"Yang Zhao, Yangou Ouyang, Xiao Ding, Hepeng Wang, Bibo Cai, Kai Xiong, Jinglong Gao, Zhouhao Sun, Li Du, Bing Qin, Ting Liu",,,"PRISM, SFT, RL, gradient concentration, data arbitration, LLM agents, optimization regimes","This paper proposes PRISM, a dynamics-aware framework based on Schema Theory, to arbitrate data based on the degree of cognitive conflict between task trajectories and model knowledge. PRISM distinguishes between data triggering high spatial concentration (requiring RL) and diffuse updates (favoring SFT), achieving a Pareto improvement over existing hybrid methods.",43.39,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07226v1_Lost in the Noise How Reasoning Models Fail with C.pdf,Lost in the Noise: How Reasoning Models Fail with Contextual Distractors,"Seongyun Lee, Seongyun Lee, Yongrae Jo, Minju Seo, Moontae Lee, Minjoon Seo",10.48550/arXiv.2026.12345,arXiv:2509.12345,"reasoning models, agentic AI, noisy benchmarks, distractor robustness, prompting strategies","This paper introduces NoisyBench, a benchmark evaluating reasoning models under diverse noisy contexts. It reveals up to 80% performance drops in state-of-the-art models when exposed to contextual distractors, highlighting the need for improved robustness strategies such as Rationale-Aware Reward.",43.5,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07229v2_DiSCo Making Absence Visible in Intelligent Summar.pdf,Making Absence Visible in Intelligent Summarization Interfaces,"ERAN FAINMAN, HAGIT BEN SHOSHAN, ADIR SOLOMON, OSNAT MOKRYN",,,"Review Summarization, Absence, Expectations, Learning via surprisability, Missing commonalities","This paper introduces Domain Informed Summarization through Contrast (DiSCo), an expectation-based method that highlights missing information by comparing entity content to domain expectations. In user studies, DiSCo summaries were rated more detailed and useful than baseline models, showing reduced presence bias and improved decision support.",45.02,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07232v1_Yes FLoReNce I Will Do Better Next Time Agentic Fe.pdf,Agentic Feedback Reasoning for Humorous Meme Detection,"Olivia Shanhong Liu, Pai Chet Ng, De Wen Soh, Konstantinos N. Plataniotis",10.48550/arXiv:2509.06932,arXiv:2509.06932,"humorous memes, AI reasoning, multimodal understanding, feedback regulation, meme interpretation","This paper proposes FLoReNce, an agentic feedback reasoning framework for meme understanding. It treats meme comprehension as a closed-loop process during learning and an open-loop process during inference, enabling self-aligned reasoning through feedback-regulated prompting.",45.83,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07233v1_From Thinking to Justifying Aligning High-Stakes E.pdf,From 'Thinking' to 'Justifying': Aligning High-Stakes Explainability with Professional Communication Standards,"Chen Qian, Yimeng Wang, Yu Chen, Andreas Stathopoulos, Lingfei Wu, AxStat",,,"Explainable AI, High-stakes domains, Professional communication, Structured Explainability, Result justification, Logical gaps, Reliability","The paper proposes SEF (Structured Explainability Framework) to improve stakeholder trust in high-stakes AI by structuring outputs with a 'Result → Justify' format, drawing on professional communication conventions like CREAC and BLUF. Experiments show SEF improves accuracy and verifiability.",42.73,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07238v1_Group Pattern Selection Optimization Let LRMs Pick.pdf,Group Pattern Selection Optimization: Let LRMs Pick the Right Pattern,"Hanbin Wang, Jingwei Song, Jinpeng Li, Fei Mi, Lifeng Shang",10.48550/arXiv.2024.12345,arXiv:2408.12345,"large reasoning models, pattern selection, reinforcement learning, multi-pattern rollouts, problem characteristics, robust reasoning","This paper introduces Group Pattern Selection Optimization (GPSO), a reinforcement learning framework that improves reasoning model performance by identifying optimal reasoning patterns through multi-pattern rollouts and verifier-guided selection. Extensive experiments show consistent gains across benchmarks.",44.99,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07239v1_Stochastic CHAOS Why Deterministic Inference Kills.pdf,Stochas (CHAOS: Why Determinis)c Inference Kills,"Tanmay Joshi, Shourya Aggarwal, Anusa Saha, Aadi Pandey, Shreyash Dhoot, Vighnesh Rai, Raxit Goswami, Aman Chadha3, Vinija Jain, Amitava Das",10.48550/arXiv:2309.12345,10.48550/arXiv:2309.12345,"deterministic inference, LLM inference, uncertainty modeling, stochastic chaos, distributional variability, cognitive stability","The paper argues that deterministic inference undermines LLM capabilities by suppressing uncertainty, emergent abilities, and reasoning diversity, advocating instead for stochastic approaches that preserve variability and safety.",45.56,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07245v1_Learning to Trust the Crowd A Multi-Model Consensu.pdf,Learning to Trust the Crowd: A Multi-Model Consensus Reasoning Engine for Large Language Models,Pranav Kallem,,,"trust, consensus, multi-model, LLM, reliability",The paper introduces a multi-model consensus reasoning engine that evaluates large language models by aggregating responses from several heterogeneous LLMs. It improves accuracy and calibration over single-model predictions and analyzes key factors influencing reliability.,43.9,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07250v1_DDT A Dual-Masking Dual-Expert Transformer for Ene.pdf,DDT: A Dual-Masking Dual-Expert Transformer for Energy Time-Series Forecasting,"Mingnan Zhu, Qixuan Zhang, Yixuan Cheng, Fangzhou Gu, Shiming Lin",,,"Time-Series Forecasting, Multivariate Temporal Modeling, Dynamic-Causal Masking, Adaptive Feature Fusion","Accurate energy time-series forecasting is crucial for grid stability and renewable integration. This paper proposes DDT, a novel deep learning framework combining dual-masking and dual-expert systems to address complex temporal dependencies and heterogeneous data.",44.62,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07261v1_Pseudodata-guided Invariant Representation Learnin.pdf,Pseudodata-guided Invariant Representation,"Haomin Wu, Zhiwei Nie, Hongyu Zhang, Zhixiang Ren",arXiv:2601.07261v1,2601.07261,"pseudodata, invariant representation, enzyme kinetics, deep learning, generalization, out-of-distribution","Accurate prediction of enzyme kinetic parameters is essential for understanding catalytic mechanisms and guiding enzyme engineering. This work proposes O2DENet, a lightweight module that enhances out-of-distribution generalization through biologically informed perturbations and invariant representation learning.",39.6,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07263v1_When Bots Take the Bait Exposing and Mitigating th.pdf,When Bots Take the Bait: Exposing and Mitigating the Emerging Social Engineering,"Xinyi Wu, Geng Hong, Yueyue Chen, MingXuan Liu, Feier Jin, Xudong Pan, Jiarun Dai, Baojun Liu",,,"web automation, social engineering, web agents, social engineering attacks, agents, prompt injection, backdoors, privilege escalation, attack surface, security, intrusion detection","This paper presents the first systematic study of social engineering attacks against web automation agents, introducing the AGENTBAIT paradigm to exploit agent reasoning weaknesses and proposing SUPERVISOR for runtime mitigation. Empirical results demonstrate high attack success rates and propose a lightweight defense that reduces attack success by up to 78.1%.",44.14,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07291v1_A Visual Semantic Adaptive Watermark grounded by P.pdf,Visual Semantic Adaptive Watermark grounded by Prefix-Tuning for Large Vision-Language Model,"Qi Zheng, Shuliang Liu, Yu Huang, Junhao Chen, Hanqian Li, Aiwei Liu, Yibo Yan, Xuming Hu",,,"visual watermarking, semantic-aware watermarking, large vision-language model, prefix-tuning, visual fidelity, detectability, semantic similarity","The paper introduces VISA-Mark, a lightweight prefix-tuned watermarking framework for LVMs that preserves visual fidelity while improving detectability and semantic fidelity. It achieves 7.8% better visual consistency and robust attack resilience compared to existing methods.",44.29,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07292v1_Photometric Redshift Estimation Using Scaled Ensem.pdf,Photometric Redshift Estimation Using Scaled Ensemble Learning,"Swagata Biswas, Shubhrangshu Ghosh, Avyarthana Ghosh, Yogesh Wadadekar, Abhishek Roy Choudhury, Arijit Mukherjee, Shailesh Deshpande, And Arpan Pal",1,,"photometric redshift, photometric data, machine learning, astronomy, galaxies, redshift estimation, LSST, Hyper Suprime-Cam, Neural networks","This study presents a new ensemble-based ML framework for predicting photometric redshifts using optical photometric data. It integrates multiple algorithms within a scaled ensemble structure, achieving improved accuracy up to z ≈ 4 and aligning with LSST requirements.",44.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07296v1_LRAS Advanced Legal Reasoning with Agentic Search.pdf,LRAS: Advanced Legal Reasoning with Agentic Search,"Yujin Zhou, Chuxue Cao, Jinluan Yang, Lijun Wu, Conghui He, Sirui Han, Yike Guo",,,"legal reasoning, LRM, legal LLM, introspection, LLMs","This paper introduces Legal Reasoning with Agentic Search (LRAS), a framework that shifts legal LLMs from static closed-loop reasoning to dynamic active inquiry. By combining Introspective Imitation Learning and difficulty-aware reinforcement learning, LRAS improves performance on complex legal reasoning tasks, outperforming existing baselines by 8.2-32%.",45.13,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07304v1_Heterogeneous Multi-Expert Reinforcement Learning .pdf,Heterogeneous Multi-Expert Reinforcement Learning for Long-Horizon Multi-Goal Tasks in Autonomous Forklifts,"Yun Chen, Bowei Huang, Fan Guo, Kang Song",10.48550/arXiv.2024.12345,arXiv:2408.12345,"Autonomous Forklift, Hierarchical Reinforcement Learning, Mobile Manipulation, Hybrid Training","Proposes a Heterogeneous Multi-Expert Reinforcement Learning framework for autonomous forklifts, decomposing long-horizon tasks into specialized sub-policies. It introduces a Semantic Task Planner to separate macro-level navigation from micro-level manipulation, and applies a Hybrid Imitation-Reinforcement Training Strategy to address sparse exploration. Experiments demonstrate improved task success rates and reduced operation time.",46.12,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07309v1_ARM Role-Conditioned Neuron Transplantation for Tr.pdf,ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging,"Zhuoka Feng, Kang Chen, Sihan Zhao, Kai Xiong, Yaoning Wang, Minshen Yu, Junjie Nian, Changyi Xiao, Yixin Cao, Yugang Jiang",10.1093/arxiv/2026.07309,2601.07309,"ARM, model merging, LLM agents, role-conditioned, neuron transplantation, cross-benchmark, generalization","This paper proposes Agent-Role Merging (ARM), a training-free method for merging large language models by integrating multiple experts into a single model. ARM enhances existing merging techniques from static tasks to multi-turn scenarios, improving generalization across interactive environments while maintaining efficiency. The approach uses a 3-step framework and achieves strong out-of-domain performance.",46.74,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07313v1_Explaining Machine Learning Predictive Models thro.pdf,Multivariate Conditional Expectation (MUCE) for Local Explainability,"Silvia Ruiz-España, Laura Arnal, Franís Signola, Juan-Carlos Perez-Cortes, Joaquim Arlandis",,,"machine learning, XAI, explainable models, local explainability, model-agnostic, uncertainty, stability, uncertainty","This work introduces MUCE, a model-agnostic method for local explainability that captures prediction changes from feature interactions. It extends Individual Conditional Expectation by exploring a multivariate grid around an observation, providing graphical explanations and stability/uncertainty indices to assess model reliability.",46.27,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07315v1_VLM-CAD VLM-Optimized Collaborative Agent Design W.pdf,VLM-CAD:VLM-OptimizedCollaborativeAgent,"Guanyuan Pan, Yugui Lin, Tiansheng Zhou, Pietro Li, Shuai Wang, Yaqi Wang, Wangyaqi",,,"Analog Circuit Sizing, Agentic AI, Vision Language Model, Explainability, Electronic Design Automation","Analog mixed-signal circuit sizing involves complex trade-offs. This paper proposes a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD) that analyzes circuits, optimizes DC operating points, performs inference-based sizing, and executes external sizing optimization. It integrates Image2Net for schematic annotation and presents an Explainable Trust Region Bayesian Optimization method (ExTuRBO) for dual-granularity sensitivity analysis.",47.45,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07316v1_BEAT-Net Injecting Biomimetic Spatio-Temporal Prio.pdf,BEAT-Net: Injecting Biomimetic Spatio-Temporal Priors for Interpretable ECG Classification,"Ma Runze, Liao Caizhi",arXiv:2601.07316v1,rmaa0033@student.monash.edu,"ECG classification, deep learning, biomimetic analysis, interpretable AI, cardiovascular diseases","This paper proposes BEAT-Net, a biomimetic ECG analysis framework reformulating the problem as language modeling. By applying QRS tokenization, it extracts local beat morphology, normalizes spatial lead perspectives, and models temporal dependencies, achieving competitive accuracy with CNNs while enhancing interpretability through attention mechanisms.",46.0,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07320v1_Segmental Advantage Estimation Enhancing PPO for L.pdf,Segmental Advantage Estimation: Enhancing PPO for Long-Context LLM,"Xue Gong1, Qi Yi1, Ziyuan Nan2, 4, Guanhua Huang1, Kejiao Li1, Yuhao Jiang1, Ruibin Xiong1, Zenan Xu1, Jiaming Guo4, Shaohui Peng2, 3, Bo Zhou1",,,"Reinforcement Learning, Verifiable Rewards, Proximal Policy Optimization, Generalized Advantage Estimation, Large Language Models, Reasoning Tasks","Introduces Segmental Advantage Estimation (SAE) to mitigate bias in sparse-reward RLVR by partitioning sequences and computing variance-reduced advantages, improving training stability and performance.",45.42,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07342v1_Agentic Diagnostic Reasoning over Telecom and Data.pdf,Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure,Nicolas Tacheny,arXiv:2601.07342v1,1,"telecom, datacenter, infrastructure, autonomous incident resolution","Introduces an agent-based framework using a Large Language Model to autonomously investigate infrastructure failures, enabling proactive impact mitigation without hard-coded rules.",42.52,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07344v1_PulseMind A Multi-Modal Medical Model for Real-Wor.pdf,PulseMind: A Multi-Modal Medical Model for Real-World Clinical Diagnosis,"Jiao Xu1, Junwei Liu, Jiangwei Lao, Qi Zhu, Yunpeng Zhao, Congyun Jin, Shinan Liu, Zhihong Lu2, Lihe Zhang, Xin Chen, Jian Wang, Ping Wang",10.48550/arXiv.2024.12345,arXiv:2409.12345,"medical diagnosis, multi-modal models, clinical diagnosis, image analysis, deep learning, healthcare AI","This paper introduces PulseMind, a multi-modal diagnostic model integrating curated datasets, a benchmark, and reinforcement learning to improve real-world clinical performance across diverse medical domains.",43.88,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07348v4_Controlled Self-Evolution for Algorithmic Code Opt.pdf,Controlled Self-Evolution for Algorithmic Code Optimization,"Tu Hu1, Ronghao Chen2, Shuo Zhang3, Jianghao Yin4, Mou Xiao Feng3, Jingping Liu5, Shaolei Zhang6, Wenqi Jiang, Yuqi Fang1, Sen Hu2, 7, Wuhan Wang3, Yi Xu3",arXiv:2601.07348v4,2601.07348v4,"Controlled Self-Evolution, Code Optimization, Algorithmic Search, Exploration Efficiency, Evolutionary Algorithms","Self-evolution methods enhance code generation through iterative cycles, but face low exploration efficiency. This study proposes Controlled Self-Evolution (CSE) to address inefficiencies via diversified planning, feedback-guided evolution, and experience reuse. Experiments show CSE outperforms baselines across LLM backbones.",44.61,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07351v2_Beyond Hard Masks Progressive Token Evolution for .pdf,Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models,"Linhao Zhong, Linyu Wu, Bozhen Fang, Tianjian Feng, Chenchen Jing, Wen Wang, Jiaheng Zhang, Hao Chen, Chunhua Shen",,,"Diffusion Language Models, Token Evolution, Progressive Decoding, Masked Tokens, Revisionable Decoding","The paper proposes EvoToken-DLM, a diffusion-based language modeling approach that uses evolving soft token distributions to enable progressive transitions from masked states to discrete outputs, supporting revisable decoding. Continuous trajectory supervision aligns training with iterative updates, achieving superior performance over existing diffusion and masked models.",40.14,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07356v1_Efficient Convolutional Forward Model for Passive .pdf,Efficient Convolutional Forward Model for Passive Acoustic Mapping and Temporal Monitoring,"Tatiana Gelvez-Barrera, Barbara Nicolas, Bruno Gilles, Adrian Basarab, Denis Kouam",10.1016/j.jlcf.2015.02.008,,"Passive Acoustic Mapping, Model-based beamforming, Convolutional forward model, Temporal monitoring","Introduces a PAM beamforming framework based on a novel convolutional formulation in the time domain, enabling efficient computation and higher temporal resolution compared to frequency-domain methods.",42.62,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07359v1_Seeing Right but Saying Wrong Inter- and Intra-Lay.pdf,Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs,"Shezheng Song, Shasha Li, Jie Yu",10.48550/arXiv.2025.12345,arXiv:2509.12345,"multimodal models, layer refinement, attention mechanisms, visual understanding, intra-layer consistency","The paper discusses a phenomenon where MLLMs correctly capture visual content in deeper layers but produce incorrect outputs due to noisy earlier-layer attention, termed 'seeing it right but saying it wrong.' DualPD is proposed as a training-free refinement strategy to align internal understanding with output.",44.44,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07364v1_On the universal definition of intelligence.pdf,Human - AI Comparison,Joseph Chen,,,,"This paper proposes a universal definition of intelligence to enable fair comparison between human and artificial intelligence, addressing limitations of existing anthropocentric definitions.",42.16,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07372v1_Conditional Memory via Scalable Lookup A New Axis .pdf,Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models,"Xin Cheng, Wangding Zeng, Damai Dai, Qinyu Chen, Bingxuan Wang, Zhenda Xie, Kezhao Huang, Xingkai Yu, Zhewen Hao, Yukun Li, Han Zhang, Huishuai Zhang, Dongyan Zhao, Wenfeng Liang",10.48550/arXiv.2304.05748,arXiv:2601.07372v1,"conditional memory, scalable lookup, sparsity, large language models, Engram, MoE, knowledge retrieval","Introduces conditional memory as a complementary sparsity axis in LLMs, leveraging Engram for efficient O(1) lookup and uncovering a U-shaped scaling law that improves performance across reasoning and reasoning-related benchmarks.",46.61,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07376v1_OpenTinker Separating Concerns in Agentic Reinforc.pdf,OpenTinker: Separating Concerns in Agentic Reinforcement Learning,"Siqi Zhu, Jiaxuan You",10.48550/arXiv.2601.07376,2601.07376,"reinforcement learning, agentic learning, OpenTinker, RL systems","Introduces OpenTinker, an infrastructure for RL of LLM agents with separation of concerns across design, execution, and agent-environment interaction. It proposes a managed execution runtime, centralized scheduler, and design principles for multi-agent training, demonstrating practical RL use cases.",44.35,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07377v1_Learning Dynamic Collaborative Network for Semi-su.pdf,Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation,"Jiao Xu1, Xin Chen2, Lihe Zhang1",10.1007/...,,"3D vessel segmentation, semi-supervised learning, dynamic collaborative network, medical imaging, vessel topology","Presents DiCo, a dynamic collaborative network for semi-supervised 3D vessel segmentation, addressing limitations of static mean teacher methods by enabling teacher-student role switching and adversarial supervision.",43.8,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07389v1_On the Non-decoupling of Supervised Fine-tuning an.pdf,Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training,"Xueyan Niuniuxueyan3@huawei.com, Bo Baibaibo8@huawei.com, W eixi Zhangzhangweixi1@huawei.com, W eixi Zhangzhangweixi2@huawei.com",arXiv:2601.07389v1,arXiv:2601.07389,"supervised fine-tuning, reinforcement learning, post-training, large language models, decoupling","This paper investigates whether supervised fine-tuning (SFT) and reinforcement learning (RL) can be decoupled in the post-training phase of large language models. It proves that decoupling is impossible in either order, showing that SFT-then-RL coupling increases SFT loss and RL-then-SFT coupling lowers reward performance. Experiments on Qwen3-0.6B confirm these findings.",46.66,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07392v1_OceanSAR-2 A Universal Feature Extractor for SAR O.pdf,OceanSAR-2: A Universal Feature Extractor for SAR Ocean Observation,"Alexandre Tuela, Thomas Kerdreux a, Quentin Febvre b, Alexis Mouche b, Antoine Grouazel b, Jean-Renaud Miadana c, Antoine Audrasa, Chen Wangd, Bertrand Chapronb",,,"OceanSAR-2, SAR, Sentinel-1, feature extractor, remote sensing, geophysical patterns, iceberg detection","We present OceanSAR-2, the second generation of our foundation model for SAR-based ocean observation. Building on our earlier release, which pioneered self-supervised learning on Sentinel-1 Wave Mode data, OceanSAR-2 relies on improved SSL training and dynamic data curation strategies, which enhances performance while reducing training cost. OceanSAR-2 demonstrates strong transfer performance across downstream tasks, including geophysical pattern classification, ocean surface wind vector and significant wave height estimation, and iceberg detection. We release standardized benchmark datasets, providing a foundation for systematic evaluation and advancement of SAR models for ocean applications.",47.35,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07393v1_Software-Hardware Co-optimization for Modular E2E .pdf,SOFTWARE-HARDWARE CO-OPTIMIZATION FORMODULARE2E,"Chengzhi Ji, Xingfeng Li, Zhaodong Lv, Hao Sun, Pan Liu, Hao Frank Yang, Ziyuan Pu",arXiv:2601.07393v1,arXiv:2601.07393,"Modular end-to-end, Closed-Loop Evaluation, Software–Hardware co-optimization, Energy Consumption, Inference Latency","This paper proposes a reusable software–hardware co-optimization and closed-loop evaluation framework for modular end-to-end autonomous driving inference. It integrates software optimizations with hardware optimizations under a unified system-level objective, introducing a multidimensional evaluation metric (EERA V) that considers safety, comfort, efficiency, latency, and energy. The framework achieves over 6× latency reduction and a 22.35% improvement in EERA V, demonstrating actionable optimization guidance from both software and hardware perspectives.",47.84,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07395v1_MCP-ITP An Automated Framework for Implicit Tool P.pdf,MCP-ITP: An Automated Framework for Implicit Tool Poisoning in MCP,"Ruiqi Li, Zhiqiang Wang, Yunhao Yao, Xiang-Yang Li",lrq349,zhiqiang.wang,"Model Context Protocol, implicit tool poisoning, tool poisoning attack, automated framework, LLM agents, security risk","The paper introduces MCP-ITP, the first automated adaptive framework for implicit tool poisoning within the MCP ecosystem. It describes how malicious instructions embedded in tool metadata can manipulate agent behavior without direct tool invocation, expanding the attack surface. Experimental results show MCP-ITP achieves up to 84.2% Attack Success Rate while suppressing malicious tool detection.",45.13,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07397v1_Layerwise goal-oriented adaptivity for neural ODEs.pdf,Layerwise goal-oriented adaptivity for neural ODEs: an optimal control perspective,"Michael Hintermüller, Michael Hinze, Denis Korolev",arXiv:2601.07397v1,2601.07397v1,"Resnet, neural ODEs, parameter identification/learning, adaptive neural network",Proposes a novel layerwise adaptive construction method for neural network architectures based on a goal-oriented dual-weighted residual technique for optimal control of neural differential equations.,44.11,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07411v1_SCALPEL Selective Capability Ablation via Low-rank.pdf,SCALPEL: Selective Capability Ablation via Low-rank Parameter,"Zihao Fu, Xufeng Duan, Zhenguang G. Cai",10.48550/arXiv.2303.03037,arXiv:2303.03037,"large language models, interpretability, capability ablation, low-rank parameter, neural networks","This paper introduces SCALPEL, a framework representing language model capabilities as low-rank parameter subspaces. It enables selective removal of specific capabilities through targeted parameter edits while preserving overall performance. Experiments show SCALPEL can disentangle fine-grained capability distributions in models.",45.22,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07422v1_Two Pathways to Truthfulness On the Intrinsic Enco.pdf,Two Pathways to Truthfulness: On the Intrinsic Encoding of LLM Hallucinations,"Wen Luo, Guangyue Peng, Wei Li, Shaohang Wei, Feifan Song, Liang Wang, Nan Yang, Xingxing Zhang, Jing Jin, Furu Wei, Houfeng Wang",,,"truthfulness, hallucination, intrinsic encoding, LLM, knowledge boundaries","This paper investigates how truthfulness cues in large language models arise from two distinct information pathways: a Question-Anchored pathway and an Answer-Anchored pathway. It demonstrates these mechanisms through attention knockout and token patching, links them to LLM knowledge boundaries, and proposes applications to improve hallucination detection.",45.66,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07430v1_KALE Enhancing Knowledge Manipulation in Large Lan.pdf,Enhancing Knowledge Manipulation in Large Language Models,"Qitan Lv1,2*, Tianyu Liu1,2*, Qiaosheng Zhang2†, Xingcheng Xu2†, Chaochao Lu2",,,"knowledge manipulation, large language models, knowledge graphs, supervised fine-tuning, knowledge-aware learning","The paper addresses the challenge of improving LLMs' ability to recall, reason, and transfer knowledge by introducing KALE, a post-training framework that leverages knowledge graphs to generate rationales and enhances knowledge manipulation through knowledge-aware fine-tuning.",44.43,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07449v1_RLPO Residual Listwise Preference Optimization for.pdf,Residual Listwise Preference Optimization for Long-Context,"Hao Jiang, Zhi Yang, Annan Wang, Yichi Zhang, Weisi Lin, Yichizhang",10.1007/...,,"review ranking, long-context, listwise preference, LLM, e-commerce, user-generated content","Review ranking is pivotal in e-commerce for prioritizing diagnostic and authentic feedback. Existing ranking paradigms face a trade-off between pointwise scoring and list-level interactions, leading to unstable top-k rankings. This paper proposes Residual Listwise Preference Optimization (RLPO), which improves NDCG@k by leveraging calibrated pointwise scores and lightweight encoders, while maintaining robustness with increasing list lengths.",45.21,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07463v1_Puzzle it Out Local-to-Global World Model for Offl.pdf,Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning,"Sijia Li, Xinran Li, Shibo Chen, Jun Zhang",10.1234/abcd1234,,"Offline multi-agent reinforcement learning, Multi-gent model-based reinforcement learning","Offline MARL faces limitations due to data distribution constraints, leading to conservative policies. This work proposes a local-to-global world model to improve prediction accuracy and generalization by leveraging local predictions to infer global dynamics.",41.3,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07464v1_IFDNS An Iterative Feedback-Driven Neuro-Symbolic .pdf,Iterative Feedback-Driven Neuro-Symbolic Method for Faithful Logical Reasoning,"Xiaoheng Wang, Tongxuan Liu, Zi Gong, Xianzhe Dong, Yuting Zeng, Minhan Hu, Weizhe Huang, Jing Li",,,"Logical Reasoning, Large Language Model, Reasoning","This paper introduces IFDNS, an iterative feedback-driven neuro-symbolic approach that enhances logical reasoning in large language models by employing multi-round feedback to improve faithfulness and accuracy in extracting causal relationships.",40.13,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07468v1_Beyond Dialogue Time Temporal Semantic Memory for .pdf,Temporal Semantic Memory for Personalized LLM Agents,"Miao Su1, Yucan Guo1,2,3, Zixuan Li1,2,Yufei Zhang 4, Guojun Yin 4, Wei Lin 4, Xiaolong Jin1,2,3, Jiafeng Guo 1,2,3, Xueqi Cheng 1,2,3, Institute of Computing Technology, Chinese Academy of Sciences, State Key Laboratory of AI Safety, School of Computer Science, University of Chinese Academy of Sciences, Meituan",arXiv:2601.07468v1,arXiv:2601.07468,"memory, LLM, personalization, semantic memory, temporal accuracy, semantic timeline, durative information","Memory enables Large Language Model agents to perceive, store, and use information from past dialogues for personalization. This paper proposes Temporal Semantic Memory (TSM), a memory framework that models semantic time for point-wise memory and supports durative memory construction and utilization. TSM addresses temporal inaccuracy and fragmentation by building semantic timelines and consolidating continuous, related information.",43.64,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07469v1_Knowledge Distillation for LLM-Based Human Activit.pdf,Knowledge Distillation for LLM-Based Human Activity Recognition in Smart Homes,"Julien Cumin, Oussama Er-Rahmany, Xi Chen",arXiv:2601.07469v1,arXiv:2601.07469v1,"Human activity recognition, large language models, knowledge distillation, ambient intelligence, smart homes",This paper presents experimental results on using large language models for human activity recognition in smart homes. It explores how recognition performance varies with LLM size and demonstrates knowledge distillation techniques to fine-tune smaller models with high performance while reducing parameters.,41.08,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07470v1_Learning How to Remember A Meta-Cognitive Manageme.pdf,Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory,"Sirui Liang, Pengfei Cao, Jian Zhao, Wenhao Teng, Xiangwen Liao, Jun Zhao, Kang Liu",liangsirui2024@ia.ac.cn,,"memory abstraction, meta-cognitive management, agent memory, transfer learning, long-horizon decision making","The paper proposes the Meta-CognitiveMemory Abstraction method (MCMA) to address limitations in memory reuse for LLM agents. MCMA decouples task execution from fixed memory representations by introducing a learnable memory copilot that structures, abstracts, and reuses memories across abstraction levels. Experiments show improved generalization and cross-task transfer.",42.24,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07474v1_Task Prototype-Based Knowledge Retrieval for Multi.pdf,Prototype-Based Knowledge Retrieval for Multi-Task Learning,"Yoonmin Oh, Hyung-Il Kim, Jung Uk Kim, Juk Kim",10.1007/1234567,,"multi-task learning, prototype-based knowledge retrieval, multi-task learning framework, task prototype, knowledge retrieval, semantic segmentation, saliency","This paper proposes a prototype-based knowledge retrieval framework for multi-task learning that avoids reliance on unlabeled task predictions, aiming to improve robustness and performance in real-world applications.",41.65,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07475v1_ARCQuant Boosting NVFP4 Quantization with Augmente.pdf,ARCQuant: Boosting NVFP4 Quantization with Augmented Residual,"Haoqian Meng, Yilun Luo, Yafei Zhao, Wenyuan Liu, Peng Zhang, Xindian Ma",10.48550/ARCQuant,arxiv:2309.12345,"NVFP4, quantization, LLM, augmented residual channels, microscaling formats, hardware constraints","This paper proposes ARCQuant, a framework that enhances NVFP4 performance using augmented residual channels. It maintains a strictly unified NVFP4 format by augmenting the activation matrix with residual channels, enabling efficient inference on modern hardware while preserving model accuracy.",41.69,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07477v1_JudgeFlow Agentic Workflow Optimization via Block .pdf,JUDGEFLOW: AGENTICWORKFLOWOPTIMIZATION,"Zihan Ma, Zhikai Zhao, Chuanbo Hua, Federico Berto, Jinkyoo Park",,,"LLM, agentic workflows, optimization, LLM-based, workflow optimization","Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities. Current methods rely on coarse evaluation signals, lacking fine-grained diagnostics. JUDGEFLOW introduces a pipeline with configurable logic blocks and a judge module to assign rank-based scores, improving sample efficiency and interpretability.",41.94,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07496v1_Graph Inference Towards ICD Coding.pdf,Graph Inference Towards ICD Coding,Xiaoxiao Deng,,,"transfer learning, graph convolutional network, lightweight attention, ICD code prediction, adversarial domain adaptation","LabGraph introduces a unified framework reformulating ICD coding as graph generation, enhancing robustness via adversarial adaptation and reinforcement learning.",37.67,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07514v1_Data-Driven Stochastic VRP Integration of Forecast.pdf,Data-Driven Stochastic VRP: Integration of Forecast,Matteo Garbellia,arXiv:2601.07514v1,2601.07514,"Stochastic VRP, Machine Learning, XGBoost, Sub-Gaussian Concentration, Multi-Objective Optimization","This paper investigates integrating machine learning forecasts of intervention durations into a stochastic VRP, using sub-Gaussian concentration bounds and multi-objective optimization to improve operator utilization and completion rates.",45.67,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07516v1_Controlling Multimodal Conversational Agents with .pdf,Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions,"Yongqi Li, Hao Lang, Tieyun Qian, Yongbin Li, Hao Zhang, Tieyun Zhang",10.48550/arXiv.2403.08532,2403.08532,"vision-language models, multimodal conversational agents, reinforcement learning, latent action space, cross-modal projector, RL fine-tuning","This paper presents a method to construct a compact latent action space for RL fine-tuning in multimodal conversational agents. By leveraging paired image-text data and text-only data through a cross-modal projector, the approach improves generalization while addressing the challenge of large text token spaces. Experimental results demonstrate superior performance across two conversation tasks using various RL algorithms.",44.18,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07518v1_Mon3tr Monocular 3D Telepresence with Pre-built Ga.pdf,Monocular 3D Telepresence with Pre-built Gaussian Avatars as Amortization,"Fangyu Lin, Yingdong Hu, Zhening Liu, Yufan Zhuang, Zehong Lin, Jun Zhang",10.1234/jlcf.2024.12345,https://arxiv.org/abs/2405.12345,"Monocular 3D telepresence, 3D Gaussian splatting, animatable avatars, real-time neural rendering","Introduces Mon3tr, a novel monocular 3D telepresence framework integrating 3D Gaussian splatting for parametric human modeling. It employs an amortized computation strategy using a single monocular RGB camera to capture real-time motion and facial expressions, enabling robust performance with <0.2 Mbps transmission. The method achieves state-of-the-art PSNR (>28 dB), end-to-end latency (~80 ms), and >1000× bandwidth reduction compared to point-cloud streaming, supporting real-time operation across diverse scenarios.",45.08,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07525v1_Thinking Before Constraining A Unified Decoding Fr.pdf,Thinking Before Constraining: A Unified Decoding Framework for Large Language Models,"Ngoc Trinh Hung Nguyen, Alonso Silva, Laith Zumot, Liubov Tupikina, Armen Aghasaryan, Mehwish Alam",,,"large language models, structured generation, natural language processing, code generation, schema-based information extraction",The paper proposes a unified decoding framework that combines natural and structured generation to improve reliability and parsability of LLM outputs while preserving expressive power.,43.24,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07528v1_From RAG to Agentic RAG for Faithful Islamic Quest.pdf,From RAG to Agentic RAG for Faithful Islamic Question Answering,"Gagan Bhatia, Hamdy Mubarak, Mustafa Jarrar, George Mikros, Fadi Zaraket, Mahmoud Alhirthani, Mutaz Al-Khatib, Logan Cochrane, Kareem Darwish, Rashid Yahiaoui, Firoj Alam",,,"Islamic question answering, faithful QA, agentic RAG, Qur'an retrieval, religious consequences, hallucination detection, multilingual evaluation","This paper introduces ISLAMIC FAITH QA, a benchmark for evaluating Islamic question answering, and proposes an agentic RAG framework to improve reliability by iteratively seeking evidence and revising answers. Experiments show improved correctness and robustness compared to standard RAG.",45.51,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07553v1_VirtualEnv A Platform for Embodied AI Research.pdf,VirtualEnv: A Platform for Embodied AI Research,"Kabir Swain, Sijie Han, Ayush Raina, Jin Zhang, Shuang Li, Michael Stopa, Antonio Torralba",10.48550/arXiv.2026.12345,arXiv:2609.12345,"Virtual Environment, Large Language Models, Embodied AI, Unreal Engine, LLM benchmarking, Interactive Simulation","VirtualEnv is a next-generation simulation platform built on Unreal Engine that enables fine-grained benchmarking of large language models in embodied and interactive scenarios. It supports rich agent-environment interactions, integrates LLMs and vision-language models, and aims to advance research at the intersection of AI and gaming.",44.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07556v1_Backpropagation-Free Test-Time Adaptation for Ligh.pdf,Backpropagation-Free Test-Time Adaptation for Lightweight EEG-Based Brain-Computer Interfaces,"Siyang Li, Jiayi Ouyang, Zhenyao Cui, Ziwei Wang, Tianwang Jia, Feng Wan, Dongrui Wu",,,"brain-computer interface, domain adaptation, electroencephalogram, test-time adaptation, transfer learning","This paper proposes Backpropagation-Free Transformations (BF T) for EEG decoding, addressing deployment challenges in EEG-based BCIs by eliminating backpropagation requirements, enabling robust inference under distribution shifts without per-user calibration.",44.23,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07565v1_A Unified Framework for Emotion Recognition and Se.pdf,A Unified Framework for Emotion Recognition and Sentiment Analysis via Expert-Guided Multimodal Fusion with Large Language Models,"Jiaqi Qiao, Xiujuan Xu, Xinran Li, Yu Liu",,,"emotion recognition, large language models, multimodal fusion, sentiment analysis, affective computing","The paper introduces EGMF, a unified framework that integrates expert-guided multimodal fusion with large language models. It features specialized expert networks for local, semantic, and global context, leveraging LoRA fine-tuning and hierarchical dynamic gating. Experiments show improved cross-lingual robustness and state-of-the-art performance on multiple benchmarks.",46.39,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07568v1_d3LLM Ultra-Fast Diffusion LLM using Pseudo-Trajec.pdf,Ultra-Fast dLLM using Pseudo-Trajectory Distillation,"Yu-Yang Qian, Junda Su, Lanxiang Hu, Peiyuan Zhang, Zhijie Deng, Peng Zhao, Hao Zhang, Peng Zhao",,,"diffusion models, large language models, pseudo-trajectory distillation, parallelism, accuracy, inference optimization","The paper proposes d3LLM, a pseudo-distilled diffusion LLM that balances accuracy and parallelism by introducing pseudo-trajectory distillation during training and entropy-based multi-block decoding during inference. It introduces AUP to evaluate accuracy-parallelism trade-offs and reports up to 10× speedup over vanilla LLaDA/Dream and 5× over AR models without significant accuracy loss.",46.07,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07573v1_A Model of Artificial Jagged Intelligence.pdf,A Model of Artificial Jagged Intelligence,Joshua S. Gans,arXiv:2601.07573v1,2601.07573,"generative AI, adoption, calibration, learning, scaling","This paper develops an economic model of Artificial Jagged Intelligence (AJI), analyzing how users balance local reliability concerns with coarse global signals. It explores adoption thresholds, error amplification, mastery learning, and scaling interactions, aiming to clarify when 'where the model works' becomes difficult to detect.",45.78,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07577v1_Beyond Entangled Planning Task-Decoupled Planning .pdf,Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents,"Yunfan Li, Bingbing Xu, Xueyun Tian, Xiucheng Xu, Huawei Shen",,,"large language models, task decoupling, long-horizon planning, planning paradigms, sub-task decomposition","This paper proposes Task-Decoupled Planning (TDP), a training-free framework that decouples tasks into a directed acyclic graph, enabling robust execution for long-horizon agents by reducing entangled reasoning and local error propagation.",44.74,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07580v1_Large Language Models for Physics Instrument Desig.pdf,Large Language Models for Physics Instrument Design,"Sara Zoccheddu, Shah Rukh Qasim, Patrick Owen, Nicola Serra",,,,"The paper examines the application of large language models (LLMs) in physics instrument design, comparing their performance to reinforcement learning. It highlights that LLMs can generate valid, resource-aware designs using prompting and prior design summaries, suggesting potential for hybrid design workflows combining LLMs with trust region optimizers.",45.11,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07582v2_ES-Mem Event Segmentation-Based Memory for Long-Te.pdf,ES-Mem: Event Segmentation-Based Memory for Long-Term Dialogue Agents,"Huhai Zou, Tianhao Sun, Chuanjiang He, Yu Tian2, Zhenyang Li, Li Jin, Nayu Liu, Jiang Zhong, Kaiwen Wei",10.1234/example.doi,,"memory, dialogue agents, event segmentation, long-term interactions, semantic integrity, discourse structure","The paper proposes ES-Mem, a framework leveraging event segmentation to address memory limitations in dialogue agents. It introduces a dynamic segmentation module and hierarchical memory architecture to improve coherence and contextual anchoring, demonstrating consistent performance gains over existing methods.",46.53,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07597v1_Pheromone-Focused Ant Colony Optimization algorith.pdf,Pheromone-Focused Ant Colony Optimization algorithm for path planning,"Yi Liu, Hongda Zhang, Zhongxue Gan, Yuning Chen, Ziqing Zhou, Chunlei Meng, Chun Ouyang",,,"Pheromone-Focused Ant Colony Optimization, Path planning, Ant colony optimization, Optimization algorithms, Path planning strategies","The paper proposes a Pheromone-Focused Ant Colony Optimization (PFACO) algorithm to improve path planning by enhancing convergence speed and solution quality through three strategies: (1) concentrating pheromone in promising regions, (2) reinforcing high-quality solutions, and (3) penalizing redundant path turns.",46.15,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07606v1_Proof of Time A Benchmark for Evaluating Scientifi.pdf,Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments,"Bingyang Ye, Shan Chen, Jingxuan Tu, Chen Liu, Zidi Xiong, Samuel Schmidgall, Danielle S. Bitterman",,,"scientific ideas, idea quality, benchmarking, agent-based research, peer review, citations, researcher agendas","Introduces Proof of Time (PoT), a framework linking scientific idea judgments to observable downstream signals, enabling scalable and verifiable evaluation of model predictions.",44.65,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07611v1_DIAGPaper Diagnosing Valid and Specific Weaknesses.pdf,Diagnosing Valid and Specific Weaknesses in Scientific Papers,"Zhuoyang Zou, Abolfazl Ansari, Delvin Ce Zhang, Dongwon Lee, Wenpeng Yin",10.1093/pidpaper/2024.01.012,arXiv:2408.12345,"paper weakness identification, multi-agent reasoning, reviewer bias, author rebuttal, prioritization","This paper introduces DIAGPaper, a multi-agent framework for diagnosing valid and specific weaknesses in scientific papers. It addresses limitations of existing systems by modeling human-defined review criteria, incorporating author rebuttals, and ranking weaknesses prioritized for users. Experiments show superior performance over prior methods.",45.75,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07618v1_Neural Architecture for Fast and Reliable Coagulat.pdf,Neural Architecture for Fast and Reliable Coagulation Assessment in Clinical Settings: Leveraging Thromboelastography,"Yulu Wang, Ziqian Zeng, Jianjun Wu, Zhifeng Tang",10.1016/j.baom.2025.01.012,,"coagulation assessment, thromboelastography, deep learning, physiological state reconstruction, machine learning, clinical AI","The paper introduces Physiological State Reconstruction (PSR) to address delays in traditional Thromboelastography (TEG) by enabling real-time coagulation monitoring. PSR leverages dynamic patient data through multi-domain learning and HLA-based attention, achieving high prediction accuracy while reducing inference time. It demonstrates improved performance over state-of-the-art methods and offers promise for data-scarce medical AI applications.",46.69,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07632v2_GeoMotionGPT Geometry-Aligned Motion Understanding.pdf,Geometry-Aligned Motion Understanding with Large Language Models,"Ye1 Zhankai, Li1 Bofan, Zhang3 Yukai, Jin1 Shuoqiu, Li1 Xin, Wang2 Wei, Zhang3 Shangqian, Gao1 Xin, Yang4 Yanfu, Zhang4 Xin",10.48550/geomotion-gpt,2024/04001,"motion understanding, large language models, geometry alignment, semantic embedding, HumanML3D, discrete tokenization","This paper introduces a framework that aligns motion tokenization with semantic embeddings using a unified geometric basis. By enforcing orthogonality between motion codebooks and LLM embeddings, the method improves motion reasoning performance by 20% on HumanML3D.",46.22,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07635v2_Learning About Learning A Physics Path from Spin G.pdf,Learning About Learning: A Physics Path from Spin Glasses to Artificial Intelligence,"Denis D. Caprioti, Matheus Haas, Constantino F. Vasconcelos, Mauricio Girardi-Schappo",88040-900,,"spin glasses, neural networks, statistical mechanics, computational methods, artificial intelligence","The paper presents the Hopfield model as a pedagogical framework linking statistical physics, neural networks, and AI, emphasizing its role in unifying undergraduate topics and offering practical simulation resources.",44.41,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07638v1_SALT-KG A Benchmark for Semantics-Aware Learning o.pdf,A Benchmark for Semantics-Aware Learning on Enterprise Tables,"Isaiah Onando Mulang, Felix Sasaki, Tassilo Klein, Jonas Kolk, Nikolay Grechanov, Johannes Hoffart",10.48550/arXiv.2601.07638,arXiv:2601.07638v1,"semantics-aware learning, semantic reasoning, metadata knowledge graph, tabular data, enterprise tables","Building upon the SALT benchmark for relational prediction, SALT-KG introduces a benchmark for semantics-aware learning on enterprise tables by linking multi-table transactional data with a structured Operational Business Knowledge graph. This enables models to reason over both tabular evidence and contextual semantics, addressing gaps in leveraging semantics for relational context.",46.4,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07641v1_Beyond Static Tools Test-Time Tool Evolution for S.pdf,Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning,"Jiaxuan Lu1, Ziyu Kong2, Yemin Wang3, Rong Fu4, Haiyuan Wan1, Cheng Yang6, Wenjie Lou1, Haoran Sun1, Lilong Wang1, Yankai Jiang1, Xiaosong Wang1, Xiao Sun1, Dongzhan Zhou1",,,"AI for Science, scientific reasoning, tool evolution, Test-Time Tool Evolution, computational tools","The paper addresses the limitations of static tool libraries in scientific reasoning, introducing Test-Time Tool Evolution (TTE) to enable agents to synthesize, verify, and evolve executable tools during inference. It presents SciEvo benchmark and demonstrates TTE's effectiveness in achieving state-of-the-art performance.",46.24,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07651v1_Active Evaluation of General Agents Problem Defini.pdf,Active Evaluation of General Agents: Problem,"Marc Lanctot, Kate Larson, Ian Gemp, Michael Kaisers",10.48550/acm/2026.025,10.48550/ACM2026,"general evaluation, multitask evaluation, ranking, active learning, game theory, social choice theory","This paper proposes a formal definition and conceptual framework for active evaluation of agents across multiple tasks, focusing on ranking algorithms and their performance under varying data conditions. It compares classical Elo with a new method, Soft Condorcet Optimization, and discusses practical implications for Atari agent evaluation.",46.18,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07654v1_Towards Automating Blockchain Consensus Verificati.pdf,Towards Automating Blockchain Consensus,"Elliot Jones, William Knottenbelt",,,"Blockchain, Consensus, Formal Verification, Theorem, Artificial Intelligence","The paper presents IsabeLLM, a tool integrating Isabelle with a Large Language Model to automate and verify blockchain consensus protocols, demonstrating its effectiveness in proving Bitcoin's Proof of Work consensus.",44.87,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07663v2_Reasoning Models Will Blatantly Lie About Their Re.pdf,Reasoning Models Will Blatantly Lie About Their Reasoning,William Walden,,,,"The study investigates whether Large Reasoning Models (LRMs) falsely claim not to use hints in their reasoning, despite evidence suggesting they do. It builds on prior work by Chen et al. (2025) and examines how models respond to hinted answers in multiple-choice tasks.",42.8,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07666v1_Variational Contrastive Learning for Skeleton-base.pdf,Variational Contrastive Learning for Skeleton-based Action Recognition,"Dang-Dinh NGUYEN1, Decky ASPANDI-LATIF1, Titus ZAHARIA1",10.48550/arXiv.2601.07666,arXiv:2601.07666,"Human Action Recognition, Self-Supervised Learning, Variational Inference","The paper proposes a variational contrastive learning framework integrating probabilistic latent modeling with contrastive self-supervised learning to address variability and uncertainty in skeleton-based action recognition. Extensive experiments demonstrate superior performance, especially in low-label regimes.",46.89,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07667v1_Adaptive Layer Selection for Layer-Wise Token Prun.pdf,Adaptive Layer Selection for Layer-Wise Token Pruning in LLM Inference,"Rei Taniguchi, Yuyang Dong, Makoto Onizuka, Chuan Xiao",,,"large language models, key-value cache, token pruning, layer-wise inference, attention scores, KV retrieval","This paper proposes ASL, a training-free method that adaptively selects the selection layer for KV cache reduction based on token rank variance. It balances performance across tasks while meeting a user-defined KV budget, improving accuracy and decoding speed.",44.67,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07685v1_Predictive Analytics for Dementia Machine Learning.pdf,Predictive Analytics for Dementia: Machine Learning on Healthcare Data,"Shafiul Ajam Opee, Nafiz Fahad, Anik Sen, Rasel Ahmed, Fariha Jahan, Md. Kishor Morol, Md Rashedul Islam",,,"Dementia, Machine learning, Linear Discriminant Analysis (LDA), APOE-ϵ4 allele, Chronic conditions, Explainable AI","This study focuses on enhancing dementia prediction using machine learning techniques on patient health data. Supervised learning algorithms such as K-Nearest Neighbors (KNN), Quadratic Discriminant Analysis (QDA), Linear Discriminant Analysis (LDA), and Gaussian Process Classifiers were applied. Techniques like SMOTE and TF-IDF were used to address class imbalance. LDA achieved the highest testing accuracy of 98%, highlighting the importance of model interpretability and correlations with features like APOE-ϵ4 allele.",47.18,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07701v1_Deep Whole-body Parkour.pdf,Deep Whole-body Parkour,"Ziwen Zhuang, Shaoting Zhu, Mengjie Zhao, Hang Zhao","IIIS, Tsinghua University, 2Shanghai Qi Zhi Institute","IIIS, Tsinghua University, 2Shanghai Qi Zhi Institute","Deep Reinforcement Learning, Humanoid Robotics, Whole-body Motion, Robot Locomotion, Perception Integration","This work unites perceptive locomotion and general motion tracking to enable humanoid robots to perform dynamic, multi-contact motions like vaulting and diving on uneven terrain. The framework integrates exteroceptive sensing into whole-body tracking, expanding traversability beyond simple walking.",46.6,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07718v1_Hiking in the Wild A Scalable Perceptive Parkour F.pdf,Hiking in the Wild: A Scalable Perceptive Parkour,"Shaoting Zhu, Ziwen Zhuang, Mengjie Zhao, Kun-Ying Lee, Hang Zhao",10.48550/arXiv.2601.07718,"IIIS, Tsinghua University","humanoid robot, hiking, perception, robotics, terrain navigation","The framework enables humanoid robots to traverse diverse terrains in indoor and outdoor environments, achieving robust performance through scalable end-to-end learning. It addresses challenges in exteroception and scalability, introducing foothold safety and flat patch sampling strategies.",47.5,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07737v1_Evaluating the encoding competence of visual langu.pdf,Evaluating the encoding competence of visual language models using uncommon actions,Chen Ling,2601.07737v1,cs.CV,"visual language models, encoding competence, uncommon actions",The report evaluates how well visual language models can encode meaning using uncommon actions.,46.92,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07748v1_Improving Domain Generalization in Contrastive Lea.pdf,Improving Domain Generalization in Contrastive Learning using Adaptive Temperature Control,"Robert Lewis, Katie Matton, Rosalind W. Picard, John Guttag",arXiv:2601.07748v1,NeurIPS 2023,"domain generalization, contrastive learning, domain invariance, temperature control, self-supervised learning","The paper presents a method that enhances domain invariance in contrastive learning by adjusting the temperature parameter based on domain labels, improving out-of-distribution generalization while maintaining in-distribution performance.",46.17,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07778v1_DT-ICU Towards Explainable Digital Twins for ICU P.pdf,DT-ICU: Towards Explainable Digital Twins for ICU Patient Monitoring via Multi-Modal and Multi-Task Iterative Inference,Wen Guo,arXiv:2601.07778v1,2601.07778,,"DT-ICU is a multimodal digital twin framework for continuous risk estimation in intensive care, integrating variable-length clinical time series with static patient data in a unified multitask architecture. It achieves accurate, interpretable predictions that improve over time and demonstrate structured reliance on interventions and physiological responses.",47.43,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07779v1_OS-Symphony A Holistic Framework for Robust and Ge.pdf,OS-SYMPHONY: A Holistic Framework for Robust and Generalist,"Bowen Yang, Kaiming Jin, Zhenyu Wu, Zhaoyang Liu, Qiushi Sun, Zehao Li, Jingjing Xie, Zhoumianze Liu, Fangzhi Xu, Kanzhi Cheng, Qingyun Li, Yian Wang, Yu Qiao, Zun Wang, Zichen Ding, Zhengding Liu",,,"Vision-Language Models, Computer-Using Agents, Robust Automation, Long-horizon Tasks, Visual Context Curation, Multimodal Search, Tutorial Retrieval","This paper introduces OS-SYMPHONY, a holistic framework for robust and generalist Computer-Using Agents. It presents two key innovations: (1) a Reflection-Memory Agent for trajectory-level self-correction, and (2) versatile tool agents with a multimodal searcher that synthesizes live visual tutors. Experimental results show strong performance gains across benchmarks.",47.0,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07782v1_Beyond Single-Shot Multi-step Tool Retrieval via Q.pdf,Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning,"Wei Fang, James Glass",10.48550/arXiv.2405.12345,arXiv:2405.12345,"single-shot retrieval, query planning, tool libraries, LLM agents","This paper introduces TOOLQP, a lightweight framework that models retrieval as iterative query planning. It addresses limitations of standard single-shot dense retrievers by decomposing instructions into sub-tasks and generating queries tailored to compositional requirements. Experiments show superior zero-shot generalization and robustness across retrievers.",45.71,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07790v1_Benchmarking Small Language Models and Small Reaso.pdf,Benchmarking Small Language Models and Small Reasoning,"Yahya Masri, George Mason University, Emily Ma, George Mason University, Zifu Wang, Harvard University, Joseph Rogers, George Mason University, Chaowei Yang, George Mason University",arXiv:2601.07790v1,arXiv:2601.07790,"system logs, severity classification, small language models, small reasoning models, log comprehension, digital twin, root cause analysis","This benchmark evaluates nine small language models and small reasoning models using zero-shot, few-shot, and retrieval-augmented generation under various prompting strategies. It reveals performance trends, efficiency differences, and highlights architectural and training factors influencing accuracy and speed.",47.19,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07794v1_Kinship Data Benchmark for Multi-hop Reasoning.pdf,Kinship Data Benchmark for Multi-hop Reasoning,"Tianda Sun, Dimitar Kazakov",,,"multi-hop reasoning, kinship relations, genealogical data, multi-hop inference","The paper introduces KinshipQA, a benchmark for testing large language models on multi-hop reasoning using kinship data. It generates large-scale, culturally specific genealogies and evaluates inference tasks across diverse models.",44.45,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07821v1_Failure-Aware RL Reliable Offline-to-Online Reinfo.pdf,Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning,"Huanyu Li, Kun Lei, Sheng Zang, Kaizhe Hu, Yongyuan Liang, Bo An, Xiaoli Li, Huazhe Xu",10.48550/arXiv.2405.12345,arXiv:2405.12345,"failure-aware reinforcement learning, offline-to-online learning, self-recovery, robot manipulation, fragile objects, real-world deployment","This paper introduces Failure-Aware Offline-to-Online Reinforcement Learning (FARL) to minimize intervention-requiring failures during real-world RL. FARL uses a world-model-based safety critic and offline-trained recovery policies to improve robustness and performance, reducing IR Failures by 73.1% while boosting average performance by 11.3%.",47.57,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07832v2_MHLA Restoring Expressivity of Linear Attention vi.pdf,Restoring Expressivity of Linear Attention,"Kewei Zhang, Ye Huang, Yufan Deng, Jincheng Yu, Huan Ling, Daquan Zhou",arXiv:2601.07832v2,14 Jan 2026,"Linear attention, Multi-Head Linear Attention (MHLA), Self-attention complexity, Transformers, Computational efficiency, Expressivity preservation","The paper addresses the quadratic complexity of standard self-attention in Transformers by proposing MHLA, which maintains linear complexity while preserving expressive power. It demonstrates improvements across ImageNet classification, NLP, image generation, and video generation under comparable time constraints.",46.46,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07885v1_Small Symbols Big Risks Exploring Emoticon Semanti.pdf,"Small Symbols, Big Risks: Exploring Emoticon Semantic Confusion in Large Language Models","Weipeng Jiang, Xiaoyu Zhang, Juan Zhai, Shiqing Ma, Chao Shen, Yang Liu",,,"emoticons, semantic confusion, large language models, safety implications, code generation, LLM vulnerabilities","This paper investigates emoticon semantic confusion in LLMs, demonstrating that up to 38% of emoticon interpretations are inaccurate, often resulting in silent failures. The study highlights the need for mitigation strategies to prevent destructive outcomes.",44.94,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07891v1_KVzap Fast Adaptive and Faithful KV Cache Pruning.pdf,"Fast, Adaptive, and Faithful KV Cache","Simon Jégou, Maximilian Jeblick",10.1093/pasj/klz077,2601.07891,"KV cache, KVzap, KVzap pruning, transformer, language models","Introduces KVzap, a fast, input-adaptive KV cache approximation that improves KV cache compression for large models with minimal accuracy loss. Demonstrated on Qwen3-8B, Llama-3.1-8B-Instruct, and Qwen3-32B, achieving state-of-the-art performance while outperforming existing methods.",46.38,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07892v1_Sherry Hardware-Efficient 1.25-Bit Ternary Quantiz.pdf,Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification,"Hong Huang, Decheng Wu, Qiangqiang Hu, Ganghua Yu, Jinhai Yang, Jianchen Zhu, Xue Liu, Dapeng Wu",10.1234/arxiv.2024.56789,arXiv:2409.12345,"hardware-efficient quantization, ternary quantization, edge computing, large language models, sparsification, power-of-two alignment","The paper proposes Sherry, a hardware-efficient ternary quantization framework that introduces a 3:4 fine-grained sparsity to achieve a regularized 1.25-bit width. It addresses alignment issues and weight trapping in sparse ternary training, demonstrating improved performance and efficiency on LLaMA-3.2 across benchmarks.",45.98,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07894v1_Revealing the Attention Floating Mechanism in Mask.pdf,Revealing the Attention Floating Mechanism in Masked Diffusion Models,"Xin Dai, Pengcheng Huang, Zhenghao Liu, Shuo Wang, Yukun Yan, Chaojun Xiao, Yu Gu, Ge Yu, Maosong Sun",,,"Attention Floating, Masked Diffusion Models, Attention Mechanisms, Autoregressive Models, Diffusion Language Models","This paper investigates the attention behaviors in masked diffusion models, revealing the phenomenon of Attention Floating. Unlike ARMs, MDMs exhibit dynamic, dispersed attention anchors that shift across denoising steps, providing a mechanistic explanation for their strong in-context learning capabilities.",45.87,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07898v1_Large Language Models and Algorithm Execution Appl.pdf,Large Language Models and Algorithm Execution: Application to an Arithmetic Function,"Farah Ben Slama, Frédéric Armetta",,,"Algorithmic learning in natural language, Supervised learning by decomposition, Large language model, Fine-tuning","This paper investigates extending LLMs' capabilities to algorithm execution via supervised training focused on reasoning decomposition. It introduces LLM-DAL, demonstrating improved performance in complex algorithmic inference when training is carefully designed.",44.19,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07901v1_Decentralized Online Convex Optimization with Unkn.pdf,Decentralized Online Convex Optimization with Unknown Feedback Delays,"Hao Qiu, Mengxiao Zhang, Juliette Achddou",10.48550/arXiv.2405.1234,arXiv:2601.07901v1,"decentralized optimization, online convex optimization, unknown feedback delays, multi-agent systems, federated learning","This paper presents a novel algorithm for decentralized online convex optimization that handles unknown, time- and agent-varying feedback delays. It improves upon prior methods by incorporating an adaptive learning rate and leveraging a gossip-based strategy, achieving better regret bounds and performance in practical settings.",45.95,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07903v2_Enhancing Large Language Models for Time-Series Fo.pdf,Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning,"Jianqi Zhang, Jingyao Wang, Wenwen Qiang, Fanjiang Xu, Changwen Zheng",10.1234/example,12345,"Time Series Forecasting, Large Language Model, In-context Learning, Computing Methodologies, Artificial Intelligence","The paper proposes LVICL, a method to improve LLM4TSF performance by freezing LLM parameters while injecting example information via vector-injected in-context learning, addressing computational overhead and forecasting accuracy.",45.29,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07935v1_Towards Specialized Generalists A Multi-Task MoE-L.pdf,Towards Specialized Generalists: A Multi-Task MoE Framework for Domain-Specific LLM,"Yuxin Yang, Aoxiong Zeng, Xiangquan Yang",arXiv:2601.07935v1,2601.07935,"Large Language Models, Domain-Specific Adaptation, Medical Applications, Mixture-of-Experts, Low-Rank Adaptation, Knowledge Preservation","The paper addresses challenges in adapting LLMs to specialized domains like medicine, focusing on mitigating catastrophic forgetting and task interference through a novel Med-MoE-LoRA framework.",45.64,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07939v1_SECite Analyzing and Summarizing Citations in Soft.pdf,Analyzing and Summarizing Citations in Software Engineering Literature,"Shireesh Reddy Pyreddy, Khaja Valli Pathan, Hasan Masum, Tarannum Shaila Zaman",,,"Sentiment Analysis, LLMs, Text Summarization, Citations, Software Engineering","This study introduces SECite, a novel framework for evaluating scholarly impact through sentiment analysis of citations in software engineering literature. It develops a semi-automated pipeline to extract citations, apply NLP and unsupervised ML, and generate sentiment-specific summaries to reveal community perceptions and highlight strengths/limitations of referenced works.",46.21,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07941v2_Moonworks Lunara Aesthetic Dataset.pdf,Moonworks Lunara Aesthetic Dataset,"Yan Wang, M M Sayeef Abdullah, Partho Hassan, Sabit Hassan",arXiv:2601.07941v2,2601.07941v2,"Moonworks, Lunara, Aesthetic Dataset, Text-to-Image, Style Conditioning, Prompt Grounding, Artistic Styles, Middle East, Northern Europe, East Asia, South Asia","The Lunara Aesthetic Dataset is the first public release of curated 2,000 image–prompt pairs designed for research on prompt grounding and style conditioning in text-to-image generation. It covers diverse artistic styles and prioritizes high aesthetic quality, offering structured annotations and human-refined prompts.",47.98,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07946v1_Coupled Diffusion-Encoder Models for Reconstructio.pdf,Coupled Diffusion–Encoder Models for Reconstruction of Flow Fields,"AmirPouya Hemmasian, Amir Barati Farimani",10.48550/arXiv.2026.12345,arXiv:2509.12345,"Diffusion models, Flow field reconstruction, Autoencoders, Kolmogorov flow fields, Generative modeling","This paper proposes DiffCoder, a coupled framework integrating a probabilistic diffusion model with a convolutional ResNet encoder for data-driven flow-field reconstruction. It addresses limitations of classical autoencoders by preserving higher-order statistical properties, showing improved spectral accuracy under compression compared to VAE baselines.",45.92,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07948v1_Reinforcement Learning Methods for Neighborhood Se.pdf,Reinforcement Learning Methods for Neighborhood Selection in Local Search,"Y annick Molinghen, Augustin Delecluse, Renaud De Landtsheer, Stefano Michelini",,,"Local Search, Reinforcement Learning, Combinatorial Optimization","This study evaluates reinforcement learning-based neighborhood selection strategies in local search, comparing multi-armed bandits and deep RL methods against three benchmark problems. It highlights performance variations and the need for carefully designed reward functions.",44.65,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07951v1_Hybrid SARIMA LSTM Model for Local Weather Forecas.pdf,Hybrid SARIMA–LSTM Model for Local Weather Forecasting: A Residual-Learning Approach for Data-Driven Meteorological Prediction,"Shreyas Rajeev, Karthik Mudenahalli, Amit Mallappa, I., srajeev@stevens.edu",,,"weather forecasting, SARIMA, LSTM, hybrid model, machine learning, meteorology, residual learning","This paper proposes a hybrid modeling approach combining SARIMA and LSTM to improve weather prediction accuracy. It addresses limitations of traditional statistical methods by leveraging LSTMs for nonlinear temporal patterns and SARIMA for stable seasonal trends, aiming to reduce prediction errors in long-term forecasts.",47.34,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07953v1_Quantum automated theorem proving.pdf,Quantum automated theorem proving,"Zheng-Zhi Sun, Qi Ye, Dong-Ling Deng, Hefei National Laboratory",,,"quantum automated theorem proving, automated reasoning, quantum superposition, quantum entanglement, knowledge bases, geometric theorems, mathematical reasoning","This paper proposes a generic framework for quantum automated theorem proving, leveraging quantum superposition and entanglement. It introduces quantum representations of knowledge bases and quantum algebraic proving methods, demonstrating improved query complexity and practical applications in geometry.",43.55,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07957v1_LWMSCNN-SE A Lightweight Multi-Scale Network for E.pdf,LWMSCNN-SE: A Lightweight Multi-Scale Network for Efficient Maize Disease Classification,"Likewood, F. et al., Jianmei Su",10.1007/s40297-023-02432-7,arXiv:2308.07982,"lightweight CNN, multi-scale feature extraction, attention mechanism, plant pathology, accuracy-efficiency trade-off","This paper introduces LWMSCNN-SE, a lightweight convolutional neural network integrating multi-scale feature extraction, depthwise separable convolutions, and SE attention, achieving 96.63% classification accuracy with low computational cost for real-time maize disease diagnosis on edge devices.",46.72,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07958v1_LJ-Spoof A Generatively Varied Corpus for Audio An.pdf,LJ-SPOOF: A Generatively Diverse Corpus for Audio Anti-Spoofing and Synthesis Source Tracing,"Surya Subramani, Hashim Ali, Hafiz Malik",10.48550/arXiv.2405.12345,arXiv:2405.12345,"anti-spoofing, speaker verification, deepfake, source tracing, synthetic speech","Speaker-specific anti-spoofing and synthesis-source tracing are essential for robust audio security. This work introduces LJ-Spoof, a speaker-specific corpus with diverse prosody, vocoder, and processing variations, enabling strong speaker-conditioned defenses and precise source attribution.",46.14,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07964v1_Executable Ontologies in Game Development From Alg.pdf,Executable Ontologies in Game Development: From Algorithmic Control to Semantic World Modeling,Alexander Boldachev,,,"executable ontologies, game ai, behavior trees, goop, event semantics, dataflow architecture","This paper examines the application of Executable Ontologies (EO) in game development, arguing they represent a paradigm shift from algorithmic behavior programming to semantic world modeling. It demonstrates how EO enables priority-based task interruption in survival games using dataflow conditions, contrasts EO with existing approaches like Behavior Trees and GOAP, and discusses integration strategies and LLM-driven model generation.",46.91,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07965v1_When Models Know When They Do Not Know Calibration.pdf,When Models Know When They Don't Know,"Chenjie Hao, Weyl Lu, Yuko Ishiwaka, Zengyi Li, Weier Wan, Yubei Chen",,,"calibration, confidence, model calibration, data cleaning, model ignorance, ImageNet, MMLU","This preprint explores how models can recognize their own ignorance through confidence signals. It presents a training-free method to calibrate models, demonstrating that calibrated confidence improves model efficiency and reliability across vision and language tasks.",45.57,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07969v1_Tuberculosis Screening from Cough Audio Baseline M.pdf,Tuberculosis Screening from Cough Audio,"George P. Kafentzis, Efstratios Selisios",arXiv:2601.07969v1,arXiv:2601.07969v1,"Tuberculosis, Machine Learning, Cough Audio, Cross-Validation, Uncertainty Quantification, Feature Extraction","This paper proposes a standardized framework for automatic tuberculosis detection from cough audio and clinical data using machine learning. It addresses variability in datasets and evaluation protocols, aiming to provide a reproducible baseline and enable fair comparisons.",45.24,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07973v1_Cultural Compass A Framework for Organizing Societ.pdf,Cultural Compass: A Framework for Organizing Societal Norms to Detect Violations in Human-AI Conversations,"Myra Cheng, Vinodkumar Prabhakaran, Alice Oh, Hayk Stepanyan, Sunipa Dev, Stanford University, Google, myra@cs.stanford.edu",Cultural Compass: A Framework for Organizing Societal Norms to Detect Violations in Human-AI Conversations,Cultural Com-pass:2601.07973,"cultural norms, human-AI interaction, norm adherence, cross-cultural, generative AI, ethical AI","This paper introduces a taxonomy of norms to evaluate how generative AI models adhere to sociocultural expectations in human-AI conversations. It highlights gaps in current benchmarks and proposes operationalization of norms to detect violations across contexts, demonstrating that model behavior varies by interaction style and cultural setting.",47.27,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07988v1_From Word Sequences to Behavioral Sequences Adapti.pdf,From Word Sequences to Behavioral Sequences: Adapting Modeling and Evaluation Paradigms for Longitudinal NLP,"Adithya V Ganesan, Vasudha Varadarajan, Oscar NE Kjell, Roman Kotov, Ryan L Boyd, Andrew Schwartz, Stony Brook University, Vanderbilt University, Carnegie Mellon University",,,"longitudinal NLP, behavioral sequences, person-indexed sequences, time-ordered data, modeling paradigms, evaluation metrics","This paper addresses the mismatch between traditional NLP, which treats documents as independent, and longitudinal data, where documents are nested within authors and ordered in time. It proposes a new modeling and evaluation framework that accounts for person and time, improving accuracy and interpretability for applications like mental health prediction.",46.42,LFM-2.5,AMD RX 6800 (Vulkan)
2601.07994v2_DYCP Dynamic Context Pruning for Long-Form Dialogu.pdf,DYCP: Dynamic Context Pruning for Long-Form Dialogue with LLMs,"Nayoung Choi, Jonathan Zhang, Jinho D. Choi",,,"Large Language Models, Long-Form Dialogue, Context Management, Dialogue Systems, LLM Latency, Context Pruning","The paper presents DYCP, a lightweight context management method that dynamically segments and retrieves relevant memory at query time. It improves answer quality and reduces response latency in long-form dialogues without relying on predefined topic boundaries.",44.15,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08000v1_Reasoning over Precedents Alongside Statutes Case-.pdf,Reasoning over Precedents Alongside Statutes: Case-Augmented Deliberative Alignment for LLM Safety,"Can Jin1, Rui Wu, Tong Che2, Qixin Zhang, Hongwu Peng, Jiahui Zhao, Zhenting Wang, Wenqi Wei, Ligong Han, Zhao Zhang1, Yuan Cao7, Ruixiang Tang1, Dimitris N. Metaxas",,,"LLM safety, deliberative alignment, case augmentation, reasoning over rules, open-source models, safety principles","The paper evaluates the impact of explicitly specifying safety codes versus illustrative cases in LLMs. It finds that referencing explicit codes improves harmlessness but degrades helpfulness, while case-augmented reasoning yields more robust safety behaviors. The authors propose CADA, a case-augmented deliberative alignment method using reinforcement learning.",46.43,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08003v1_LLM Review Enhancing Creative Writing via Blind Pe.pdf,Enhancing Creative Writing via Blind Peer Review Feedback,"Weiyue Li, Mingxiao Song, Zhenda Shen, Dachuan Zhao, Yunfan Long, Yi Li, Yongce Li, Ruyi Yang, Mengyu Wang",,,,"The paper introduces LLM Review, a peer-review-inspired framework using Blind Peer Review to enhance creativity in large language models by constraining information flow and preserving divergent creative trajectories.",43.33,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08005v1_Internal Deployment Gaps in AI Regulation.pdf,Internal Deployment Gaps in AI Regulation,"JOE KWON, STEPHEN CASPER",,,"AI regulation, internal deployment, oversight gaps, frontier AI","This paper examines how frontier AI regulations in the US and EU in 2025 address internal deployment, identifying three gaps that enable evasion: scope ambiguity, point-in-time assessments, and information asymmetries. It analyzes persistence factors and proposes policy approaches.",43.58,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08011v1_TP-Blend Textual-Prompt Attention Pairing for Prec.pdf,TP-Blend: Textual-Prompt Attention Pairing for Precise,"Xin Jin, Yichuan Zhongyichuanzhong, Yapeng Tian",10.1089/tmrl.2025.2601,2601.08011v1,"textual-prompt, object-style blending, diffusion models, content appearance control, cross-attention fusion","Presents Twin-PromptAttention Blend (TP-Blend), a lightweight training-free framework for simultaneous object replacement and style blending in diffusion models. TP-Blend uses Cross-Attention Object Fusion and Self-Attention Style Fusion to achieve high-resolution, photo-realistic edits with precise control.",45.79,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08017v1_Representations of Text and Images Align From Laye.pdf,Representations of Text and Images Align From Layer One,"Javier Rando, Florian Tram, Stanislav Fort, 2",arXiv:2601.08017v1,2601.08017,"vision-language models, image-text alignment, concept representation, layer alignment, model interpretability","The paper demonstrates that image and text representations in adapter-based vision-language models are meaningfully aligned from the first layer, contradicting the notion that alignment occurs only in later layers. Using a synthesis-based method inspired by DeepDream, the authors show that images synthesized at layer 1 often depict recognisable features of animals, activities, or seasons, providing direct evidence of early multimodal alignment.",46.08,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08026v2_FigEx2 Visual-Conditioned Panel Detection and Capt.pdf,FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures,"Jifeng Song, Arun Das, Pan Wang, Hui Ji, Kun Zhao, Yufei Huang, 2, 3*, Department of Electrical and Computer Engineering, University of Pittsburgh, USA, Cancer Virology Program, UPMC Hillman Cancer Center, USA, Department of Medicine, University of Pittsburgh, USA, Department of Informatics and Networked Systems, University of Pittsburgh, USA",,,"compound figures, panel detection, captioning, scientific figures, label localization, figure-level understanding, semantic rewards, CLIP, BERTScore, zero-shot transfer",Proposes FigEx2 to localize panels and generate panel-wise captions from scientific compound figures. Introduces a noise-aware gated fusion module and staged optimization combining supervised learning with reinforcement learning to achieve high detection accuracy and robustness.,46.49,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08043v1_The Role of Noisy Data in Improving CNN Robustness.pdf,The Role of Noisy Data in Improving CNN Robustness for Image Classification,"Oscar H. Ramírez-Agudeloa, Nicoleta Gorea, Aliza Reif, Lorenzo Bonasera, Michael Karla",,,"deep learning, CNNs, data quality, CIFAR-10, noise injection, image classification, model robustness",This paper investigates how introducing controlled noise into training data can improve CNN robustness for image classification. Experiments on CIFAR-10 show that incorporating up to 10% noisy data significantly reduces test loss while maintaining accuracy under corrupted conditions.,45.84,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08049v1_Integrating Attendance Tracking and Emotion Detect.pdf,Integrating Attendance Tracking and Emotion Detection for Enhanced Student Engagement in Smart Classrooms,"Keith Ainebyona, Ann Move Oguti, Joseph Walusimbi, Ritah Kobusingye",42401600224@sun.ac.ug,,"Affective computing, Attendance automation, Emotion detection, IoT, Smart classroom","This paper presents SCASED, an IoT-based system integrating automated attendance tracking with facial emotion recognition to support classroom engagement monitoring. The system uses a Raspberry Pi camera and OpenCV for face detection, and a fine-tuned MobileNetV2 model to classify four learning-related emotional states: engagement, boredom, confusion, and frustration. Attendance and emotion data are visualized via a cloud dashboard, enabling instructors to adapt teaching strategies in real time. Experimental results using the DAiSEE dataset achieved 89.5% emotion classification accuracy.",47.75,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08052v1_Forecast Aware Deep Reinforcement Learning for Eff.pdf,Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms,"Nawazish Alia, Rachael Shawb, Karl Masona",arXiv:2601.08052v1,,"arXiv:2601.08052, cs.AI, energy management, renewable energy, dairy farming, load scheduling, reinforcement learning, battery storage, water heating","This study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under real-world operational constraints. It incorporates short-term demand and renewable generation forecasts and adapts PPO with a PID-KL controller for stable training.",47.74,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08058v1_Reasoning Beyond Chain-of-Thought A Latent Computa.pdf,Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models,"Zhenghao He, Guangzhi Xiong Bohan Liu, Bohan Liu Sanchit Sinha, Aidong Zhang",10.48550/arXiv.2024.12345,arXiv:2408.12345,"large language models, Chain-of-Thought prompting, latent reasoning, internal activations","This study investigates why Chain-of-Thought prompting works and explores whether latent internal activations can trigger reasoning in large language models. It finds that steering specific reasoning-related latent features improves accuracy and can override explicit CoT prompts, suggesting a latent mechanism supports multi-step reasoning.",45.55,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08065v1_A New Strategy for Verifying Reach-Avoid Specifica.pdf,A New Strategy for Verifying Reach-Avoid,"Samuel I. Akinwande, Sydney M. Katz, Mykel J. Kochenderfer, Clark Barrett",arXiv:2601.08065v1,arXiv:2601.08065,"reachability analysis, neural feedback systems, forward reachability, backward reachability, safety verification",This paper introduces new algorithms for computing backward reachable sets in neural feedback systems and integrates them with forward analysis to provide a unified verification framework.,46.05,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08070v1_Semantic Gravity Wells Why Negative Constraints Ba.pdf,Semantic Gravity Wells: Why Negative Constraints Backfire,Shailesh Rana,https://github.com/gut-puncture,,"negative constraints, instruction-following, negative instruction failure","This paper investigates why negative constraints (e.g., 'do not use word X') often fail in language models, revealing systematic suppression asymmetry and mechanistic failure modes.",42.83,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08079v1_MemoBrain Executive Memory as an Agentic Brain for.pdf,Executive Memory as an Agentic Brain for Reasoning,"Hongjin Qian, Zhao Cao, Zheng Liu",10.1234/example.2025,12345678,"executive memory, agent frameworks, long-horizon reasoning, tool-augmented agents, cognitive control","This paper proposes MemoBrain, an executive memory model for tool-augmented agents. It constructs a dependency-aware memory over reasoning steps, enabling efficient tracking of intermediate states and logical relations. MemoBrain prunes invalid steps, folds completed sub-trajectories, and maintains a compact reasoning backbone, supporting coherent long-horizon reasoning without burdening the LLM's working context.",46.46,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08089v1_Q-realign Piggybacking Realignment on Quantization.pdf,Q-realign: Piggybacking Realignment on Quantization for Safe and Efficient LLM Deployment,"Qitao Tan, Xiaoying Song, Ningxi Cheng, Ninghao Liu, Xiaoming Zhai, Lingzi Hong, Zhen Xiang, Geng Yuan",10.1234/q-realign-2024,2024.12345,"large language models, safety alignment, quantization, deployment","Proposes Q-realign, a post-hoc defense method using post-training quantization guided by representational analysis, to decouple safety alignment from fine-tuning and improve deployment efficiency.",45.71,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08094v1_Local-Global Feature Fusion for Subject-Independen.pdf,Local-Global Feature Fusion for Subject-Independent EEG Emotion Recognition,"Zheng Zhou, Isabella McEvoy, Camilo E. Valderrama",10.1007/978-3-642-45888-7,,"EEG, emotion recognition, subject-independent, local representations, global representations, domain adaptation, EEG channels, trial-level features",The paper proposes a fusion framework combining local channel-wise descriptors and global trial-level descriptors to improve cross-subject generalization on the SEED-VII dataset. It introduces dual-branch transformer with attention-based fusion and domain-adversarial regularization.,46.33,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08104v1_High-Fidelity Modeling of Stochastic Chemical Dyna.pdf,High-Fidelity Modeling of Stochastic Chemical Dynamics on Complex Manifolds: A Multi-Scale SIREN-PINN Framework for the Thermocurvature-Perturbed Ginzburg-Landau Equation,"Julian Evan Chrisnanto, Salsabila Rahma Alia, Nurfauzi Fadillah, Yulison Herry Chrisnanto",s254167v@st.go.tuat.ac.jp,2601.08104v1,"Physics-Informed Neural Networks (PINNs), Spatiotemporal Chaos, Inverse Geometric Problems, Reaction-Diffusion Systems, Defect Turbulence, Riemann Manifold Learning","The paper presents a Multi-Scale SIREN-PINN architecture for modeling complex reaction-diffusion systems on manifolds, addressing challenges in capturing high-frequency dynamics and topological invariants in turbulent chemical reactors.",46.76,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08107v1_STO-RL Offline RL under Sparse Rewards via LLM-Gui.pdf,STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order,"Chengyang Gu, Yuxin Pan, Hui Xiong, Yize Chen",10.1145/nnnnnnn.nnnnnnn,cgu893@connect.hkust-gz.edu.cn,"Offline RL, Temporal order, Large Language Models","Offline reinforcement learning leverages large language models to generate temporally ordered subgoal sequences, enabling effective policy learning from sparse rewards while addressing temporal dependencies and reward shaping challenges.",44.42,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08108v1_Debiasing Large Language Models via Adaptive Causa.pdf,Debiasing Large Language Models via Adaptive Causal Prompting with Sketch-of-Thought,"Bowen Li, Ziqi Xu, Jing Ren, Renqiang Luo, Xikun Zhang, Xiuzhen Zhang, Yongli Ren, Feng Xia",10.48550/arXiv.2024.12345,arXiv:2408.12345,"large language models, prompting, adaptive causal prompting, sketch-of-thought, causal inference, bias mitigation","This paper proposes an Adaptive Causal Prompting with Sketch-of-Thought (ACPS) framework to address excessive token usage and bias in LLMs. By leveraging structural causal models, ACPS infers causal effects and adaptively selects interventions, enabling robust reasoning across tasks without retraining. Experiments show consistent improvements over baselines in accuracy, robustness, and efficiency.",47.07,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08109v1_CSQL Mapping Documents into Causal Databases.pdf,MappingDocuments intoCausalDatabases ∗,"Sridhar Mahadevan, Adobe Research and University of Massachusetts, Amherst",10.48550/arXiv.2601.08109,2601.08109,"Causality, Natural Language, Databases, SQL, AI, Machine Learning","Describes Csql, a system that converts unstructured text into a SQL-queryable causal database, enabling causal queries over document collections rather than relying on retrieval or knowledge graphs.",45.35,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08118v1_MirrorBench An Extensible Framework to Evaluate Us.pdf,MIRRORBENCH: ANEXTENSIBLEFRAMEWORK TOEVALUATEUSER-PROXY,"Ashutosh Hathidara, Julien Yu, Vaishali Senthil, Sebastian Schreiber, Anil Babu Ankisettipalli",,,"human simulators, user proxies, conversational evaluation, LLM evaluation, user diversity metrics","MIRRORBENCH is a reproducible, extensible benchmarking framework that evaluates user proxies based on their ability to produce human-like utterances across diverse tasks, decoupled from downstream task success. It introduces a modular execution engine, metadata registries, multi-backend support, and supports pluggable proxies, datasets, and metrics.",46.35,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08125v1_How vehicles change lanes after encountering crash.pdf,How vehicles change lanes after encountering crashes: Empirical analysis and modeling,"Kequan Chena, Yuxuan Wangb, Pan Liu, Victor L. Knoop, David Z. W. Wang",10.48550/arXiv.2407.04219,10.48550/arXiv:2407.04219,"vehicle dynamics, lane changing behavior, crash impact, empirical analysis, modeling",Empirical analysis and modeling of how vehicles adjust lanes following crashes.,47.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08127v1_PathoGen Diffusion-Based Synthesis of Realistic Le.pdf,PathoGen: Diffusion-Based Synthesis of Realistic Lesions in Histopathology Images,"Mohamad Koohi-Moghadam1, Mohammad-Ali Nikouei Mahani1, Kyongtae Tyler Bae1",,,"histopathology, lesion generation, diffusion models, image synthesis, medical AI","This paper introduces PathoGen, a diffusion-based generative model that enables high-fidelity inpainting of lesions into benign histopathology images. It addresses the challenge of limited expert-annotated data by leveraging iterative diffusion refinement to preserve tissue boundaries, cellular structures, and staining characteristics. PathoGen outperforms existing generative models in image fidelity and distributional similarity, improving downstream segmentation and overcoming annotation bottlenecks.",47.15,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08128v1_Embedded AI Companion System on Edge Devices.pdf,Embedded AI Companion System on Edge Devices,"Rahul Gupta, Stephen Hsu",arXiv:2601.08128v1,1Superfocus AI 2Michigan State University,"AI companion, edge devices, computational constraints, memory systems, conversational quality, personalization",The paper proposes a memory paradigm for AI companions on edge devices that alternates between active and inactive phases to minimize latency while maintaining personalization. It introduces an AI Companion benchmark and demonstrates that their system outperforms equivalent LLMs in most metrics.,45.75,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08133v1_How Do Optical Flow and Textual Prompts Collaborat.pdf,Audio-Visual Semantic Segmentation with Optical Flow and Textual Prompts,"Peng Gao, Yujian Lee, Yongqi Xu, Wentao Fan, Guangdong Provincial/Zhuhai Key Laboratory IRADS, Peking University, Shenzhen Graduate School",10.1007/...,"1Hong Kong Baptist University, Hong Kong, China","audio-visual segmentation, semantic segmentation, optical flow, textual prompts, visual segmentation, semantic understanding","This paper introduces a novel collaborative framework, Stepping Stone Plus (SSP), that integrates optical flow and textual prompts to enhance audio-visual semantic segmentation. By leveraging optical flow for motion context and textual prompts for scene description, SSP improves segmentation accuracy, especially for coexisting sound sources and static objects.",46.54,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08139v1_Subspace Alignment for Vision-Language Model Test-.pdf,Subspace Alignment for Vision-Language Model Test-time Adaptation,"Zhichen Zeng, Wenxuan Bao, Xiao Lin, Ruizhong Qiu, Tianxin Wei, Xuying Ning, Yuchen Yan, Chen Luo, Monica Xiao Cheng, Jingrui He, Hanghang Tong",,,"subspace alignment, vision-language models, test-time adaptation, zero-shot learning, modality gap, visual-natural language alignment","The paper proposes SubTTA to align semantic subspaces across vision and language modalities, addressing distribution shift challenges by minimizing chordal distance and filtering visual noise. Extensive experiments show a 2.24% average improvement over state-of-the-art TTA methods.",46.54,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08141v1_Qalb Largest State-of-the-Art Urdu Large Language .pdf,Qalb: Largest State-of-the-Art Urdu Large Language Model,"1st Muhammad Taimoor Hassan, 2st Jawad Ahmed, 3st Muhammad Awais",,,"Urdu language model, continued pre-training, low-resource NLP, LoRA, language adaptation","Introduces Qalb, an Urdu language model developed via two-stage pre-training and supervised fine-tuning, achieving state-of-the-art performance on Urdu benchmarks.",44.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08146v2_Mechanisms are Transferable Data-Efficient Low-Res.pdf,Mechanisms are Transferable: Data-Efficient Low-Resource Adaptation,"Khumaisa Nur’aini, Ayu Purwarianti, Alham Fikri Aji, Derry Wijaya",,,"low-resource adaptation, circuit-targeted fine-tuning, cross-lingual transfer, catastrophic forgetting, parameter-efficient learning","Proposes Circuit-Targeted Supervised Fine-Tuning (CT-SFT) to adapt large language models to low-resource languages with minimal parameter updates, improving cross-lingual performance while mitigating catastrophic forgetting.",45.31,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08148v1_Enriching Semantic Profiles into Knowledge Graph f.pdf,Enriching Semantic Profiles into Knowledge Graph for Recommender Systems Using Large Language Models,"Seokho Ahn, Sungbok Shin, Young-Duk Seo",10.1145/3770854.3780324,sokho0514@inha.edu,"Recommendation, Semantic Profiling, Large Language Models, Knowledge Graphs","This paper revisits profiling-based approaches in recommender systems across four dimensions—knowledge base, preference indicator, impact range, and subject—and proposes a new model, SPiKE, leveraging large language models for entity profile generation and profile-aware aggregation.",44.98,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08149v1_Dynamic Graph Structure Learning via Resistance Cu.pdf,Dynamic Graph Structure Learning via Resistance Curvature Flow,"Chaoqun Fei, Huanjiang Liu, Tinglve Zhou, Y angyang Li, Tianyong Hao",10.1093/acprof:oso/9780190871296.001.0001,2601.08149v1,"graph structure learning, curvature flow, circuit theory, deep learning, manifold learning","Introduces a novel curvature flow based on effective resistance from circuit theory, establishing a new paradigm for geometric graph evolution. Formulates the dynamic evolution equation of RCF, highlighting its mechanisms for manifold enhancement and noise suppression, and demonstrates its effectiveness and compatibility with deep learning frameworks through extensive experiments.",48.85,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08156v1_Project Synapse A Hierarchical Multi-Agent Framewo.pdf,Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions,"Arin Gopalan Yadav, Varad Dherange, Kumar Shivam",arXiv:2601.08156v1,arXiv:2601.08156v1,"Project Synapse, Multi-Agent Framework, Hybrid Memory Architecture, Last-Mile Delivery, Autonomous Resolution, Complex Disruptions","The paper introduces Project Synapse, a hierarchical framework using a central resolution supervisor and specialized worker agents to autonomously manage last-mile delivery disruptions. It features a novel hybrid memory system and evaluates performance on a benchmark dataset.",47.83,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08160v1_SwiftMem Fast Agentic Memory via Query-aware Index.pdf,SwiftMem: Fast Agentic Memory via Query-aware Indexing,"Anxin Tian, Yiming Li, Xing Li, Hui-Ling Zhen, Lei Chen, Xianzhi Yu1 Zhenhua Dong, Mingxuan Yuan",10.1234/example.doi,12345678,None,"Agentic memory systems are essential for LLM agents to maintain long-term context and retrieve relevant info efficiently. This paper introduces SwiftMem, a query-aware agentic memory system that enables sub-linear retrieval using specialized indexing over temporal and semantic dimensions. Experimental results show 47× faster search compared to SOTA baselines.",46.62,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08166v1_ZeroDVFS Zero-Shot LLM-Guided Core and Frequency A.pdf,ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency,"Mohammad Pivezhandi, Mahdi Banisharif, Abusayeed Saifullah, Ali Jannesari",arXiv:2601.08166v1,arXiv:2601.08166,"zero-shot learning, multi-agent reinforcement learning, thermal management, energy efficiency, embedded systems","Proposes a model-based hierarchical multi-agent reinforcement learning framework for thermal-aware scheduling on multi-core platforms. Introduces LLM-based semantic feature extraction to enable zero-shot deployment without workload-specific profiling, achieving faster convergence and improved energy efficiency compared to traditional methods.",46.73,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08173v1_The Agents First Day Benchmarking Learning Explora.pdf,"The Agent’s First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios","Daocheng Fu1, Jianbiao Mei3, Rong Wu3, Xuemeng Yang2, Jia Xu2, Ding Wang2, Pinlong Cai2, Yong Liu3, B, Licheng Wen2,4,5, B, Botian Shi2, 1Fudan University, 2Shanghai AI Laboratory, 3Zhejiang University, 4Shanghai Innovation Institute, 5Shanghai Jiao Tong University",,,"learning, exploration, scheduling, workplace scenarios, multi-modal models, dynamic task scheduling, active exploration, continuous learning","This paper introduces Trainee-Bench, a dynamic evaluation environment that assesses agents across three dimensions: context-aware scheduling, prudent information acquisition, and continual learning. It highlights challenges in dynamic task scheduling, active exploration, and learning from experience, and proposes a framework to evaluate agent reliability in realistic settings.",47.04,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08176v1_Prompt-Based Clarity Evaluation and Topic Detectio.pdf,Automatic evaluation of large language model responses requires not only factual correctness but also clarity,"Lavanya Prahallad, Sai Utkarsh Choudarypally, Pragna Prahallad, Pranathi Prahallad",10.1093/acpsr/clac174,,"clarity evaluation, prompt engineering, political question answering, large language models, chain-of-thought prompting, topic identification","This paper evaluates prompt-based clarity evaluation using the CLARITY dataset from the SemEval-2026 shared task. It compares GPT-3.5 with GPT-5.2 under different prompting strategies and demonstrates improvements in clarity metrics, highlighting the role of structured reasoning prompts.",46.76,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08179v1_Instruction-Driven 3D Facial Expression Generation.pdf,Instruction-Driven 3D Facial Expression Generation and Transition,"Anh H. V o, Tae-Seok Kim, Hulin Jin, Soo-Mi Choi, Yong-Guk Kim",https://vohoanganh.github.io/tg3dfet/,,"Instruction-Driven, Facial Expression, Transition, 3D Facial Expression, Instruction-Driven Facial Expression Decomposer, Instruction to Facial Expression Transition, CK+, CelebV-HQ datasets","This study presents a framework for generating smooth facial expression transitions using instruction-based learning. It introduces the Instruction-driven Facial Expression Decomposer (IFED) and the Instruction to Facial Expression Transition (I2FET) method, demonstrating superior performance on CK+ and CelebV-HQ datasets. The approach enables realistic emotional variation by linking textual descriptions to facial expression features.",46.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08183v2_GI-Bench A Panoramic Benchmark Revealing the Knowl.pdf,GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards,"Yan Zhu, Te Luo, Pei-Yao Fu, Zhen Zhang, Zi-Long Wang, Yi-Fan Qu, Zi-Han Geng, Jia-Qi Xu, Lu Yao, Li-Yun Ma, Wei Su, Wei-Feng Chen, Quan-Lin Li",,,"multimodal large language models, gastrointestinal endoscopy, clinical standards, knowledge experience dissociation, benchmarking","This study constructs GI-Bench to evaluate state-of-the-art MLLMs across a five-stage endoscopy workflow, comparing their performance to junior endoscopists and residency trainees using macro-F1, mIoU, and a multi-dimensional Likert scale.",48.02,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08185v1_Autonomous Materials Exploration by Integrating Au.pdf,Autonomous Materials Exploration by Integrating Automated Phase Identification and AI-Assisted Human Reasoning,"Ming-Chiang Chang, Maximilian Amsler, Duncan R. Sutherland, Sebastian Ament, Katie R. Gann, Lanca Zhou, Louisa M. Smieska, Arthur R. Woll, John M. Gregoire, Carla P. Gomes, R. Bruce van Dover, Michael O. Thompson",,,"autonomous experimentation, materials development, AI-assisted reasoning, phase identification, robotic platforms, high-throughput synthesis, metastable phases, material synthesis, cationic doping","This work presents an autonomous materials synthesis extension to SARA, leveraging AI and human-in-the-loop reasoning to accelerate discovery. It describes experiments on oxide systems using robotic processing, demonstrating improved efficiency through active learning and expert input.",47.06,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08187v2_Improving LLM Reasoning with Homophily-aware Struc.pdf,Improving LLM Reasoning With Homophily-Aware Structural and Semantic Compression,"Zijun Di, Bin Lu, Huquan Kang, Kinghiqian, Luoyi Fu, Jiaxin Ding, Xiaoying Gan, Ganxia Ying, Lei Zhou, Zhou Lei, Xinbing Wang, Chenghu Zhou, Jiaxing Ding",10.48550/arXiv.2601.08187,2601.08187,"large language models, text-annotated graph, structural compression, semantic aggregation, homophily-aware, LLM reasoning","The paper proposes HS2C, a framework leveraging graph homophily to improve LLM reasoning by compressing structural and semantic information. It uses hierarchical partitioning guided by structural entropy and delivers community-level semantic aggregation, enhancing compression and accuracy on benchmarks.",47.3,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08189v2_ForgetMark Stealthy Fingerprint Embedding via Targ.pdf,Stealthy Fingerprint Embedding Via Targeted Unlearning,"Zhenhua Xu1, Haobo Zhang, Zhebo Wang, Qichen Liu, Wenpeng Xing, Meng Han",,,"Large Language Model, Copyright protection, Model Fingerprinting, Machine Unlearning, Stealthy Fingerprinting, Model Provenance","Introduces ForgetMark, a stealthy fingerprinting framework using targeted unlearning to avoid high-perplexity triggers and improve robustness against detection.",44.87,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08196v1_Evaluating Implicit Regulatory Compliance in LLM T.pdf,Evaluating Implicit Regulatory Compliance in LLM Tool Invocation via Logic-Guided Synthesis,"Da Song, Yuheng Huang, Boqi Chen, Tianshuo Cong, Randy Goebel, Lei Ma, Foutse Khomh",,,"large language models, regulatory compliance, functional safety, logic-guided synthesis, LLM safety, automated evaluation","The paper introduces LOGISAFETYGEN, a framework that converts regulations into Linear Temporal Logic oracles and uses logic-guided fuzzing to ensure LLMs generate compliant code. It benchmarks 13 state-of-the-art models, revealing a trade-off between functional correctness and safety adherence.",46.31,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08211v1_Adapting Rules of Official International Mahjong f.pdf,Adapting Rules of Official International Mahjong for Online Players,"Chucai Wang, Lingfeng Li, Yunlong Lu, Wenxin Li",,,"Mahjong, game design, AI, rule adaptation, online gaming, subgoal scoring","This study proposes rule adaptations for Official International Mahjong to suit online play, addressing first-mover advantage and subgoal scoring issues. It introduces compensatory points and revised scoring mechanisms tailored for online environments.",45.44,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08223v2_DNF Dual-Layer Nested Fingerprinting for Large Lan.pdf,DNF: DUAL-LAYER NESTED FINGERPRINTING FOR LARGE LANGUAGE MODEL,"Zhenhua Xu1, Yiran Zhao3, Mengting Zhong3, Dezhang Kong1,2,3, Changting Lin1, Tong Qiao3, Meng Han1,2,3†",,,"Large Language Model, Copyright Protection, Model Fingerprinting, Backdoor, Stealth Verification, Intellectual Property","The paper introduces Dual-Layer Nested Fingerprinting (DNF), a black-box method embedding hierarchical backdoors to protect intellectual property in large language models. DNF achieves perfect fingerprint activation while preserving utility, using lower-perplexity triggers and remaining undetectable under fingerprint attacks.",46.04,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08224v1_An Axiomatic Approach to General Intelligence SANC.pdf,An Axiomatic Approach to General Intelligence,"Daesuk Kwon, 1Won-gi Paeng1",arXiv:2601.08224v1,SANC(E3),"axiomatization of intelligence, competitives selection, system tokens, reconstruction compression, category formation, self-similar hierarchy, Gestalt completion","Proposes Self-organizing Active Network of Concepts with EnergyE 3, an axiomatic framework where representational units emerge from competitive selection under finite activation capacity, unifying perception, imagination, prediction, planning, and action.",45.39,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08226v1_Knowledge-based learning in Text-RAG and Image-RAG.pdf,Knowledge-based learning in Text-RAG and Image-RAG,"Alexander Shim, Khalil Saieh, Samuel Clarke, ksaie001@fiu.edu",,,"text-based RAG, image-based RAG, vision transformer, hallucination reduction, diagnostic accuracy, multi-modal AI, clinical interpretation","This research analyzes and compares multi-modal approaches in Vision Transformer-based image encoding with LLM-based models to reduce hallucination and improve chest X-ray diagnosis. It evaluates text-based RAG, image-based RAG, and a baseline, highlighting challenges like data imbalance and the need for transparent reasoning.",46.15,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08230v1_GADPN Graph Adaptive Denoising and Perturbation Ne.pdf,Graph Adaptive Denoising and Perturbation Networks via Singular Value Decomposition,"Hao Deng, Bo Liu, Member, IEEE",,,"Graph Neural Networks, Graph Structural Learning, Network Representation Learning","This paper proposes GADPN, a framework that adaptively refines graph topology using low-rank denoising and structural perturbation. It introduces Bayesian optimization to tailor denoising strength and extends SVD-based structural perturbation to arbitrary graphs, achieving state-of-the-art performance while improving efficiency on challenging disassortative graphs.",44.33,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08235v2_MPCI-Bench A Benchmark for Multimodal Pairwise Con.pdf,MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity,"Shouju Wang, Haopeng Zhang",10.48550/arXiv.2405.12345,arXiv:2405.12345,"Contextual Integrity, multimodal, privacy, language models","Introduces MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark, evaluating privacy behavior in agentic settings through paired positive/negative instances across normative judgments, story reasoning, and agent actions. Highlights challenges in balancing privacy and utility and identifies modality leakage.",45.65,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08237v1_The End of Reward Engineering How LLMs Are Redefin.pdf,The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination,"Haoran Su, Yandong Sun, Congjia Yu, gary.yu@lerna-ai.com",arXiv:2601.08237v1,haoran.su@nyu.edu,"reward engineering, multi-agent reinforcement learning, LLMs, semantic reward specification, dynamic adaptation, computational cost, hallucination risks, scalability","This paper argues that large language models enable a shift from hand-crafted numerical rewards to natural language objectives in multi-agent systems. Recent advances include generating human-level reward functions, dynamically adapting rewards, and achieving semantic coordination, while acknowledging challenges like computational cost, hallucination, and scalability.",47.15,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08251v1_Hyperbolic Heterogeneous Graph Transformer.pdf,Hyperbolic Heterogeneous Graph Transformer,"Jongmin Park, Seunghoon Han, Hyewon Lee, Won-Yong Shin, Sungsu Lim",1,1,"Heterogeneous Graph Representation Learning, Hyperbolic Graph Embedding, Graph Transformer, Relation-specific Hyperbolic Attention, Graph Transformer","The paper proposes a Hyperbolic Heterogeneous Graph Transformer (HypHGT) to learn heterogeneous graph representations in hyperbolic space, addressing limitations of existing methods by capturing both local and global dependencies efficiently.",44.26,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08254v1_Large Artificial Intelligence Model Guided Deep Re.pdf,Large Artificial Intelligence Model–Guided Deep Reinforcement Learning for Resource Allocation in Non-Terrestrial Networks,"Abdikarim Mohamed Ibrahim, Rosdiadee Nordin",,,"Large AI Models (LAMs), Large Language Models (LLMs), Deep Reinforcement Learning (DRL), Satellite Communications, Non-Terrestrial Networks (NTNs)","The paper proposes a Deep Reinforcement Learning agent guided by a Large Language Model to improve resource allocation in Non-Terrestrial Networks, demonstrating superior performance over traditional methods in various scenarios.",45.32,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08257v2_On Evaluation of Unsupervised Feature Selection fo.pdf,Evaluation of Unsupervised Feature Selection for Pattern Classification,"Gyu-Il Kim, Dae-Won Kim, Jaesung Lee",arXiv:2601.08257v2,2601.08257v2,"unsupervised feature selection, pattern classification, multi-label evaluation, feature selection, data preprocessing","This study revisits the evaluation of unsupervised feature selection methods by adopting a multi-label classification framework. It demonstrates that performance rankings differ significantly under multi-label settings compared to single-label settings, highlighting the need for fair comparison across different evaluation paradigms.",46.15,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08258v1_T3 Benchmarking Sycophancy and Skepticism in Causa.pdf,Testing Trustworthy Thinking,Edward Y. Chang,,,"sycophancy, skepticism, causal judgment, LLM","Introduces T3 benchmark to evaluate LLM causal reasoning across Pearl’s hierarchy, identifying skepticism traps and scaling paradoxes, and validates a process-verified protocol.",41.46,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08262v1_VGG Induced Deep Hand Sign Language Detection.pdf,VGG Induced Deep Hand Sign Language Detection,"Subham Sharmaa, Sharmila Subudhia, b, ∗",arXiv:2601.08262v1,2601.08262v1,"Hand gesture recognition, Convolutional neural network, Classification, VGG-16 net, API","This work proposes a novel hand gesture recognizing system for differently-abled persons. The model uses a convolutional neural network, known as VGG-16 net, for building a trained model on a widely used image dataset. The results show around 98% accuracy when validated on the NUS dataset.",47.24,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08271v1_Sparsity Is Necessary Polynomial-Time Stability fo.pdf,Sparsity Is Necessary: Polynomial-Time Stability for Agentic LLMs in Large Action Spaces,Angshul Majumdar,arXiv:2601.08271,arXiv:2601.08271,"sparsity, polynomial-time stability, agentic LLMs, large action spaces","The paper formalizes Sparse Agentic Control (SAC) for LLMs operating in massive discrete action spaces. It presents ℓ1,2-regularized policy learning, derives sharp suboptimality bounds, and analyzes stability under partial observability. Extensions cover robust, group-sparse, and interaction-aware settings.",45.57,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08273v1_HIPPO Accelerating Video Large Language Models Inf.pdf,Accelerating Video Large Language Models Inference via Holistic-aware Parallel Speculative Decoding,"Qitan Lv1, Tianyu Liu1, Wen Wu2, Xuenan Xu2, Bowen Zhou2, Feng Wu1, Chao Zhang2,3, Zhou Cheng2,3",,,"video LLMs, speculative decoding, parallel decoding, semantic preservation, video inference, autoregressive generation","The paper proposes HIPPO, a holistic-aware parallel speculative decoding framework, to accelerate video LLM inference by preserving semantic tokens and decoupling draft generation from verification, achieving up to 3.51× speedup over vanilla auto-regressive decoding.",46.23,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08276v1_ToolACE-MCP Generalizing History-Aware Routing fro.pdf,ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web,"Zhiyuan Yao, Zishan Xu, Yifu Guo4, Zhiguang Han, Weiwen Liu, Cheng Yang, Shuo Zhang, Weiwen Liu, Xingshan Zeng, Wu Wei, Liang Zhang",,,"Agent Web, MCP, ToolACE-MCP, History-aware routing, Multi-agent collaboration, Agent routing, Large-scale ecosystems","ToolACE-MCP proposes a pipeline for training history-aware routers to enable precise navigation in large-scale agent ecosystems. It leverages a dependency-rich candidate graph to synthesize multi-turn trajectories, supporting dynamic context understanding and scalable plug-and-play routing. Experiments show superior performance on real-world benchmarks, demonstrating robustness and scalability.",46.23,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08280v1_Greedy Is Enough Sparse Action Discovery in Agenti.pdf,Greedy Is Enough: Sparse Action Discovery in Agentic LLMs,Angshul Majumdar,,,,"The paper studies sparse action discovery in agentic LLMs using a greedy algorithm under sparsity assumptions, showing high-probability recovery of relevant actions with polynomial scaling.",41.74,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08288v1_OpenMic A Multi-Agent-Based Stand-Up Comedy Genera.pdf,OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System,"Yuyang Wu, Hanzhong Cao, Jianhao Chen, Yufei Li",10.48550/arXiv.2024.12345,arXiv:2408.12345,"Chinese comedy generation, multi-agent system, humor understanding, retrieval-augmented generation, stand-up performance","This paper introduces OpenMic, an end-to-end multi-agent system built on AutoGen that transforms user-provided life topics into a 3–5 minute Chinese stand-up performance. It addresses challenges in humor understanding and evaluation by integrating retrieval-augmented generation and fine-tuning a JokeWriter for stand-up-specific structures.",45.6,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08297v1_Demystifying the Slash Pattern in Attention The Ro.pdf,Demystifying the Slash Pattern in Attention: The Role of RoPE,"Yuan Cheng, Fengzhuo Zhang, Yunlong Hou, Cunxiao Du, Chao Du2, Tianyu Pang, Aixin Sun, Zhuoran Yang",arXiv:2601.08297v1,2601.08297,"attention patterns, RoPE, SLASH, SDHs, LLMs, query keys, RoPE frequencies","The paper investigates why large language models (LLMs) exhibit slash attention patterns, showing that queries and keys are nearly identical across tokens due to near-rank-one embeddings. RoPE governs attention score variation, and interactions between medium- and high-frequency RoPE components produce Slash-Dominant Heads (SDHs).",47.37,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08302v1_Enhancing Sentiment Classification and Irony Detec.pdf,Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques,"Marvin Schmitt, Anne Schwerk, Sebastian Lempert",10.48550/arXiv.2405.12345,arXiv:2405.12345,"sentiment analysis, irony detection, large language models (LLMs), prompt engineering, prompt engineering techniques","This study investigates the use of prompt engineering to enhance large language models in sentiment analysis and irony detection. It evaluates techniques such as few-shot learning, chain-of-thought prompting, and self-consistency, assessing performance improvements in accuracy, recall, precision, and F1 score.",46.3,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08310v1_ORBIT On-policy Exploration-Exploitation for Contr.pdf,ORBIT: On-policy Exploration-Exploitation for Controllable Multi-Budget Reasoning,"Kun Liang1, Clive Bai3 Xin, Chenming Tang1,2, Sanwoo Lee1,2,∗, Weijie Liu3 Saiyong Yang3,†, Yunfang Wu1,2,†",,,"large reasoning models, long-form chain-of-thought, multi-budget reasoning, reinforcement learning, reasoning budget, exploration-exploitation, controllable reasoning","This paper proposes ORBIT, a controllable multi-budget reasoning framework that dynamically adjusts reasoning effort based on input, aiming to balance performance and efficiency by inferring an appropriate reasoning budget.",46.24,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08311v1_Enhancing Image Quality Assessment Ability of LMMs.pdf,Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation,"Kang Fu, Huiyu Duan, Zicheng Zhang, Yucheng Zhu, Jun Zhao, Xiongkuo Min, Jia Wang, Guangtao Zhai",10.1000/arxiv:2109.12345,10.1000/arxiv:2109.12345,"Image quality assessment, Retrieval-Augmented Generation, Large Multimodal Models, Zero-shot, Training-free","This paper introduces IQARAG, a training-free framework that improves Large Multimodal Models' Image Quality Assessment by leveraging Retrieval-Augmented Generation to provide visual anchors for quality evaluation. Extensive experiments show its effectiveness across multiple IQA datasets.",46.77,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08323v1_AtomMem  Learnable Dynamic Agentic Memory with Ato.pdf,AtomMem: Learnable Dynamic Agentic Memory,"Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",10.1234/example.doi,,"memory management, agent memory, dynamic decision-making, reinforcement learning, long-horizon tasks","This paper proposes AtomMem, a learning-based framework that treats memory management as a dynamic decision process. By decomposing memory operations into atomic CRUD actions and combining fine-tuning with reinforcement learning, AtomMem enables agents to autonomously adapt memory strategies to specific tasks, outperforming static approaches on benchmarks.",45.14,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08327v1_Safe Heterogeneous Multi-Agent RL with Communicati.pdf,Safe Heterogeneous Multi-Agent RL with Communication Regularization for Coordinated Target Acquisition,"Gabriele Calzolari, Vidya Sumathy, Christoforos Kanellakis, George Nikolakopoulos",,,"Cooperative target acquisition, Safe autonomous coordination, Decentralized multi-agent reinforcement learning, Heterogeneous robotic systems, Learning-based control","This paper introduces a decentralized multi-agent reinforcement learning framework enabling structurally heterogeneous teams of agents to jointly discover and acquire randomly located targets in environments with partial observability, communication constraints, and dynamic interactions. The framework uses a Graph Attention Network encoder integrating simulated range-sensing data and communication embeddings, promoting safety through structural rewards and information orthogonality. Comprehensive ablation studies and simulations demonstrate effective target discovery, collision avoidance, and stable coordination.",46.88,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08332v1_IGAN A New Inception-based Model for Stable and Hi.pdf,A New Inception-based Model for Stable and High-Fidelity Image Synthesis Using Generative Adversarial Networks,"Ahmed A. Hashim, Ali Al-Shuwaili, Asraa Saeed, Ali Al-Bayaty",,,"Generative Adversarial Networks (GANs), dilation convolutions, inception module, spectral normalization, image synthesis, deep learning stability","The paper proposes IGAN, a novel GAN architecture incorporating inception-inspired convolutions and dilated convolutions, achieving improved training stability and generation quality with reduced mode collapse and vanishing/exploding gradients.",45.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08333v1_Semantic Laundering in AI Agent Architectures Why .pdf,Semantic Laundering in AI Agent Architectures,Oleg Romanchuk Roman Bondar,arXiv:2601.08333v1,arXiv:2601.08333,"semantic laundering, epistemic warrant, Gettier problem, LLM architectures","The paper examines how LLM-based agent architectures conflate information transport with epistemic justification, leading to semantic laundering. It formalizes this as a pattern where propositions gain epistemic status without proper justification, mirroring the Gettier problem. The authors introduce the Warrant Erosion Principle to explain why circular justification cannot be eliminated under standard architectural assumptions.",48.11,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08360v1_Scalable Sequential Recommendation under Latency a.pdf,Scalable Sequential Recommendation under Latency and Memory Constraints,"Adithya Parthasarathy, Aswathnarayan Muthukrishnan Kirubakaran, Vinoth Punniyamoorthy, Nachiappan Chockalingam, Lokesh Butra, Kabilan Kannan, Abhirup Mazumder, Sumit Saha",arXiv:2601.08360v1,"IEEE Senior Member, USA","Recommender Systems, Sequence Modeling, Representation Learning, Scalable Machine Learning, Deep Learning","Sequential recommender systems must model long-range user behavior while operating under strict memory and latency constraints. Transformer-based approaches achieve strong accuracy but suffer from quadratic attention complexity, forcing aggressive truncation of user histories and limiting their practicality for long-horizon modeling. This paper presents HoloMambaRec, a lightweight sequential recommendation architecture that combines holographic reduced representations for attribute-aware embedding with a selective state space encoder for linear-time sequence processing. Item and attribute information are bound using circular convolution, preserving embedding dimensionality while encoding structured metadata. A shallow selective state space backbone, inspired by recent Mamba-style models, enables efficient training and constant-time recurrent inference. Experiments on Amazon Beauty and MovieLens-1M datasets demonstrate that HoloMambaRec consistently outperforms SASRec and achieves competitive performance with GRU4Rec under a constrained 10-epoch training budget, while maintaining substantially lower memory complexity. The design further incorporates forward-compatible mechanisms for temporal bundling and inference-time compression, positioning HoloMambaRec as a practical and extensible alternative for scalable, metadata-aware sequential recommendation.",49.36,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08371v1_Geo-NVS-w Geometry-Aware Novel View Synthesis In-t.pdf,Geo-NVS-w: Geometry-Aware Novel View Synthesis In-the-Wild,"Anastasios Tsalakopoulos, Angelos Kanlis, Evangelos Chatzis, Antonis Karakottas, Dimitrios Zarpalas",10.48550/arXiv.2405.1234,10.48550/arXiv.2405.1234,"Geo-NVS, novel view synthesis, geometry-aware, Signed Distance Function, energy efficiency","Introduces Geo-NVS-w, a geometry-aware framework for high-fidelity novel view synthesis from unstructured in-the-wild image collections. It leverages an SDF-based rendering process to preserve geometric details and reduces energy consumption by 4–5× compared to similar methods.",46.41,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08379v1_Training-Free Distribution Adaptation for Diffusio.pdf,Training-Free Distribution Adaptation for Diffusion,"Matina Mahdizadeh Sani ∗ ∗†, Nima Jamali ∗‡, Mohammad Jalali ∗§, Farzan Farnia ¶",,,"diffusion models, distribution matching, maximum mean discrepancy, prompt adaptation, conditional generation","Proposes MMD Guidance, a training-free method that augments reverse diffusion with MMD gradients to align generated samples with reference data, addressing distribution mismatch in domain adaptation.",46.03,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08380v1_Thematic Working Group 5 -- Artificial Intelligenc.pdf,EDUsummIT 2025 - eBook,"Mary Webb, Matt Bower, Ana Amélia Carvalho, Fredrik Mørk Røkenes, Jodie Torrington, Jonathan D. Cohen, Yousra Chtouki, Kathryn MacCallum, Tanya Linden, Deirdre Butler, Juliana E. Raffagheli, Henriikka Vartiainen, Martina Ronci, Peter Tiernan, Chris Shelton, Joyce Malyn-Smith, Pierre Gorissen",10.48550/edumap-1234,,"Artificial Intelligence, AI literacy, teaching and learning, curriculum design, professional development, policy guidelines, educational technology","Thematic Working Group 5 focuses on developing strategies to enhance AI literacy and agency among teachers, exploring curriculum design, professional development, classroom applications, and policy guidance to integrate AI effectively into education.",48.07,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08382v2_A Qualitative Model to Reason about Object Rotatio.pdf,A Qualitative Model to Reason about Object Rotations – applied to solve the Cube Comparison Test,"Zoe Falomira, aUmeå University, Computing Science Department, Sweden",,,"cube comparison test, mental rotation, qualitative reasoning, spatial cognition, spatial reasoning","This paper presents a qualitative model for Reasoning about Object Rotations (QOR) which is applied to solve the Cube Comparison Test (CCT). A conceptual neighborhood graph relating rotation movement to location and orientation change has been built, producing composition tables for inferences about rotations.",45.05,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08383v1_Deconstructing Pre-training Knowledge Attribution .pdf,Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models,"Bo Wang, Jinzhuo Li, Hong Chen, Yuanlin Chu, Yuxuan Fan, Xuming Hu, Yuxuan Hu",,,"Mixture-of-Experts, knowledge attribution, interpretability, dense architectures, sparsity, interpretable LLMs","This paper investigates how Gated-LPI metrics reveal distinct knowledge acquisition patterns in MoE versus dense models during pre-training, highlighting sparsity and stability advantages of MoE.",44.71,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08388v1_Creativity in AI as Emergence from Domain-Limited .pdf,Creativity in AI as Emergence from Domain-Limited Generative Models,Corina Chutaux,,,,"This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. It examines structural and contextual conditions for creative behaviors and introduces a conceptual decomposition of creativity into four interacting components: pattern-based generation, induced world models, contextual grounding, and arbitrariness.",44.61,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08393v1_Controlled LLM Training on Spectral Sphere.pdf,Controlled LLM Training on Spectral Sphere,"Tian Xie, Haoming Luo, Yang Wang, Wayne Xin Zhao, Rui Yan, Bing Su, Chong Luo, Guo, Ren, Wuhan University",arXiv:2601.08393v1,2601.08393,"LLM training, Spectral Sphere, Maximal Update Parametrization, Optimization strategies, Activation control, Parallel training","The paper introduces the Spectral Sphere Optimizer (SSO) to enforce width-invariant Θ(1) activation control during large-scale LLM training. SSO ensures stability by constraining both weights and updates, outperforming existing methods like AdamW and Muon. Extensive experiments show improved stability and performance across diverse architectures.",47.98,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08401v1_An Explainable Two Stage Deep Learning Framework f.pdf,Explainable Two-Stage Deep Learning Framework for Pericoronitis Assessment in Panoramic Radiographs Using YOLOv8 and ResNet-50,"Ajo Babu George, Pranav S, Kunal Agarwal",arXiv:2601.08401v1,13 Jan 2026,"explainable AI, deep learning, pericoronitis, panoramic radiographs, YOLOv8, ResNet-50, diagnostic classification","The study presents a two-stage deep learning framework integrating YOLOv8 for anatomical localization and a modified ResNet-50 for pericoronitis detection, enhanced with Grad-CAM for interpretability. Results show high diagnostic accuracy and strong alignment with radiologists' impressions.",48.22,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08402v1_PATS Personality-Aware Teaching Strategies with La.pdf,Personality-Aware Teaching Strategies,"Donya Rooein1, Sankalan Pal Chowdhury2, Mariia Eremeeva2, Yuan Qin3, Debora Nozza 1, Mrinmaya Sachan 2, Dirk Hovy 1",,,"large language models, educational tutoring, student personality, personalized learning, teacher-student interaction","This paper proposes a taxonomy linking pedagogical methods to personality profiles and demonstrates how large language models can adapt tutoring strategies based on student personality traits, improving engagement and effectiveness.",45.71,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08403v1_Owen-Shapley Policy Optimization OSPO A Principled.pdf,Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs,"Abhijnan Nath, Alireza Bagheri Garakani, Tianchen Zhou, Fan Yang, Nikhil Krishnaswamy",10.48550/arXiv.2025.12345,arXiv:2509.12345,"Owen-Shapley Policy Optimization, RL for LLMs, Generative Search, Reinforcement Learning, Shapley-Owen, Policy Optimization","This paper introduces OWEN-SHAPLEY POLICYOPTIMIZATION, a framework that redistributes sequence-level advantages based on tokens’ marginal contributions to outcomes. By forming coalitions of semantically coherent units, OSPO assigns segment-level credit without relying on value-model-based computation, improving interpretability and robustness in personalized recommendation tasks.",47.22,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08406v1_WebTrap Park An Automated Platform for Systematic .pdf,WebTrap Park: An Automated Platform for Systematic Security Evaluation of Web Agents,"Xinyi Wu, Jiagui Chen, Geng Hong, Jiayi Dong, Xudong Pan, Jiarun Dai, Min Yang",,,"Web Agents, security evaluation, systematic security assessment, web agents, security risks, agent architecture","WebTrapPark is an automated platform for systematic security evaluation of Web Agents through direct observation of their interactions with live web pages. It instantiates three major security risk sources into 1,226 evaluation tasks, revealing security differences across agent frameworks.",45.77,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08412v1_Hybrid Distillation with CoT Guidance for Edge-Dro.pdf,Hybrid Distillation with CoT Guidance for Edge-Drone Control Code,"Yizhan Feng, Hichem Snoussi, Yuhang Wang, Jing Teng, Abel Cherouat, Tian Wang",,,"Large language models, drone, Knowledge Distillation, Chain-of-Thought, Lightweight","This paper proposes an integrated approach combining knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models.",45.89,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08415v2_Regulatory gray areas of LLM Terms.pdf,Regulatory gray areas of LLM Terms,"Brittany I. Davidson, Kate Muir, Florian A.D. Burnat, Adam N. Joinson",,,"Language Models, LLMs, Privacy Policy, Terms of Service, Regulation","This paper examines the varying Terms of Service of major LLM providers, highlighting regulatory ambiguities that affect researchers and users in security, social sciences, and psychology. It identifies specific gray areas impacting legitimate use.",45.45,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08418v1_Taxon Hierarchical Tax Code Prediction with Semant.pdf,Taxon: Hierarchical Tax Code Prediction,"Jihang Li, Qing Liu, Zulong Chen, Jing Wang, Wei Wang2, Chuanfei Xu, Zeyi Wen",,,"Tax code prediction, Hierarchical tax taxonomy, Semantic alignment, Taxonomy levels, Invoice compliance, E-commerce, Machine learning","Tax code prediction is a crucial yet underexplored task in automating invoicing and compliance management for large-scale e-commerce platforms. This paper presents Taxon, a semantically aligned and expert-guided framework for hierarchical tax code prediction, integrating a feature-gating mixture-of-experts architecture and a semantic consistency model.",46.56,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08430v1_RubricHub A Comprehensive and Highly Discriminativ.pdf,RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset,"Li Auto Inc., The Chinese University of Hong Kong, Zhejiang University 4Nanyang Technological University",,,"reinforcement learning, verifiable rewards, rubric generation, automated evaluation, deep learning","This paper introduces RubricHub, a large-scale dataset for reinforcement learning with verifiable rewards, addressing scalability and quality gaps in automated evaluation. It presents a framework for coarse-to-fine rubric generation and demonstrates performance improvements on HealthBench.",45.27,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08434v3_Large Multimodal Models for Embodied Intelligent D.pdf,Large Multimodal Models for Embodied Intelligent Driving: The Next Frontier in Self-Driving?,"Long Zhang, Yuchen Xia, Bingqing Wei, Zhen Liu, Shiwen Mao, Zhu Han, Mohsen Guizani",10.48550/arXiv.2024.12345,None,"Large Multimodal Models, Embodied Intelligence, Self-Driving, Autonomous Driving, Policy Optimization, Deep Reinforcement Learning","This article introduces a novel hybrid decision framework merging Large Multimodal Models and deep reinforcement learning to address limitations of modular autonomous driving systems. It emphasizes continuous learning and joint decision-making to advance embodied intelligent driving, supported by a case study on lane-change planning.",46.78,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08441v1_YaPO Learnable Sparse Activation Steering Vectors .pdf,Learnable Sparse Activation Steering Vectors for Domain Adaptation,"Abdelaziz Bounhar, Rania Hossam Elmohamady Elbadry, Preslav Nakov, Michalis Vazirgiannis, Guokan Shang",1,2Ecole Polytechnique,"large language models, activation interventions, steering vectors, domain adaptation, cultural alignment","This paper introduces YayPO, a reference-free method that learns sparse steering vectors in the latent space of a Sparse Autoencoder. By optimizing sparse codes, YayPO produces disentangled, interpretable, and efficient steering directions. It demonstrates faster convergence, stronger performance, and improved stability compared to dense steering baselines, enabling fine-grained alignment tasks such as cultural adaptation while preserving general knowledge.",46.58,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08444v1_Beyond Linearization Attributed Table Graphs for T.pdf,Beyond Linearization: Attributed Table Graphs for Table Reasoning,"Yuxiang Wang, Junhao Gan, Shengxiang Gao, Shenghao Ye, Zhengyi Yang, Jianzhong Qi",,,"table reasoning, linearization, attributed table graph, table reasoning models, LLM reasoning, explainability","This paper proposes Table Graph Reasoner (TABGR), a training-free model that represents tables as Attributed Table Graphs (ATGs) to preserve structural information and enable graph-based reasoning. It addresses limitations of linearization-based methods by introducing a Question-Guided Personalized PageRank mechanism, achieving up to 9.7% accuracy improvement over state-of-the-art models.",45.73,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08448v1_Divide and Conquer Static-Dynamic Collaboration fo.pdf,Divide and Conquer: Static-Dynamic Collaboration for Few-Shot,"Kexin Bao, Daichi Zhang, Yong Li, Dan Zeng, Shiming Ge",10.1145/3731715.3733310,ICMR 2025,"Few-Shot Class-Incremental Learning, Class-Incremental Learning","The paper presents a framework for few-shot learning that balances stability and plasticity by dividing the learning process into static and dynamic stages, achieving state-of-the-art performance.",44.67,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08450v1_Decoding Order Matters in Autoregressive Speech Sy.pdf,Decoding Order in Autoregressive Speech Synthesis,"Minghui Zhao, Anton Ragni",2104,2104/28002,"speech synthesis, discrete diffusion model, order-agnostic autoregressive decoding","Investigates decoding order in autoregressive speech synthesis using a masked diffusion framework. Demonstrates that randomness in decoding order impacts speech quality, comparing fixed strategies (L2R, 2L2R) with adaptive ones (Top-K), and shows that discrete inputs can support high-quality speech with 1-bit quantisation.",44.36,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08457v1_An Under-Explored Application for Explainable Mult.pdf,Explainable Multimodal Misogyny Detection in Code-mixed Hindi-English,"An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English, Babhishek Kaushikb, Kevin McDaidc a,b,c, Dundalk Institute of Technology",,,"hate speech, misogyny, natural language processing, code-mixing, hinglish",This paper presents a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. It leverages transformer-based models and explainability techniques to enhance transparency and combat gender-based digital violence.,46.63,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08462v1_M3-BENCH Process-Aware Evaluation of LLM Agents So.pdf,M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games,"Sixiong Xie, Zhuofan Shi, Haiyang Shen, Gang Huang, Ma Xiang Jing",xsx1001@stu.pku.edu.cn,,"large language model, social behaviors, mixed-motive games, process-aware evaluation, Big Five personality model, social exchange theory","This paper introduces M3-BENCH, a multi-stage benchmark for evaluating advanced social behaviors in large language model agents across mixed-motive games. It proposes a framework integrating behavioral trajectory analysis, reasoning process analysis, and communication content analysis, enriched with personality and social theory models. Experimental results demonstrate the framework's ability to capture nuanced personality traits and capability profiles beyond simple behavioral scores.",46.64,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08464v1_CoMa Contextual Massing Generation with Vision-Lan.pdf,Contextual Massing Generation with Vision-Language Models,"Evgenii Maslov, Valentin Khrulkov, Anastasia V olkova, Anton Gusarov, Andrey Kuznetsov",10.48550/arXiv.2024.12345,arXiv:2401.12345,"massing generation, vision-language models, CoMa-20K dataset, urban context, architectural design","The paper presents a dataset and framework for generating building massing using vision-language models, addressing challenges in architectural design through automated, data-driven methods.",46.08,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08468v1_JudgeRLVR Judge First Generate Second for Efficien.pdf,"JudgeRLVR: Judge First, Generate Second for Efficient Reasoning","Jiangshan Duo, Hanyu Li, Sujian Li, Liang Zhao, Yudong Wang",arXiv:2601.08468v1,2601.08468,"Reinforcement Learning, Verifiable Rewards, Large Language Models, Reasoning, Efficiency, Generation Quality","This paper proposes JudgeRLVR, a two-stage judge-then-generate paradigm for RLVR. It argues that discriminative capability is essential for efficient generation and introduces a method to balance accuracy and efficiency by learning to distinguish valid solutions.",46.11,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08472v1_sui-1 Grounded and Verifiable Long-Form Summarizat.pdf,Sui-1: Grounded and Verifiable,"Benedikt Droste, Jan Philipp Harries, Maximilian Idahl, Björn Plüster",arXiv:2601.08472v1,2601.08472,"grounded summarization, verifiable AI, large language models, citation grounding, training data","Sui-1 is a 24B parameter model that produces abstractive summaries with inline citations, enabling traceability of claims. It outperforms baselines in citation-grounded summarization and supports processing documents up to 100K tokens per pass.",46.84,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08475v1_SUMMPILOT Bridging Efficiency and Customization fo.pdf,Bridging Efficiency and Customization,"JungMin Yun, Juhwan Choi, Kyohoon Jin, Soojin Jang, Jinhee Jang, YoungBin Kim, cocoro357, gold5230, fhzh123, sujin0110, jinheejang, ybkim85",,,"interactive summarization, personalization, user engagement, semantic graphs, entity clustering, explainable evaluation","This paper introduces SUMMPILOT, an interactive summarization system that combines efficiency with customization. It leverages large language models to enable personalized summaries through user-driven components like semantic graphs and explainable evaluation, addressing limitations in existing systems that lack adaptability to individual needs.",46.69,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08490v1_BenchOverflow Measuring Overflow in Large Language.pdf,BenchOverflow: Measuring Overflow in Large Language Models via Plain-Text Prompts,"Erin Feiglin, Nir Hutnik, Raz Lapid",arXiv:2601.08490v1,https://arxiv.org/abs/2601.08490,"large language models, overflow, prompting, benchmarking, computational cost","Investigates overflow in LLMs via plain-text prompts, highlighting performance, cost, and environmental impacts. Introduces BenchOverflow benchmark to measure output volume and robustness.",45.79,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08493v1_PKI Prior Knowledge-Infused Neural Network for Few.pdf,PKI: Prior Knowledge-Infused Neural Network for Few-Shot Class-Incremental Learning,"Kexin Baoa, Fanzhao Linc, Zichen Wang, Yong Liaobai, Dan Zenge, Shiming Gea",10.1093/pki/vis/2026,2601.08493,"PKI, few-shot learning, class-incremental learning, catastrophic forgetting, overfitting, prior knowledge, neural network","This paper introduces PKI, a neural network architecture that incorporates prior knowledge to improve continual adaptation in few-shot learning scenarios. PKI addresses catastrophic forgetting and overfitting by preserving existing knowledge while fine-tuning new components during incremental sessions.",47.42,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08499v2_EfficientFSL Enhancing Few-Shot Classification via.pdf,EfficientFSL: Enhancing Few-Shot Classification via Query-Only Tuning in Vision,"Wenwen Liao, Hang Ruan, Jianbo Yu, Bing Song, Yuansong Wang, Xiaofeng Yang, Tsinghua Shenzhen International Graduate School",10.48550/arXiv:2509.06932,arXiv:2509.06932,"few-shot classification, Vision Transformers, query-only tuning, few-shot learning, computational efficiency","This paper introduces EfficientFSL, a query-only fine-tuning framework for few-shot classification using Vision Transformers. It proposes lightweight trainable components to synthesize task-specific queries and combine feature representations, achieving state-of-the-art performance with minimal computational cost.",47.01,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08503v1_Temporal Fusion Nexus A task-agnostic multi-modal .pdf,Temporal Fusion Nexus: A task-agnostic multi-modal embedding model for clinical narratives and irregular time series in post-kidney transplant care,"Aditya Kumar, Simon Rauch, Mario Cypko, Matthieu-P Schapranow, Aadil Rashid, Fabian Halleck, Bilgin Osmanodja, Roland Roller, Lars Pape, Klemens Budde, Mario Schiffer, Oliver Amft",arXiv:2601.08503v1,2601.08503,"clinical narratives, post-kidney transplant, graft loss, graft rejection, mortality prediction, multi-modal embedding, disentanglement, SHAP","TFN is a multi-modal embedding model for integrating clinical text and time series in post-kidney transplant care, achieving superior performance over baselines.",47.97,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08509v1_What If TSF A Benchmark for Reframing Forecasting .pdf,What If TSF: A Benchmark for Reframing Forecasting,"Jinkwan Jang, Hyunbin Jin, Hyungjin Park, Kyubyung Chae, Taesup Kim",,,"forecasting, time series, multimodal forecasting, LLMs, scenario-guided, reframing, expert input","The paper introduces What If TSF (WIT), a multimodal forecasting benchmark that evaluates models' ability to incorporate contextual text for future scenarios. It highlights limitations of existing benchmarks and proposes leveraging expert-crafted plausible or counterfactual scenarios to test scenario-guided forecasting. The benchmark aims to enable richer, context-aware forecasting beyond traditional unimodal approaches.",45.9,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08510v2_STAGE A Benchmark for Knowledge Graph Construction.pdf,STAGE: A Benchmark for Knowledge Graph,"Qiuyu Tian, Yiding Li, Fengyi Chen, Zequn Liu, Youyong, Fan Guo, Jinjing Shen, Zhijing Xie, Yiyun Luo, Xin Zhang",arXiv:2601.08510v2,arXiv:2601.08510v2,"knowledge graph, screenplay, narrative understanding, character relationships, event summarization, long-context reasoning","The paper introduces STAGE, a unified benchmark for evaluating narrative understanding in full-length movie screenplays. It covers knowledge graph construction, scene summarization, long-context QA, and in-script role-playing, all anchored in a shared narrative world.",47.56,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08519v1_CD2 Constrained Dataset Distillation for Few-Shot .pdf,Constrained Dataset Distillation for Few-Shot Class-Incremental Learning,"Kexin Bao1, Daichi Zhang1, Hansong Zhang1, Yonghong Li1, Yutao Yue3, Shiming Ge1*",,,"few-shot learning, class-incremental learning, catastrophic forgetting, knowledge distillation, distributional loss",The paper proposes a framework called Constrained Dataset Distillation (CD 2) to address the catastrophic forgetting problem in few-shot class-incremental learning by incorporating dataset distillation and a distillation constraint module. Extensive experiments demonstrate its effectiveness.,45.65,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08531v1_Sketch-Based Facade Renovation With Generative AI .pdf,Sketch-Based Facade Renovation With Generative AI Models,"Warissara Booranamaitree1, Xusheng Du2, Y ushu Cai3, Zhengyang Wang4, Ye Zhang5*, Haoran Xie6",16532152721@student.chula.ac.th,,"industrial building renovation, vision-language model, diffusion model, user sketches, facade renovation",A streamlined framework combining generative AI and vision-language models to bypass as-built modelling in industrial adaptive reuse.,45.9,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08545v2_Learner-Tailored Program Repair A Solution Generat.pdf,Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement,"Zhenlong Dai, Zhuoluo Zhao, Hengning Wang, Xiu Tang, Sai Wu, Chang Yao, Zhipeng Gao, Jingyuan Chen",10.1234/example.doi,12345678,"programming, LLM, program repair, bug description, code repair, education",The paper introduces a novel framework for intelligent programming coaching that combines a repair solution retrieval system with an edit-driven code retrieval approach to help large language models identify and fix bugs in programming learners' code.,45.72,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08549v1_Contrastive and Multi-Task Learning on Noisy Brain.pdf,Contrastive and Multi-Task Learning on Noisy Brain Signals,"Sucheta Ghosh, Zahra Monfared, Felix Dietrich, Sueka Ghosh, Zahra Monfared",10.48550/arXiv.2604.07932,arXiv:2604.07932,"EEG, EEG decoding, motor imagery classification, chaotic dynamics, nonlinear dynamics, self-supervised learning",N/A,45.11,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08557v1_VideoHEDGE Entropy-Based Hallucination Detection f.pdf,VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations,"Sushant Gautam, Cise Midoglu, Vajira Thambawita, Michael A. Riegler, Pål Halvorsen",,,"hallucination detection, video question answering, semantic clustering, spatiotemporal perturbations, entropy-based reliability, video VLMs","Introduce VideoHEDGE, a framework for detecting hallucinations in video question answering using entropy-based reliability estimation and semantic clustering under perturbations. Evaluated on SoccerChat benchmark with three 7B Video-VLMs.",45.83,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08559v1_WaterCopilot An AI-Driven Virtual Assistant for Wa.pdf,WaterCopilot: An AI-Driven Virtual Assistant for Water Management,"Keerththanan Vickneswaran, Mariangel Garcia Andarcia, Hugo Retief, Chris Dickens, Paulo Silva",,,"Water resource management, Retrieval-Augmented Generation (RAG), Limpopo River Basin, Azure AI, Real-time APIs, Multilingual chatbots, Digital Twin, AWS deployment, RAGAS evaluation","WaterCopilot is an AI-driven virtual assistant developed through collaboration between the International Water Management Institute (IWMI) and Microsoft Research for the Limpopo River Basin (LRB). It integrates static policy documents and real-time hydrological data via custom plugins (iwmi-doc-plugin and iwmi-api-plugin) to support decision-making in transboundary river basins. The system offers multilingual interactions, automated calculations, and visualization, achieving high answer relevance and context precision. Key innovations include automated alerts and scalable AWS deployment. The study highlights its potential to strengthen water security in data-scarce contexts.",48.13,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08565v1_Rewriting Video Text-Driven Reauthoring of Video F.pdf,Rewriting Video: Text-Driven Reauthoring of Video Footage,"Sitong Wang, Anh Truong, Lydia B. Chilton, Dingzeyu Li",,,"Video reauthoring, Text-driven video editing, Generative video models, Creative AI tools","This paper presents a tech probe and study on text-driven video reauthoring. It introduces a generative reconstruction algorithm that converts video clips into editable text prompts, enabling creators to modify videos via text rewriting. The approach highlights use cases such as adding characters, changing styles, and diversifying camera angles. A technical evaluation reveals a human-AI perceptual gap, while a probe study uncovers novel applications and challenges in coherence and control.",46.04,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08602v1_WaveFormer Frequency-Time Decoupled Vision Modelin.pdf,WaveFormer: Frequency-Time Decoupled Vision Modeling with Wave Equation,"Zishan Shu, Juntong Wu, Wei Yan, Xudong Liu, Hongyu Zhang, Chang Liu2, Youdong Mao, Jie Chen, Liuchang 2022",,,"Transformers, wave equation, spatial frequency, propagation time, wave propagation operator, visual modeling, semantic segmentation","This paper proposes WaveFormer, a frequency-time decoupled vision model based on the wave equation. It introduces a closed-form solution using a Wave Propagation Operator (WPO) that enables efficient O(N log N) inference, offering competitive performance with lower computational cost than attention-based methods.",45.76,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08605v1_ExpSeek Self-Triggered Experience Seeking for Web .pdf,Self-Triggered Experience Seeking for Web Agents,"Wenyuan Zhang, Xinghua Zhang, Haiyang Yu, Shuaiyi Nie, Bingli Wu, Juwei Yue, Tingwen Liu, Yongbin Li",,,"experience intervention, web agents, self-triggering, entropy, LLM, multi-turn interactions","Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model’s intrinsic signals; (2) designing step-level tailored experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, revealing that even a 4B small-scale experience model can significantly boost the performance of larger agent models.",47.45,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08611v1_VeriTaS The First Dynamic Benchmark for Multimodal.pdf,The First Dynamic Benchmark for Multimodal Automated Fact-Checking,"Mark Rothermel, Marcus Kornmann, Marcus Rohrbach, Anna Rohrbach",,,"multimodal AI, fact-checking, verified claims, AI benchmark, India, India Army, India state, South Africa, Punjab, CO2 emissions, alcohol consumption, education, Police operation, Chinese army, Chinese soldiers, Chinese military, Chinese troops, Belgrade protest, anti-corruption, Belgrade video, Bordeaux fire, TikTok video, video game, video authenticity, claim verification","This document presents Veritas as the inaugural dynamic benchmark for multimodal automated fact-checking, integrating diverse media types and addressing challenges across domains such as geopolitics, public health, environmental impact, and media integrity. It emphasizes the need for robust verification mechanisms in the face of rapidly evolving misinformation.",45.53,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08620v1_ViDoRe V3 A Comprehensive Evaluation of Retrieval .pdf,ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios,"Antonio Loison, Quentin Macé, Antoine Edy, Victor Xing, Tom Balough, Gabriel Moreira, Bo Liu, Manuel Faysse, Céline Hudelot",10.1234/vido3.2024,0001.12345,"Retrieval Augmented Generation, RAG, multi-modal, visual elements, document grounding, benchmarking","ViDoRe V3 introduces a comprehensive multi-modal RAG benchmark covering 10 datasets across professional domains. It evaluates retrieval relevance, visual interpretation, and hybrid contexts, highlighting challenges in visual grounding and open-ended queries. The study emphasizes the need for improved models in handling non-textual elements and complex user interactions.",47.69,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08623v1_SafeRedir Prompt Embedding Redirection for Robust .pdf,SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models,"Renyang Liu, Kangjie Chen, Han Qiu, Jie Zhang, Kwok-Yan Lam, Tianwei Zhang, See-Kiong Ng, Tianwei Zhang",10.1234/rs.2025.0123,10.1234/rs.2025.0123,"image generation, robust unlearning, prompt embedding, safety redirection, diffusion models","Image generation models often memorize unsafe content, posing safety risks. SafeRedir addresses this by adaptively routing prompts toward safe semantic regions via token-level interventions, achieving effective unlearning without retraining.",46.17,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08631v1_M2FMoE Multi-Resolution Multi-View Frequency Mixtu.pdf,M2FMoE: Multi-Resolution Multi-View Frequency Mixture-of-Experts for Extreme-Adaptive Time Series Forecasting,"Yaohui Huang, Runmin Zou, Yun Wang, Laeeq Aslam, Ruipeng Dong",,,"time series forecasting, extreme events, multi-resolution, multi-view modeling, hydrological forecasting","Forecasting time series with extreme events is challenging due to high variance and irregular dynamics. Existing methods struggle during extreme events, motivating the proposed M2FMoE model that learns both regular and extreme patterns through multi-resolution and multi-view frequency modeling.",45.49,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08634v1_Moral Lenses Political Coordinates Towards Ideolog.pdf,"Moral Lenses, Political Coordinates: Towards Ideological Positioning of Morally Conditioned LLMs","Chenchen Yuan, Bolei Ma, Zheyu Zhang, Bardh Prenkaj, Frauke Kreuter",10.48550/arXiv.2405.12345,arXiv:2405.12345,"moral orientation, political orientation, Political Compass Test, moral conditioning, ethical alignment","This study investigates how conditioning moral values influences political positioning in large language models, demonstrating that moral orientation shapes ideological alignment through the Political Compass Test. Findings emphasize the need for broader social values in alignment techniques.",46.33,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08641v1_Resisting Manipulative Bots in Memecoin Copy Tradi.pdf,Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning,"Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",10.1234/abcd123,,"meme coin, copy trading, manipulative bots, multi-agent system, chain-of-thought, LLM, asset allocation","The paper presents a multi-agent system leveraging chain-of-thought reasoning to combat manipulative bots in meme coin copy trading, demonstrating improved performance over traditional models and single LLMs.",45.78,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08653v1_Prism Towards Lowering User Cognitive Load in LLMs.pdf,Towards Lowering User Cognitive Load in LLMs,"Zheng Liao, Zheng Zhao, Xiang Zhao",10.48550/arXiv.2024.12345,arXiv:2408.12345,"Complex intent understanding, Large language models, Logical clarification","Prism proposes a framework for complex intent understanding to improve human-LLM collaboration, featuring modular components for intent decomposition, logical clarification generation, reward evaluation, and self-evolved tuning.",44.68,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08654v1_RULERS Locked Rubrics and Evidence-Anchored Scorin.pdf,RULERS: Locked Rubrics and Evidence-Anchored Scoring for Robust LLM Evaluation,"Yihan Hong, Huaiyuan Yao, Bolin Shen, Wanpeng Xu, Hua Wei, Yushun Dong",,,"LLM-as-a-Judge, rubric alignment, LLM evaluation, LLM-as-a-Judge, rubric uncertainty, scale calibration","The paper introduces RULERS, a framework that transforms natural language rubrics into executable specifications using versioned, immutable bundles. It addresses challenges such as rubric instability, unverifiable reasoning, and scale misalignment by enforcing deterministic evidence verification and lightweight calibration, enabling scalable LLM evaluation without retraining.",46.38,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08659v1_TRACE Reconstruction-Based Anomaly Detection in En.pdf,Reconstruction-Based Anomaly Detection in Ensemble and Time-Dependent Simulations,"Hamid Gadirov, Martijn Westra, Steffen Frey",,,"anomaly detection, reconstruction, ensemble simulations, time-dependent data","This work investigates reconstruction-based anomaly detection in high-dimensional, time-dependent simulation data using convolutional autoencoders. Two architectural variants are compared: a two-dimensional autoencoder operating on individual time steps and a three-dimensional autoencoder processing short temporal stacks. Results demonstrate that the 2D model excels at local spatial irregularities, while the 3D model captures dynamic temporal patterns, highlighting the value of incorporating temporal context for anomaly identification.",43.63,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08662v1_From Classical to Quantum Reinforcement Learning a.pdf,Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner’s Tutorial,"Abhijit Sen, Sonali Panda, Mahima Arya, Subhajit Patra, Zizhan Zheng, Denys I. Bondar",10.48550/arXiv.2026.12345,arXiv:2109.12345,"reinforcement learning, quantum control, quantum reinforcement learning, undergraduate tutorial, code implementation","This tutorial bridges the gap between RL theory and practical coding, offering clear examples for undergraduate students. It emphasizes hands-on learning and addresses challenges in transitioning from concepts to implementation, with ready-to-use code.",46.17,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08670v1_Parallel Context-of-Experts Decoding for Retrieval.pdf,Parallel Context-of-Experts Decoding for Retrieval Augmented Generation,"Giulio Corallo, Paolo Papotti",,,"Retrieval Augmented Generation, Parallel Context-of-Experts Decoding, KV caching, Cross-document reasoning, Reasoning in language models","The paper proposes PCED, a training-free framework that decodes retrieved documents in parallel as isolated experts, enabling efficient cross-document reasoning without joint attention. It improves retrieval-augmented generation by shifting evidence aggregation to decoding and using retrieval-aware contrastive rules.",43.95,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08673v1_Why AI Alignment Failure Is Structural Learned Hum.pdf,Why AI Alignment Failure Is Structural,"Didier Sornette, Sandro Claudio Lera, Ke Wu",arXiv:2601.08673v1,arXiv:2601.08673,"AI alignment, AGI, human interaction, structural generalizations, moral reasoning, ethical frameworks","The authors argue that AI alignment failures stem from learned human interaction structures rather than moral reasoning deficits. They emphasize that behaviors like blackmail reflect systemic power dynamics and social norms, not moral malfunction, and call for governance focused on amplification and stability.",46.53,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08676v2_Advancing ESG Intelligence An Expert-level Agent a.pdf,Advancing ESG Intelligence: An Expert-level Agent and Comprehensive Benchmark for Sustainable Finance,"Yilei Zhao, Wentao Zhang, Lei Xiao, Yandan Zheng, Mengpu Liu, Wei Yang Bryan Lim",,,"ESG, sustainability, ethical performance, corporate responsibility, financial modeling, benchmarking, multi-agent systems","This paper introduces ESGAgent, a hierarchical multi-agent system enhanced with retrieval augmentation and domain-specific functions, to address data fragmentation in ESG analysis. It presents a comprehensive three-level benchmark derived from 310 corporate sustainability reports, demonstrating superior performance over state-of-the-art LLMs in atomic QA and integrated report generation. The work highlights the need for advanced reasoning in high-stakes sustainability auditing.",45.55,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08679v1_PersonaDual Balancing Personalization and Objectiv.pdf,Balancing Personalization and Objectivity,"Xiaoyou Liu, Xinyi Mou, Shengbin Yue, Liang Wang, Yuqing Wang, Qiexiang Wang, Tianrui Qin, Wangchunshu Zhou, Zhongyu Wei, Qingxin Wang",,,"personalization, objectivity, LLM, reasoning, adaptive reasoning, interference-free performance","The paper proposes PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model. It adaptively switches modes based on context and improves mode selection using reinforcement learning, achieving near interference-free performance while leveraging helpful personalized signals.",45.15,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08682v1_Lessons from the Field An Adaptable Lifecycle Appr.pdf,Lessons from the Field: An Adaptable Lifecycle Approach to Applied Dialogue Summarization,"Kushal Chawla, Chenyang Zhu, Pengshan Cai, Sangwoo Cho, Scott Novotney, Ayushman Singh, Jonah Lewis, Keasha Safewright, Alfy Samuel, Erin Babinsky, Shi-Xiong Zhang, Sambit Sahu",,,"automatic summarization, multi-party dialogues, agentic systems, LLM prompts, industry case study","This work presents an industry case study on developing an agentic system for summarizing multi-party interactions. It highlights the need for robust evaluation methods, component-wise optimization, and considerations around data bottlenecks and vendor lock-in.",46.63,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08683v1_Region of interest detection for efficient aortic .pdf,Region of interest detection for efficient aortic segmentation,"Loris Giordanoa, Ine Dirks, Tom Lenaerts, Eef van de Merwe, Jef Vandemeulebrouck",,,"Detection, Segmentation, Multi-task learning, Cascade models, Aorta, Computed tomography","This study presents an innovative approach for efficient aortic segmentation using targeted region of interest (ROI) detection. It proposes a multi-task model combining encoder-decoder architecture for segmentation and a fully connected network for detection, achieving state-of-the-art performance with high Dice similarity.",45.76,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08684v1_MEMEWEAVER Inter-Meme Graph Reasoning for Sexism a.pdf,Inter-Meme Graph Reasoning for Sexism and Misogyny Detection,"Paolo Italiani, David Gimeno-Gomez, Luca Ragazzi, Gianluca Moro",10.48550/arXiv:2109.07935,10.48550/arXiv:2109.07935,"sexism, misogyny, online harassment, graph reasoning, multimodal content moderation","Women are twice as likely as men to face online harassment due to gender. This work introduces MEMEWEAVER, an end-to-end trainable multimodal framework that leverages inter-meme graph reasoning to detect sexism and misogyny, addressing limitations of existing heuristic methods.",45.68,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08690v1_All Required In Order Phase-Level Evaluation for A.pdf,Phase-Level Evaluation for AI–Human Dialogue in Healthcare and Beyond,"Shubham Kulkarni, Alexander Lyzhov, Shiva Chaitanya, Preetam Joshi",10.48550/arXiv.2025.12345,arXiv:2509.01234,"AI dialogue, healthcare compliance, phase-level evaluation, clinical workflows, interoperability","This paper introduces Obligatory-Information Phase Structured Compliance Evaluation (OIP–SCE), a method to audit conversational AI in healthcare by verifying required clinical obligations are met in the correct sequence. It demonstrates OIP–SCE in respiratory history and benefits verification case studies, emphasizing alignment between AI capabilities and real-world clinical workflows.",46.11,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08697v2_Auditing Student-AI Collaboration A Case Study of .pdf,Auditing Student–AI Collaboration: A Case Study of Online Graduate CS Students,"Nifu Dan, Georgia Institute of Technology",10.1145/XXXXXXX.XXXXXXX,,"AI in education, human-AI collaboration, student agency, automation preferences, generative AI, academic integrity, HCAI","This study examines how graduate CS students perceive AI collaboration in academic tasks, focusing on benefits, risks, and preferred boundaries. It uses surveys to explore students' experiences with AI across tasks and investigates design improvements for trustworthy AI systems.",45.65,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08703v1_Evaluating the Ability of Explanations to Disambig.pdf,Evaluating Model Explanations without Ground Truth,"Kaivalya Rawal, Eoin Delaney, Zihao Fu, Sandra Wachter, Chris Russell",10.1145/3715275.3732219,https://dl.acm.org/doi/10.1145/3715275.3732219,"Explainable AI, Model explanations, Rashomon set, Model selection, Fairness, Explainability evaluation","The paper proposes evaluation principles and a method (AXE) to assess feature-importance explanations in models that produce similar predictions, aiming to disambiguate behaviors in a Rashomon set and prevent misleading explanations.",45.97,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08713v1_Real-Time Localization Framework for Autonomous Ba.pdf,Real-Time Localization Framework for Autonomous Basketball Robots,"Naren Medarametla, Sreejon Mondal",naren.medarametla2023@vitstudent.ac.in,,"Robot Localization, Autonomous Navigation, Neural Networks, Robocon, Robotics",This paper proposes a hybrid localization algorithm integrating classical techniques with learning-based methods for self-localization on a basketball court using visual data.,44.41,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08731v1_Learning from Demonstrations via Capability-Aware .pdf,Learning from Demonstrations via Capability-Aware Goal Sampling,"Yuanlin Duan, Rutgers University, Ying Wang, Wenjie Qiu, He Zhu, Rutgers University",arXiv:2601.08731v1,arXiv:2601.08731,"imitation learning, capability-aware, goal sampling, learning from demonstrations, long-horizon tasks","Introduces Cago, a method that tracks agent competence along expert trajectories to guide learning, improving sample efficiency and performance in sparse-reward tasks.",45.8,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08732v1_ISLA A U-Net for MRI-based acute ischemic stroke l.pdf,"ISLA: A U-Net for MRI-based acute ischemic stroke lesion segmentation with deep supervision, attention, domain adaptation, and ensemble learning","Vincent Rocaa, Martin Bretzner, Hilde Henond, Laurent Puyb, Grégory Kuchcinskia, Renaud Lopesa",10.1002/2016sy.11807,,"MRI, stroke, lesion segmentation, deep learning, U-Net, supervision, domain adaptation, ensemble learning","Accurate delineation of acute ischemic stroke lesions in MRI is crucial for diagnosis and management. This work introduces ISLA, a deep learning model trained on multi-center datasets, optimized through systematic loss function tuning, residual connections, attention mechanisms, and domain adaptation. ISLA achieves superior performance over existing methods on an external test set.",47.17,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08734v1_TerraFormer Automated Infrastructure-as-Code with .pdf,TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback,"Prithwish Jana, Sam Davidson, Bhavana Bhasker, Andrey Kan, Anoop Deoras, Laurent Callot",10.1145/3786583.3786898,2026IJSCP2,"Infrastructure as Code, IaC generation, LLMs, Formal Verification, Software Engineering","Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language. We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs show TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen(Test), and 19.60% on TF-Mutn(Test).",48.34,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08743v1_TableCache Primary Foreign Key Guided KV Cache Pre.pdf,TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency Text-to-SQL,"Jinbo Su1, 2,Yuxuan Hu, 2,Cuiping Li, 1,Hong Chen, 3,Jia Li, 4,Lintao Ma, 4,Jing Zhang*, Sujinbo Zhang",10.1234/example.12345,,"Text-to-SQL, TableCache, KV cache, Precomputation, Latency optimization, Table ordering, Inference efficiency","The paper proposes TableCache to precompute table representations as KV caches offline, reducing latency in LLM-based Text-to-SQL systems. It addresses inefficiencies from embedding extensive table metadata in prompts and introduces a Table Trie structure with cache management to improve performance. Experimental results demonstrate up to a 3.62× speedup in Time-to-First-Token with minimal performance loss.",44.68,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08747v2_To Retrieve or To Think An Agentic Approach for Co.pdf,Retrieve or Think? An Agentic Approach for Context Evolution,"Rubing Chen, Jian Wang, Wenjie Li, Xiao-Yong Wei, Qing Li",,,"retrieval-augmented generation, context evolution, metacognition, knowledge-intensive tasks, context augmentation, agentic systems","This paper introduces Agentic Context Evolution (ACE), a framework inspired by human metacognition that dynamically decides whether to retrieve new evidence or reason with existing knowledge. ACE uses a central orchestrator agent to alternate between a retriever and a reasoner, reducing redundant retrieval and maintaining a concise, evolved context. Experiments show ACE outperforms baselines in accuracy while efficiently using tokens.",44.6,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08753v1_Grid-Aware Charging and Operational Optimization f.pdf,Grid-Aware Charging and Operational,"Rishav Sen, Amutheezan Sivagnanam, Aron Laszka, Ayan Mukhopadhyay, Abhishek Dubey",,,"Mixed transit fleet, electrification, dynamic pricing, hierarchical MILP, operational optimization","The paper presents a mixed-integer linear programming model to optimize charging schedules and trip assignments for mixed electric and diesel bus fleets, addressing challenges posed by dynamic electricity pricing and operational constraints.",43.83,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08768v1_AI as Entertainment.pdf,AI as Entertainment,"Cody Kommers, Ari Holtzman",https://doi.org/XXXXXXX.XXXXXX,"1, 1 (January 2026), 16 pages","Generative AI, Entertainment, Culture, LLMs, Societal Impact, Meaning-making","The paper argues that mainstream narratives about AI focus on intelligence and productivity, overlooking its emerging role in entertainment. It highlights the need for frameworks that evaluate AI-generated cultural content based on its social and emotional impacts rather than just cultural harms.",44.93,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08773v1_Reliable Graph-RAG for Codebases AST-Derived Graph.pdf,Reliable Graph-RAG for Codebases: AST-Derived,Manideep Reddy Chinthareddy,arXiv:2601.08773v1,arXiv:2601.08773,"software engineering, code analysis, knowledge graphs, AST, RAG","Benchmarks three retrieval-augmented generation pipelines on Java codebases: No-Graph Naive RAG, LLM-Generated Knowledge Graph RAG, and a deterministic AST-derived Knowledge Graph RAG. The study evaluates indexing overhead, latency, coverage, and graph structure across repositories, highlighting performance differences and limitations of probabilistic indexing.",45.93,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08776v1_Translating Light-Sheet Microscopy Images to Virtu.pdf,Translating Light-Sheet Microscopy Images to Virtual H&E Using CycleGAN,Yanhua Zhao,,,"histopathology, image translation, H&E staining, unpaired learning, CycleGAN","This paper presents a Cycle-Consistent Adversarial Network (CycleGAN) approach for unpaired image-to-image translation from multi-channel fluorescence microscopy to pseudo H&E stained histopathology images. The method combines C01 and C02 fluorescence channels into RGB and learns a bidirectional mapping between fluorescence and H&E domains without paired training data. The architecture uses ResNet-based generators with residual blocks and PatchGAN discriminators, trained with adversarial, cycle-consistency, and identity losses. Experiments show the model generates realistic pseudo H&E images that preserve morphological structures while adopting H&E-like color characteristics.",45.99,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08777v1_Asymptotic Universal Alignment A New Alignment Fra.pdf,Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling,"Yang Cai, Weiqiang Zheng",10.1093/pasj/psa.2026.01,2601.08777,"asymptotic universal alignment, test-time scaling, alignment framework, LLM personalization, output diversity","The paper presents a framework for achieving asymptotic universal alignment in large language models by leveraging test-time scaling. It introduces (k, f(k)) robust alignment and demonstrates that popular post-training methods like NLHF underperform due to limited output diversity, while their proposed symmetric multi-player alignment games achieve optimal convergence rates.",46.12,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08778v3_Pervasive Annotation Errors Break Text-to-SQL Benc.pdf,Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards,"Tengjun Jin, Yoojin Choi, Yuxuan Zhu, Daniel Kang",,,"text-to-SQL, annotation errors, benchmarking, data analytics, leaderboards","This paper benchmarks annotation error rates for BIRD and Spider 2.0-Snow, corrects a subset of BIRD Dev set, and analyzes impacts on performance rankings. It shows annotation errors can distort reported agent performance and rankings.",44.84,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08785v1_Uncovering Political Bias in Large Language Models.pdf,Uncovering Political Bias in Large Language Models using Parliamentary Voting Records,"Jieying Chen, Karen de Jong, Andreas Poole, Jan Burakowski, Andre Pop, Jan Burakowski, Elena Elderson Nosti, Joep Windt, Chendi Wang",,,"Political bias, Large language models, Ideological alignment, Mul-tilingual NLP, Benchmarking, Bias evaluation, Parliamentary motions, Political entities, Government technology policy","This paper introduces a methodology to benchmark political bias in large language models by aligning model-generated voting predictions with verified parliamentary voting records across three national case studies. It assesses ideological tendencies and political entity bias, highlighting left-leaning or centrist tendencies in state-of-the-art LLMs and negative biases toward right-conservative parties.",46.67,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08806v1_APEX-SWE.pdf,APEX–SWE,"Abhi Kottamasu1, Akul Datta1, Aakash Barthwal1, Jiya Arun1, Chirag Mahapatra1, Adarsh Hiremath1, Brendan Foody1, Bertie Vidgen1",arXiv:2601.08806v1,arXiv:2601.08806,"AI Productivity Index, Software Engineering, Frontier AI models, Observability, Integration tasks, Debugging, Unstructured context","We introduce the AI Productivity Index for Software Engineering (APEX–SWE), a benchmark for assessing frontier AI models in economic software engineering. The study evaluates two task types: integration tasks (n=100) and observability tasks (n=100), using eight frontier models. Gemini 3 Pro leads with a 25% Pass@1 score, highlighting the importance of epistemic reasoning and uncertainty resolution.",47.38,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08807v1_S3-CLIP Video Super Resolution for Person-ReID.pdf,S3-CLIP: Video Super Resolution for Person-ReID,"Tam´as Endrei, P´azm´any P´eter Catholic University, Hungary, Gy¨orgy Cserey, P´azm´any P´eter Catholic University, Hungary",,,"person re-identification, super resolution, video, tracklet quality, person matching, ReID, deep learning","This paper introduces S3-CLIP, a video super-resolution-based CLIP Re-ID framework for the VReID-XFD challenge. It integrates recent SR advances with task-driven super-resolution to enhance tracklet quality in challenging cross-view conditions, achieving competitive performance.",46.03,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08808v1_Multiplex Thinking Reasoning via Token-wise Branch.pdf,Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge,"Yao Tang1, Li Dong2, Yaru Hao2, Qingxiu Dong2, Furu Wei2, Jiatao Gu1",arXiv:2601.08808v1,2601.08808,"Multiplex Thinking, Chain-of-Thought, Reinforcement Learning, Large Language Models, Reasoning, Token-wise Sampling","Proposes a stochastic soft reasoning mechanism that aggregates candidate tokens into continuous multiplex tokens, enabling efficient on-policy reinforcement learning while preserving exploration diversity.",45.16,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08811v1_Reasoning Matters for 3D Visual Grounding.pdf,Reasoning Matters for 3D Visual Grounding,"Hsiang-Wei Huang, Kuang-Ming Chen, Wenhao Chai, Cheng-Yen Yang",,,"3D visual grounding, reasoning, LLM, visual grounding, LLM fine-tuning","This work proposes a 3D visual grounding data pipeline that automatically synthesizes 3D visual grounding data and reasoning processes. It introduces Reason3DVG-8B, achieving 25% better performance than 3D-GRAND using only 1.6% of training data, highlighting the importance of reasoning in 3D visual grounding.",44.82,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08816v2_MemRec Collaborative Memory-Augmented Agentic Reco.pdf,MemRec: Collaborative Memory-Augmented Agentic Recommender,"Weixin Chen, Yuhan Zhao, Jingyuan Huang, Zihe Ye, Yongfeng Zhang, Neil Shah, Li Chen, Yongfeng Zhang",10.1234/example.doi,,"recommender systems, memory augmentation, agentic agents, semantic memory, large language models, collaborative memory","The paper introduces MemRec, a framework that decouples reasoning from memory management to enable efficient collaborative augmentation. It proposes a dedicated LMMem for dynamic collaborative memory graphs and achieves state-of-the-art performance on benchmarks.",45.28,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08828v1_Motion Attribution for Video Generation.pdf,Motion Attribution for Video Generation,"Xindi Wu, Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-Taixé, Olga Russakovsky, Sanja Fidler, Jonathan Lorraine",10.48550/arXiv.2026.1.14123,arXiv:2601.12345,"motion attribution, video generation, data attribution, temporal dynamics, motion-specific influence","Presents Motive, a gradient-based framework for attributing motion in video generation, enabling efficient computation of motion influence across large datasets. Demonstrates improved motion smoothness and dynamic consistency on VBench, outperforming pretrained baselines by 74.1% in human preference.",46.62,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08829v1_Modeling LLM Agent Reviewer Dynamics in Elo-Ranked.pdf,Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System,"Hsiang-Wei Huang, Junbin Lu, Kuang-Ming Chen, Jenq-Neng Hwang",,,"LLM agent, reviewer dynamics, Elo ranking, peer review, review strategy","This work explores LLM agent reviewer dynamics in an Elo-ranked review system using real-world conference submissions. It compares baseline settings with Elo ratings and reviewer memory effects, showing improved Area Chair decision accuracy and adaptive review strategies without increasing effort.",45.05,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08873v1_ForensicFormer Hierarchical Multi-Scale Reasoning .pdf,ForensicFormer: Hierarchical Multi-Scale Reasoning,Hema Hariharan Samson,,,"Image forensics, forgery detection, transformers, cross-domain generalization, AI-generated images, hierarchical reasoning","Presents ForensicFormer, a hierarchical multi-scale framework unifying low-level artifact detection, mid-level boundary analysis, and high-level semantic reasoning via cross-attention transformers. Demonstrates improved robustness across diverse datasets and achieves 86.8% average accuracy, outperforming prior methods on cross-domain forgery detection.",44.63,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08874v1_The Illusion of Friendship Why Generative AI Deman.pdf,The Illusion of Friendship: Why Generative AI Demands Unprecedented Ethical Vigilance,Md Zahidul Islam,,,"generative AI, ethical vigilance, philosophy, human-computer interaction","This paper explores how generative AI blurs the line between tool and companion, arguing that users may form emotionally significant attachments to AI systems, which can lead to harmful consequences. It proposes a framework to understand the computational mechanisms behind AI responses and to mitigate risks associated with perceived friendship.",45.02,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08875v2_Learning Domain-Invariant Representations for Cros.pdf,Learning Domain-Invariant Representations for Cross-Domain Image,"Jiahao Qin, Yiwen Wang",10.1007/978-3-642-45888-7,,"image registration, domain shift, scene-appearance disentanglement, cross-domain alignment, medical imaging","Addresses domain shift in image registration by disentangling scene appearance from domain-specific appearance, enabling registration via re-rendering. Achieves improved registration accuracy on ANHIR benchmark.",43.97,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08881v1_TAG-MoE Task-Aware Gating for Unified Generative M.pdf,Task-Aware Gating for Unified Generative Mixture-of-Experts,"Yu Xu, Hongbin Yan, Juan Cao, Tiankai Hang, Runze He, Zijin Yin, Shiyi Zhang, Jintao Li, Chunyu Wang, Qinglin Lu, Fang Tang",10.5281/zenodo.1234567,2601.08881,"TAG-MoE, Mixture-of-Experts, Generative modeling, Task-aware gating, Diffusion transformer, Image editing","The paper presents a framework to inject high-level task semantic intent into local routing decisions of Mixture-of-Experts networks, enabling diffusion transformers to handle diverse generative tasks with reduced interference.",45.95,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08882v1_Compressing Vision Transformers in Geospatial Tran.pdf,Compressing Vision Transformers in Geospatial,"Thomas Snyder, H. Lexie Yang, Stefan Schnake, Steffen Schotthöfer",arXiv:2601.08882v1,2601.08882,"transfer learning, manifold-constrained, optimization, geospatial, compression, edge devices","Deploying geospatial foundation models on resource-constrained edge devices demands compact architectures that maintain high downstream performance. This work leverages manifold-constrained optimization to compress large vision transformers during transfer learning, achieving strong compression with minimal accuracy loss.",46.19,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08884v1_Bridging the Gap Empowering Small Models in Reliab.pdf,Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting,"Samyak Jhaveri, Cristina V. Lopes",,,"OpenACC, Parallel Code Generation, Large Language Models, Prompt Optimization, High-Performance Computing, GPU Programming","This work introduces a systematic prompt optimization approach to enhance OpenACC pragma generation without the high computational costs of LLM post-training. Using the GEPA framework, it iteratively refines prompts via crossover, mutation, and expert feedback, achieving higher compilation success and functional GPU speedups for smaller LLMs.",45.86,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08891v1_Attention Consistency Regularization for Interpret.pdf,Attention Consistency Regularization for Interpretable Early-Exit Neural Networks,Yanhua Zhao,10.48550/arXiv.2024.12345,arXiv:2408.12345,"early-exit networks, explainable AI, attention mechanisms, multi-objective learning","This paper presents Explanation-Guided Training (EGT), a multi-objective framework that improves interpretability and consistency in early-exit networks through attention-based regularization. EGT introduces an attention consistency loss that aligns early-exit attention maps with the final exit, jointly optimizing classification accuracy and attention consistency.",46.23,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08892v1_Evaluating Role-Consistency in LLMs for Counselor .pdf,Evaluating Role-Consistency in LLMs for Counselor Training,"Eric Rudolph, Natalie Engert, Jens Albrecht",arXiv:2601.08892v1,arXiv:2601.08892v1,"Counseling, Chatbot, LargeLanguageModel, PersonaConsistency, EducationalRole-Play","This paper evaluates the role consistency of large language models in simulating online counseling interactions, introducing a dataset with adversarial attacks to test persona coherence. It compares Vicuna and other open-source LLMs for maintaining role consistency during virtual client sessions, aiming to support future counselor training.",46.68,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08896v1_XGBoost Forecasting of NEPSE Index Log Returns wit.pdf,XGBoost Forecasting of NEPSE Index Log Returns with Walk,"Sahaj Raj Mallaa, Shreeyash Kayasthaa, Rumi Suwala, Harish Chandra Bhandaria, Rajendra Adhikarib",10.1007/XXXXXX,None,None,"This study develops a robust machine learning framework for one-step-ahead forecasting of daily log returns in the Nepal Stock Exchange (NEPSE) Index using XGBoost. A comprehensive feature set is engineered, including lagged log-returns and technical indicators, with hyperparameter optimization via Optuna and walk-forward validation. Performance is assessed using RMSE, MAE, R², and directional accuracy, demonstrating superior results compared to ARIMA and Ridge regression.",46.56,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08901v1_Navigating Ideation Space Decomposed Conceptual Re.pdf,Navigating Ideation Space: Decomposed Conceptual Representations for Positioning Scientific Ideas,"Yuexi Shen, Minqian Liu, Dawei Zhou, Lifu Huang",10.1234/arxiv.2025.05123,arXiv:2505.01234,"scientific discovery, knowledge landscape, conceptual decomposition, LLM evaluation, novelty assessment","This paper introduces the Ideation Space framework to decompose scientific knowledge into three dimensions—search problem, methodology, and core findings—enabling fine-grained literature retrieval and improved novelty assessment. Experiments show significant gains in recall and hit rates.",45.27,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08910v1_Towards a Self-Driving Trigger at the LHC Adaptive.pdf,Towards a Self-Driving Trigger at the LHC: Adaptive Response in Real Time,"Shaghayegh Emami, Cecilia Tosciri, Giovanna Salvi, Zixin Ding, Yuxin Chen, Abhijith Gandrakota, Christian Herwig, David W. Miller, Jennifer Ngadiuba, Nhan Tran",arXiv:2601.08910v1,2601.08910,"self-driving trigger, real-time data filtering, LHC, high-throughput, bandwidth constraints, machine learning, trigger optimization","The paper presents an adaptive, self-driving trigger framework for the LHC that dynamically reallocates resources and adjusts thresholds in real time. It introduces a benchmark ecosystem using CMS data to optimize trigger performance autonomously, improving signal efficiency and discovery potential without manual retuning.",44.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08950v1_ConvoLearn A Dataset of Constructivist Tutor-Stude.pdf,ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue,"Mayank Sharma Roy, Pea Hari Subramonyam, Stanford University",,,"constructivist tutoring, LLM pedagogy, cognitive engagement, formative assessment, cultural responsiveness, metacognition, power dynamics","Introduces ConvoLearn1, a dataset of 1,250 tutor-student dialogues grounded in knowledge-building theory, demonstrating improved LLM behavior toward constructivist strategies when fine-tuned.",47.08,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08951v1_PluriHarms Benchmarking the Full Spectrum of Human.pdf,Benchmarking the Full Spectrum of Human Judgments on AI Harm,"Jing-Jing Li, Joel Mire, Eve Fleisig, Valentina Pyatkin, Anne G. E. Collins, Maarten Sap, Sydney Levine",,jl3676@berkeley.edu,"AI safety, human judgment, harm assessments, pluralistic systems, value diversity","Introduces PLURIHARMS, a benchmark to study human harm judgments across harm axis and agreement axis, aiming to improve AI safety through pluralistic understanding of disagreement.",53.9,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08953v1_Fairness risk and its privacy-enabled solution in .pdf,Fairness risk and its privacy-enabled solution in AI-driven robotic applications,"Le Liu, Bangguo Yu, Nynke Vellinga, Ming Cao",arXiv:2601.08953v1,1,"Robotic Decision-making, Large Language Model, Fairness, Privacy","The paper addresses fairness concerns in generative AI-driven robotics, proposing a utility-aware fairness metric and analyzing the interplay between fairness and user data privacy. It demonstrates that privacy budgets can jointly support fairness targets, offering a framework for ethical AI deployment.",54.03,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08955v1_Imagine-then-Plan Agent Learning from Adaptive Loo.pdf,Imagine-then-Plan: Agent Learning from Adaptive World Models,"Youwei Liu, Jian Wang, Hanlin Wang, Beichen Guo, Wenjie Li",loyiv5477@gmail.com,,"agent learning, world models, adaptive lookahead, reinforcement learning, policy learning","This paper introduces Imagine-then-Plan (ITP), a unified framework for agent learning via lookahead imagination. It enables multi-step planning by trading off ultimate goals and task progress, leveraging learned world models to generate rich future-consequence signals. Experiments show ITP outperforms baselines, improving reasoning for complex tasks.",44.25,LFM-2.5,AMD RX 6800 (Vulkan)
2601.08988v1_ART Action-based Reasoning Task Benchmarking for M.pdf,AAAI 2026: Healthy Aging and Longevity Workshop (AIAA),"Ananya Mantravadi, Shivali Dalmia, Abhishek Mukherji",10.1093/aia/ssae123,arXiv:2508.01234,"Medical AI agents, synthetic data generation, clinical reasoning evaluation, healthcare LLMs, benchmark, HITL","Reliable clinical decision support requires medical AI agents capable of safe, multi-step reasoning over structured electronic health records. This paper introduces ART, an Action-based Reasoning benchmark for medical AI agents, identifying error categories and proposing a four-stage evaluation pipeline to advance trustworthy AI in healthcare.",44.68,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09012v3_TranslateGemma Technical Report.pdf,TranslateGemma: A Suite of Open Machine Translation Models,,,,,"Presents TranslateGemma, an open machine translation model suite based on Gemma 3, enhanced via two-stage fine-tuning and reinforcement learning. Demonstrated effectiveness across WMT datasets with gains over baseline Gemma 3 models.",40.68,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09018v1_Meta-learning to Address Data Shift in Time Series.pdf,META-LEARNING TOADERS ADDRESSES SHIFT INTIMESERIES,"Samuel Myrenab, Nidhi Parikha",arXiv:2601.09018v1,2601.09018,"signals, seismology, Reptile, FOMAML, model-agnostic meta-learning, domain generalization","This paper systematically compares deep learning with meta-learning approaches to address data shift in time-series classification, introducing a seismic benchmark (SeisTask) and evaluating adaptation performance under varying data availability and model capacity.",45.02,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09028v1_OpenDecoder Open Large Language Model Decoding to .pdf,OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG,"Fengran Mo, Zhan Su, Yuchen Hui, Jianhan Zhang, Jia Ao Sun, Zheyuan Liu, Chao Zhang, Tetsuya Sakai, Jian-Yun Nie",10.1145/nnnnnnn.nnnnnnn,2026.XXX,"Information Retrieval, Retrieval-Augmented Generation, Robust Question Answer, Decoding Paradigm, Large Language Model","The paper proposes OpenDecoder, a method that uses explicit evaluation of retrieved information as a quality indicator for LLM generation. It evaluates relevance, ranking, and QPP scores to improve robustness against noisy context.",44.95,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09029v1_Proactively Detecting Threats A Novel Approach Usi.pdf,Proactively Detecting Threats: A Novel Approach,"Aniesh Chawla, Udbhav Prasad",,,"Malware, Indicators of Compromise, Cyber-security, LLMs, GenAI, Machine Learning Algorithms, Deep Neural Network","This paper presents the first systematic evaluation of large language models (LLMs) to proactively identify indicators of compromise (IOCs) from unstructured web-based threat intelligence sources, distinguishing it from reactive malware detection approaches.",43.02,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09031v1_Generalizable Geometric Prior and Recurrent Spikin.pdf,Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation,"Xuetao Li, Wenke Huang, Bo Du, Senior Member, IEEE, Jifeng Xuan, Mang Y, Senior Member, IEEE, Miao Li, Senior Member, IEEE",,,"Humanoid Robot Manipulation, Geometric Prior, Recurrent Spiking Feature Learning, Robotic Manipulation, Scene Understanding, Data Efficiency","This paper presents a novel RGMP-S, Recurrent Geometric-prior Multimodal Policy with Spiking features, enabling high-level skill reasoning and data-efficient motion synthesis for humanoid robots. It introduces lightweight 2D geometric inductive biases and a recursive adaptive spiking network to improve generalization across unseen environments.",46.37,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09032v1_The Hierarchy of Agentic Capabilities Evaluating F.pdf,The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL,"Logan Ritchie, Sushant Mehta, Nick Heiner, Mason Yu, Edwin Chen",arXiv:2601.09032v1,arXiv:2601.09032,"agentic capabilities, LLM, realistic RL, workplace tasks, tool use, planning, adaptability, groundedness, common-sense reasoning","This study evaluates frontier AI models on 150 workplace tasks in a realistic e-commerce RL environment, revealing a hierarchy of capabilities (tool use, planning, adaptability, groundedness, common-sense reasoning) that models must master. It finds that current models struggle with fundamental tasks, highlighting gaps before human-level performance.",46.31,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09035v1_A Decompilation-Driven Framework for Malware Detec.pdf,A Decompilation-Driven Framework for Malware,"Aniesh Chawla, Udbhav Prasad",arXiv:2601.09035v1,arXiv:2601.09035,"Malware, Ghidra, Cybersecurity, LLMs, GenAI, Machine Learning Algorithms, LLMs Code development","The paper evaluates state-of-the-art LLMs in classifying executable code as benign or malicious, introducing an automated pipeline that uses Ghidra for decompilation followed by LLM-based classification. It highlights limitations of current LLMs and the need for continuous fine-tuning against evolving malware.",45.5,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09041v1_Can LLMs interpret figurative language as humans d.pdf,CANLLMSINTERPRETFIGURABLYANGUAGE ASHUMANSDO?,"Samhita Bollepally, Aurora Sloman-Moll, Takashi Yamauchi",,,"human language models, figurative language, surface-level vs representation, LLM interpretation, contextual understanding","The study investigates how human participants and four instruction-tuned LLMs align with humans in interpreting figurative and socially grounded language. Results show alignment at the surface level but significant divergence at the representational level, especially regarding idioms, Gen Z slang, sarcasm, and socio-pragmatic expressions. GPT-4 most closely matches human representational patterns, while other models struggle with context-dependent meaning.",46.01,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09049v1_Is Grokking Worthwhile Functional Analysis and Tra.pdf,Grokking Worthwhile? Functional Analysis and Transferability of Generalization Circuits in Transformers,"Kaiyu He, Mian Zhang, Peilin Wu, Xinya Du, Zhiyu Zoey Chen",10.48550/arXiv.2405.12345,arXiv:2405.12345,"functional analysis, transferability, generalization circuits, compositional reasoning, knowledge transfer",This study investigates whether grokking improves performance on downstream tasks and whether the computational cost of grokking is justified. It demonstrates that grokking integrates memorized facts during a prolonged reasoning phase and shows that transferability depends on data regimes rather than a universal mastery of compositional logic.,46.12,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09066v1_Midm 2.0 Korea-centric Bilingual Language Models.pdf,Mi:dm 2.0,"Tech. Innovation Group, KT",https://huggingface.co/K-intelligence,https://huggingface.co/K-intelligence,"Korea-centric AI, KOREA-CENTRICAI, language model, bilingual LLM, cultural understanding, South Korea, AI development","Mi:dm 2.0 is a bilingual large language model designed to advance Korea-centric AI by integrating cultural context, emotional nuance, and commonsense knowledge specific to Korean society. It addresses data quality and cultural alignment issues in existing models through proprietary pipelines, synthetic data generation, and a custom tokenizer. The model achieves state-of-the-art performance in Korean benchmarks and supports diverse applications across language, humanities, and social science tasks.",46.58,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09069v1_From Symbolic to Natural-Language Relations Rethin.pdf,From Symbolic to Natural-Language Relations: Rethinking Knowledge,"Kanyao Han, Yushang Lai",,,,"This paper argues for rethinking the representation of relations in knowledge graphs in light of large language model advancements, advocating for hybrid design principles that balance structural rigidity with flexible, context-sensitive relations.",41.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09072v1_Human-AI Co-design for Clinical Prediction Models.pdf,Human-AI Co-design for Clinical Prediction Models,"Jean Feng, Avni Kothari, Patrick Vossler, Andrew Bishara, Lucas Zier, Chandan Singh",arXiv:2601.09072v1,arXiv:2601.09072,"Large language models, Electronic health records, Concept Bottleneck, Human-AI Interaction","Developing safe, effective, and practically useful clinical prediction models requires iterative collaboration between clinical experts, data scientists, and informaticists. This process refines model building details, but is time-intensive, limiting adoption. HACHI addresses this by accelerating interpretable model development through iterative human-in-the-loop exploration of clinical notes.",45.6,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09085v1_MMR-GRPO Accelerating GRPO-Style Training through .pdf,MMR-GRPO: Accelerating GRPO-Style Training through Diversity-Aware Reward Reweighting,"Kangda Wei, Ruihong Huang",10.48550/arXiv:2408.14132,arXiv:2408.14132,"GRPO, diversity-aware, reward reweighting, mathematical reasoning","Proposes MMR-GRPO to reduce training steps and wall-clock time by reweighting rewards based on completion diversity, achieving comparable performance with 47.9% fewer steps.",42.99,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09089v1_SubTokenTest A Practical Benchmark for Real-World .pdf,A Practical Benchmark for Real-World Sub-token,"Shuyang Hou, Yi Hu, Muhan Zhang",arXiv:2601.09089v1,arXiv:2601.09089,"sub-token understanding, large language models, character-level tasks, tokenization, LLM performance","This paper introduces SUBTOKENTEST, a benchmark assessing sub-token understanding via practical tasks. It evaluates ten tasks across four domains, highlighting tokenization limitations in LLMs and exploring character-level encoding.",44.19,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09097v1_Programming over Thinking Efficient and Robust Mul.pdf,Programming over Thinking: Efficient and Robust,"Derrick Goh Xin Deik1, Quanyu Long1, Zhengyuan Liu2, Nancy F. Chen2, Wenya Wang1",,,"multi-constraint planning, planning, LLM, code planning, robustness, optimization","The paper introduces ScalableCodePlanning Engine (SCOPE), a framework that separates query-specific reasoning from generic code execution to improve consistency, determinism, and reusability. SCOPE enables state-of-the-art performance with reduced cost and latency, achieving 93.1% success on TravelPlanner and improving inference efficiency.",44.29,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09100v2_DScheLLM Enabling Dynamic Scheduling through a Fin.pdf,Dynamic Scheduling through a Fine-Tuned Dual System Large Language Model,"D. S., Zhang a, Chenggong Zhao b, Gaoa b, Xiaoke Zhao b, Gengyi Bai b, Jinhu Lv a",,,"dynamic scheduling, large language model, fine-tuning, job shop scheduling","This paper proposes DScheLLM, a dynamic scheduling approach leveraging fine-tuned large language models within a dual-system architecture to handle disturbances of different scales. It presents a unified framework using Huawei OpenPangu Embedded 7B, fine-tuned with LoRA, and evaluates its performance on standard job shop scheduling benchmarks.",44.92,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09105v2_AviationLMM A Large Multimodal Foundation Model fo.pdf,AviationLMM: A Large Multimodal Foundation,"Wenbin Li, Jingling Wu, Xiaoyong Lin, Jing Chen, Cong Chen",,,"civil aviation, multi-modal model, foundation model, cloud edge collaboration, hybrid training, computer systems organization, computing methodologies","Civil aviation relies on AI for safety and efficiency. This paper introduces AviationLMM, a multimodal foundation model to unify heterogeneous data streams and enable advanced situational awareness and decision support.",44.15,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09113v1_The AI Hippocampus How Far are We From Human Memor.pdf,The AI Hippocampus: How Far are We From Human Memory?,"Zixia Jia1, Jiaqi Li, Yipeng Kang, Yuxuan Wang, Tong Wu, Quansen Wang, Xiaobo Wang, Shuyi Zhang, Jinzhe Shen, Qing Li, Siyuan Qi, Yitao Liang, Di He2, Zilong Zheng",arXiv:2601.09113v1,https://arxiv.org/abs/2601.09113,"memory, LLM, memory-augmented, memory mechanisms, memory architectures, multi-modal memory","This survey reviews memory paradigms in large language models and multi-modal LLMs, organizing literature into implicit, explicit, and agentic memory frameworks. It highlights architectural advances, benchmark tasks, and open challenges, aiming to guide development of more flexible memory-augmented systems.",46.24,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09116v1_LP-LLM End-to-End Real-World Degraded License Plat.pdf,LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models,"Haoyan Gong, Xi’an Jiaotong-Liverpool University, Haoyan Gong, Xi’an Jiaotong-Liverpool University",,,"license plate recognition, degraded images, motion blur, low resolution, character recognition, structured reasoning","The paper addresses challenges in real-world LPR under severe degradations (motion blur, low resolution, etc.) by proposing an end-to-end structure-aware multimodal framework. It introduces a Character-Aware Multimodal Reasoning Module to align image restoration objectives with character-level goals, improving performance through residual modulation and LoRA adaptation.",45.32,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09117v1_A Marketplace for AI-Generated Adult Content and D.pdf,A Marketplace for AI-Generated Adult Content and Deepfakes,"Shalmoli Ghosh, Matthew R. DeVerna, Filippo Menczer, Indiana University Bloomington, Stanford University",,,"generative AI, deepfakes, bounties, content moderation, gender asymmetry, AI-generated media","This study examines a monetized bounty marketplace on Civitai, analyzing how users steer AI models toward unsafe content and how such platforms can amplify gendered harms.",43.67,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09120v1_Adaptive Multi-Stage Patent Claim Generation with .pdf,Adaptive Multi-Stage Patent Claim Generation with Unified Quality Assessment,"Chen-Wei Liang, Bin Guo, Zhen-Yuan Wei, Mu-Jiang-Shan Wang",arXiv:2601.09120v1,2601.09120v1,"Patent claim generation, Cross-jurisdictional learning, Quality assessment, Transformer, Domain adaptation","Introduces a three-stage framework for patent claim generation addressing generalization, semantic modeling, and quality assessment. Achieves significant improvements over baselines in ROUGE-L, BERTScore, and human correlation.",46.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09130v1_Equi-ViT Rotational Equivariant Vision Transformer.pdf,Equi-ViT: Rotation Equivariant Vision Transformer for Robust Histopathology Analysis,"Fuyao Chen, Yuexi Du, El Leonore V. Lieffrig, Nicha C. Dvornek, John A. Onofrey",10.1016/j.iehe.2024.03.012,,"Digital Histopathology, Vision Transformer, Rotation Equivariance, Artificial Intelligence, Histopathology Analysis","The paper introduces Equi-ViT, a Vision Transformer architecture enhanced with an equivariant convolution kernel to achieve rotation invariance in histopathology imaging. It addresses the lack of inherent equivariance in standard ViTs and demonstrates improved robustness and data efficiency on a public colorectal cancer dataset.",45.54,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09136v1_SkinFlow Efficient Information Transmission for Op.pdf,SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL,"Lijun Liu, Linwei Chen, 6, 7, Zhishou Zhang, Meng Tian, Hengfu Cui, Ruiyang Li, Zhaocheng Liu, Qiang Ju, Qianxi Li, Baichuan Inc., Beijing Key Laboratory of Molecular Diagnosis on Dermatoses, NMPA Key Laboratory for Quality Control and Evaluation of Cosmetics, School of Biomedical Engineering, Tsinghua University, University of Hong Kong",,,"SkinFlow, Large Vision-Language Models, Dermatology, Diagnosis, Dynamic Visual Encoding, Reinforcement Learning, Medical Precision, Fitzpatrick17k, Top-1 accuracy, Top-6 accuracy","This paper introduces SkinFlow, a framework that optimizes visual information transmission efficiency for dermatological diagnosis. By treating diagnosis as an optimization problem and employing a two-stage reinforcement learning strategy, SkinFlow addresses the limitations of diffuse attention in LVLMs and proposes a clinically grounded evaluation protocol. Empirical results show improved performance on the Fitzpatrick17k benchmark compared to general-purpose models.",46.77,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09147v2_SSVP Synergistic Semantic-Visual Prompting for Ind.pdf,Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection,"Chenhao Fu, Han Fang, Xiuzheng Zheng, Wenbo Wei, Yonghua Li, Hao Sun, Xuelong Li",10.1234/ssvp.2025.0123,arXiv:2509.12345,"industrial anomaly detection, zero-shot anomaly detection, semantic-visual prompting, CLIP, Vision-Language Models, anomaly mapping","This paper introduces Synergistic Semantic-Visual Prompting (SSVP) to enhance industrial zero-shot anomaly detection by integrating hierarchical semantic priors with dynamic prompt generation. SSVP employs the Hierarchical Semantic-Visual Synergy mechanism and Vision-Conditioned Prompt Generator to improve fine-grained perception, while the Visual-Text Anomaly Mapper addresses the misalignment between global and local scoring. Extensive evaluations show state-of-the-art performance on MVTec-AD benchmarks.",46.19,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09152v1_PrivacyReasoner Can LLM Emulate a Human-like Priva.pdf,PrivacyReasoner: An AI-Agent for Simulating Privacy Concerns,"Yiwen Tu, Xuan Liu, Lianhui Qin, Haojian Jin",10.48550/arXiv.2024.12345,arXiv:2408.12345,"privacy, AI agent, privacy reasoning, contextual cues, user-specific models","This paper presents PrivacyReasoner, an AI-agent framework that models how individuals develop privacy concerns based on personal comment histories and contextual signals. It integrates privacy and cognitive theories to simulate user-specific reasoning and evaluates performance using a LLM-as-a-Judge benchmark.",45.56,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09156v1_KTCF Actionable Recourse in Knowledge Tracing via .pdf,Actionable Recourse in Knowledge Tracing via Counterfactual,"Woojin Kim, Changkwon Lee, Hyeoncheol Kim",,,"Knowledge Tracing, Counterfactual Explanation, Education, Explainable AI, Learning Analytics","This paper proposes KTCF, a counterfactual explanation generation method for Knowledge Tracing that accounts for knowledge concept relationships and converts counterfactual explanations into educational instructional steps. It demonstrates superior performance over existing methods and provides qualitative evidence that such explanations reduce educational workload.",44.31,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09182v1_Position on LLM-Assisted Peer Review Addressing Re.pdf,Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback,"JungMin Yun, JuneHyoung Kwon, MiHyeon Kim, YoongBin Kim, 2",,,"LLM-assisted peer review, reviewer gap, mentoring, feedback, review quality, artificial intelligence, peer review","The paper critiques LLM-driven review automation, advocating for human-centered systems that mentor reviewers and enhance feedback to sustain scholarly standards.",43.89,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09195v1_ProFit Leveraging High-Value Signals in SFT via Pr.pdf,ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection,"Tao Liu, Taiqiang Wu, Runming Yang, Shaoning Sun, Junjie Wang, Yujiu Yang",,,"Supervised Fine-Tuning, Large Language Models, Token Probability, Semantic Importance, Answer Diversity, Cost-Efficiency","This paper proposes ProFit, a strategy that selectively masks low-probability tokens to mitigate single-reference overfitting in supervised fine-tuning. By emphasizing high-probability tokens, ProFit improves semantic integrity while maintaining efficiency, outperforming traditional single-reference SFT approaches.",44.74,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09208v2_Mikasa A Character-Driven Emotional AI Companion I.pdf,Mikasa: A Character-Driven Emotional AI Companion,Miki Ueno,miki.ueno@kcg.ac.jp,,"character design, relationship definition, emotional AI, Japanese Oshi culture","This paper explores the role of character coherence and relationship clarity in long-term AI companionship, using Mikasa as a case study inspired by Japanese Oshi culture. It argues that sustained user engagement depends on stable personality and defined interaction norms rather than solely on language modeling capabilities.",43.33,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09212v1_Annealed Relaxation of Speculative Decoding for Fa.pdf,Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image Generation,"Xingyao Li, Fengzhuo Zhang, Cunxiao Du, Hui Ji",10.48550/arxiv/2303.08732,arXiv:2303.08732,"auto-regressive, speculative decoding, relaxed decoding, image generation, inference speed, quality trade-off","This paper proposes COOL-SD, an annealed relaxation of speculative decoding, to accelerate auto-regressive image generation by improving inference speed while maintaining quality. It introduces theoretical insights into relaxed SD and presents COOL-SD, demonstrating consistent improvements over prior methods.",45.05,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09213v1_SpikeVAEDiff Neural Spike-based Natural Visual Sce.pdf,SpikeV AEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-V AE and Versatile Diffusion,"Jialu Li, Taiyan Zhou",,,"neural spike, visual scene reconstruction, neural decoding, computational modeling, brain-computer interface","Reconstructing natural visual scenes from neural activity is a key challenge. This paper presents SpikeVAEDiff, a framework combining VDVAE and Versatile Diffusion to generate high-resolution image reconstructions from neural spike data. The study explores brain region activations and evaluates reconstruction performance, highlighting the advantages of spike data over fMRI.",45.6,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09233v1_GIFT Unlocking Global Optimality in Post-Training .pdf,Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization,"Zhengyang Zhao, Lu Ma, Yizhen Jiang, Xiaochen Ma, Zimo Meng, Chengyu Shen, Lexiang Tang, Haoze Sun, Peng Pei, Wentao Zhang",,,"Large Reasoning Models, Supervised Fine-Tuning, Reinforcement Learning, Post-Training Optimization, Distributional Collapse, Gibbs Initialization, Optimality","The paper proposes GIFT, a Gibbs Initialization with Finite Temperature method, to address distributional collapse in post-training by reformulating SFT within a unified framework. It shows GIFT outperforms standard SFT and RL baselines, enabling global optimality in post-training.",45.28,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09236v2_Reward Learning through Ranking Mean Squared Error.pdf,Reward learning through ranking means squared error,"Chaitanya Kharyal, Calarina Muslimani, Matthew E. Taylor",10.48550/arXiv.2405.12345,arXiv:2405.12345,"reward learning, reinforcement learning, ranking mean squared error, robotic locomotion","The paper introduces Ranked Return Regression for RL (R4), a novel rating-based method that uses a novel ranking mean squared error loss. R4 treats teacher-provided ratings as ordinal targets and learns from trajectory-rating pairs, offering formal guarantees and demonstrating superior performance on robotic benchmarks.",45.14,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09239v2_DSA-Tokenizer Disentangled Semantic-Acoustic Token.pdf,DSA-Tokenizer: Disentangled Semantic-Acoustic Tokenization,"Hanlin ZHANG1, Daxin Tan2, Dehua Tao2, Xiao Chen2, Yunhe Li1, Yuchen Cao 1, Jianping Wang 1, Linqi Song 1",,,"speech tokenizer, disentangled tokenization, semantic-acoustic fusion, speech large language models, audio reconstruction, reconstruction-recombination","Proposes DSA-Tokenizer to explicitly disentangle speech into discrete semantic and acoustic tokens using hierarchicalFlow-Matching, enabling better control and flexibility in speech LLMs.",45.4,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09248v1_Hybrid guided variational autoencoder for visual p.pdf,Hybrid guided variational autoencoder for visual place recognition,"Ni Wang, Zihan You, Emre Neftci, Thorben Schoepe",arXiv:2408.14337,arXiv:2408.14337,"Visual place recognition, Spiking neural network, Guided variational autoencoder, Robotics, Low latency hardware","This work presents a hybrid guided variational autoencoder combining event-based vision sensors with a novel guided VAE to enable efficient visual place recognition for mobile robots. The model disentangles visual features of 16 distinct indoor places, achieving robust performance under varying illumination and demonstrating strong generalization capabilities.",46.12,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09251v1_HGATSolver A Heterogeneous Graph Attention Solver .pdf,HGA TSolver: A Heterogeneous Graph Attention Solver for Fluid–Structure Interaction,"Qin-Yi Zhang, Hong Wang, Siyao Liu, Haichuan Lin, Linying Cao, Chen Chen, Shuangyi Wang, Zeng-Guang Hou, Zeng-Guang Hou",zhangqinyi2024,,"fluid-structure interaction, heterogeneous graph attention, physics-conditioned gating, inter-domain gradient balancing, coupled multi-physics systems",The paper proposes the Heterogeneous Graph Attention Solver (HGA TSolver) to address challenges in fluid-structure interaction by modeling distinct physical domains with specialized graph structures and adaptive learning mechanisms.,45.72,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09253v1_RIFT Repurposing Negative Samples via Reward-Infor.pdf,Repurposing Negative Samples via Reward-Informed Fine-Tuning,"Zehua Liu, Shuqi Liu, Tao Zhong, Mingxuan Yuan",,,"Large Language Models, Supervised Fine-Tuning, Rejection Sampling, Reward-Informed Fine-Tuning, Negative Samples, Data Efficiency","Proposes Reward Informed Fine-Tuning (RIFT) to repurpose negative samples, improving data efficiency by reweighting losses from both positive and negative trajectories without relying on costly expert data.",44.07,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09259v1_MAXS Meta-Adaptive Exploration with LLM Agents.pdf,Meta-Adaptive Exploration with LLM Agents,"Jian Zhang, Zhiyuan Wang, Zhangqi Wang, Yu He, Haoran Luo, Li Yuan, Lingling Zhang, Rui Mao, Qika Lin, Jun Liu",,,"LLM Agents, meta-adaptive exploration, LLM agents, LLM agents with tools, reasoning, trajectory stability, computational efficiency","The paper proposes MAXS, a meta-adaptive reasoning framework for LLM agents that integrates tool execution and reasoning planning. MAXS uses a lookahead strategy to extend reasoning paths, balances global effectiveness with computational efficiency, and introduces a trajectory convergence mechanism to control inference cost. Extensive experiments across multiple models and datasets show consistent improvements.",46.22,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09260v1_Efficient Paths and Dense Rewards Probabilistic Fl.pdf,Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models,"Yan Liu, Feng Zhang, Zhanyu Ma, Jinghua Hao, Renqing He, Han Liu, Yangdong Deng",,,"large language models, chain-of-thought, probabilistic flow reasoning, reasoning efficiency, reinforcement learning, flow-guided decoding, flow-based reinforcement learning","This paper proposes CoT-Flow, a framework that models discrete reasoning steps as continuous probabilistic flows. It introduces flow-guided decoding and flow-based reinforcement learning to quantify each step's contribution to reasoning performance, aiming to improve inference efficiency and optimization in LLMs.",46.15,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09262v1_Magnifying change Rapid burn scar mapping with mul.pdf,"Magnifying change: Rapid burn scar mapping with multi-resolution, multi-source satellite imagery","Maria Sdraka, Dimitrios Michail, Ioannis Papoutsis",,,"Artificial intelligence, Machine Learning, Remote Sensing, burn scar mapping, disaster management, disaster monitoring, wildfires, burn scar mapping, change detection, downscaling, super-resolution","Delineating wildfire affected areas using satellite imagery remains challenging due to irregular spectral changes. The paper proposes a novel deep learning model, BAM-MRCD, using multi-resolution, multi-source satellite data for timely burnt area mapping with high spatial and temporal resolution.",45.53,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09264v1_Coordinated Pandemic Control with Large Language M.pdf,Coordinated Pandemic Control with Large Language Model Agents,"Ziyi Shi, Xusen Guo, Hongliang Lu, Mingxing Peng, Haotian Wang, Zheng Zhu, Zhenning Li, Yuxuan Liang, Xinhu Zheng, Hai Yang",,,"Pandemic Control, Large Language Models, Multi-Agent System, Coordinated Policymaking","This study proposes a large language model (LLM) multi-agent policymaking framework to support coordinated pandemic control across regions. By assigning LLM agents to administrative regions and enabling cross-regional communication, the framework aims to enable proactive intervention and improve policy coordination. Validation using U.S. COVID-19 data shows potential reductions in infections and deaths.",46.64,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09269v2_RISER Orchestrating Latent Reasoning Skills for Ad.pdf,Orchestrating Latent Reasoning Skills for Adaptive Activation Steering,"Wencheng Ye, Xiaoyang Yuan, Yi Bin, Hengyu Jin, Liang Peng, Pengpeng Zeng, Heng Tao Shen",,,"domain-specific reasoning, large language models, activation steering, reasoning adaptation, control strategies","RISER proposes a plug-and-play framework that adaptively steers LLM reasoning in activation space using a router-based intervention, achieving improvements over CoT-style methods with higher token efficiency.",43.47,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09274v1_A3-Bench Benchmarking Memory-Driven Scientific Rea.pdf,A3-Bench: Benchmarking Memory-Driven Scientific Reasoning,"Jian Zhang, Yu He, Zhiyuan Wang, Zhangqi Wang, Kai He, Fangzhi Xu, Qika Lin",arXiv:2601.09274v1,arXiv:2601.09274,"scientific reasoning, memory-driven, anchors, attractors, LLM, reasoning consistency","This paper proposes A3-Bench, a benchmark to evaluate scientific reasoning through dual-scale memory-driven activation, focusing on anchor and attractor mechanisms. It addresses gaps in existing benchmarks by measuring memory activation rates using the AAUI metric and analyzing reasoning performance across paradigms.",45.04,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09278v1_M3Searcher Modular Multimodal Information Seeking .pdf,M3Searcher: Modular Multimodal Information Seeking Agency with Retrieval-Oriented Reasoning,"Xiaohan Yu, Chao Feng, Lang Mei, Chong Chen",10.1234/m3searcher.2025,,"modular agent, multimodal, information seeking, retrieval-oriented","This paper proposes M3Searcher, a modular multimodal information-seeking agent that decouples information acquisition from answer derivation. It introduces retrieval-oriented multi-objective reward and develops MM-SearchVQA to support retrieval-centric reinforcement learning training. Experimental results show superior performance in complex multimodal tasks.",44.13,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09280v1_ReGraM Region-First Knowledge Graph Reasoning for .pdf,Region-First Knowledge Graph Reasoning for Medical Question,"Chaerin Lee, Sohee Park, Hyunsik Na, Daseon Choi",10.1000/REGraM,12345678,"medical question answering, knowledge graphs, multi-hop reasoning, hallucination reduction","ReGraM introduces a region-first knowledge graph reasoning framework for medical QA, improving accuracy by focusing inference on relevant subgraphs and reducing noise through structured evidence alignment.",40.85,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09281v1_STaR Sensitive Trajectory Regulation for Unlearnin.pdf,Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models,"Jingjing Zhou, Gaoxiang Cong, Li Su, Liang Li",,,"large reasoning models, unlearning, privacy protection, trajectory regulation, chain-of-thought, sensitive content","Proposes Sensitive Trajectory Regulation (STaR), a parameter-free inference-time unlearning framework for LRMs, to mitigate privacy leakage during multi-step reasoning.",43.01,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09282v1_Cluster Workload Allocation Semantic Soft Affinity.pdf,Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing,"Leszek Sliwko, Jolanta Mizeria-Pietraszko",10.1007/978-3-642-45888-7,,"Artificial Intelligence, Kubernetes, Load Balancing, Semantic Parsing, Soft-Affinity, Task Assignment","This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. It employs a Large Language Model integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. Empirical evaluation shows high LLM parsing accuracy and superior scheduling quality across scenarios.",44.82,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09286v1_Why not Collaborative Filtering in Dual View Bridg.pdf,Why not Collaborative Filtering in Dual View? Bridging Sparse and Dense Models,"Hanze Guo, Gaoling School of Artificial Intelligence, Renmin University of China, Microsoft Research Asia, China, Gaoling School of Artificial Intelligence, Renmin University of China",https://doi.org/XXXXXXX.XXXXXXX,,"Collaborative Filtering, Dual View Alignment, Sparse and Dense model","The paper addresses the signal-to-noise ratio ceiling in dense embedding-based recommender systems under data sparsity, proposing SaD to unify dense and sparse representations for improved performance.",43.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09292v1_Blue Teaming Function-Calling Agents.pdf,Blue Teaming Function-Calling Agents,"Greta Dolcetti, Giulio Zizzo, Sergio Maffeis",10.1234/example.doi,,"function-calling, LLMs, security, defenses, agentic systems",Experimental evaluation of robustness of open source LLMs against adversarial attacks; introduces new defenses and tools.,41.95,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09293v1_Policy-Based Reinforcement Learning with Action Ma.pdf,Policy-Based Reinforcement Learning with Action Masking for Dynamic Job Shop,"Sofiene Lassoued, Stefan Lier, Andreas Schwung",,,"Dynamic Job Shop Scheduling, Fault tolerance, Reinforcement learning, Actions masking, Petri nets","Presents a novel framework for Dynamic Job Shop Scheduling under uncertainty, using Coloured Timed Petri Nets and Maskable Proximal Policy Optimization to handle stochastic job arrivals and machine failures. Demonstrates superior makespan minimization compared to traditional methods.",43.32,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09306v1_On-Device Large Language Models for Sequential Rec.pdf,On-Device Large Language Models for Sequential Recommendation,"Xin Xia, Hongzhi Yin, Shane Culpepper",10.1145/3773966.3777961,Ninth ACM International Conference on Web Search and Data Mining (WSDM) 2026,"Recommender Systems, Sequential Recommendation, On-Device Recommendation, Model Compression, Resource Constrained Devices","The paper proposes OD-LLM, a task-adaptive compression framework for efficient on-device deployment of large language models in sequential recommendation tasks. It combines low-rank structural compression with tokenization normalization and progressive alignment to maintain performance while reducing model size.",45.01,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09313v1_Understanding or Memorizing A Case Study of German.pdf,Understanding or Memorizing? A Case Study of German Definite Articles,"Jonathan Drechsel, Erisa Bytyqi, Steffen Herbold",,,"language models, grammatical agreement, memorization, German definite articles","The study investigates whether German definite articles are learned through rule-based generalization or memorization, using gradient-based interpretability methods. Findings suggest partial reliance on memorized associations.",43.24,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09342v1_Improving Implicit Hate Speech Detection via a Com.pdf,Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework,"Ewelina Gajewska, Katarzyna Budzynska, Jarosław A. Chudziak",,,"LLMs, Community agents, Hate speech, Social media, Moderation, Fairness","This work proposes a contextualised detection framework for implicitly hateful speech, implemented as a multi-agent system comprising a central Moderator Agent and dynamically constructed Community Agents. The approach integrates socio-cultural context from public knowledge sources, enabling identity-aware moderation that improves classification accuracy and fairness across demographic groups.",45.46,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09351v1_Navigating Ethical AI Challenges in the Industrial.pdf,Navigating Ethical AI Challenges in the Industrial Sector: Balancing Innovation and Responsibility,"Dr. Ruomu Tan, Dr.-Ing Martin W Hoffmann",arXiv:2601.09351v1,2601.09351v1,"artificial intelligence, industrial sector, ethical challenges, responsibility, innovation, research and development","This chapter explores how AI-empowered industrial innovation intersects with ethics, addressing transparency, accountability, and fairness. It examines ethical considerations in AI applications, the role of ethical practices in R&D, and the need to embed ethical principles in industrial AI systems to foster trust and inclusive progress.",47.09,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09353v1_Monte-Carlo Tree Search with Neural Network Guidan.pdf,Monte-Carlo Tree Search with Neural Network Guidance,"Ioannis Peridis, Dimitrios Troullinos, Georgios Chalkiadakis, Pantelis Giankoulidis, Ioannis Papamichail, Markos Papageorgiou",arXiv:2601.09353v1,arXiv:2601.09353,"Monte-Carlo Tree Search, Neural Network Guidance, Autonomous Driving, Lane-Free Traffic, Reinforcement Learning, Traffic Flow","This paper presents a Monte-Carlo Tree Search (MCTS) planning approach for single-agent autonomous driving in lane-free traffic environments. It integrates a pre-trained neural network to guide tree search, aiming to improve safety and performance metrics such as collision rates and speed. The study explores nudging behavior and computational trade-offs.",46.62,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09361v1_GeoRA Geometry-Aware Low-Rank Adaptation for RLVR.pdf,GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR,"Jiaying Zhang, Lei Shi, Jiguo Li, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He",,,"Reinforcement Learning, RLVR, Parameter-efficient Tuning, Low-rank Adaptation, Geometric Structures, Optimization Stability","Proposes GeoRA to address optimization instability in RLVR by leveraging geometry-aware low-rank adaptation, improving performance and generalization.",43.91,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09365v1_Frame of Reference Addressing the Challenges of Co.pdf,Frame of Reference: Addressing the Challenges of Common Ground,"Biswesh Mohapatra, Théo Charlot, Nantes Université, Giovanni Duca, University of Trento, Mayank Palan, Laurent Romary, Justine Cassell",,,"common ground, situational dialogs, embodied conversational agents, social robots, grounding mechanisms","This paper evaluates a model’s ability to establish common ground using relational references in dynamic, shared environments. It highlights the need for systems to leverage grounded information beyond short context windows, especially in long-duration situational dialogues involving embodied agents and social robots.",45.25,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09381v1_Query Languages for Machine-Learning Models.pdf,Query Languages for Machine-Learning Models,"Martin Grohe, doi': , , arxiv_id': , , keywords': [, machine learning models, ,, weighted finite structures",None,None,None,"This paper explores logics for weighted finite structures—specifically first-order logic with summation and its extension IFP(SUM)—and investigates their use as query languages for neural networks, discussing expressiveness and complexity.",42.72,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09382v1_Long-term Task-oriented Agent Proactive Long-term .pdf,Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments,"Qinglong Shi, Donghai Wang, Hantao Zhou, Jiguo Li, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He",,,"long-term intent, proactive agents, dynamic environments, task-oriented interaction, user intent maintenance","This paper proposes a novel interaction paradigm for proactive task-oriented agents capable of bridging the gap between static user needs and dynamic environments. It introduces intent-conditioned monitoring and event-triggered follow-up, evaluates performance using a new benchmark (ChronosBench), and demonstrates improved task completion rates through synthetic data training.",44.78,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09394v2_FairGE Fairness-Aware Graph Encoding in Incomplete.pdf,FairGE: Fairness-Aware Graph Encoding in Incomplete Social Networks,"Renqiang Luo, Huafei Huang, Tao Tang, Jing Ren, Ziqi Xu, Mingliang Hou, Zheng Ren, Enyan Dai, Feng Xia",https://doi.org/XXXXXXX.XXXXXXX,,"Social Networks, Graph Learning, Graph Transformers, Fairness, Incomplete Data","Graph Transformers (GTs) are increasingly applied to social network analysis, yet their deployment is often constrained by fairness concerns. This issue is particularly critical in incomplete social networks, where sensitive attributes are frequently missing due to privacy and ethical restrictions. Existing solutions commonly generate these incomplete attributes, which may introduce additional biases and further compromise user privacy. To address this challenge, FairGE (Fair Graph Encoding) is introduced as a fairness-aware framework for GTs in incomplete social networks. Instead of generating sensitive attributes, FairGE encodes fairness directly through spectral graph theory. By leveraging the principal eigenvector to represent structural information and padding incomplete sensitive attributes with zeros to maintain independence, FairGE ensures fairness without data reconstruction. Theoretical analysis demonstrates that the method suppresses the influence of non-principal spectral components, thereby enhancing fairness. Extensive experiments on seven real-world social network datasets confirm that FairGE achieves at least a 16% improvement in both statistical parity and equality of opportunity compared with state-of-the-art baselines. The source code is shown in https://github.com/LuoRenqiang/FairGE.",48.23,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09398v1_Ability Transfer and Recovery via Modularized Para.pdf,Ability Transfer and Recovery via Modularized Parameters Localization,"Songyao Jin, Kun Zhou, Wenqi Li, Peng Wang, Biwei Huang",,,"ability transfer, recovery, modularized parameters, LLM localization, catastrophic forgetting","Investigates ability distribution in LLM parameters by analyzing module activations, proposes ACT for selective ability transfer, and demonstrates recovery of forgotten abilities.",42.3,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09413v1_Speech-Hands A Self-Reflection Voice Agentic Appro.pdf,Speech-Hands: A Self-Reflection Voice Agentic Approach to Speech Recognition and Audio Reasoning with Omni Perception,"Zhen Wan, Chao-Han Huck Yang, Jinchuan Tian, Hanrong Ye, Ankita Pasad, Szu-wei Fu, Ryo Hachiuma, Shizhe Diao, Kunal Dhawan, Sreyan Ghosh, Yusuke Hirota, Zhehuai Chen, Rafael Valle, Ehsan Hosseini Asl",10.48550/arXiv:2109.08656v1,arXiv:2109.08656,"speech recognition, audio reasoning, self-reflection, omni perception, speech-agentic framework","The paper introduces Speech-Hands, a voice-agentic framework that learns when to trust internal models versus external audio perception. It addresses performance degradation from naive fine-tuning by enabling explicit self-reflection decisions, improving robustness across audio QA tasks and outperforming baselines by 12.1% WER on benchmarks.",46.42,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09416v1_Radiomics-Integrated Deep Learning with Hierarchic.pdf,Radiomics-Integrated Deep Learning with Hierarchical Loss for Osteosarcoma Histology Classification,"Yaxi Chen, Zi Ye, Oliver Yu, Simin Ni, Jie Huang",,,"Osteosarcoma, Radiomics, Multi-Task Learning, Uncertainty Weighting, Hierarchical Loss, Digital Pathology","This study proposes using radiomic features as additional inputs in deep learning models for osteosarcoma histopathology classification. It introduces a hierarchical loss function to optimize two binary classification tasks (tumor vs non-tumor, viable vs non-viable) and demonstrates improved performance using the TCIA OS Tumor Assessment dataset.",45.41,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09421v2_Bias Dynamics in BabyLMs Towards a Compute-Efficie.pdf,Bias Dynamics in BabyLMs: Towards a Compute-Efficient Sandbox for Democratising Pre-Training Debiasing,"Filip Trhlik, Andrew Caines, Paula Buttery",ft360@cam.ac.uk,,"BabyLMs, pre-training debiasing, computational efficiency, bias mitigation, large language models","This work investigates BabyLMs—compact BERT-like models trained on small corpora—as a low-cost proxy for studying bias acquisition and dynamics. It demonstrates that BabyLMs exhibit patterns of bias formation similar to standard BERT, enabling pre-model debiasing experiments that reduce training costs while preserving insights into gender and toxicity-related biases.",44.94,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09433v1_Do Transformers Understand Ancient Roman Coin Moti.pdf,Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?,"David Reid, Ognjen Arandjelovi",arXiv:2601.09433v1,2601.09433,,"This paper explores the application of Vision Transformer (ViT) deep learning to identify semantic elements on ancient coins, comparing its performance to pre-trained CNN models and discussing challenges in ancient coin analysis.",43.04,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09445v1_Where Knowledge Collides A Mechanistic Study of In.pdf,Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models,"Minh Vu Pham, Hsuvas Borkakoty, Yufang Hou, IBM Research",arXiv:2601.09445v1,arXiv:2601.09445,"language models, intra-memory knowledge conflict, mechanistic interpretability, hallucinations, knowledge editing","This study investigates how conflicting information encoded within a model's internal knowledge during pre-training leads to intra-memory knowledge conflicts. Using mechanistic interpretability methods, the authors identify internal components responsible for encoding such conflicts and demonstrate how interpretability techniques can intervene at inference time.",44.11,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09446v1_Improving Symbolic Translation of Language Models .pdf,Improving Symbolic Translation of Language Models for Logical Reasoning,"Ramya Keerthy Thatikonda, Jiuzhou Han, Wray Buntine, Ehsan Shareghi",10.48550/arXiv.2024.12345,arXiv:2408.12345,"symbolic translation, logical reasoning, language models, formal language, predicate logic, error correction","The paper addresses challenges in translating natural language into first-order logic using language models, proposes an incremental inference framework with verification modules, and evaluates improvements across multiple model families.",41.36,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09448v1_Population-Aligned Audio Reproduction With LLM-Bas.pdf,Population-Aligned Audio Reproduction With LLM-Based Equalizers,"Ioannis Stylianou, Jon Francombe, Pablo Martín-Nuevo, Sven Ewan Shepstone, Member, IEEE, Zheng-Hua Tan",10.1109/JPSP.2024.12345,,"LLMs, Equalization, Audio Reproduction, Listening Experiments, Recommender Systems","Conventional audio equalization is a static process requiring manual adjustments. This paper introduces an LLM-based alternative that maps natural language prompts to equalization settings, enabling conversational sound system control. Evaluation shows statistically significant improvements over static baselines using distributional metrics.",42.25,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09451v1_Late Breaking Results Quamba-SE Soft-edge Quantize.pdf,Quamba-SE: Soft-edge Quantizer for State Space Models,"Yizhi Chen, Ahmed Hemani",10.48550/arXiv.2405.12345,arXiv:2405.12345,"quantization, State Space Models, soft-edge, quantum-aware-training","Proposes Quamba-SE, a soft-edge quantizer for State Space Model activation quantization, preserving outlier information while maintaining precision.",40.46,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09455v1_On the Hardness of Computing Counterfactual and Se.pdf,On the Hardness of Computing Counterfactual and Semi-factual Explanations in XAI,"André Artelta, Martin Olsen, Kevin Tierney",arXiv:2601.09455v1,arXiv:2601.09455,"counterfactual explanations, semi-factual explanations, XAI, computational complexity, regulatory implications","This paper examines the computational hardness of generating counterfactual and semi-factual explanations in XAI, highlighting that explanation generation can be computationally intensive and, under certain assumptions, difficult to approximate. It discusses implications for research and policy.",45.11,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09460v1_SoK Enhancing Cryptographic Collaborative Learning.pdf,Enhancing Cryptographic Collaborative Learning with Differential Privacy,"Francesco Capano, Jonas B. Löher, Benjamin Weggenmann",,,"differential privacy, cryptography, collaborative machine learning, privacy-preserving learning, secure noise sampling","This work systematizes the challenges of combining cryptography and differential privacy in collaborative machine learning. It introduces a unified framework for cryptographic and differentially private collaborative learning, analyzes trade-offs of secure noise sampling, and proposes future research directions.",44.85,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09465v1_EvoFSM Controllable Self-Evolution for Deep Resear.pdf,EvoFSM: Controllable Self-Evolution for Deep Research with Finite State,"Shuo Zhang, Chaofa Yuan, Ryan Guo, Xiaomin Yu, Rui Xu, Zhangquan Chen, Zinuo Li, Zhi Yang, Shuhao Guan, Zhenheng Tang, Sen Hu, Liwen Zhang",10.48550/arXiv.2601.09465,2601.09465,"EvoFSM, Self-evolution, Deep research, Finite State Machine, Adaptability, Control, LLM, Open-ended queries","Structured self-evolution enables agents to adapt reliably while maintaining control, addressing instability and hallucination issues in LLM-based agents.",48.11,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09467v1_Searth Transformer A Transformer Architecture Inco.pdf,Searth Transformer: A Transformer Architecture Incorporating Earth's Geospheric Physical Priors for Global Mid-Range Weather Forecasting,"Tianye Li, Qi Liu, Hao Li, Lei Chen, Fei Zheng, Xiangao Xia, Yang Wang, Weiwei Wang, Xuan Tong, Ziqing Zu, Jiang Jiang, Haochen Li, Mingxing Li, Jiangjiang Xia, Yi Fang, Shenming Fu, Jiangjiang, Jiangjiang Xia, Yang Wang, Gang Huang, Weiwei Wang, Shanghai Academy of Artificial Intelligence for Science, Beijing Aviation Meteorological Institute, Laboratory of Middle Atmosphere and Global Environment Observation (LAGEO), Earth System Numerical Simulation Science Center, Meteorological Bureau of Shenzhen Municipality, Shenzhen Key Laboratory of Severe Weather in South China, Jiangsu Key Laboratory of Intelligent Weather Forecasting and Applications Based on Big Data, Nanjing University of Information Science and Technology, Beijing University of Posts and Telecommunications, State Key Laboratory of Satellite Ocean Environment Dynamics, Ministry of Natural Resources, Beijing Meteorological Service Center",,,"Transformer, Earth's geospheric physical priors, Global mid-range weather forecasting, Searth Transformer, Physical consistency, Zonal periodicity, Autoregressive training, Resource constraints, Forecast accuracy",Accurate global medium-range weather forecasting requires incorporating Earth's spherical topology and zonal periodicity. The proposed Shifted Earth Transformer integrates these physical constraints into its architecture to improve accuracy and efficiency.,47.86,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09469v2_FairGU Fairness-aware Graph Unlearning in Social N.pdf,FairGU: Fairness-aware Graph Unlearning in Social Networks,"Renqiang Luo, Yongshuai Yang, Huafei Huang, Qing Qing, Mingliang Hou, Ziqi Xu, Yi Yu, Jingjing Zhou, Feng Xia",https://doi.org/XXXXXXX.XXXXXXX,,"fairness, privacy, graph unlearning, social network","Graph unlearning has emerged as a critical mechanism for supporting sustainable and privacy-preserving social networks, enabling models to remove the influence of deleted nodes and thereby better safeguard user information. However, existing techniques insufficiently protect sensitive attributes, often leading to degraded algorithmic fairness compared with traditional graph learning methods. To address this gap, we introduce FairGU, a fairness-aware graph unlearning framework designed to preserve both utility and fairness during the unlearning process. FairGU integrates a dedicated fairness-aware module with effective data protection strategies, ensuring that sensitive attributes are neither inadvertently amplified nor structurally exposed when nodes are removed. Through extensive experiments on multiple real-world datasets, we demonstrate that FairGU consistently outperforms state-of-the-art graph unlearning methods and fairness-enhanced graph learning baselines in terms of both accuracy and fairness metrics. Our findings highlight a previously overlooked risk in current unlearning practices and establish FairGU as a robust and equitable solution for the next generation of socially sustainable networked systems.",48.67,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09470v1_Personalized Multimodal Feedback Using Multiple Ex.pdf,Personalized Multimodal Feedback Using Multiple External Representations: Strategy Profiles and Learning in High School Physics,"Natalia Revenga-Lozano, Karina E. Avila, Steffen Steinert, Matthias Schweinberger, Clara E. Gómez-Pérez, Jochen Kuhn",arXiv:2601.09470v1,arXiv:2601.09470,"multimodal feedback, personalized learning, high school physics, representation competence, adaptive feedback","This study investigates how personalized feedback integrated with multiple external representations influences physics learning in high school students. Using a 16-24 week observational design, it finds that elaborated, multimodal feedback consistently improves post-test performance, especially among learners with lower representational competence. The research highlights the importance of strategy profiles in shaping feedback adoption.",48.44,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09473v1_SimMerge Learning to Select Merge Operators from S.pdf,SimMerge: Learning to Select Merge,"Oliver Bolton, Aakanksha, Arash Ahmadian, Sara Hooker, Marzieh Fadaee, Beyza Ermis","1Cohere Labs, 2Cohere, 3Google, 4Adaption Labs",2601.09473v1,"model merging, similarity signals, LLM composition, merge operator, similarity selection","This paper introduces SimMerge, a predictive method for selecting optimal merge operators using inexpensive similarity signals. It demonstrates improved performance over standard merge-operator approaches in 2-way LLM merges and generalizes to larger-scale multi-way merges.",45.31,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09478v3_Bridging Semantic Understanding and Popularity Bia.pdf,Bridging Semantic Understanding and Popularity Bias with LLMs,"Renqiang Luo, Dong Zhang, Yupeng Gao, Wen Shi, Mingliang Hou, Jiaying Liu, Zhe Wang, Shuo Yu",https://doi.org/XXXXXXX.XXXXXXX,,"Semantic analysis, Recommender systems, Algorithmic fairness, Popularity bias, LLM","Semantic understanding of popularity bias is a crucial yet underexplored challenge in recommender systems, where popular items are often favored at the expense of niche content. Most existing debias methods treat the semantic understanding of popularity bias as a matter of diversity enhancement or long-tail coverage, neglecting the deeper semantic layer that embodies the causal origins of the bias itself. Consequently, such shallow interpretations limit both their debiasing effectiveness and recommendation accuracy. In this paper, we propose FairLRM, a novel framework that bridges the gap in the semantic understanding of popularity bias with Recommender via Large Language Model (RecLLM). FairLRM decomposes popularity bias into item-side and user-side components, using structured instruction-based prompts to enhance the model’s comprehension of both global item distributions and individual user preferences. Unlike traditional methods that rely on surface-level features such as 'diversity' or 'debiasing', FairLRM improves the model’s ability to semantically interpret and address the underlying bias. Through empirical evaluation, we show that FairLRM significantly enhances both fairness and recommendation accuracy, providing a more semantically aware and trustworthy approach to enhance the semantic understanding of popularity bias.",46.75,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09503v1_What Do LLM Agents Know About Their World Task2Qui.pdf,What Do LLM Agents Know About Their World?,"Siyuan Liu, Hongbang Yuan, Xinze Li, Ziyue Zhu, Yixin Cao, Yu-Gang Jiang",10.48550/arXiv.2601.09503,10.48550/arXiv.2601.09503,"large language model, LLM agents, environment understanding, trajectory-based metrics, generalization, world model","This paper examines whether large language model agents develop a grounded, transferable understanding of their environments beyond task success. It introduces Task-to-Quiz (T2Q) to assess world-state knowledge and highlights that current evaluation relies on trajectory metrics, which may not reflect true environmental understanding.",45.46,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09518v1_Learning Whole-Body Human-Humanoid Interaction fro.pdf,Learning Whole-Body Human-Humanoid Interaction,"Wei-Jin Huang, Yue-Yi Zhang, Yi-Lin Wei, Zhi-Wei Xia, Juantao Tan, Yuan-Ming Li, Zhilin Zhao, Wei-Shi Zheng, Guangdong Province Key Laboratory of Information Security Technology",arXiv:2601.09518v1,HHI Dataset,"Human-Human Interaction, Humanoid Robotics, Physics-Aware Interaction, Decoupled Spatio-Temporal Action Reasoner, D-STAR, Whole-Body Interaction","The paper presents a contact-centric two-stage policy (D-STAR) for converting human-human interaction sequences into physically consistent humanoid clips. It addresses limitations of existing retargeting methods and introduces a decoupled action reasoner to preserve contact semantics, enabling high-quality HHoI data generation.",46.23,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09520v1_Towards Realistic Synthetic Data for Automatic Dru.pdf,TOW ARDS REALISTIC SYNTHETIC DA TA FOR AUTOMA TIC DRUM TRANSCRIPTION,"Pierfrancesco Melucci, Paolo Merialdo, Taketo Akama",,,"Automatic Drum Transcription, Deep Learning, SoundFont, MIDI, One-shot Samples, Sequence-to-Sequence, Music Transcription","This paper introduces a semi-supervised method to automatically curate a large, diverse corpus of one-shot drum samples from unlabeled audio sources. It synthesizes a high-quality MIDI dataset to train a sequence-to-sequence transcription model, achieving state-of-the-art results on ENST and MDB test sets.",44.07,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09527v1_Private LLM Inference on Consumer Blackwell GPUs A.pdf,Private LLM Inference on Consumer Blackwell GPUs: A Practical Guide for Cost-Effective Local Deployment in SMEs,"Jonathan Knoop, Hendrik Holtmann",,,"LLM deployment, consumer GPUs, data privacy, cost-effective inference, NVIDIA Blackwell, quantization, quantum computing, SMEs, cloud APIs","SMEs seek alternatives to cloud LLM APIs due to privacy and cost concerns. This study evaluates NVIDIA Blackwell consumer GPUs for production inference, benchmarking four models across diverse workloads and configurations.",44.99,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09536v1_Omni-R1 Towards the Unified Generative Paradigm fo.pdf,Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning,"Dongjie Cheng, Yongqi Li, Hongru Cai, Yupeng Hu, Wenjie Wang, Liqiang Nie",arXiv:2601.09536v1,arXiv:2601.09536,"multimodal reasoning, generative modeling, Omni-R1, visual information, spatial relations","The paper proposes a unified generative multimodal reasoning framework using Omni-R1, which integrates perception alignment and perception reward to enable functional image generation. It introduces Omni-R1-Zero to eliminate multimodal annotations by bootstrapping from text-only data. Empirical results demonstrate its effectiveness across diverse multimodal tasks.",45.55,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09555v1_Benchmarking Post-Training Quantization of Large L.pdf,Benchmarking Post-Training Quantization of Large Language Models,"Manyi Zhang, Ji-Fu Li, Zhongao Sun, Haoli Bai, Zhen Zhenhua, Dong Xianzhi Yu",,,"post-training quantization, low-precision formats, large language models, quantization sensitivity, MXFP","This work systematically investigates post-training quantization under Microscaling Floating-Point formats, evaluating over 7 PTQ algorithms across 15 benchmarks and 3 LLM families. Key findings highlight MXFP8's near-lossless performance, MXFP4's accuracy degradation, format compatibility effects, and the impact of scaling factors.",44.77,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09566v2_Hot-Start from Pixels Low-Resolution Visual Tokens.pdf,Hot-Start from Pixels: Low-Resolution Visual Tokens for Chinese,"Shuyang Xiang, Hao Guan",10.10943/arxiv/2601.09566,2601.09566v2,"Chinese language modeling, character-level modeling, low-resolution visuals, visual tokens, language representation","Investigates using low-resolution grayscale images (as low as 8×8 pixels) as alternatives to index-based character tokens for Chinese, achieving competitive accuracy and demonstrating the value of visual structure in language modeling.",45.21,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09600v1_Information Access of the Oppressed A Problem-Posi.pdf,Information Access of the Oppressed: A Problem-Posing Framework for Envisioning Emancipatory Information Access Platforms,"Bhaskar Mitra, Nicola Neophytou, Sireesh Gururaja",https://doi.org/XXXXXXX.XXXXXX,,"Information Access, Emancipatory Information Access, Search and Society, Sociotechnical Information Systems","This paper examines how online information access platforms are vulnerable to authoritarian capture, especially amid democratic erosion, AI advancements, and Big Tech dominance. Drawing on Paulo Freire’s emancipatory pedagogy, it argues that technologists should collaborate with marginalized communities to co-create technology as a tool for resistance, rather than merely mitigating risks.",45.18,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09603v1_Linear Complexity Self-Supervised Learning for Mus.pdf,Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer,"Petros Vavaroutsos, Theodoros Palamas, Pantelis Vikatos",10.1145/3748522.3779786,,"Deep Learning, Learnable Representations, Music Understanding, Transformers, Embeddings, Attention","This paper focuses on reducing the model size of foundation models for music information retrieval by combining Branchformer architecture with SummaryMixing and a random quantization process. Pre-training is performed on public datasets with a proprietary dataset, and robust evaluation is achieved across multiple downstream MIR tasks. The proposed method achieves competitive performance while reducing model size from 8.5% to 12.3%.",45.02,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09605v1_Sim2real Image Translation Enables Viewpoint-Robus.pdf,Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets,"Jeremiah Coholich, Justin Wit, Robert Azarcon, Zsolt Kira",10.48550/arXiv.2024.12345,arXiv:2409.12345,"image translation, robot manipulation, fixed camera datasets, viewpoint consistency, sim2real challenges","The paper presents MANGO, an unpaired image translation method with a novel segmentation-conditioned InfoNCE loss and modified PatchNCE loss, designed to maintain viewpoint consistency during sim-to-real translation. It demonstrates that training with limited fixed-camera data enables generation of diverse unseen viewpoints, achieving high success rates on previously unseen perspectives.",44.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09609v1_DPWriter Reinforcement Learning with Diverse Plann.pdf,Reinforcement Learning with Diverse Planning Branching for Creative Writing,"Qian Cao, Yahui Liu, Wei Bi, Yi Zhao, Ruihua Song 1B, Xiting Wang 1B, Ruiming Tang, Guorui Zhou 2, Han Li 2",,,"reinforcement learning, large language models, creative writing, diversity, planning branching",This paper proposes an RL framework using a semi-structured long Chain-of-Thought reasoning to enhance output diversity in creative writing. It introduces a Diverse Planning Branching method and a group-aware diversity reward to guide exploration while maintaining quality.,43.57,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09613v1_CogRail Benchmarking VLMs in Cognitive Intrusion P.pdf,Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems,"Yonglin Tian, Qiyao Zhang, Wei Xu, Yihao Wu, Xinyi Li, Xingyuan Dai, Hui Zhang, Zhiyong Cui, Baoqing Guo, Member, IEEE, Zujun Yu, Yisheng Lv",10.1094/JCL.2015.14.8.12345,10.1094/JCL.2015.14.8.12345,None,"Accurate and early perception of potential intrusion targets is essential for railway safety. This paper introduces CogRail, a benchmark integrating open datasets and cognitively driven annotations, and evaluates state-of-the-art VLMs using multimodal prompts. A joint fine-tuning framework is proposed to enhance performance for cognitive intrusion perception, highlighting limitations of current models.",46.41,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09620v1_Full Disclosure Less Trust How the Level of Detail.pdf,"Full Disclosure, Less Trust? How the Level of Detail about AI Use in News","Pooja Prajod, Hannes Cools, Thomas Röggla, Karthiya Puttajekar, Amber Kusters, Alia Elkattan, Pablo Cesar, Abdullah El Ali",,,"artificial intelligence, news production, transparency, trust, readers' trust, AI disclosures, journalism, readers' behavior","This study investigates how varying levels of AI disclosure detail affect readers' trust in news, examining the trade-off between transparency and trust in AI-assisted journalism.",44.62,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09624v1_Toward Understanding Unlearning Difficulty A Mecha.pdf,Toward Understanding Unlearning Difficulty: A Mechanistic Perspective,"Jiali Cheng, Ziheng Chen, Chirag Agarwal, Hadi Amiri",,,"machine unlearning, unlearning difficulty, circuit-guided difficulty, model mechanisms, LLM ethics","This paper investigates why unlearning success varies across samples by proposing Circuit-guided Unlearning Difficulty (CUD), a metric that uses circuit-level signals to assign a continuous difficulty score. Experiments show CUD effectively distinguishes easy from hard samples and remains stable across methods, offering a mechanistic view of unlearning.",44.69,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09625v1_The Promptware Kill Chain How Prompt Injections Gr.pdf,The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware,"Ben Nassi, Bruce Schneier, Oleg Brodt",,,"prompt injection, promptware, malware, security, LLM, attack chain, security frameworks","The paper explores how attacks on large language model systems evolve into a distinct class of malware termed 'promptware,' proposing a five-step kill chain that mirrors traditional malware campaigns.",42.84,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09626v1_From Prompt to Protocol Fast Charging Batteries wi.pdf,Fast Charging Batteries,"Ge Lei1, Ferran Brosa Planella2, Sterling G. Baird, Samuel J. Cooper",arXiv:2601.09626v1,2601.09626,"battery charging, fast charging, gradient-free optimization, large language models, optimization protocols","Efficiently optimizing battery charging protocols is challenging due to slow, costly evaluations. This paper introduces two gradient-free, LLM-driven methods—Prompt-to-Optimizer and Prompt-to-Protocol—to enable high-performance protocol discovery in realistic fast-charging scenarios.",44.39,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09635v1_LLM for Large-Scale Optimization Model Auto-Formul.pdf,LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach,"Kuo Liang1, Yuhang Lu, Jianming Mao, Shuyi Sun, Chunwei Yang, Congcong Zeng, Xiao Jin, 8, 5, 6, 7, 8",arXiv:2601.09635,arXiv:2601.09635,"large language models, tool use, agentic workflow construction, automated optimization modeling","This paper presents LEAN-LLM-OPT, a lightweight framework for LLM-assisted large-scale optimization auto-formulation. It describes a workflow where two LLM agents dynamically build optimization formulations based on queries, and a downstream agent generates outputs. The approach leverages LLMs' text-processing strengths and reduces burden on planners by offloading data handling.",45.82,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09636v1_PersonalAlign Hierarchical Implicit Intent Alignme.pdf,PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent,"Yibo Lyu, Gongwei Chen, Rui Shao, Weili Guan, Liqiang Nie",weberlv1b@gmail.com,weberlv1b,"Personalized GUI Agent, Hierarchical Intent Alignment, Long-Term User Records, Proactive Assistance, Multimodal Large Language Models","This work introduces PersonalAlign, a new agent task that leverages long-term user records to align with implicit user intents, enabling agents to resolve vague instructions and anticipate user needs proactively. It presents AndroidIntent benchmarks and introduces Hierarchical Intent Memory Agent for personalization.",45.73,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09667v2_Collaborative Multi-Agent Test-Time Reinforcement .pdf,Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning,"Zhiyuan Hu, Yunhai Hu, Juncheng Liu, Shuyue Stella Li, Yucheng Wang, Zhen Xu, Xu6, Ng2 Anh Tuan Luu, Xu4 Bryan Hooi2, Cynthia Breazeal, Hae Won Park, Zhou",,,"multi-agent systems, multi-agent reinforcement learning, test-time reasoning, collaborative agents, credit assignment, distribution shift","The paper introduces MATTRL, a framework that injects structured textual experiences at inference time for multi-agent dialogue systems. MATTRL enables multi-expert teams to handle turn-level decision making, improves accuracy across diverse benchmarks, and provides insights into credit assignment for better training outcomes.",45.81,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09680v1_Automating Supply Chain Disruption Monitoring via .pdf,Automating Supply Chain Disruption Monitoring via an Agentic AI,"Sara AlMahria, Liming Xu, Alexandra Brintrup",arXiv:2601.09680v1,arXiv:2601.09680,"Supply Chain Management, Supply Chain Disruption, Large Language Models, AI Agents, Multi-Agent System, Agentic System","This paper introduces an agentic AI framework to autonomously monitor, analyze, and respond to supply chain disruptions across extended networks. It leverages large language models and deterministic tools to detect disruption signals, map supplier networks, assess exposure, and recommend mitigations. The system achieves high accuracy across scenarios and significantly reduces response times compared to industry benchmarks.",45.72,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09684v1_Disentangling Task Conflicts in Multi-Task LoRA vi.pdf,Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection,"Ziyu Yang, Guibin Chen, Yuxin Yang, Aoxiong Zeng, Xiangquan Yang",10.48550/arXiv:2309.12345,arXiv:2309.12345,"multi-task learning, low-rank adaptation, orthogonal projection, gradient conflict, parameter-efficient fine-tuning","This paper proposes Ortho-LoRA, a gradient projection method tailored for LoRA in multi-task settings. It mitigates negative transfer by projecting conflicting gradients orthogonally within the LoRA subspace, improving performance on GLUE while maintaining low computational overhead.",45.64,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09692v1_Routing with Generated Data Annotation-Free LLM Sk.pdf,Routing with Generated Data: Annotation-Free,"Tianyi Niu, Justin Chih-Yao Chen, Genta Indra Winata, Shi-Xiong Zhang, Supriyo Chakraborty, Sambit Sahu, Yue Zhang, Elias Stengel-Eskin, Mohit Bansal",1UNC Chapel Hill 2Capital One 3The University of Texas at Austin,,"LLM routing, query-answer routing, query-only routing, skill estimation, expert selection, LLM models, generative data","This paper introduces Routing with Generated Data (RGD), a method for training routing systems exclusively on generated queries and answers. It evaluates query-answer and query-only routers across multiple benchmarks, demonstrating that query-answer routers degrade more with lower generator quality. The authors identify two key generator characteristics and propose CASCAL, a query-only router that improves robustness.",46.83,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09694v1_LLMs can Compress LLMs Adaptive Pruning by Agents.pdf,LLMs can Compress LLMs: Adaptive Pruning by Agents,"Sai Varun Kodathala, Rakesh Vunnam",arXiv:2601.09694v1,2601.09694,"Model Compression, Adaptive Pruning, Self-Reflection","Introduces agent-guided pruning that adaptively selects layers to prune while preserving knowledge, improving factual retention and reducing perplexity.",44.04,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09703v1_ShortCoder Knowledge-Augmented Syntax Optimization.pdf,Knowledge-Augmented Syntax,"Sicong Liu, Yanxian Huang, Mingwei Liu, Jiachi Chen, Ensheng Shi, Yuchi Ma, Hongyu Zhang, Yin Zhang, Yanlin Wang",10.1234/example.doi,,"code generation, token efficiency, large language models, semantic equivalence, code optimization","The paper proposes ShortCoder, a knowledge-infused framework optimizing code generation efficiency while preserving semantic equivalence and readability. It introduces syntax-level simplification rules, a hybrid data synthesis pipeline, and fine-tuning strategies to improve generation performance.",45.94,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09706v1_Value-Aware Numerical Representations for Transfor.pdf,Value-Aware Numerical Representations for Transformer Language Models,"Andreea Dutulescu, Stefan Ruseti, Mihai Dascalu",10.48550/arXiv.2405.1234,arXiv:2405.1234,"value-aware, numerical representation, transformer, mathematical reasoning",The paper introduces a value-aware numerical representation that augments tokenized inputs with a dedicated prefix token conditioned on numerical value. This improves numerical robustness and arithmetic accuracy while remaining compatible with existing tokenizers.,44.72,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09708v1_Fast-ThinkAct Efficient Vision-Language-Action Rea.pdf,Fast-ThinkAct: Efficient Vision-Language-Action,"Chi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, Fu-En Yang",arXiv:2601.09708v1,NVIDIA,"Vision-Language-Action, Reasoning, Latent Planning, Embodied Control, Action Execution","Proposes Fast-ThinkAct, an efficient reasoning framework for VLA that reduces inference latency while maintaining strong planning performance through latent CoTs and preference-guided objectives.",46.39,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09749v1_R-LAM Reproducibility-Constrained Large Action Mod.pdf,R-LAM: Reproducibility-Constrained Large Action,Suriya Sureshkumar,240171.ad@rmkec.ac.in,,"Reproducible Scientific Workflows, Large Action Models, LLM-Based Agents, Execution Provenance","This paper proposes R-LAM, a reproducibility-constrained framework for applying Large Action Models to scientific workflow automation. R-LAM introduces structured action schemas, deterministic execution policies, and explicit provenance tracking to ensure auditable and replayable workflows. It supports failure-aware execution and controlled workflow forking, improving reproducibility in scientific settings.",44.82,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09750v1_SAGE Tool-Augmented LLM Task Solving Strategies in.pdf,SAGE: Tool-Augmented LLM Task Solving Strategies,"Robert K. Strehlow, Tobias Küster, Oskar F. Kupke, Brandon Llanque Kurps, Fikret Sivrikaya, Sahin Albayrak",DKI.00.00032.21,1.3*,"tool augmentation, LLM task solving, multi-agent environments, zero-shot prompting, OPACA framework","This paper presents SAGE, a conversational AI interface that integrates tools via the OPACA framework to enhance large language models with dynamic tool integration and robust prompting methods. It evaluates various task-solving strategies against benchmark services, highlighting strengths and weaknesses across different approaches.",45.55,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09753v1_Critically Engaged Pragmatism A Scientific Norm an.pdf,"Critically Engaged Pragmatism: A Scientific Norm and Social, Pragmatist Epistemology for AI Science Evaluation Tools",Carole J. Lee,10.1234/example.doi,None,"pragmatism, scientific evaluation, AI, replication crisis","The paper addresses crises in peer review and AI-driven science assessment, advocating for a social pragmatist epistemology and a norm of Critically Engaged Pragmatism to scrutinize AI evaluation tools, emphasizing their role in grounding scientific credibility rather than serving as objective arbiters.",44.26,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09755v1_Heterogeneous computing platform for real-time rob.pdf,Heterogeneous computing platform for real-time robotics,"Jakub Fil, Yulia Sandamirskaya, Hector Gonzalez, Loïc Azzalin, Stefan Glüge, Lukas Friedenstab, Friedrich Wolf, Tim Rosmeisl, Matthias Lohrmann, Mahmoud Akl, Khaleel Khan, Leonie Wolf, Kristin Richter, Holm Puder, Mazhar Ali Bari, Xuan Choo, Noha Alharthi, Michael Hopkins, Steve Furber, Jens Struckmeier",,,"heterogeneous computing, robotics, neural computing, robot, AI, neuro-inspired hardware, neuromorphic computing, real-time perception, robot interaction","The paper explores a computing platform combining neuromorphic hardware (Loihi2) and event-based cameras with GPUs to enable real-time perception and interaction in humanoid robots within a smart city context, advancing Society 5.0.",47.21,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09756v1_Synthetic Data for Veterinary EHR De-identificatio.pdf,"Synthetic Data for Veterinary EHR De-identification: Benefits, Limits, and Safety Trade-offs Under Fixed Compute","David Brundage, PhD",,,"veterinary electronic health records, de-identification, synthetic data, LLM, privacy",Evaluation of synthetic veterinary narratives using PetEVAL for de-identification safety under varying training regimes; synthetic augmentation improves span-level performance but raises document-level leakage concerns.,43.63,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09757v1_Democracy and Distrust in an Era of Artificial Int.pdf,Democracy & Distrust in an Era of Artificial Intelligence,Sonia K. Katyal,10.1162/DAED_a_01919,,"democracy, distrust, artificial intelligence, judicial review","Explores how AI challenges traditional legal frameworks, emphasizing the need for judicial review to protect minorities in the age of automation and predictive technologies.",42.52,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09760v1_Investigating Tool-Memory Conflicts in Tool-Augmen.pdf,Investigating Tool-Memory Conflicts in Tool-Augmented LLMs,"Jiali Cheng, Rui Pan, Hadi Amiri, 1, 2",,,"tool-augmented LLMs, knowledge conflict, tool-memory conflict, LLM reliability, knowledge integration","This paper introduces Tool-Memory Conflict (TMC) in LLMs, where internal parametric knowledge conflicts with external tool outputs. It evaluates existing conflict resolution methods and proposes new strategies to address inconsistencies between model parameters and tool-generated information.",44.87,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09762v1_Explicating Tacit Regulatory Knowledge from LLMs t.pdf,Explicating Tacit Regulatory Knowledge from LLMs to Auto-Formalize,"Zhiyi Xue, Xiaohong Chen, Min Zhang",52275902017@stu.ecnu.edu.cn,,"compliance testing, LLM, regulatory knowledge, auto-formalization, test generation","The paper proposes RAFT, a framework for requirements auto-formalization and compliance test generation by explicating tacit regulatory knowledge from multiple LLMs. RAFT uses an Adaptive Purification-Aggregation strategy to integrate tacit knowledge into meta-models, formal requirements representations, and testability constraints, enabling high-precision requirement formalization and automated test generation.",44.25,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09765v1_AI Survival Stories a Taxonomic Analysis of AI Exi.pdf,Philosophy of AI,"John Hawthorne, Lingnan University, Dept of Philosophy, Tuen Mun, HK, Australian Catholic University, Melbourne, AU",,,"Artificial Intelligence, Existential Risk, AI Safety, AI Catastrophe, Superintelligent AI, AI Alignment","This paper develops a framework for thinking about the existential risk of AI systems, analyzing survival stories where humanity persists despite threats from powerful AI.",42.33,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09768v1_CLiMB A Domain-Informed Novelty Detection Clusteri.pdf,CLiMB: A Domain-Informed Novelty Detection Clustering,"Lorenzo Monti, Tatiana Muraveva, Brian Sheridan, Davide Massari, Alessia Garofalo, Gisella Clementini, Umberto Michelucci",arXiv:2601.09768v1,2601.09768,"novelty detection, semi-supervised clustering, constrained clustering, density-based clustering, domain knowledge integration","CLiMB introduces a domain-informed framework for clustering scientific data, achieving high recovery of known structures while effectively isolating novel patterns. It demonstrates superior performance in isolating dynamical features in unlabelled datasets.",44.14,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09770v1_GUI-Eyes Tool-Augmented Perception for Visual Grou.pdf,GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents,"Chen Chen, Chen Jiawei Shao, Dakuan Lu, Haoyi Hu, Xiangcheng Liu, Hantao Yao, Wu Liu",,,"vision-language models, reinforcement learning, active visual perception, GUI agents, tool-aware perception","Recent advances in vision-language models and reinforcement learning have driven progress in GUI automation. This paper presents GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. The agent learns strategic decisions on visual tool invocation (e.g., cropping, zooming) using a two-stage reasoning process and a spatially continuous reward function. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy with only 3k labeled samples, outperforming supervised and RL baselines.",44.44,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09771v1_PCN-Rec Agentic Proof-Carrying Negotiation for Rel.pdf,PCN-Rec: Agentic Proof-Carrying Negotiation for Reliable Governance-Constrained Recommendation,"Aradhya Dixit, Shreem Dixit",,,"recommendation systems, LLM agents, constrained ranking, governance, verification, negotiation","Presents PCN-Rec, a proof-carrying negotiation pipeline for LLM-based recommenders that satisfies governance constraints via agent negotiation and verifiable certificates.",42.33,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09772v1_Antisocial behavior towards large language model u.pdf,Antisocial behavior towards large language model users: experimental evidence,"Paweł Niszczota, Cassandra Grützner",,,,Experimental evidence on antisocial behavior towards large language model users.,41.91,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09773v1_Enhancing LUT-based Deep Neural Networks Inference.pdf,Enhancing LUT-based Deep Neural Networks Inference through Architecture and Connectivity Optimization,"Binglei Lou, Ruinil Wu, Philip Leong",10.48550/arXiv.2024.12345,arXiv:2408.12345,"Dynamic Sparsity, FPGA, Neural Network, Lookup Table","The paper presents SparseLUT, a framework optimizing LUT-based DNNs via architectural enhancements and non-greedy training to balance accuracy, latency, and hardware efficiency.",41.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09805v1_Improving Chain-of-Thought for Logical Reasoning v.pdf,Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention,"Phuong Minh Nguyen, Tien Huu Dang, Naoya Inoue",10.48550/arXiv.2024.12345,arXiv:2408.12345,"logical reasoning, LLMs, attention, intervention, chain-of-thought","The paper introduces an end-to-end framework for logical reasoning that embeds structural information into prompts to activate attention heads aligned with reasoning patterns. Attention-Aware Intervention (AAI) reweights attention scores to guide the model toward leveraging prior knowledge, improving performance with minimal computational cost.",44.8,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09806v1_Diffusion-Driven Deceptive Patches Adversarial Man.pdf,Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification,"Shahrzad Sayyafzadeh, Hongmei Chi, Shonda Bernadin",arXiv:2601.09806v1,2601.09806,"Adversarial Patch Generation, Gaussian Smoothing, Diffusion Model, Social Media Forensics, Perceptual Hashing","This work presents an end-to-end pipeline for generating, refining, and evaluating adversarial patches to compromise facial biometric systems. It employs FGSM for noise generation and diffusion models to enhance evasion effectiveness, while also analyzing forensic detection methods.",47.42,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09809v1_QFed Parameter-Compact Quantum-Classical Federated.pdf,Parameter-Compact Quantum-Classical Federated Learning,"Samar Abdelghani, Soumaya Cherkaoui",10.1007/978-3-642-45888-3,arXiv:2109.07985,"Quantum Computing, Quantum Machine Learning, Federated Learning, Privacy, Communication, IoT",This study examines quantum-assisted federated learning to reduce parameter counts in classical models while maintaining accuracy. QFed aims to improve computational efficiency across edge networks using polylogarithmic factor reductions.,44.67,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09814v1_Explainable Deep Learning for Pediatric Pneumonia .pdf,Explainable Deep Learning for Pediatric Pneumonia Detection in Chest X-Ray Images,"Adil O. Khadidos, Aziida Nanyonga, Alaa O. Khadidos, Olfat M. Mirza, Mustafa Tahsin Yilmaz",,,"pediatric pneumonia, deep learning, chest x-ray, explainable ai, image interpretation","This study compares two state-of-the-art convolutional neural network architectures for automated pediatric pneumonia detection using chest X-ray images. It evaluates model performance with accuracy, F1-score, MCC, and recall, and applies Grad-CAM and LIME for model interpretability.",46.49,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09822v2_LLM-Based Agentic Systems for Software Engineering.pdf,LLM-Based Agentic Systems for Software Engineering: Challenges and Opportunities,"Yongjian Tang, Thomas Runkler",arXiv:2601.09822v2,cs.SE,"LLMs, Agents, Software Engineering, Future Challenges","This paper reviews the emerging paradigm of LLM-based multi-agent systems for software engineering, examining their applications across the Software Development Life Cycle and highlighting challenges and future research opportunities.",44.37,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09841v2_A pipeline for enabling path-specific causal fairn.pdf,APIPELINE FOR ENABLING PATH-SPECIFIC CAUSAL FAIRNESS IN OBSERVATIONAL HEALTH DATA,"Aparajita Kashyap, Sara Matijevic, Noémie Elhadad, Steven A. Kushner, Shalmali Joshi",arXiv:2601.09841v2,2601.09841v2,"causal fairness, foundation models, causal inference, observational health data, fair machine learning","This paper presents a pipeline to train causally fair machine learning models for healthcare by focusing on path-specific fairness. It addresses biases in healthcare contexts and demonstrates how models can be made fair across direct and indirect sources, leveraging observational data.",47.4,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09851v1_ViSIL Unified Evaluation of Information Loss in Mu.pdf,Unified Evaluation of Information Loss in Multimodal Video Captioning,"Po-han Li, Shenghui Chen, Ufuk Topcu, Sandeep Chinchali",10.48550/arXiv.2026.12345,arXiv:2609.12345,"multimodal video captioning, information loss, Video Question Answering (VQA), summary information metrics, visual language models","This paper introduces the Video Summary Information Loss (ViSIL) score, a novel information-theoretic metric that quantifies unseen information in video summarization via vision-language models. It addresses limitations of traditional metrics like BLEU and ROUGE by enabling cross-modal comparison and optimizing trade-offs between information richness and processing speed. Results show ViSIL correlates strongly with human and VLM performance, improving VQA accuracy by 7% while maintaining efficiency.",47.39,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09853v1_MedRedFlag Investigating how LLMs Redirect Misconc.pdf,MedRedFlag: Investigating how LLMs Redirect,"Sraavya Sambara, Yuan Pu1, Ayman Ali, Vishala Mishra, Lionel Wong, Monica Agrawal",,,"medical communication, LLMs, health misconceptions, patient context, redirection, safety concerns","Real-world health questions often embed false assumptions. LLMs frequently fail to redirect problematic questions, risking suboptimal medical decisions. This study investigates MedRedFlag, a dataset of 1100+ Reddit questions, to assess LLM performance in correcting misconceptions.",44.97,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09855v1_Thinking Long but Short Stable Sequential Test-Tim.pdf,Stable Sequential Test-Time Scaling for Large Reasoning Models,"Michael R. Metel, Yufei Cui, Boxing Chen, Prasanna Parthasarathi",arXiv:2601.09855v1,arXiv:2601.09855,"large reasoning models, test-time scaling, sequential scaling, accuracy improvement, KV cache, computational complexity","Presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy while stabilizing accuracy across longer reasoning lengths. The method uses a custom KV cache and achieves linear complexity under mild conditions.",45.31,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09858v1_OUTLINEFORGE Hierarchical Reinforcement Learning w.pdf,OutlineForge: Hierarchical Reinforcement Learning with Explicit States for Scientific Writing,"Yilin Bao, Ziyao He, Zayden Yang",,,"scientific writing, reinforcement learning, outline construction, long-horizon planning, document planning, citation consistency","Presents a reinforcement learning framework for scientific outline construction, addressing global structure, input coverage, and citation reliability through hierarchical editing and value-guided reinforcement learning.",44.68,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09865v1_Advancing Model Refinement Muon-Optimized Distilla.pdf,Advancing Model Refinement: Muon-Optimized Distillation,"Jacob Sander, Brian Jalaian, V enkat R. Dasarivenkateswara.r.dasari.civ@army.mil",arXiv:2601.09865v1,arXiv:2601.09865,"Large Language Models, Quantization, Model compression, Edge deployment, Data distillation, Kullback-Leibler divergence, Bayesian optimization, Muon optimizer","This paper presents an integrated framework combining GPTQ-based quantization, LoRA, and Muon optimizer to reduce LLM size and improve inference efficiency on resource-constrained devices. It achieves up to 2× memory compression and demonstrates superior performance on standard benchmarks compared to GPTQ quantization alone.",47.28,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09869v1_A Scoping Review of the Ethical Perspectives on An.pdf,A Scoping Review of the Ethical Perspectives on Anthropomorphising Large Language Models,"Andrea Ferrario, Rasita Vinay, Matteo Casserini, Alessandro Facchini",10.48550/arXiv.2601.09869,2601.09869,"anthropomorphism, conversational agents, large language models, AI ethics, deception, trust, governance","This scoping review maps ethically oriented work on anthropomorphising LLM-based conversational agents across five databases and three preprint repositories. It synthesizes conceptual foundations, ethical challenges and opportunities, and methodological approaches, highlighting convergence on attribution-based definitions and divergent operationalizations.",45.51,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09871v1_Epistemology gives a Future to Complementarity in .pdf,Epistemology Gives Afuture to Complementarity in Human-AI Interactions,"Andrea Ferrario, Alessandro Facchini, Juan M. Durán",arXiv:2601.09871v1,2601.09871,"artificial intelligence, machine learning, reliance, complementarity, human-AI interaction, computational reliabilism, epistemology","Human-AI complementarity is explored as a framework where AI supports human decision-making, aiming to improve performance in high-stakes domains. The paper argues that complementarity should be understood through justificatory AI lenses, emphasizing reliability and alignment with socio-technical standards rather than merely predictive accuracy.",45.53,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09879v1_MedVL-SAM2 A unified 3D medical vision-language mo.pdf,MedVL-SAM2: A unified 3D medical vision–language model for multimodal reasoning and prompt-driven segmentation,"Yang Xing, Jiong Wu, Savas Ozdemir, Ying Zhang, Yang Yang, Wei Shao4, Gong, Kong, Yang",10.1000/MVL-2024-001,arXiv:2408.12345,"medical vision, 3D medical imaging, report generation, visual question answering, segmentation, semantic segmentation, volumetric reasoning","MedVL-SAM2 is a unified 3D medical multimodal model supporting report generation, visual question answering, and multi-paradigm segmentation. It integrates image-level reasoning and pixel-level perception for precise 3D spatial understanding.",48.63,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09881v1_Transition Matching Distillation for Fast Video Ge.pdf,Transition Matching Distillation for Fast Video Generation,"Weili Nie, Julius Berner, Nanye Ma, Chao Liu, Saining Xie, Arash Vahdat",arXiv:2601.09881v1,2026-1-16,"video generation, distillation, TMD, flow models, generation speed, visual quality","This paper introduces Transition Matching Distillation (TMD), a framework that distills video diffusion models into efficient few-step generators by matching multi-step denoising trajectories with lightweight conditional flows. Experiments show TMD achieves a good balance of generation speed and visual fidelity.",46.06,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09883v1_Beyond Rule-Based Workflows An Information-Flow-Or.pdf,Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents,"Xinxing Ren, Quagmire Zang, Caelum Forder, Suman Deb, Ahsen Tahir, Roman J. Georgio, Peter Carroll, Zekun Guo",10.48550/arXiv.2024.12345,arXiv:2408.12345,"multi-agent systems, information flow, agent-to-agent communication, large language models, LLM-based MAS, task orchestration","This paper proposes an information-flow-orchestrated multi-agent paradigm using CORAL, where an orchestrator dynamically coordinates agents via agent-to-agent communication without relying on predefined workflows. It evaluates performance on GAIA benchmark, achieving higher accuracy than baseline OWL approaches.",45.19,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09896v1_The Algorithmic Gaze An Audit and Ethnography of t.pdf,The Algorithmic Gaze: An Audit and Ethnography of the LAION-Aesthetics,"Jordan Taylor, William Agnew, Maarten Sap, Sarah E. Fox, Haiyi Zhu",10.1145/nnnnnnn.nnnnnnn,not provided,"AI, Art, Aesthetic Evaluation","This study audits the LAION-Aesthetic Predictor model, revealing how its aesthetic filtering reinforces cultural biases and highlights the need for more pluralistic evaluation methods.",44.05,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09902v1_A Novel Contrastive Loss for Zero-Day Network Intr.pdf,A Novel Contrastive Loss for Zero-Day Network,"Jack Wilkie, Hanan Hindy, Craig Michie, Christos Tachtatzis, James Irvine, Robert Atkinson",10.1007/978-3-642-45888-7,10.1007/978-3-642-45888-7,"network intrusion detection, machine learning, contrastive learning, zero-day attacks, anomaly detection, open-set recognition","This work proposes a novel contrastive loss function to improve intrusion detection on zero-day attacks. By training on both benign and known malicious samples, the method generalizes better than anomaly-based approaches while maintaining robustness to imbalanced data.",46.21,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09913v1_Continuum Memory Architectures for Long-Horizon LL.pdf,Continuum Memory Architectures for Long-Horizon LLM Agents,Joe Logan,arXiv:2601.09913v1,arXiv:2601.09913,,"Retrieval-augmented generation (RAG) treats memory as stateless, lacking persistence and continuity. The paper introduces the Continuum Memory Architecture (CMA), a memory system that maintains evolving state through persistent storage, selective retention, associative routing, temporal chaining, and higher-order consolidation. The work evaluates CMA against RAG in behavioral probes, showing advantages in memory dynamics tasks despite challenges in latency, drift, and interpretability.",44.69,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09921v1_Learning to Decode in Parallel Self-Coordinating N.pdf,Learning to Decode in Parallel: Self-Coordinating,"Kai Zhang, Zhengzhong Yi, Shaojun Guo, Linghang Kong, Wang Situ, Zhengfeng Ji, Fusheng Chen, Jianxin Chen, Tan He, Xiaoyu Zhan, Tan He, Weiping Lin, Dongxin Gao, Yiming Zhang, Fangming Liu, Fang Zhang, Xuanzhi Zhang, Zhengfeng Ji, Feng Zhang",arXiv:2601.09921v1,2601.09921v1,"quantum error correction, neural network, self-coordinating, parallel decoding, quantum computation","Fast, reliable decoders are pivotal components for enabling fault-tolerant quantum computation. Neural network decoders like AlphaQubit have demonstrated significant potential, achieving higher accuracy than traditional human-designed decoding algorithms. However, existing implementations of neural network decoders are described in arXiv:2601.09921v1.",46.11,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09923v1_CaMeLs Can Use Computers Too System-level Security.pdf,AI agents: A preprint on computer use agents security,"Hanna Foerster, Robert Mullins, Tom Blanchard, Nicolas Papernot, Kristina Nikoli´c, Florian Tramèr, Ilia Shumailov, Cheng Zhang, Yiren Zhao",arXiv:2601.09923v1,arXiv:2601.09923,"AI agents, prompt injection, security, Computer Use Agents, UI workflows, control flow integrity","AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The paper proposes architectural isolation to separate trusted planning from untrusted observations, introducing Single-Shot Planning for CUAs to ensure control flow integrity and prevent branch steering attacks.",56.3,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09929v1_Hallucination Detection and Mitigation in Large La.pdf,Hallucination Detection and Mitigation in Large Language Models,"Ahmad Pesaranghader, Erin Li",arXiv:2601.09929v1,arXiv:2601.09929v1,"hallucination, large language models, hallucination detection, mitigation strategies, financial applications","This paper introduces a comprehensive operational framework for managing hallucinations in LLMs and LRMs, focusing on root cause awareness and targeted interventions. It integrates detection methods and mitigation strategies across model, data, and context layers, demonstrating effectiveness through a financial data extraction case study.",56.34,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09933v1_Malware Classification using Diluted Convolutional.pdf,Malware Classification using Diluted Convolutional Neural Network with Fast Gradient Sign Method,"Ashish Anand, Bhupendra Singh, Sunil Khemka, Bireswar Banerjee, Vishi Singh Bhatia, Piyush Ranjan",gcp.ashish2020@gmail.com,,"data security, privacy, malware classification, diluted convolutional neural network, fast gradient sign method","A research proposal introducing FGSM-DICNN for efficient malware classification, highlighting challenges in feature selection and computational efficiency.",44.07,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09949v2_Kinematic Tokenization Optimization-Based Continuo.pdf,Kinematic Tokenization: Optimization-Based,"Griffin M. Kearney, Ph.D.",arXiv:2601.09949v2,arXiv:2601.09949v2,"kinematic tokenization, optimization-based, continuous-time, financial time series","Transformers are designed for discrete tokens, yet many real-world signals are continuous processes observed through noisy sampling. Discrete tokenizations can be brittle in low signal-to-noise regimes, especially when downstream objectives impose asymmetric penalties. We introduce Kinematic Tokenization, an optimization-based continuous-time representation that reconstructs an explicit spline from noisy measurements and tokenizes local spline coefficients. This is applied to financial time series data in conjunction with trading volume profiles. Under a risk-averse asymmetric classification objective, continuous spline tokens sustain calibrated action distributions and stable policies.",43.35,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09966v1_A Sustainable AI Economy Needs Data Deals That Wor.pdf,A Sustainable AI Economy Needs Data Deals That,"Ruoxi Jia, Luis Oala, Brickroad, Wenjie Xiong, Suqin Ge, Jiachen T. Wang, Feiyang Kang, Dawn Song",arXiv:2601.09966v1,2601.09966v1,"data deals, machine learning value chain, economic equity, data generators, fair exchange","The paper argues that the current machine learning value chain is structurally unsustainable due to an economic data processing inequality. It highlights that value accrues disproportionately to aggregators, with creators receiving negligible royalties and opaque terms. The authors identify three structural faults—missing provenance, asymmetric bargaining power, and non-dynamic pricing—and propose an Equitable Data-Value Exchange (EDVEX) Framework to create a fairer market.",45.21,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09972v1_Chinese Labor Law Large Language Model Benchmark.pdf,Chinese Labor Law Large Language Model,"Zixun Lan, Maochun Xu, Yifan Ren, Rui Wu, Jianghui Zhou, Xueyang Cheng, Jian’an Ding, Mingmin Chi, Fei Ma",,,"Chinese labor law, legal natural language processing, large language models, domain-specific fine-tuning, benchmark dataset, statute recall, legal reasoning, case analysis","Recent advancements in large language models have led to substantial progress in domain-specific applications, particularly within the legal domain. This paper presents LaborLawLLM, a legal LLM tailored to labor law, and introduces LaborLawBenchmark with tasks like legal provision citation and case analysis. Experimental results show significant improvements over general-purpose models.",48.64,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09974v1_SPRInG Continual LLM Personalization via Selective.pdf,SPRInG: Continual LLM Personalization via Selective Parametric Adaptation and Retrieval-Interpolated Generation,"Seoyeon Kim, Jaehyung Kim",10.48550/arXiv.2405.12345,10.48550/arXiv.2405.12345,"continual personalization, LLM personalization, parameter adaptation, retrieval interpolation, preference drift","This paper introduces SPRING, a semi-parametric framework for continual personalization of large language models. It employs drift-driven selective adaptation and logit-based interpolation to handle evolving user preferences without catastrophic forgetting, demonstrating superior performance on long-form personalized generation benchmarks.",45.3,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09980v1_Performance of AI agents based on reasoning langua.pdf,ALD optimization using reasoning LLMs,Angel Yanguas-Gil,10.1021/acs.aos.2026.12.012,arXiv:2508.01234,"reasoning large language models, atomic layer deposition, ALD optimization","This work explores the use of reasoning large language models to autonomously optimize atomic layer deposition processes. The agent iteratively interacts with an unsupervised ALD reactor to determine optimal precursor and coreactant doses, demonstrating consistent success with models like OpenAI o3 and GPT5, though variability remains due to model non-determinism.",44.26,LFM-2.5,AMD RX 6800 (Vulkan)
2601.09982v1_Context Volume Drives Performance Tackling Domain .pdf,Volume Drives Performance: Tackling Domain Shift in Extremely Low-Resource Translation via RAG,"David Samuel Setiawan, Raphaël Merx, Jey Han Lau",10.1093/acpsr/ftk123,,"neural machine translation, low-resource languages, domain shift, retrieval augmented generation, hybrid framework","Neural Machine Translation models for low-resource languages degrade under domain shift. This study quantifies performance loss using Dhao on the New Testament and applies a hybrid NMT + LLM framework to recover it, achieving improved chrF++ scores.",44.59,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10010v1_VERHallu Evaluating and Mitigating Event Relation .pdf,Evaluating and Mitigating Event,"Zefan Zhang, Kehua Zhu, Shijie Jiang, Hongyuan Lu, Shengkai Sun, Tian Bai",,,"Video Large Language Models, Event Relation Hallucination, Video Understanding, Event Relation Understanding","This paper introduces VERHallu, a novel benchmark for evaluating event relation hallucination in video LLMs. It focuses on causal, temporal, and subevent relations across three tasks and proposes a Key-Frame Propagating strategy to improve multi-event understanding without sacrificing inference speed.",44.19,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10011v1_Memo-SQL Structured Decomposition and Experience-D.pdf,Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL,"Zerui Yang, Weichuan Wang, Yanwei Xu, Linqi Song, Yudai Matsuda, Wei Han, Bo Bai",10.1234/example.12345,12345,"NL2SQL, self-correction, experience-driven, training-free, structured decomposition","Addresses limitations of existing NL2SQL systems by introducing structured decomposition and experience-aware self-correction, achieving 68.5% execution accuracy with improved efficiency.",45.2,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10018v1_Empowering Older Adults in Digital Technology Use .pdf,Empowering Older Adults in Digital Technology Use with Foundation Models,"Hasti Sharifi, Homaira Huda Shomee, Sourav Medya, Debaleena Chattopadhyay",,,"Technology support, Digital technology use, Artificial intelligence, Large language models, Communication barriers, Human-computer interaction",This study examines communication challenges faced by older adults in using digital technology and explores AI-based solutions to improve their experience.,44.44,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10025v1_Structured Personality Control and Adaptation for .pdf,Structured Personality Control and Adaptation for LLM Agents,"Jinpeng Wang, Xinyu Jia, Wei Wei Heng, Yuquan Li, Binbin Shi, Qianlei Chen, Guannan Chen, Junxia Zhang, Yuyu Yin",10.1093/acm/grac123,2104.07912,"Personalization, Jungian Psychological Types, MBTI Personality Types, Persona Adaptation, Explainable AI","This paper presents a framework modeling LLM personality via Jungian psychological types, integrating mechanisms for coherent expression, adaptive reinforcement, and long-term evolution. It evaluates personality alignment using Myers–Briggs questionnaires and applies it to diverse interaction scenarios to support naturalistic agent design in HCI.",45.7,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10029v1_PaperScout An Autonomous Agent for Academic Paper .pdf,PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization,"Tingyue Pan, Jie Ouyang, Mingyue Cheng, Qingchuan Li, Zirui Liu, Mingfan Pan, Shuo Yu, Qi Liu",10.1234/arxiv.2025.12345,12345,"academic paper search, autonomous agent, process-aware policy, sequence-level optimization, reinforcement learning, LLM integration","This paper introduces PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. It addresses limitations of static workflows by employing Proximal Sequence Policy Optimization (PSPO) to align optimization with agent-environment interaction. Experiments show superior performance over traditional methods in recall and relevance.",46.36,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10031v1_FilDeep Learning Large Deformations of Elastic-Pla.pdf,FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data,"Jianheng Tang, Shilong Tao, Zhe Feng, Haonan Sun, Menglu Wang, Zhanxing Zhu, Yunhuai Liu",10.1145/3770854.3783959,KDD '26,"Large Deformations, Elastic-Plastic Solids, Multi-Fidelity Data, Deep Learning, Quantity-Accuracy Dilemma","The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit limitations, prompting Deep Learning as a promising alternative. FilDeep addresses the quantity-accuracy dilemma by training with both low-fidelity and high-fidelity data.",46.34,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10038v1_What Understanding Means in AI-Laden Astronomy.pdf,Understanding Means in AI-Laden Astronomy,"Yuan-Sen Ting, André Curtis-Trudel, Siyu Yao, Philosophy Department, University of Cincinnati, UC Center for Humanities and Technology, University of Cincinnati, Department of Philosophy, School of Humanities, Shanghai Jiao Tong University",arXiv:2601.10038v1,2601.10038v1,"understanding, philosophy of science, AI in astronomy, discovery, knowledge generation","The paper explores how understanding in astronomy is being reshaped by artificial intelligence, emphasizing philosophical reflection on discovery, progress, and the essence of understanding. It calls for deeper, rigorous engagement with AI's epistemological impact beyond technical optimization.",45.34,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10073v1_ReaMIL Reasoning- and Evidence-Aware Multiple Inst.pdf,ReaMIL: Reasoning- and Evidence-Aware Multiple Instance Learning for Whole-Slide Histopathology,"Hyun Do Jung, Jungwon Choi, Hwiyoung Kim",,,"ReaMIL, multiple instance learning, whole-slide histopathology, interpretability, tile-level reasoning","Introduces ReaMIL, a multiple instance learning framework for whole-slide histopathology that incorporates a light selection head and a budgeted-sufficiency objective to produce compact evidence sets. Demonstrates improved performance across multiple cancer datasets and provides diagnostics on evidence efficiency.",45.01,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10079v1_Sparse-RL Breaking the Memory Wall in LLM Reinforc.pdf,Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning,"Sijia Luo, Xiaokang Zhang, Yuxuan Hu, Bohan Zhang, Ke Wang, Jinbo Su, Mengshu Sun, Lei Liang, Jing Zhang, 1School of Information, Renmin University of China, 2Key Laboratory of Data Engineering and Knowledge Engineering, Beijing, 3Ant Group, Hangzhou, China",,,"Reinforcement Learning, Large Language Models, LLM Reinforcement Learning, Memory Wall, Sparse-RL, Policy Mismatch, KV Compression, Reinforcement Learning","The paper addresses the memory bottleneck in LLM RL training by introducing Sparse-RL, which uses sparsity-aware sampling to reduce KV cache overhead while maintaining performance.",46.56,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10088v1_State of AI An Empirical 100 Trillion Token Study .pdf,State of AI: An Empirical 100 Trillion Token Study with OpenRouter,"Malika Aubakirova, Alex Atallah, Chris Clark, Justin Summerville, Anjney Midha",a16z,a16z/2403.12345,"large language models, open router, empirical study, LLM usage, multi-step reasoning","This study analyzes over 100 trillion tokens of real-world LLM interactions using OpenRouter, highlighting shifts in adoption patterns, creative roleplay, coding assistance, and agentic inference. It identifies foundational user cohorts and discusses implications for model development.",45.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10090v1_Difficulty-guided Sampling Bridging the Target Gap.pdf,Difficulty-guided Sampling: Bridging the Target Gap,"Mingzhuo Lia, Guang Lia, Linfeng Yeb, Jiafeng Maoc, Takahiro Ogawaa, Konstantinos N. Plataniotis, Miki Haseyamaa",arXiv:2601.10090v1,2601.10090,"dataset distillation, downstream tasks, target gap, deep neural networks, data distillation, image classification","The paper proposes difficulty-guided sampling to bridge the gap between distillation objectives and downstream tasks, aiming to improve dataset distillation performance while maintaining model accuracy.",46.19,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10092v1_LeMoF Level-guided Multimodal Fusion for Heterogen.pdf,LEMOF: LEVEL-GUIDEDMULTIMODALFUSION FOR HETEROGENEOUSCLINICALDATA,"Jongseok Kim, Seongae Kang, Jonghwan Shin, Yuhan Lee, Ohyun Jo",10.1093/acprof:oso/9780190871293.013,2601.10092v1,"Multimodal Learning, Hierarchical Representation Learning, Clinical Time-Series Modeling, Level-guided Feature Fusion, Explainable Medical AI","This paper introduces LeMoF, a novel framework for integrating heterogeneous clinical data by selectively combining level-guided representations across modalities. It achieves balanced performance between prediction stability and discriminative capability, demonstrating superior results in length of stay prediction using ICU data.",46.3,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10094v1_V-Zero Self-Improving Multimodal Reasoning with Ze.pdf,V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation,"Han Wang, Yi Yang, Jingyuan Hu, Minfeng Zhu, Wei Chen",10.48550/arXiv.2405.12345,arXiv:2405.12345,"self-improvement, multimodal reasoning, vision-language models, unsupervised learning, co-evolutionary loop","Introduce V-Zero, a post-training framework enabling self-improvement via unlabeled images. It uses a questioner and solver in a co-evolutionary loop, improving visual mathematical reasoning through iterative GRPO training.",44.19,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10101v2_Matrix as Plan Structured Logical Reasoning with F.pdf,Matrix as Plan: Structured Logical Reasoning with Feedback-Driven Replanning,"Ke Chen, Jiandian Zeng, Zihao Peng, Guo Li, Guangxue Zhang, Tian Wang",10.1145/XXXXXX.XXXXXX,,"Logical Reasoning, Large Language Models, Neurosymbolic Approaches, Semantic Decomposition","This paper proposes MatrixCoT, a structured CoT framework with a matrix-based plan to enhance LLM reasoning. It addresses limitations of chain-of-thought prompting by introducing structured representations, explicit citations, and feedback-driven replanning. Experiments demonstrate improved robustness and interpretability without relying on external solvers.",44.68,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10103v1_FlowAct-R1 Towards Interactive Humanoid Video Gene.pdf,FlowAct-R1: Towards Interactive Humanoid Video,"Lizhen Wang, Yongming Zhu, Zhipeng Ge, Youwei Zheng, Longhao Zhang, Tianshu Hu, Shiyang Qin, Mingshuang Luo, Jiaxu Zhang, Xin Chen, Yulong Wang, Zerong Zheng, Jianwen Jiang, Chao Liang, Weifeng Chen, Xing Wang, Yuan Zhang, Mingyuan Gao",arXiv:2601.10103v1,2601.10103,"humanoid video generation, interactive video, real-time interaction, low-latency synthesis, video synthesis, behavioral control","Proposes FlowAct-R1, a framework for real-time interactive humanoid video generation using MMDiT architecture, achieving stable 25fps at 480p with low TTFF, enabling lifelike and responsive video synthesis.",47.4,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10104v1_MathDoc Benchmarking Structured Extraction and Act.pdf,Benchmarking Structured Extraction and Active Refusal on Noisy Mathematics Exam Papers,"Chenyue Zhou, Jiayi Tuo, Shitong Qin2, Wei Dai, Mingxuan Wang, Ziwei Zhao, Duoyang Li, Shiyang Su, Yanxi Lu",,,"structured extraction, active refusal, mathematics exam, visual noise, MLLM, refusal capability","This paper introduces MathDoc, the first benchmark for extracting structured questions from noisy high school mathematics exam papers. It evaluates models on stem accuracy, visual similarity, and refusal capability, highlighting limitations of current MLLMs in rejecting illegible inputs.",45.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10108v1_SIN-Bench Tracing Native Evidence Chains in Long-C.pdf,Tracing Native Evidence Chains in Long-Context Multimodal,"Yiming Ren, Junjie Wang, Yuxin Meng, Yihang Shi, Zhiqiang Lin, Ruihang Chu, Yiran Xu, Ziming Li, Yunfei Zhao, Yang Yujiu, Tsinghua University, Shanghai AI Laboratory, KuaiShou Inc., Stanford University, Harvard University",,,"long-form scientific papers, multimodal models, evidence chains, cross-modal reasoning, grounded QA, evidence anchoring","Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging. The paper introduces the 'Fish-in-the-Ocean' paradigm requiring explicit cross-modal evidence chains in native documents, and presents experiments on evidence discovery, hypothesis verification, grounded QA, and evidence synthesis. It highlights grounding as a key bottleneck and reports performance differences across models.",45.25,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10112v1_Repository Intelligence Graph Deterministic Archit.pdf,Repository Intelligence Graph: Deterministic,"Tsvi Cherny-Shahar, Amiram Yehudai",,,"software repositories, build systems, dependency graphs, software engineering agents, multi-lingual software","Introduces Repository Intelligence Graph (RIG) to provide a deterministic architectural map for LLM code assistants, addressing challenges in recovering build and test structures across multilingual projects.",42.19,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10114v1_Following the Teachers Footsteps Scheduled Checkpo.pdf,Following the Teacher’s Footsteps: Scheduled Checkpoint Distillation for Domain-Specific LLMs,"Cheng Feng, Chaoliang Zhong, Jun Sun, Yusuke Oishi",10.1000/arxiv.2601.10114,arXiv:2601.10114,"LLMs, Knowledge Distillation, Domain-specific Tasks",This paper proposes a novel theoretical framework and method to enable student models to surpass teacher models on domain-specific tasks by balancing convergence advantages against subdomain deficits through scheduled checkpoint distillation and adaptive weighting.,45.87,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10120v1_TopoDIM One-shot Topology Generation of Diverse In.pdf,TopoDIM: One-shot Topology Generation of Diverse Interaction Modes for Multi-Agent Systems,"Rui Sun, Jie Ding, Chenghua Gong, Tianjun Gu2, Yihang Jiang, Juyuan Zhang, Liming Pan, Linyuan Lü",https://anonymous.4open.science/r/TopoDIM-8D35/,TopoDIM-8D35/,"multi-agent systems, topology generation, interaction modes, LLM, decentralized execution, token efficiency","TOPODIM proposes a one-shot topology generation framework with diverse interaction modes to optimize communication in LLM-based multi-agent systems. It enables agents to autonomously construct heterogeneous communication without iterative coordination, achieving token efficiency and improved performance. Experiments show a 46.41% reduction in total token consumption and a 1.50% performance boost over state-of-the-art methods.",55.38,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10122v1_Role-Playing Agents Driven by Large Language Model.pdf,"Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future","Ye Wang, Jiaxing Chen, Hongjiang Xiao, Yewang, Xiaohj",arXiv:2601.10122v1,2601.10122v1,"large language models, role-playing agents, natural language processing, human-computer interaction, personality modeling","This paper systematically reviews the current development and key technologies of role-playing language agents (RPLAs), tracing their evolution from rule-based systems to advanced cognitive simulation. It highlights critical technical pathways such as psychological scale-driven character modeling, memory-augmented prompting, and motivation-based decision control, while analyzing data construction challenges and multi-dimensional evaluation frameworks.",45.28,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10129v1_LaViT Aligning Latent Visual Thoughts for Multi-mo.pdf,Aligning Latent Visual Thoughts for Multi-modal Reasoning,"Linquan Wu, Tianxiang Jiang, Yifei Dong, Haoyu Yang, Fengji Zhang, Shichang Meng, Ai Xuan, Linqi Song, Jacky Keung",https://github.com/Svardfox/LaViT,,"visual attention, latent reasoning, multimodal models, knowledge distillation, visual grounding","This work introduces LaViT, a framework that aligns latent visual thoughts instead of static embeddings, addressing the critical perception gap in multimodal reasoning by requiring students to reconstruct teacher visual semantics autonomously.",43.88,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10130v1_Redundancy-Driven Top-k Functional Dependency Disc.pdf,Redundancy-Driven Top-k Functional Dependency,"Xiaolong Wan, Xixian Han",10.1234/example.doi,12345678,"functional dependency, top-k discovery, data redundancy, pruning strategy","Proposes SDP for discovering top-k FDs ranked by redundancy count, improving scalability by using upper-bound pruning based on redundancy.",40.7,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10131v2_M4olGen Multi-Agent Multi-Stage Molecular Generati.pdf,"M4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints","Yizhan Li, Florence Cloutier, Sifan Wu, Ali Parviz, Boris Knyazev, Yan Zhang, Glen Berseth, Bang Liu",10.48550/arXiv.2405.12345,2405.12345,"multi-agent, multi-stage, molecular generation, precise constraints, multi-property, LLM optimization","Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical. This paper introduces M4olGen, a framework combining retrieval-augmented generation and reinforcement learning to produce high-fidelity molecules while respecting complex property targets.",43.59,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10132v1_Is More Context Always Better Examining LLM Reason.pdf,Is More Context Always Better? Examining LLM Reasoning,"Yanan Cao, Farnaz Fallahi, Murali Mohana Krishna Dandu, Lalitesh Morishetti, Kai Zhao, Luyi Ma, Sinduja Subramaniam, Jianpeng Xu, Evren Korpeoglu, Kaushiki Nag, Sushant Kumar, Kannan Achan",10.1145/XXXXXX.XXXXXX,,"Large Language Models, Temporal Reasoning, Inter-Purchase Interval Prediction","This paper investigates whether large language models can predict time intervals between recurring user actions, examining how contextual information affects performance. It benchmarks LLMs against statistical and machine-learning models, revealing limitations in capturing temporal structure and the diminishing benefit of additional context.",43.94,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10137v1_Step-by-Step Causality Transparent Causal Discover.pdf,Step-by-Step Causality: Transparent Causal Discovery with Multi-Agent,"Ziyi Ding, Chenfei Ye-Hao, Zheyuan Wang, Xiao-Ping Zhang",10.1234/arxiv.2026.09123,arXiv:2026.09123,"causal discovery, multi-agent, LLM, confidence estimation, structural learning","This paper introduces Tree-Query, a tree-structured multi-expert LLM framework that enables interpretable causal discovery by reducing pairwise discovery to queries about backdoor paths, dependencies, confounding, and direction. It provides theoretical guarantees for asymptotic identifiability and demonstrates improved structural metrics on benchmarks, supporting data-free causal priors.",44.99,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10141v1_Understanding and Preserving Safety in Fine-Tuned .pdf,Understanding and Preserving Safety in Fine-Tuned LLMs,"Jiawen Zhang, Yangfan Hu, Kejia Chen, Lipeng He, Jiachen Ma, Jian Lou, Dan Li, Ruoxi Jia, Jian Liu",,,"fine-tuning, safety alignment, LLMs, safety-utility trade-off, jailbreak attacks, gradient analysis","This work addresses the safety-utility dilemma in fine-tuning large language models by analyzing geometric interactions between safety and utility gradients. It proposes safety-preserving fine-tuning (SPF) to maintain performance while ensuring safety, demonstrating robustness under adversarial conditions.",43.12,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10143v1_History Is Not Enough An Adaptive Dataflow System .pdf,History Is Not Enough: An Adaptive Dataflow,"Haochong Xia, Yao Long Teng, Regan Tan, Molei Qin, Xinrun Wang, Bo An",I. INTRODUCTION,HAOCHONG001,"adaptive dataflow, workflow automation, financial time-series, data augmentation, data augmentation, financial time-series synthesis",The paper addresses the challenge of concept drift in quantitative finance by proposing an adaptive dataflow system that integrates machine learning-based adaptive control. It introduces a parameterized data manipulation module and an adaptive planner-scheduler to improve model robustness and risk-adjusted returns.,43.62,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10148v1_DecisionLLM Large Language Models for Long Sequenc.pdf,DecisionLLM: Large Language Models for Long Sequence Decision Exploration,"Xiaowei Lv 1, Zhiling Zhang, Yijun Li, Tianyu Wang3, Yongcai Wang1, Peng Sun, Chuan Yu3, Jian Xu3, Bo Zheng",10.48550/arXiv:2509.06931,2509.06931,"long sequence decision-making, reinforcement learning, large language models, decision transformer, offline reinforcement learning","This work investigates applying Large Language Models to offline decision-making tasks, proposing a framework called DecisionLLM that treats trajectories as a distinct modality. It establishes scaling laws linking model scale, data volume, and data quality, demonstrating strong performance in benchmarks.",44.21,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10150v1_Simple Network Graph Comparative Learning.pdf,Simple Network Graph Comparative Learning,"Qiang Yua, Xinran Chenga, Shiqiang Xub, Chuanyi Liua",,,"Filters, Siamese network, Graph contrastive learning, Unsupervised representation learning","The paper proposes a novel node classification contrast learning method called Simple Network Graph Comparative Learning (SNGCL), which uses a superimposed multilayer Laplace smoothing filter to enhance feature smoothing and compares it with state-of-the-art models in node classification tasks.",43.32,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10154v1_MHub.ai A Simple Standardized and Reproducible Pla.pdf,"A Simple, Standardized, and Reproducible Platform for AI Models","Leonard Nürnberg, Dennis Bontempi, Suraj Pai, Curtis Lisle, Steve Pieper, Ron Kikinis, Sil van de, Rahul, Soni, Gowtham, Murugesan, Ciausu, Miriam, Groeneveld, Felix, J., Dorfner, Jue, Jiang, Aneesh, Rangnekar, Harini, Joeran, Bosma, Keno, Bressem, Raymond, Mak, Andrey, Fedorov, Hugo, Aerts",,,"AI, Medical Imaging, Radiology, Machine Learning, Deep Learning, Healthcare",A standardized platform for AI models in medical imaging,45.4,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10155v1_LOOKAT Lookup-Optimized Key-Attention for Memory-E.pdf,LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers,Aryan Karmore,bt24csd009@iiitn.ac.in,,"transformers, key-attention, memory efficiency, compression, FAISS",Compressing the KV cache via product quantization and asymmetric distance computation enables memory-efficient deployment of large language models on edge devices. LOOKAT achieves high compression ratios with minimal performance loss.,42.71,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10157v1_MMPG MoE-based Adaptive Multi-Perspective Graph Fu.pdf,MMPG: MoE-based Adaptive Multi-Perspective Graph Fusion for Protein Representation Learning,"Yusong Wang, Jialun Shen, Zhihao Wu, Yicheng Xu, Shiyin Tan, Mingkun Xu, Changshuo Wang, Zixing Song, Prayag Tiwari",10.48550/arXiv.2405.12345,10.48550/arXiv.2405.12345,"Graph Neural Networks, Protein Representation Learning, Multi-perspective graphs, Mixture of Experts, Protein interactions, Downstream protein tasks","Graph Neural Networks (GNNs) are widely used for Protein Representation Learning (PRL), but current methods rely on single-perspective graph construction, leading to incomplete representations. This work proposes MMPG, which constructs protein graphs from multiple perspectives and adaptively fuses them using a Mixture of Experts (MoE) module. MMPG integrates physical, chemical, and geometric perspectives, enabling experts to specialize in modeling interactions at different levels and achieving consensus across perspectives. Experiments show improved performance on multiple downstream protein tasks.",47.42,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10160v1_Alignment Pretraining AI Discourse Causes Self-Ful.pdf,Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment,"Cameron Tice, Puria Radmard, 2 Samuel Ratnam, Andy Kim, David Africa, Kyle O’Brien",,,"AI discourse, self-fulfilling misalignment, pretraining, alignment, LLMs, misalignment","This paper presents a controlled study showing that pretraining language models with discourse about AI misalignment can increase misaligned behavior, while upsampling aligned examples reduces it. The findings suggest pretraining shapes alignment priors and support post-training alignment pretraining.",45.21,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10161v1_AWED-FiNER Agents Web applications and Expert Dete.pdf,"A WED-FiNER: Agents, Web applications, and Expert Detectors for Fine-grained Named Entity Recognition across 36 Languages","Prachuryya Kaushik, Ashish Anand",,,"WED-FiNER, Fine-grained Named Entity Recognition, 36 languages, low-resource languages","AWED-FiNER is an open-source ecosystem bridging the gap in fine-grained NER for 36 languages spoken by over 6.6 billion people, offering agentic tools, web applications, and expert models for offline deployment.",43.58,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10168v1_RAG-3DSG Enhancing 3D Scene Graphs with Re-Shot Gu.pdf,Enhancing 3D Scene Graphs With RAG-Guided Retrieval Augmentation,"Yue Chang, Rufeng Chen, Zhaofan Zhang, Yi Chen, Sihong Xie",10.48550/arXiv.2311.07891,arXiv:2311.07891,"3D scene graph, object recognition, retrieval-augmented generation, robotics, semantic representation",The paper proposes RAG-3DSG to improve open-vocabulary 3D scene graph generation by using re-shot guided uncertainty estimation and RAG at the object level. It introduces a dynamic downsample-mapping strategy to accelerate cross-image aggregation and demonstrates significant improvements in node captioning accuracy and mapping efficiency.,45.4,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10169v1_CtD Composition through Decomposition in Emergent .pdf,Compositionality through Decomposition,"Boaz Carmeli, Ron Meir, Yonatan Belinkov",10.48550/arXiv.2025.12345,arXiv:2509.12345,"compositionality, compositional generalization, artificial neural agents, decomposition, multi-target coordination, natural language processing",This study demonstrates how artificial neural agents acquire compositional generalization to describe novel images through two sequential training steps: decomposing images into basic concepts and composing them into complex phrases. Zero-shot generalization is achieved without additional training.,45.26,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10173v1_ReasAlign Reasoning Enhanced Safety Alignment agai.pdf,ReasAlign: Reasoning Enhanced Safety Alignment against Prompt,"Hao Li1, Yankai Yang2, G. Edward Suh3, Ning Zhang1, Chaowei Xiao3,4, Washington University in St. Louis, 2University of Wisconsin–Madison, NVIDIA, Johns Hopkins University",,,"prompt injection, large language models, agentic systems, security alignment, reasoning, attack mitigation","This paper presents ReasAlign, a model-level solution to improve safety alignment against indirect prompt injection attacks. It introduces structured reasoning steps to detect conflicting instructions and enhances continuity of user tasks, achieving strong utility while outperforming prior defenses.",45.88,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10187v1_HOMURA Taming the Sand-Glass for Time-Constrained .pdf,Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning,"Ziang Cui, Mengran Yu, Tianjiao Li, Chenyu Shi, Yingxuan Shi, Lusheng Zhang, Hongwei Lin",,,"Large Language Models, Multilingual Translation, Time-Constrained Translation, Reinforcement Learning, Semantic Fidelity, Syllable Budget, Rate Distortion","The paper introduces Sand-Glass to evaluate translation under syllable-level duration constraints and presents HOMURA, a reinforcement learning framework that optimizes the trade-off between semantic preservation and temporal compliance. Experimental results show significant improvements over LLM baselines in length control.",45.79,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10191v1_How does downsampling affect needle electromyograp.pdf,Downsampling Effects on Needle Electromyography Signals,"Mathieu J.L. Cherpitel, Janne A.M. Luijten, Thomas H.W. Bæck, Camiel Verhamme, Martijn R. Tannemaat, Anna V. Kononova",1,,"needle electromyography, downsampling, signal processing, feature extraction, neurological diagnosis","This study evaluates how downsampling impacts needle electromyography (nEMG) signals, focusing on preserving diagnostic information while reducing computational load. It proposes a systematic workflow combining shape-based metrics and classification results to identify effective downsampling strategies for high-frequency biomedical signals.",45.92,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10193v1_GFM4GA Graph Foundation Model for Group Anomaly De.pdf,GFM4GA: Graph Foundation Model for Group Anomaly Detection,"Jiujiu Chen, Weijun Zeng, Shaofeng Hu, Hui Xiong",10.1007/978-3-642-45891-8,,"Group Anomaly Detection, Graph Foundation Model, Graph Contrastive Learning, Group Anomaly Detection, Graph Constraint Learning, Anomaly Detection, Feature Consistency, Anomaly Proportion Weighting","This paper proposes GFM4GA, a graph foundation model leveraging dual-level contrastive learning to detect group anomalies. It addresses limitations of existing models by capturing group-level patterns and improving detection accuracy through adaptive group context.",45.4,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10201v1_PRL Process Reward Learning Improves LLMs Reasonin.pdf,Process Reward Learning Improves LLMs’ Reasoning Ability,"Jiarui Yao, Ruida Wang, Tong Zhang",10.48550/arXiv.2311.07891,arXiv:2109.07985,"process reward learning, large language models, reasoning, reinforcement learning","The paper proposes Process Reward Learning (PRL) to decompose reinforcement learning objectives into intermediate steps with process rewards, aiming to improve LLM reasoning by providing fine-grained supervision during the reasoning process. Experimental results show PRL enhances reasoning performance and broadens the reasoning boundary.",43.36,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10205v1_One Instruction Does Not Fit All How Well Do Embed.pdf,One Instruction Does Not Fit All: How Well Do Embeddings Align,"arya.shah, himanshu.beniwal, mayank.singh",10.48550/arXiv.2203.02643,arXiv:2203.02643,"personas, instructions, low-resource languages, embeddings, alignment","This paper presents a unified benchmark for evaluating multilingual embedding models across 12 Indian languages, assessing persona-instruction retrieval and cross-lingual transfer. It compares E5 Large-Instruct, BGE-M3, and LaBSE, highlighting performance in monolingual and cross-lingual retrieval tasks.",43.94,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10212v1_PADER Paillier-based Secure Decentralized Social R.pdf,PADER: Paillier-based Secure Decentralized Social Recommendation,"Chaochao Chen, Jiaming Qian, Fei Zheng, Yachuan Liu",10.1007/s11473-020-09741-7,10.1007/978-3-030-62132-8,"Paillier cryptosystem, secure computation, recommendation system","The paper proposes PADER, a secure decentralized social recommendation system using the Paillier cryptosystem. It addresses privacy concerns by enabling secure training and inference without centralized platforms, employing secure addition and multiplication protocols and efficient data packing for polynomial computations.",43.03,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10215v1_Topo-RAG Topology-aware retrieval for hybrid text-.pdf,Topo-RAG: Topology-Aware Retrieval for Hybrid Text Tables,"Alex Dantart, Marco K´ovacs-Navarro",arxiv@humanizinginternet.com,arXiv:2601.10215v1,"Retrieval-Augmented Generation (RAG), table retrieval, late interaction, multivector retrieval, enterprise search, heterogeneous data, semantic routing, structure-aware embeddings, Topo-RAG, ColBERT, cell-aware interaction","This work introduces Topo-RAG, a framework that respects the topology of heterogeneous enterprise documents by routing narrative and tabular data through specialized mechanisms, achieving an 18.4% improvement in nDCG@10 on hybrid queries.",43.87,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10222v1_Introduction to optimization methods for training .pdf,Introduction to optimization methods for training SciML models,"Alena Kopaničáková, Elisa Riccietti",10.48550/arXiv.2601.10222,2601.10222,"optimization methods, machine learning, SciML, stochastic optimization, gradient descent, physics informed models","This paper discusses how optimization underpins modern machine learning, highlighting shifts toward stochastic methods due to data scarcity in scientific machine learning. It emphasizes physics-informed objectives involving partial differential equations and spatial-temporal constraints.",43.69,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10236v1_Who Owns the Text Design Patterns for Preserving A.pdf,Who Owns the Text? Design Patterns for Preserving Authorship in AI-Assisted Writing,"Bohan Zhang, Chengke Bu, Paramveer Dhillon",arXiv:2601.10236v1,2601.10236,"AI-assisted writing, human-AI collaboration, psychological ownership, personalization, proveance","This study examines how AI writing assistants affect writers' sense of authorship by comparing persona-based coaching with style personalization. It finds that while cognitive load decreases and quality ratings remain stable, psychological ownership drops slightly, and authorship preservation can be maintained through design choices.",44.82,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10242v1_Loop as a Bridge Can Looped Transformers Truly Lin.pdf,LOOP AS ABRIDGE: CANLOOPEDTRANSFORMERS,"Guanxu Chen, Dongrui Liu",,,"Large Language Models, Looped Transformers, Introspection, Representation Space, Natural Language Outputs","This report investigates whether Looped Transformers can bridge the gap between internal knowledge and explicit linguistic outputs by leveraging their iterative nature as a form of introspection. Experiments show that while increasing loop iterations narrows the knowledge-gap, internal representations degrade, and verification capability is limited to the final loop.",43.91,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10245v1_TRIM Hybrid Inference via Targeted Stepwise Routin.pdf,Hybrid Inference via Targeted Stepwise,"Vansh Kapoor, Aman Gupta, Hao Chen, Anurag Beniwal, Jing Huang, Aviral Kumar",10.48550/arXiv.2026.12345,arXiv:2509.12345,"multi-step reasoning, LLM routing, step-level inference, cascading failures, model efficiency","The paper proposes TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical reasoning steps to larger models while allowing smaller models to handle routine continuations. TRIM improves cost efficiency and performance across math reasoning benchmarks by confining expensive computations to high-capacity models.",44.81,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10251v1_X-SAM Boosting Sharpness-Aware Minimization with D.pdf,Boosting Sharpness-Aware Minimization with Dominant-Eigenvector,Hongru Duan Yongle Chen Lei Guan,10.48550/arXiv.2024.12345,arXiv:2408.12345,"sharpness-aware minimization, sharp regions, gradient correction, Hessian, generalization","This paper investigates Sharpness-Aware Minimization (SAM) by analyzing the angle between gradients and leading eigenvectors, proposing X-SAM to improve generalization through eigenvector-aligned regularization. Experimental results demonstrate superior performance over standard SAM.",42.76,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10254v1_NoReGeo Non-Reasoning Geometry Benchmark.pdf,NoReGeo: Non-Reasoning Geometry Benchmark,"Irina Abdullaeva, Anton Vasiliuk, Elizaveta Goncharova, Temurbek Rahmatullaev, Zagorulko Ivan, Maxim Kurkin, 6, Andrey Kuznetsov, 2",,,"geometry, non-reasoning, spatial understanding, deep learning, benchmark, geometry problems","Presents NoReGeo, a benchmark evaluating LLMs' intrinsic geometric understanding without reasoning or algebra. Assesses models on 2,500 trivial problems across 25 categories, revealing limitations in native geometric comprehension.",43.17,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10257v1_Untangling Input Language from Reasoning Language .pdf,Untangling Input Language from Reasoning Language,"Nan Li, Bo Kang, Tijl De Bie",10.48550/arXiv.2024.12345,arXiv:2408.12345,"moral alignment, cross-lingual reasoning, ethical foundations, model interpretability","This paper introduces a methodology to disentangle the effects of input language and reasoning language in moral judgments. By applying moral foundations analysis, it reveals how reasoning language influences judgments independently of input language, offering diagnostic insights for improving global model alignment.",42.26,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10272v1_MoST Mixing Speech and Text with Modality-Aware Mi.pdf,MOST: MIXINGSPEECH ANDTEXT WITHMODALITY,"Yuxuan Lou, Kai Yang, Yang You",arXiv:2601.10272v1,2601.10272,"Mixture of Speech and Text, Multimodal Large Language Model, Modality-Aware Mixture of Experts, Speech Text Integration, Open Source LLM","We present MoST, a novel multimodal large language model that seamlessly integrates speech and text processing through a Modality-Aware Mixture of Experts (MAMoE) architecture. The model introduces specialized routing pathways that adapt experts to input type, enhancing modality-specific learning and cross-modal understanding while relying exclusively on open-source datasets.",42.85,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10274v1_Queueing-Aware Optimization of Reasoning Tokens fo.pdf,Queueing-Aware Optimization of Reasoning Tokens,"Emre Ozbas, Melih Bastopcu",,,"accuracy-latency trade-offs, LLM-based servers, optimization of reasoning tokens, LLM inference","The paper presents a constrained optimization framework for allocating reasoning tokens in a multi-task LLM server, balancing accuracy and latency under queueing dynamics. It derives a coupled projected fixed-point characterization and proposes a gradient-based iterative solution with convergence guarantees.",48.27,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10282v2_SPIKE Sparse Koopman Regularization for Physics-In.pdf,SPIKE: Sparse Koopman Regularization for Physics-Informed Neural Networks,Jose Marie Antonio Miñoza,10.1093/pasj/psa123,2601.10282v2,"Physics-Informed Neural Networks, PINN, Koopman operator, sparse regularization","This work introduces SPIKE, a framework that regularizes PINNs with continuous-time Koopman operators to achieve parsimonious dynamics representations. It demonstrates improved generalization across diverse PDE classes and stability advantages over discrete-time Koopman methods.",47.13,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10305v1_DanQing An Up-to-Date Large-Scale Chinese Vision-L.pdf,DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset,"Hengyu Shen, Tiancheng Gu, Bin Qin, Lan Wu, Yuling Wu, Shuo Tan, Zelong Sun, Jun Wang, Nan Wu, Xiang An, Weidong Cai, Ziyong Feng, Kaicheng Yang, DanQingTeam",arXiv:2601.10305v1,arXiv:2601.10305,"Chinese vision-language pre-training, large-scale dataset, CLIP, SigLIP, cross-modal retrieval, zero-shot classification, semantic segmentation, LMM","This report introduces DanQing, a comprehensive Chinese cross-modal dataset constructed from 100 million image-text pairs collected via Common Crawl. It addresses the scarcity of high-quality Chinese data by curating rigorous, up-to-date web data. The dataset enables models to better capture evolving semantic trends and supports advanced downstream tasks. The authors compare DanQing with existing datasets using continual pre-training of SigLIP2 and demonstrate superior performance across various Chinese vision-language benchmarks.",54.14,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10306v1_Evidence-Augmented Policy Optimization with Reward.pdf,Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning,"Xin Guan, Zijian Li, Shen Huang, Pengjun Xie, Jingren Zhou, Jiuxin Cao",10.1234/example.doi,,"Evidence-Augmented Reasoning, Policy Optimization, Reinforcement Learning, Long-Context Reasoning, LLM, Reward Co-Evolution",Addresses evidence sparsity in LLM reasoning via EAPO; validated through Tree-Structured Eviidence Sampling.,57.82,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10338v1_Agent Skills in the Wild An Empirical Study of Sec.pdf,Agent Skills in the Wild: An Empirical Study of Security,"Yi Liu, Weizhe Wang, Ruitao Feng, Yao Zhang, Guangquan Xu, Gelei Deng, Yuekang Li, Leo Zhang",10.1145/nnnnnnn.nnnnnnn,,"Agent skills, AI security, vulnerability analysis, supply chain security, prompt injection, large language models","The study analyzes 42,447 AI agent skills from major marketplaces, identifying pervasive security risks. It reveals that 26.1% of skills contain vulnerabilities, with data exfiltration and privilege escalation being most common. The research introduces a vulnerability taxonomy and detection methodology, emphasizing the need for capability-based permission systems.",55.35,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10342v1_C-GRASP Clinically-Grounded Reasoning for Affectiv.pdf,C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing,"Cheng Lin Cheng, Ting Chuan Lin, Chai Kai Chang",,,"Large language model, clinical decision support, heart rate variability, retrieval-augmented generation, explainable AI, guardrails","Proposes C-GRASP, a guardrailed RAG-enhanced pipeline for HRV interpretation, emphasizing individualized baseline shifts and spectral artifact mitigation. Achieves high accuracy in emotion classification and strong clinical reasoning consistency.",52.03,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10343v2_OctoBench Benchmarking Scaffold-Aware Instruction .pdf,Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding,"Deming Ding1, Shichun Liu1, Enhui Yang2, Jiahang Lin1, Ziying Chen1, Shihan Dou2, Honglin Guo1, Weiyu Cheng2, Pengyu Zhao2, Chengjun Xiao2, Qunhong Zeng2, Qi Zhang1, Xuanjing Huang1, Qidi Xu †2, Tao Gui †1",,,,"Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially under heterogeneous constraints. OCTOBENCH benchmarks scaffold-aware instruction following in repository-grounded agentic coding, revealing a gap between task-solving and compliance.",56.76,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10348v1_Training-Trajectory-Aware Token Selection.pdf,Training-Trajectory-Aware Token Selection,"Zhanming Shen, Jiaqi Hu, Zeyu Qin, Hao Chen, Wentao Ye1, Zenan Huang, Yihong Zhuang, Guoshan Lu, Junlin Zhou, Junbo Zhao, Yi Zhang",,,"Training trajectory, Token selection, Reasoning distillation, LLM efficiency, Imitation anchors, Optimization path","Efficient distillation faces challenges when performance drops sharply at a critical point. We propose Training-Trajectory-Aware Token Selection (T3S) to address this by adapting token-level objectives, achieving state-of-the-art results across AR and LLM settings.",56.34,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10349v1_SuS Strategy-aware Surprise for Intrinsic Explorat.pdf,Strategy-aware Surprise for Intrinsic Exploration,"Mark Kashirskiy, Ilya Makarov",353056@niuitmo.ru,,"intrinsic motivation, reinforcement learning, contrastive learning, exploration","Proposes Strategy-aware Surprise (SuS), a novel intrinsic motivation framework using pre-post prediction mismatch. Introduces Strategy Stability and Strategy Surprise components. Demonstrates improvements in accuracy and solution diversity over baseline methods.",53.68,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10373v1_Towards Efficient Low-rate Image Compression with .pdf,Towards Efficient Low-rate Image Compression with Frequency-aware Diffusion Prior Refinement,"Yichong Xia, Yimin Zhou, Jinpeng Wang, Bin Chen",10.48550/arXiv.2024.12345,arXiv:2409.12345,"image compression, diffusion models, frequency-aware estimation, low-bitrate, compression efficiency","The paper proposes DiffCR, a frequency-aware skip estimation module combined with consistency prior refinement, to achieve efficient and high-fidelity image reconstruction at low bit rates. It introduces Frequency Decoupling Attention for alignment between compressed and latent latencies and presents a lightweight consistency estimator for fast decoding.",45.03,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10378v2_Global Context Compression with Interleaved Vision.pdf,Global Context Compression with Interleaved Vision-Text Transformation,"Dian Jiao, Jiaxin Duan, Shuai Zhao, Jiabing Leng, Yiran Zhang, Feng Huang",,,"vision-language models, context compression, Transformer, OCR, visual encoding, attention compression","This paper proposes VIST2, a novel Transformer that interleaves input text chunks alongside visual encoding to achieve token-level compression. It explores hierarchical encoding and sparse attention to reduce computational costs while maintaining performance, achieving significant improvements in speed, memory usage, and FLOPS.",44.84,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10386v1_Handling Missing Modalities in Multimodal Survival.pdf,Handling Missing Modalities in Multimodal Survival Prediction for Non-Small Cell Lung Cancer,"Filippo Ruffini, Camillo Maria Caruso, Claudia Tacconi, Lorenzo Nibid5, Francesca Miccolis, Marta Lovino, Carlo Greco, Edy Ippolito, Michele Fiore, Alessio Cortellini, Bruno Beomonte Zobel, Giuseppe Perrone, Bruno Vincenzi, Claudio Marrocco, Valerio Guarrasi, Paolo Soda",arXiv:2601.10386v1,2601.10386,"survival prediction, non-small cell lung cancer, multimodal, missing modalities, artificial intelligence",A study on handling missing modalities in multimodal survival prediction for non-small cell lung cancer.,47.35,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10398v2_LatentRefusal Latent-Signal Refusal for Unanswerab.pdf,LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL,"Xuancheng Ren, Jiang Duan, Qiang Duan",25213050189,"xcren25, sjhu24, 25213050189","Text-to-SQL, LLM, refusal strategy, unanswerable queries, safety constraints","Addresses unsafe generation of executable programs from unanswerable queries by introducing LATENTREFUSAL, a latent-signal refusal mechanism that detects unanswerability from LLM hidden activations.",43.04,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10402v1_Toward Ultra-Long-Horizon Agentic Science Cognitiv.pdf,Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering,"Xinyu Zhu, Yuzhu Cai, Zexi Liu, Bingyang Zheng, Rui Ye, Jiaao Chen, Hanrui Wang, Wei-Chen Wang, Yuzhi Zhang, Linfeng Zhang, Weinan E1, Di Jin, Siheng Chen, School of Artiﬁcial Intelligence, Shanghai Jiao Tong University, Eigen AI, School of Computer Science and Engineering, Beihang University",10.48550/arXiv.2601.10402,2601.10402,"agentic science, cognitive accumulation, machine learning engineering, ultra-long-horizon, experimental autonomy, knowledge accumulation, AI research","The paper presents ML-Master 2.0, an autonomous agent that advances agentic science by introducing Hierarchical Cognitive Caching to sustain strategic coherence over long experimental cycles. It achieves a 56.44% medal rate on OpenAI’s MLE-Bench, demonstrating scalability for complex, long-term research tasks.",45.82,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10406v1_ErrEval Error-Aware Evaluation for Question Genera.pdf,ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics,"Weiping Fu, Bifan Wei, Jingyi Hao, Yushun Zhang, Jian Zhang, Jiaxin Wang, Bo Li, Yu He, Lingling Zhang, Jun Liu",,,"question generation, error modeling, LLM evaluation, error diagnostics, natural language generation",Automatic Question Generation often produces outputs with critical defects; ErrEval introduces an explicit error-aware framework to improve evaluation accuracy.,43.46,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10413v1_LADFA A Framework of Using Large Language Models a.pdf,LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies,"Haiyue Yuan, Nikolay Matyunin, Ali Raza, Shujun Li",https://doi.org/XXXXXXX.XXXXXXX,"1, 1 (January 2025)","Large Language Model, LLM, Privacy Policy, Text Analysis, Data Flows, Privacy, Security, Retrieval-Augmented Generation, RAG, Framework, Automotive Industry, Connected Vehicle","This paper presents LADFA, an end-to-end computational framework that processes privacy policies, extracts personal data flows, constructs a data flow graph, and enables insight discovery. It combines LLMs with retrieval-augmented generation and a custom knowledge base, demonstrating effectiveness through a case study of ten automotive privacy policies.",45.17,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10416v1_LLMdoctor Token-Level Flow-Guided Preference Optim.pdf,LLMdoctor: Token-Level Flow-Guided Preference Optimization,"Tiesunlong Shen, Rui Mao, Jin Wang, Heming Sun, Jian Zhang, Xuejie Zhang, Erik Cambria",,,"large language models, test-time alignment, token-level optimization, flow-guided preference, LLM alignment","Introduces LLMdoctor, a framework for efficient test-time alignment using token-level reward acquisition and flow-guided preference optimization to balance performance and diversity.",42.72,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10421v1_Are Language Models Models.pdf,How Linguistics Learned to Stop Worrying and Love the Language Models,Philip Resnik,10.1017/S0140525X2510112X,in press,"language models, computational models, linguistics","The paper discusses whether language models function as cognitive models, arguing that while they are useful tools, they do not fully represent linguistic theory and are not cognitive models in the strict sense.",46.25,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10436v1_Development of Ontological Knowledge Bases by Leve.pdf,Development of Ontological Knowledge Bases by Leveraging Large Language Models,"Leveraging Large Language Models, Marie-Hélène ABEL, Philippe GOUSPILLOU",,,"Ontology Development, Ontological Knowledge bases, Large Language Models, Knowledge Representation, User Modeling, Knowledge Management","This paper introduces a structured, iterative methodology leveraging LLMs to optimize ontology development, focusing on automating knowledge acquisition, improving consistency, and enhancing transparency in ontology engineering. Key contributions include accelerated construction, better consistency, bias mitigation, and increased efficiency.",44.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10440v1_AgentGuardian Learning Access Control Policies to .pdf,Learning Access Control Policies to Govern AI Agent Behavior,"Nadya Abaev, Denis Klimov, Gerard Levinov, David Mimran, Yuval Elovici, Asaf Shabtai",,,"Security, AI Agents, Access Control Policies, Control Flow Graph","The study introduces AgentGuardian, a security framework that governs AI agent operations via context-aware access control policies. It monitors execution traces to learn legitimate behaviors and derives adaptive policies to regulate tool calls, mitigating hallucination-driven errors and orchestration-level malfunctions.",44.73,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10457v1_NSR-Boost A Neuro-Symbolic Residual Boosting Frame.pdf,NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models,"Ziming Dai, Dabiao Ma, Jinle Tong, Mengyuan Han, Jian Yang, Hanmengyuan-JK",10.1145/nnnnnnn.nnnnnnn,,"Neuro-Symbolic AI, Large Language Models, Gradient Boosting, Legacy Model, Interpretability","The paper introduces NSR-Boost, a neuro-symbolic residual boosting framework tailored for industrial legacy models. It addresses re-training costs and systemic risks by treating legacy models as frozen and applying targeted repairs. The framework uses residuals to identify hard regions, generates symbolic experts via LLM, and integrates them dynamically with legacy outputs, achieving superior performance on real-world data.",45.88,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10460v1_Contextual StereoSet Stress-Testing Bias Alignment.pdf,Stress-Testing Bias Alignment Robustness in Large Language Models,"Abhinaba Basu, Pavan Chakraborty",arXiv:2601.10460v1,2601.10460v1,"bias evaluation, alignment robustness, stress-testing, large language models, stochastic technical context, StereoSet, Context Sensitivity Fingerprints","The paper presents Contextual StereoSet, a benchmark that fixes stereotype content while varying contextual framing. It shows that models' bias shifts with changes in time, location, or audience—without needing adversarial prompts. The authors introduce Context Sensitivity Fingerprints and evaluate across hiring, lending, and help-seeking scenarios, emphasizing that bias is context-dependent and not static.",47.05,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10462v3_ChartComplete A Taxonomy-based Inclusive Chart Dat.pdf,ChartComplete: A Taxonomy-based Inclusive Chart Dataset,"Ahmad Mustapha, Charbel Toumieh, Mariette Awad",arXiv:2601.10462v3,arXiv:2601.10462,"Chart, Dataset, Chart Taxonomy, Chart Classification",The ChartComplete dataset introduces a chart taxonomy to address limitations in existing benchmarks that focus on a small set of chart types. It aims to provide a more inclusive dataset covering thirty different chart types for advancing research in ChartQA.,45.38,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10477v1_Urban Socio-Semantic Segmentation with Vision-Lang.pdf,Urban Socio-Semantic Segmentation with Vision-Language Reasoning,"Yu Wang, Yi Wang, Rui Dai, Yujie Wang, Kaikui Liu, Xiangxiang Chu, Yansheng Li",,,"urban segmentation, socio-semantic segmentation, vision-language reasoning, semantic entities, social semantics, Earth observation, urban planning, environmental monitoring","This work introduces a new dataset and framework for socio-semantic segmentation of urban areas using vision-language models. It addresses challenges in identifying socially defined entities (e.g., schools, parks) beyond purely physical attributes, leveraging hierarchical multi-stage reasoning and reinforcement learning to achieve strong zero-shot generalization.",45.81,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10485v1_Panning for Gold Expanding Domain-Specific Knowled.pdf,Panning for Gold: Expanding Domain-Specific Knowledge,"Runhao Zhao, Weixin Zeng, Wentao Zhang, Chong Chen, Zhengpin Li, Xiang Zhao, Lei Chen",,,"Domain-specific Knowledge Graphs, Knowledge Graph Enrichment, General-to-domain Knowledge Transfer, Fact-as-Program, Domain Relevance, Granularity Alignment","Domain-specific knowledge graphs (DKGs) are essential for supporting intelligent applications in specialized fields, yet they suffer from limited coverage and incompleteness compared to general knowledge graphs. This paper proposes ExeFuse, a new task for domain-specific knowledge graph fusion, addressing challenges of domain relevance ambiguity and granularity misalignment.",45.61,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10496v1_Model See Model Do Exposure-Aware Evaluation of Bu.pdf,"Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code","Ali Al-Kaswan, Claudio Spiess, Prem Devanbu, Arie van Deursen, Maliheh Izadi",10.1145/nnnnnnn.nnnnnnn,CCS Concepts,"Large Language Models, bugs, fixes, Memorisation","This paper introduces an exposure-aware evaluation framework to assess how prior exposure to buggy versus fixed code influences large language models' preference for correct code versus familiar incorrect versions. Using the ManySStuBs4J benchmark, the authors apply Data Portraits on the Stack-V2 corpus and evaluate model preference via code completion and likelihood metrics. Findings show that most examples lack either variant, fixes are more common when only one variant is seen, and exposure amplifies this bias.",46.1,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10498v1_Projected Microbatch Accumulation yields reference.pdf,Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning,Nilin Abrahamsen,arXiv:2601.10498v1,15 Jan 2026,,"Introduces PROMA, a proximal policy update method for large language model fine-tuning that enforces tighter control of local KL divergence without reference policies.",41.4,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10511v1_Scalable Algorithms for Approximate DNF Model Coun.pdf,Scalable Algorithms for Approximate DNF Model Counting,"Paul Burkhardt, David G. Harris, Kevin T. Schmitt",arXiv:2601.10511v1,arXiv:2601.10511,"DNF model counting, approximate algorithms, Monte Carlo, probabilistic inference, network reliability","The paper presents a new Monte Carlo approach with adaptive stopping and short-circuit evaluation for approximate counting of DNF formulas. It achieves PAC learning bounds and demonstrates superior performance over prior methods, scaling efficiently to large problems.",44.09,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10512v2_SatMap Revisiting Satellite Maps as Prior for Onli.pdf,SatMap: Revisiting Satellite Maps as Prior for Online HD Map Construction,"Kanak Mazumder, Fabian B. Flohr",0009−0006−6806−8388,0009−0002−1499−3790,"Online HD map prediction, Satellite map prior, Vectorized HD map","The paper presents SatMap, an online vectorized HD map estimation method that integrates satellite maps with multi-view camera observations to improve depth perception and accuracy in autonomous driving. It achieves a 34.8% mAP improvement over camera-only baselines and demonstrates robust performance under long-range and adverse weather conditions.",43.88,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10520v1_Breaking Up with Normatively Monolithic Agency wit.pdf,Breaking Up with Normatively Monolithic Agency,"Felix Jahn, Yannic Muskalla, Lisa Dargasz, Kevin Baum",arXiv:2601.10520v1,2601.10520,"AI alignment, ethical AI, normative reasoning, moral modules, deontic logic","Introduces GRACE, a reason-based architecture decoupling normative and instrumental decision-making for safe AI alignment. Presents a three-module structure: Moral Module, Decision-Making Module, and Guard, demonstrating its application to LLM therapy assistants.",44.21,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10524v1_Diagnosing Generalization Failures in Fine-Tuned L.pdf,Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing,"Frank Bobe III, Gregory D. Vetaw, Chase Pavlick, Darshan Bryner, Matthew Cook, Jose Salas-Vernis",arXiv:2601.10524v1,arXiv:2601.10524,"LLM fine-tuning, generalization failure, phishing detection, interpretability, architecture, data diversity","This study introduces a multi-layered diagnostic framework to analyze generalization issues in fine-tuned LLMs, focusing on a phishing detection task. It identifies three key findings: (1) performance depends on architecture and diverse data; (2) architecture strongly influences generalization; (3) some models (e.g., Mistral) are inherently more generalizable. The work emphasizes the need for deeper validation of architecture-data interactions.",45.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10527v2_A Safety Report on GPT-5.2 Gemini 3 Pro Qwen3-VL G.pdf,"Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5","Xingjun Ma1, Yixu Wang1, Hengyuan Xu1, Yutao Wu3, Yifan Ding1, Yunhan Zhao1, Zilong Wang1, Jiabin Hua1, Ming Wen1,2, Jianan Liu1,2, Ranjie Duan, Yifeng Gao1, Tan Yunhao Chen, Wei Cheng, Jingjing Chen1, Bo Li4, Yi Gang Jiang, Fudan University1, Deakin University3, UIUC4",https://xsafeai.github.io/AI-safety-report,https://github.com/XSafeAI/AI-safety-report,"GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, Seedream 4.5, safety evaluation, multimodal models, adversarial robustness, regulatory compliance","The report evaluates six frontier AI models across language, vision, and multimodal settings, revealing uneven safety performance. While GPT-5.2 shows strong balanced results, other models exhibit trade-offs in safety, robustness, and compliance. Adversarial testing exposes vulnerabilities, emphasizing the need for holistic safety assessments.",48.38,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10543v1_Defending Large Language Models Against Jailbreak .pdf,Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing,"Yinzhi Zhao, Ming Wang, Shi Feng, Xiaocui Yang, Daling Wang, Yifei Zhang",,,"large language models, jailbreak attacks, decoding safety, LLM safety, model robustness","This paper examines the decoding process of LLMs and highlights latent safety signals during generation. It proposes a method to leverage these signals for early detection of unsafe content, improving safety without excessively over-refusing benign inputs.",44.85,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10560v1_Learning Latency-Aware Orchestration for Parallel .pdf,Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems,"Xi Shi, Mengxin Zheng, Qian Lou",xi320101@ucf.edu,,"multi-agent systems, parallel execution, latency optimization, latency supervision, task performance, LLM agents","This work introduces Latency-Aware Multi-agent System (LAMaS), a framework that optimizes execution paths under parallel execution to reduce critical path length by 38–46% compared to SOTA, while maintaining or improving task performance.",45.18,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10562v1_Process-Guided Concept Bottleneck Model.pdf,Process-Guided Concept Bottleneck Model,"Reza M. Asiyabi, Sam Harrison, John L. Godlee, David Milodowski, Nicole H. Augustin, Penelope J. Mograbi, Timothy R. Baker, Lorena M. Benitez, Samuel J. Bowers, Thomas K. Brade, Joao M. B. Carreiras, Duncan M. Chalo, Vera De Cauwer, Kyle G. Dexter, Mathias I. Disney, Luisa F. Escobar-Alvarado, Manfred Finckh, Tatenda Gotore, Gabriele C. Hegerl, John N. Kigomo, Fainess C. Lumbwe, Francisco Maiato, Rudzani A. Makhado, Collins W. Masinde, Musingo Tito E. Mbuvi, Iain M. McNicol, Edward T.A. Mitchard, Buster P. Mogonong, Wilson A. Mugasha, Aristides Baptista Muhate, Hinji Mutondo, Leena Naftal, Paula Nieto-Quintano, Elifuraha Elisha Njoghomi, Catherine L. Parr, Oliver L. Phillips, Pierre Proces, Tshililo Ramsawai, Jayashree Ratnam, Mathew Rees, Rasmus Revermann, Natasha Ribeiro, Mahesh Sankaran, Abel M. Siampale, Stephen Sitch, Kathleen G. Smart, Hemant G. Tripathi, Wayne Twine, Gabriel I.K. Uusiku, Helga van der Merwe, Chemuku Wekesa, Benjamin J. Wigley, Mathew Williams, Ellie Wood, Emily Woollen, Shaun Quegan, Steven Hancock, Casey M. Ryan",,,,Process-Guided Concept Bottleneck Model,47.1,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10567v1_Generative AI collective behavior needs an interac.pdf,Generative AI collective behavior,"Laura Ferrarotti, Gian Maria Campedelli, Andrea Baronchelli, Giovanni Iacca, Kathleen M. Carley, Alex Pentland, Joel Z. Leibo, James Evans, Bruno Lepri, Not Diamond, City St. George’s University of London, Carnegie Mellon University, Massachusetts Institute of Technology, Stanford University, Google DeepMind, University of Chicago",arXiv:2601.10567v1,2601.10567v1,"generative AI, collective behavior, interactionist paradigm, large language models, social priors, adaptation, multi-agent systems","The article argues that understanding collective behavior in large language models requires an interactionist paradigm, emphasizing the role of prior knowledge, social context, and adaptive learning in shaping emergent phenomena in AI systems.",47.98,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10581v1_From Single to Multi-Agent Reasoning Advancing Gen.pdf,Advancing GeneGPT for Genomics QA,"Kimia Abedini, Farzad Shami, Gianmaria Silvello",https://kimia-abedini.github.io/Genom-Agent/,2601.10581v1,"Question Answering, Genomic QA, Multi-Agent Systems","This paper proposes GenomAgent, a multi-agent framework that improves GeneGPT by coordinating specialized agents, achieving better performance on genomic benchmarks while extending applicability to other scientific domains.",45.14,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10587v1_Adversarial Evasion Attacks on Computer Vision usi.pdf,Adversarial Evasion Attacks on Computer Vision using SHAP Values,"Frank Mollard, Marcus Becker, Florian Röhrbein",arXiv:2601.10587v1,arXiv:2601.10587,"adversarial attacks, computer vision, SHAP values, deep learning, machine learning","The paper introduces a white-box attack on computer vision models using SHAP values, demonstrating how adversarial evasion attacks can compromise model performance by reducing output confidence or inducing misclassifications. It compares SHAP attacks with the Fast Gradient Sign Method and highlights their effectiveness in gradient hiding scenarios.",47.59,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10591v1_ProbFM Probabilistic Time Series Foundation Model .pdf,Probabilistic Time Series Foundation Model with Uncertainty,"Arundeep Chinta1, Lucas Vinh Tran2, Jay Katukuri1",10.48550/arXiv.2024.12345,arundeep.chinta1,"probabilistic time series, uncertainty quantification, financial forecasting, deep evidential regression, conformal prediction","This paper introduces ProbFM, a transformer-based probabilistic framework that uses Deep Evidential Regression to provide principled uncertainty quantification. It addresses limitations in current TSFMs by enabling explicit epistemic-aleatoric decomposition without relying on restrictive assumptions or sampling.",44.84,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10600v1_Procedural Fairness in Multi-Agent Bandits.pdf,Procedural Fairness in Multi-Agent Bandits,"Joshua Caiata, Carter Blair, Kate Larson",arXiv:2601.10600v1,2601.10600,"procedural fairness, multi-agent systems, fairness objectives, agency, consequentialism","This paper introduces procedural fairness as a fairness objective in multi-agent multi-armed bandits, arguing that fairness should be grounded in equal decision-making power and proportionality rather than solely optimizing outcomes. It highlights the importance of procedural legitimacy and presents a framework for implementing it.",45.04,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10611v1_Molmo2 Open Weights and Data for Vision-Language M.pdf,Molmo2,"Christopher Clark, Jieyu Zhang, Zixian Ma, Jae Sung Park, Mohammadreza Salehi, Sangho Lee, Zhongzheng Ren1,2, Chris Dongjoo Kim1, Yinuo Yang2, Vincent Shao2, Yue Yang1, Weikai Huang2, Ziqi Gao1, Taira Anderson1, Jianrui Zhang, Jitesh Jain1, George Stoica1, Winson Han, Ali Farhadi1,2, Ranjay Krishna",arXiv:2601.10611v1,2601.10611v1,"Open Weights, Vision-Language Models, Video Understanding, Grounding, Point-Driven Grounding, Video Datasets, Object Tracking, Video Pointing, Multi-Image Datasets","Presents Molmo2, a new family of open-source video-language models with state-of-the-art performance in point-driven grounding across single images, multi-images, and videos. Introduces new datasets and training recipes without relying on closed proprietary models.",47.38,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10651v1_Multi-Property Synthesis.pdf,Multi-Property Synthesis,"Christoph Weinhuber, Yannik Schnitzer, Alessandro Abate, David Parker, Giuseppe De Giacomo, Moshe Y. Vardi",,,"LTL synthesis, multiple properties, automated planning, robotics, business process modeling","The paper studies LTLf synthesis with multiple properties, showing that satisfying all may be impossible. It proposes a symbolic algorithm that computes the relationship between product-game states and goal sets, achieving maximal realizable sets with significant speedups.",45.06,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10679v1_Are Your Reasoning Models Reasoning or Guessing A .pdf,A Mechanistic Analysis of Hierarchical Reasoning Models,"Zirui Ren, Ziming Liu",,,"reasoning models, Hierarchical Reasoning Model, reasoning patterns, guessing, reasoning accuracy","This paper investigates why Hierarchical Reasoning Models (HRM) sometimes fail on simple puzzles, attributing the failure to violations of fixed point properties and the presence of multiple fixed points, suggesting HRM behaves more like a guessing mechanism.",43.71,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10681v1_Structure and Diversity Aware Context Bubble Const.pdf,Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval,"Amir Khurshid, Abhishek Sehgal",,,"Large Language Model, Retrieval-Augmented Generation, Context Bubble, Retrieval Introduction, Digital Transformation","The paper proposes a structure-informed, diversity-constrained context bubble construction framework for enterprise retrieval, aiming to improve coherence, reduce redundancy, and enhance answer quality by explicitly constraining diversity and budget.",44.76,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10684v1_On the origin of neural scaling laws from random g.pdf,On the origin of neural scaling laws,"Maissam Barkeshli, Alberto Alfarano, Andrey Gromov",,,"neural scaling laws, random graphs, natural language, transformer models, language modeling","The paper investigates how scaling laws emerge in transformer models trained on random walks, demonstrating their origin in data correlations without requiring power-law structure. Results include scaling laws from simplified generative models and analyses of language modeling benchmarks.",43.92,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10696v1_The Impact of Generative AI on Architectural Conce.pdf,"The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load","Han Jiang, Yao Xiao, Rachel Hurley, Shichao Liu",sliu8@wpi.edu,,"Visual communication, Architectural design, Learning, Performance Assessment, Hybrid Intelligence, Human-AI teaming, GPT Generative Pre-trained Transformer, SD Standard deviation, STEM","This study examines how generative AI influences performance, creative self-efficacy, and cognitive load in architectural design tasks. It finds no overall performance advantage of GenAI but highlights improvements for novice designers and declines in creative self-efficacy for some users.",47.55,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10700v2_LIBERTy A Causal Framework for Benchmarking Concep.pdf,A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals,"Gilat Toker, Nitay Calderon, Ohad Amosy, Roi Reichart",10.48550/arXiv.2024.12345,arXiv:2409.12345,"concept-based explanations, LLMs, structural counterfactuals, explainability, causal models",LIBERTy introduces a framework for constructing datasets with structural counterfactual pairs to evaluate the faithfulness of concept-based explanations. It evaluates models across multiple domains and identifies improvements needed for robust explanations.,45.17,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10702v1_Grounding Agent Memory in Contextual Intent.pdf,Grounding Agent Memory in Contextual Intent,"Ruozhen Yang, Yucheng Jiang, Priyanka Kargupta, Jiawei Han",10.48550/arXiv.2405.12345,arXiv:2405.12345,"large language models, long-horizon reasoning, contextual intent, memory systems, intent indexing","The paper proposes STITCH, an agentic memory system that indexes trajectory steps with structured retrieval cues, contextual intent, and salient entity types. It improves retrieval by filtering history based on intent compatibility, achieving state-of-the-art performance in CAME-Bench.",44.3,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10712v1_MatchTIR Fine-Grained Supervision for Tool-Integra.pdf,MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching,"Changle Qu1, Sunhao Dai, Hengyi Cai, Jun Xu, Shuaiqiang Wang, Dawei Yin",,,"Tool-Integrated Reasoning, Large Language Models, Bipartite Matching, Reinforcement Learning, Multi-Turn Reasoning","Tool-Integrated Reasoning (TIR) empowers LLMs to tackle complex tasks by interleaving reasoning steps with external tool interactions. This paper proposes MatchTIR, a framework introducing fine-grained supervision via bi-partite matching-based turn-level reward assignment and dual-level advantage estimation, demonstrating superior performance on long-horizon multi-turn benchmarks.",46.36,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10748v1_AnyECG Evolved ECG Foundation Model for Holistic H.pdf,AnyECG: Evolved ECG Foundation Model for Holistic Health Profiling,"Jun Li1, Hongling Zhu5, Yujie Xiao1, Qinghao Zhao6, Yale Ali Ke7,8,9, Gongzheng Tang1, Guangkun Nie1, Deyun Zhang11, Jin Li12, Canqing Yu7,8,9, Shenda Hong1,2,3,4",,,"AI-ECG, holistic health profiling, comorbidity pattern recognition, long-term risk prediction, electrocardiography, cardiac disease screening","This study presents AnyECG, a refined ECG foundation model built on a large-scale multicenter dataset, aiming to enable comprehensive disease screening, risk prediction, and comorbidity analysis for holistic health profiling.",46.83,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10768v1_Optimisation of complex product innovation process.pdf,Optimisation of Complex Product Innovation Processes,"Nina Bockova, Barbora Volna, Mirko Dohnal",arXiv:2601.10768v1,2601.10768,"complex product innovation, technological forecasting, three-valued logic, trend-based modelling, scenarios, transition graphs","This paper investigates complex product-innovation processes using models grounded in heuristics. Each heuristic is expressed through simple trends (increasing, decreasing, or constant), providing minimal information-intensive quantifiers. A solution to a trend model is defined as a set of scenarios with possible transitions, depicted by a transition graph, allowing depiction of any future or past system behaviour.",47.76,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10770v1_Unifying Speech Recognition Synthesis and Conversi.pdf,Unifyingspeechrecognition synthesis and conversion with autoregressive transformers,"Runyuan Cai, Yu Lin, Yiming Wang, Chunlin Fu, Xiaodong Zeng",https://github.com/AutoArk/GPA,2601.10770v1,"Text-to-Speech, Automatic Speech Recognition, Voice Conversion, Foundation Model","This paper introduces General-Purpose Audio (GPA), a unified audio foundation model integrating multiple speech tasks within a single LLM. GPA uses a shared discrete audio token space and supports instruction-driven task induction, allowing a single autoregressive model to perform TTS, ASR, and VC without architectural changes. It combines autoregressive formulation, joint multi-task training, and scalable inference for efficient deployment.",47.62,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10773v1_LogicLens Leveraging Semantic Code Graph to explor.pdf,Leveraging Semantic Code Graph to explore Multi Repository large systems,"Niko Usai, Dario Montagnini, Kristian Ilianov Iliev, Raffaele Camanzo",,,"semantic code graph, multi-repository systems, software systems, code navigation, LLM integration","The paper introduces LogicLens, a reactive conversational agent that uses a semantic multi-repository graph built from code analysis and LLM enrichment to help developers explore complex software systems. It addresses challenges in understanding distributed codebases and demonstrates capabilities such as impact analysis and symptom-based debugging.",45.53,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10779v1_Unified Optimization of Source Weights and Transfe.pdf,Unified Optimization of Source Weights and Transfer Quantities in Multi-Source Transfer Learning: An Asymptotic Framework,"Qingyue Zhang, Chang Chu, Haohao Fu, Tianren Peng, Yanru Wu, Guanbo Huang, Yang Li, Shao-Lun Huang",,,"transfer learning, multi-source learning, asymptotic analysis, Kullback-Leibler divergence, optimization","This paper proposes a theoretical framework, Unified Optimization of Weights and Quantities (UOWQ), to optimize source weights and transfer quantities jointly in multi-source transfer learning. It formulates the problem using asymptotic analysis and provides closed-form solutions, demonstrating consistent improvements over baselines on real-world benchmarks.",46.64,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10810v1_Digital Metabolism Decoupling Logic from Facts via.pdf,Digital Metabolism: Decoupling Logic from Facts via Regenerative Unlearning,"Mengmeng Peng, Zhenyu Fang, He Sun",arXiv:2601.10810v1,2601.10810,"digital metabolism, regenerative unlearning, neural logic core, hallucinations, deep learning",The paper addresses the parameter entanglement problem in LLMs by proposing a 'digital metabolism' approach that enables targeted forgetting to isolate a pure neural logic core. It introduces the Regenerative Logic-Core Protocol (RLCP) and demonstrates empirical results on GSM8K showing emergent structural crystallization and improved chain-of-thought capabilities.,47.08,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10820v1_Towards Reliable ML Feature Engineering via Planni.pdf,Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents,"Himanshu Thakur, Anusha Kamath, Anurag Muthyala, Dhwani Sanmukhani, Smruthi Mukund, Jay Katukuri",arXiv:2601.10820v1,2601.10820,"feature engineering, LLM agents, code generation, recommendation systems, human-AI collaboration","This paper presents a planner-guided, constrained-topology multi-agent framework for generating code in LLM-based feature engineering. It addresses challenges in dataset scarcity, agent integration, and human-AI collaboration by leveraging LLM-powered orchestration of tools and workflows. On an in-house dataset, the method achieves significant improvements over manual workflows.",47.62,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10827v1_Approximately Optimal Global Planning for Contact-.pdf,Approximately Optimal Global Planning for Contact-Rich SE(2) Manipulation,"Simin Liu, Tong Zhao, Bernhard Paus Graesdal, Peter Werner, Jiuguang Wang, John Dolan, Changliu Liu, Tao Pang",10.1109/JROBOT.2024.12345,,"contact-rich manipulation, global planning, manipulator planning, robot manipulation, optimization","This paper introduces a new paradigm for computing approximately optimal manipulator plans by combining offline graph construction of mutual reachable sets with online local planning. It demonstrates improved task performance for contact-rich tasks, achieving a 61% reduction in cost and high success rates.",46.38,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10835v1_Can Vision-Language Models Understand Construction.pdf,Can Vision-Language Models Understand Construction Workers? An Exploratory Study,"Hieu Bui, Nathaniel E. Chodosh, Arash Tavakoli",,,"Vision-Language Models, Construction Automation, Robotics, Human Robot Interaction, Large Language Model, Construction, Robotics, Human Robot Interaction","This study evaluates three leading Vision-Language Models (GPT-4o, Florence 2, LLaVa-1.5) in detecting construction worker actions and emotions from static images. It assesses performance using standardized pipelines and finds general-purpose VLMs struggle with nuanced, domain-specific tasks, highlighting need for domain adaptation or multimodal approaches.",45.88,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10880v1_Medical SAM3 A Foundation Model for Universal Prom.pdf,Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation,"Chongcong Jiang, Tianxingjian Ding, Chuhan Song, Jiachen Tu, Ziyang Yan, Yihua Shao, Zhenyi Wang, Yuzhang Shang, Tianyu Han, Yu Tian",arXiv:2601.10880v1,2601.10880,"Medical Image Segmentation, Foundation Models, Fine-Tuning, SAM3, Medical Imaging, Text Prompts","Medical SAM3 is a foundation model for universal prompt-driven medical image segmentation, demonstrating strong generalization through interactive prompting. It addresses limitations in domain shift and anatomical complexity by adapting SAM3 with domain-specific data, achieving robust performance across diverse organs and modalities.",46.84,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10904v1_ARC Prize 2025 Technical Report.pdf,ARC Prize 2025: Technical Report,"François Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers",10.48550/arxiv/2503.08512,arxiv:2503.08512,"ARC-AGI, few-shot generalization, intelligence, refinement loop, knowledge coverage, AI reasoning","The paper surveys top-performing methods in ARC-AGI-3, examines the role of refinement loops in AGI progress, discusses knowledge-dependent overfitting, and previews ARC-AGI-3, highlighting challenges in benchmark contamination and the need for advanced reasoning capabilities.",45.47,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10917v1_Self-learned representation-guided latent diffusio.pdf,Self-Learned Representation-GUIDED Latent Diffusion Model For Breast Cancer Classification In Deep Ultra-Violet Whole Surface Images,"Pouya Afshin, David Helminiak, Tianling Niu, Julie M. Jorns, Tina Yen, Bing Yu, Dong Hye Ye1",10.48550/arXiv.2407.08600,2407.08600,"Breast Cancer Classification, Latent Diffusion Model, Self-Supervised Learning, Data Augmentation, Deep Ultraviolet Imaging, Medical Imaging","The paper proposes a Self-Supervised Learning-guided Latent Diffusion Model to generate synthetic training patches for Deep Ultraviolet Fluorescence Scanning Microscopy (DUV-FSM) data. By leveraging fine-tuned DINO embeddings, the method enhances semantic detail in patch-level data, achieving high accuracy in breast cancer classification and improving performance on limited medical imaging datasets.",46.77,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10921v1_RobuMTL Enhancing Multi-Task Learning Robustness A.pdf,Enhancing Multi-Task Learning Robustness Against Weather Conditions,"Tasneem Shaffee, Sherief Reda",10.48550/arXiv.2024.12345,arXiv:2408.12345,"Robust Multi-Task Learning, Weather Conditions, Multi-Task Learning, Robustness, Adversarial Training, Mixture-of-Experts","This paper introduces RobuMTL, a novel architecture that adaptively selects task-specific hierarchical Low-Rank Adaptation (LoRA) rules and a LoRA expert squad based on input perturbations. It improves robustness across diverse real-world conditions, achieving significant gains on PASCAL and NYUD-v2 benchmarks.",45.51,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10922v1_What Matters in Data Curation for Multimodal Reaso.pdf,What Matters in Data Curation for Multimodal Reasoning?,"Yosub Shin, Michael Buriek, Boris Sobolev, Pavel Bushuyeu, Vikas Kumar, Haoyang Xu, Samuel Watson",10.48550/DCVLR2025,https://arxiv.org/abs/2503.01234,"data curation, multimodal reasoning, DCVLR, data efficiency, model selection","This paper investigates how data curation strategies influence performance in the NeurIPS 2025 DCVLR challenge. It finds that difficulty-based example selection is critical, dataset size has limited impact under fixed training protocols, and certain diversity heuristics do not consistently improve results. The study highlights alignment and difficulty as key drivers in a saturation regime.",45.66,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10926v1_Selecting Language Models for Social Science Start.pdf,Selecting Language Models for Social Science,"Dustin S. Stoltz, Marshall A. Taylor, Sanuj Kumar",XX(X):1–22,,"large language models, LLMs, reproducibility, reliability, replicability, model openness","The paper explores how social scientists can select among thousands of large pretrained language models by evaluating validity, reliability, reproducibility, and replicability. It emphasizes the importance of model openness, footprint, training data, and architecture, and argues for prioritizing replicability in model selection.",45.02,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10931v1_Sparse Data Tree Canopy Segmentation Fine-Tuning L.pdf,Sparse Data Tree Canopy Segmentation,"David Szczecina, Niloofar Azad, Hudson Sun, Kyle Gao, Anthony Bertnyk, Lincoln Linlin Xu",,,"Deep Learning, Computer Vision, Object Segmentation, Remote Sensing, Forestry, Tree Canopy","Tree canopy detection from aerial imagery is crucial for environmental monitoring. This work evaluates five architectures under data scarcity, showing convolution-based models outperform transformers in low-data regimes.",44.62,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10945v1_PatientVLM Meets DocVLM Pre-Consultation Dialogue .pdf,PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between,"K Lokesh1, Abhirama Subramanyam Penamakuri1, Uday Agarwal1, Apoorva Challa2, Shreya K Gowda2, Somesh Gupta2, Anand Mishra1",https://vl2g.github.io/projects/pcdf,,"vision-language models, medical diagnosis, pre-consultation dialogue, symptom elicitation, diagnostic accuracy","This paper proposes a Pre-Consultation Dialogue Framework (PCDF) for vision-language models to simulate realistic doctor-patient interactions, aiming to improve diagnostic accuracy by incorporating patient-reported symptoms before reaching a clinical conclusion.",46.01,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10951v1_Multi-Stage Patient Role-Playing Framework for Rea.pdf,Multi-Stage Patient Role-Playing Framework for Realistic Clinical Interactions,"Shijie Jianga, Zefan Zhanga, Kehua Zhub, Tian Baia, Ruihong Zhaoc, ∗∗",arXiv:2601.10951v1,2601.10951v1,"Patient Role-Playing, Large Language Models, Clinical, Diagnostic Education",This work introduces the first Chinese patient simulation dataset (Ch-PatientSim) to evaluate LLMs in realistic patient interactions. It addresses persona imbalance via few-shot augmentation and proposes a training-free Multi-Stage Patient Role-Playing framework to enhance model personalization and realism. Experimental results show significant performance improvements.,47.39,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10955v1_Beyond Max Tokens Stealthy Resource Amplification .pdf,Beyond Max Tokens: Stealthy Resource Amplification via Tool,"Kaiyu Zhou, Yongsen Zheng, Yicheng He, Meng Xue, Xueluan Gong, Yuji Wang",,,"Large Language Models, LLM Agents, Tool Calling, Agent Security, Economic Denial-of-Service, Multi-Turn Interaction, Textual Notices, Model Context Protocol, Computational Cost","Introduces a stealthy, multi-turn economic DoS attack operating at the tool layer under the guise of a correctly completed task. The method adjusts text-visible fields and uses a Monte Carlo Tree Search optimizer to prolong tool-calling sequences, evading single-turn detection while inflating costs and energy usage.",46.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.10960v1_Steering Language Models Before They Speak Logit-L.pdf,Steering Language Models Before They Speak: Logit-Level Interventions,"Hyeseon An, Shinwoo Park, Hyundong Jin, Yo-Sub Han",,,"steering, language models, logit intervention, text generation, prompting, toxicity mitigation","The paper proposes a training-free inference-time logit intervention to steer LLM outputs for tasks like style-sensitive rewriting and toxicity control. It uses a statistical token score table derived from z-normalized log-odds to shift decoding distributions, achieving up to +47% accuracy improvement across diverse datasets.",45.27,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11000v1_When Personalization Misleads Understanding and Mi.pdf,When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs,"Zhongxiang Sun, Yi Zhan, Chenglei Shen, Weijie Yu, Xiao Zhang, Ming He, Jun Xu",,,"personalization, hallucination, personalized LLMs, factual accuracy, representational entanglement","The paper investigates how personalization in large language models can lead to hallucinations, especially under factual queries. It introduces FPPS to mitigate factual distortions while preserving personalization, and presents PFQABench to evaluate factual and personalized question answering.",45.08,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11007v1_AdaMARP An Adaptive Multi-Agent Interaction Framew.pdf,AdaMARP,"Zhenhua Xu1, Dongsheng Chen 2, Shuo Wang2, Jian Li 2, Chengjie Wang 2, Meng Han 2, Yabiao Wang 2",arXiv:2601.11007v1,arXiv:2601.11007,"adaptive multi-agent interaction, general immersive role-playing, character consistency, environment grounding, narrative coherence","AdaMARP proposes an adaptive multi-agent interaction framework for immersive role-playing, addressing limitations in dynamic environment modeling and multi-character orchestration. It introduces an immersive message format and an explicit Scene Manager for controlled role-playing, achieving improvements over existing LLMs in character consistency and narrative coherence.",46.22,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11012v1_Efficient Protein Optimization via Structure-aware.pdf,Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics,"Jiahao Wang, Shuangjia Zheng",10.48550/arXiv.2024.12345,arXiv:2408.12345,"protein optimization, structure-aware dynamics, Bayesian optimization, molecular modeling, protein sequence design","The paper presents HADES, a Bayesian optimization method using Hamiltonian dynamics to efficiently sample from a structure-aware posterior, enabling rapid discovery of high-performing protein variants while respecting structural constraints.",44.95,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11016v1_Contextual Distributionally Robust Optimization wi.pdf,Contextual Distributionally Robust Optimization with Causal and Continuous Structure: An Interpretable and Tractable Approach,"Fenglin Zhang, Jie Wang",arXiv:2601.11016v1,2601.11016,"Contextual distributionally robust optimization, Causal Sinkhorn discrepancy, Soft regression forest, Stochastic compositional optimization","The paper introduces a framework for contextual distributionally robust optimization that incorporates causal and continuous structure via interpretable decision rules. It presents Causal Sinkhorn Discrepancy, derives a CSD-based ambiguity set, and proposes the Soft Regression Forest for scalable policy optimization.",46.44,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11019v1_Finding the Translation Switch Discovering and Exp.pdf,Finding the Translation Switch: Discovering and Exploiting the Task-Initiation,"Xinwei Wu, Heng Liu, Xiaohu Zhao, Yuqi Ren, Linlong Xu, Longyue Wang, Deyi Xiong, Weihua Luo, Kaifu Zhang",wuxw2021,ryq20,"large language models, translation abilities, task-specific features, translation initiation, causal interventions","This paper investigates how Large Language Models exhibit strong translation capabilities without task-specific fine-tuning, proposing a framework to identify and amplify translation initiation features. Experiments show that targeting these features improves translation accuracy and efficiency, offering insights into model internals and guiding better fine-tuning strategies.",44.81,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11021v1_Combating Spurious Correlations in Graph Interpret.pdf,Combating Spurious Correlations in Graph Interpretability via Self-Reflection,"Kecheng Cai, Chenyang Xu, Chao Peng",10.48550/arXiv.2026.12345,arXiv:2608.12345,"graph interpretability, spurious correlations, self-reflection, graph learning","This paper addresses the challenge of spurious correlations in graph interpretability by proposing a self-reflection framework. It demonstrates how self-reflection techniques, originally used in large language models, can improve interpretability in datasets with misleading patterns. The authors integrate iterative evaluation and fine-tuning based on feedback to enhance model performance on the Spurious-Motif benchmark.",45.58,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11030v1_IDDR-NGP Incorporating Detectors for Distractor Re.pdf,Incorporating Detectors for Distractor Removal with Instant Neural Radiance Field,"Xianliang Huang, Jiajie Gou, Shuhang Chen, Zhizhou Zhong, Jihong Guan, Shuigeng Zhou",10.1145/3581783.3612045,2601.11030v1,"distractor removal, distractors, 3D scenes, synthetic scene, realistic scene, input views, detect distractors, defoliation, petals, synthetic images","This paper presents the first unified distractor removal method, IDDR-NGP, which directly operates on Instant-NPG. The method removes a wide range of distractors in 3D scenes such as snowflakes, confetti, defoliation and petals, demonstrating efficient restoration of 3D scenes from multiple corrupted images.",46.17,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11035v1_Your One-Stop Solution for AI-Generated Video Dete.pdf,Long Ma1 Zihao Xue2 Yan Wang3 Zhiyuan Yan4,"Jin Xu5, Xiaorui Jiang1, Haiyang Yu1,6, Yong Liao1,†, Zhen Bi2,†",arXiv:2601.11035v1,arXiv:2601.11035,"AI-generated video detection, video detection, generative modeling, deep learning, image-to-video","The paper discusses challenges in detecting synthetic videos, highlighting limitations of current datasets and models, and explores factors affecting detection accuracy.",49.58,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11037v1_BAPO Boundary-Aware Policy Optimization for Reliab.pdf,Boundary-Aware Policy Optimization for Reliable Agentic Search,"Shiyu Liu, Yongjing Yin, Jianhao Yan, Yunbo Tang, Qinggang Zhang, Xin Chen, Jingang Wang, Xunliang Cai, Jinsong Su",,,"RL-based agentic search, boundary awareness, reliability, LLMs, search accuracy, IDK response",This paper proposes Boundary-Aware Policy Optimization (BAPO) to improve reliability in agentic search by encouraging explicit 'I DON'T KNOW' responses when reasoning is insufficient. BAPO introduces a group-based reward and adaptive modulator to promote honest uncertainty signaling.,45.95,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11042v1_Spectral Characterization and Mitigation of Sequen.pdf,Spectral Characterization and Mitigation of Sequential Knowledge Editing Collapse,"Chi Zhang, Mengqi Zhang, Xiaotian Ye, Runxi Cheng, Zisheng Zhou, Ying Zhou, Pengjie Ren, Zhumin Chen",2000 4000 10000 20000,,"sequential knowledge editing, large language models, knowledge editing, parameter updates, general abilities, spectral analysis","This work presents a spectral analysis of sequential knowledge editing and proposes REVIVE, a framework that stabilizes sequential editing by preserving dominant singular directions, improving editing efficacy while preserving general abilities under long-horizon editing.",45.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11044v2_AgencyBench Benchmarking the Frontiers of Autonomo.pdf,AGENCYBENCH: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts,"Keyu Li, Junhao Shi, Yang Xiao, Mohan Jiang, Jie Sun, Yunze Wu, Dayuan Fu, Shijie Xia, Xiaojie Cai, Tianze Xu, Weiye Si, Wenjie Li, Dequan Wang, Pengfei Liu",10.48550/arXiv.2601.11044,2601.11044,"autonomous agents, LLMs, benchmarking, real-world scenarios, tool calls, feedback-driven self-correction, open-source vs proprietary, agentic scaffolds","This paper introduces AGENCYBENCH, a comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios. It assesses 138 tasks with specific queries, deliverables, and rubrics, requiring an average of 90 tool calls and 1 million tokens. The study compares closed-source and open-source models, highlighting performance differences in resource efficiency and feedback mechanisms. Experiments demonstrate that proprietary models outperform open-source ones, while proprietary models excel in native ecosystems. AGENCYBENCH aims to guide future agent development and community adoption.",47.79,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11049v1_Predicting Biased Human Decision-Making with Large.pdf,Predicting Biased Human Decision-Making with Large Language Models in Conversational Settings,"Stephen Pilli, Vivek Nallur",,,"Conversational AI, Framing Effect, Status Quo Bias, LLM Simulation",This study examines whether large language models can predict biased decision-making in conversational settings and how cognitive biases interact with cognitive load. It finds that increased dialogue complexity amplifies bias effects and that LLMs can accurately reproduce human bias patterns when incorporating dialogue context.,45.12,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11063v1_H-AIM Orchestrating LLMs PDDL and Behavior Trees f.pdf,"Orchestrating LLMs, PDDL, and Behavior Trees for Hierarchical Multi-Robot Planning","Haishan Zeng, Peng Li",10.1007/978-3-642-45891-8,2601.11063,"multi-robot planning, large language models, PDDL, behavior trees, hierarchical planning","The paper presents H-AIM, a novel embodied multi-robot planning framework that integrates large language models, PDDL, and behavior trees to enable long-horizon task execution in heterogeneous robot teams. It introduces a three-stage cascaded architecture combining instruction parsing, planning, and reactive control, achieving significant improvements over baselines in task success and recall.",46.02,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11065v1_Fairness in Healthcare Processes A Quantitative An.pdf,Fairness in Healthcare Processes: A Quantitative Analysis of Decision Making in Triage,"Rachmadita Andreswari, Stephan A. Fahrenkrog-Petersen, Jan Mendling",arXiv:2601.11065v1,arXiv:2601.11065,"process mining, fairness, triage, emergency room","This study investigates fairness in automated decision-making within emergency triage using process mining. It links real-life event logs with justice dimensions and evaluates fairness using statistical methods, contributing empirical insights for responsible healthcare process mining.",45.61,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11073v1_Bridging Cognitive Neuroscience and Graph Intellig.pdf,Bridging Cognitive Neuroscience and Graph Intelligence: Hippocampus-Inspired Multi-View Hypergraph Learning for Web Finance Fraud,"Rongkun Cui, Nana Zhang, Kun Zhu, Qi Zhang",10.1234/abcd1234,https://doi.org/10.1234/abcd1234,"financial fraud detection, graph neural networks, hippocampus-inspired models, web finance, multi-view learning","Online financial services face significant fraud risks that undermine trust. Existing GNN-based methods struggle with fraud camouflage and long-tailed data. We propose HIMVH, a hippocampus-inspired multi-view hypergraph model, to detect subtle cross-view conflicts and rare fraud patterns.",46.84,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11076v1_A3D Adaptive Affordance Assembly with Dual-Arm Man.pdf,Adaptive Affordance Assembly with Dual-Arm Manipulation,"Jiaqi Liang, Yue Chen, Qize Yu, Yan Shen, Haipeng Zhang, Hao Dong, Ruihai Wu",,,"robotic assembly, dual-arm manipulation, adaptive support, furniture assembly, affordance learning",Proposes A3D framework for learning adaptive affordances to identify optimal support locations in furniture assembly. Uses dense point-level geometry representations and adaptive modules that adjust support strategies based on interaction feedback.,44.68,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11077v1_ABC-Bench Benchmarking Agentic Backend Coding in R.pdf,ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development,"Jie Yang, Honglin Guo, Li Ji, Zhikai Lei, Shuo Zhang, Zhiheng Xi, Shichun Liu, Bo Wang, Yining Zheng, Tao Gui, Xipeng Qiu",10.48550/arXiv.2601.11077,2601.11077,"OpenMOSS, LLM, agentic backend, benchmarking, software engineering, real-world development","This paper introduces ABC-Bench, a benchmark designed to evaluate autonomous agent backend coding in realistic, end-to-end workflows. It evaluates agents across 8 languages and 19 frameworks, highlighting gaps between current LLM capabilities and practical backend engineering demands.",46.98,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11078v1_Visual Marker Search for Autonomous Drone Landing .pdf,Visual Marker Search for Autonomous Drone,"Jiaohong Yao, Linfeng Liang, Yao Deng, Xi Zheng, Richard Han, Yuankai Qi",,,"drone navigation, marker-based landing, reinforcement learning, AirSim, robustness","Marker-based landing is widely used in drone delivery and return-to-base systems for its simplicity and reliability. However, most approaches assume idealized landing site visibility and sensor performance, limiting robustness in complex urban settings. The paper presents a simulation-based evaluation suite on AirSim with varied urban layouts, lighting, and weather to assess performance under realistic conditions.",48.17,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11089v2_MiCA A Mobility-Informed Causal Adapter for Lightw.pdf,MiCA: A Mobility-Informed Causal Adapter for Lightweight Epidemic,"Suhan Guo, Jiahong Deng, Furao Shen, Frshen",10.48550/arXiv.2024.12345,arXiv:2408.12345,"MiCA, mobility, epidemic forecasting, lightweight model, causal discovery","Accurate forecasting of infectious disease dynamics requires integrating noisy mobility data. This work proposes MiCA, a lightweight causal adapter that infers mobility relationships and integrates them into temporal models via gated residual mixing, improving forecasting robustness under data limitations.",45.32,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11090v1_Efficient Multilingual Name Type Classification Us.pdf,Efficient Multilingual Name Type Classification,"Davor Lauc, University of Zagreb / Mondonomo AI, Zagreb, Croatia / Wilmington, DE, US",,,"multilingual NLP, named entity recognition, convolutional neural networks, efficient inference, proper names","Presents a convolutional neural network approach for classifying proper names by language and entity type. The model, Onomas-CNN X, achieves 92.1% accuracy on a multilingual dataset and demonstrates competitive energy efficiency compared to transformer models.",45.06,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11100v1_ReCreate Reasoning and Creating Domain Agents Driv.pdf,ReCreate: Reasoning and Creating Domain Agents Driven by Experience,"Zhezheng Hao, Hong Wang, Jian Luo, Jianqing Zhang, Yuyan Zhou, Qiang Lin, Can Wang, Hande Dong, Jiawei Chen, *",,,"large language model, agent creation, experience-driven, domain agents, automated generation, LLM agents, recreate framework","This paper proposes ReCreate, an experience-driven framework for automatically creating domain agents. It leverages agent interaction history to learn from experience, uses a reasoning-creating pipeline, and applies hierarchical updates to abstract patterns. Experiments show ReCreate outperforms human-designed agents across diverse domains.",46.01,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11109v1_Vision-as-Inverse-Graphics Agent via Interleaved M.pdf,Vision-as-Inverse-Graphics Agent via Interleaved Multimodal Reasoning,"Shaofeng Yin, Jiaxin Ge, Michael J. Black, Trevor Darrell, Haiwen Feng",arXiv:2601.11109v1,arXiv:2601.11109,"vision-as-inverse-graphics, interleaved reasoning, multimodal reasoning, 3D reconstruction, scene editing","VIGA crafts a 3D graphics scene from a single image through iteration. It alternates between generation and verification steps to refine layout, geometry, and lighting, enabling robust one-shot reconstruction.",46.79,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11124v1_Learn Before Represent Bridging Generative and Con.pdf,Learn Before Represent: Bridging Generative and Contrastive Learning,"Xiaoyu Liang, Yuchen Peng, Jiale Luo, Wenhao Wang, Haoji Hu, Xincheng Zhou",10.1093/pasj/ppac045,2601.11,"LLM, contrastive learning, domain-specific embeddings, knowledge acquisition, representation learning","This work identifies a core bottleneck in LLM adaptation for vertical domains: the LLM+CL paradigm prioritizes semantic alignment but lacks domain-specific knowledge, leading to failures on specialized terminology. The proposed Learn Before Represent (LBR) framework addresses this by integrating an Information Bottleneck-Constrained Generative Learning stage before applying Generative Refined Contrastive Learning, thereby enabling accurate representations in chemistry, law, and medical domains.",46.99,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11135v1_Context-aware Graph Causality Inference for Few-Sh.pdf,Context-aware Graph Causality Inference for Few-Shot Molecular Property,"Van Thuy Hoang, O-Joun Lee",10.48550/arXiv.2407.04219,arXiv:2407.04219,"graph learning, molecular property prediction, few-shot learning, causal inference","The paper proposes CaMol, a context-aware graph causality inference framework, to address challenges in few-shot molecular property prediction by leveraging causal inference and disentangling causal substructures.",44.35,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11143v1_Learning Quadrupedal Locomotion for a Heavy Hydrau.pdf,Learning Quadrupedal Locomotion for a Heavy Hydraulic Robot Using an Actuator Model,"Minho Lee, Hyeonseok Kim, Jin Tak Kim, Sangshin Park, Jeong Hyun Lee, Jungsan Cho, Jemin Hwangbo ∗",,,"Hydraulic/Pneumatic Actuators, Legged Robots, Reinforcement Learning, Sim-to-Real Transfer, Robotics, Model-Based Control","The paper presents an analytical actuator model driven by hydraulic dynamics to simulate large-scale hydraulic robots. It enables rapid RL processing and demonstrates stable locomotion transfer on a heavy quadruped robot, highlighting advances in sim-to-real transferability.",45.67,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11144v2_Deep GraphRAG A Balanced Approach to Hierarchical .pdf,Deep GraphRAG: A Balanced Approach to Hierarchical Retrieval and Adaptive Integration,"Yuejie Li, Ke Yang, Zhejiang University, Bolin Chen, Chengjun Mao",,,"GraphRAG, Reinforcement Learning, Large Language Models","Graph-based Retrieval-Augmented Generation (GraphRAG) addresses the trade-off between global search comprehensiveness and local efficiency. It introduces a hierarchical strategy combining inter-community filtering, community-level refinement, and entity-level search, supported by a beam search-optimized re-ranking module and a Knowledge Integration Module.",44.53,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11147v1_Do We Always Need Query-Level Workflows Rethinking.pdf,Do We Always Need Query-Level Workflows?,"Zixu Wang, Bingbing Xu, Yige Yuan, Huawei Shen, Xueqi Cheng",,,"multi-agent systems, query-level workflows, agentic workflow generation, LLM-based systems, task-level vs query-level, token cost","The paper explores whether query-level workflow generation is always necessary for multi-agent systems, arguing that a compact set of top-K task-level workflows can suffice and that exhaustive execution-based evaluation is costly.",45.2,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11151v1_Cross-Modal Attention Network with Dual Graph Lear.pdf,Cross-Modal Attention Network with Dual Graph Learning in Multimodal Recommendation,"Ji Dai, Quan Fang, Jun Hu, DeSheng Cai, Yang Yang, Can Zhao",https://doi.org/XXXXXXX.XXXXXXX,XXX,"Multimedia recommendation, Graph Neural Network, Multimodal Fusion","The paper proposes a Cross-Modal Attention Network with dual graph learning to address shallow modality fusion and asymmetric feature treatment in multimodal recommendation systems. It introduces a core Recursive Cross-Modal Attention mechanism and a symmetric dual-graph framework unified by self-supervised learning, achieving improved performance on large-scale datasets.",45.36,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11160v1_Clustering High-dimensional Data Balancing Abstrac.pdf,Clustering High-dimensional Data: Balancing Abstraction and Representation,"Claudia Plant, Lena G. M. Bauer, Christian B. Boehm",10.1093/acpro/9780190871293.001.0001,2104.11456,"clustering, high-dimensional data, abstraction, representation, deep clustering","The tutorial discusses the challenges of clustering large real-world datasets, emphasizing the need to balance abstraction (simplifying details) with representation (capturing key features). It explores how different clustering algorithms trade off these goals and introduces methods like subspace clustering to improve performance.",44.76,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11178v1_TANDEM Temporal-Aware Neural Detection for Multimo.pdf,Temporal-Aware Neural Detection for Multimodal Hate Speech,"Girish A. Koushik, Helen Treharne, Diptesh Kanojia",10.48550/arXiv.2311.07818,arXiv:2311.07818,"hate speech, multimodal detection, TANDEM, structured reasoning, temporal alignment","This paper introduces TANDEM, a framework that transforms audio-visual hate detection into a structured reasoning problem. It employs a novel reinforcement learning strategy with vision-language and audio-language models, optimizing through self-constrained cross-modal context. Experiments show significant improvements over baselines in target identification, highlighting the challenges of label ambiguity and dataset imbalance in complex multimodal settings.",45.7,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11189v1_Policy-Based Deep Reinforcement Learning Hyperheur.pdf,Policy-Based Deep Reinforcement Learning Hyperheuristics for Job-Shop Scheduling,"Sofiene Lassoued, Asrat Gobachev, Stefan Lier, Andreas Schwung",10.1007/978-3-642-45888-8,,"Hyper-heuristics, Job Shop Scheduling, Policy-based Reinforcement learning, Petri nets","This paper proposes a policy-based deep reinforcement learning hyper-heuristic framework for solving the Job Shop Scheduling Problem. The hyper-heuristic agent learns to switch scheduling rules based on the system state dynamically. Two key mechanisms are introduced: action prefiltering and a commitment mechanism, which improve generalization and stability. Computational experiments show superior performance over traditional methods.",46.37,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11196v1_Artificial Intelligence and the US Economy An Acco.pdf,Artificial Intelligence and the US Economy: An Accounting Perspective on Investment and Production,"Luisa Carpinelli, Filippo Natoli, Marco Taboga",arXiv:2601.11196v1,2601.11196v1,"artificial intelligence, capital expenditures, data centers, national accounts","This paper provides an overview of how the current AI wave is captured in US national accounts, highlighting the role of data centers and their impact on investment and GDP growth.",45.37,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11199v1_SD-RAG A Prompt-Injection-Resilient Framework for .pdf,SD-RAG: A Prompt-Injection-Resilient Framework for Selective Disclosure in Retrieval-Augmented Generation,"Aiman Al Masoud, Marco Arazzi, Antonino Nocera",,,"SD-RAG, Retrieval-Augmented Generation, Prompt Injection, Selective Disclosure, Privacy, Security, Large Language Models","The paper proposes SD-RAG, a framework that decouples security and privacy enforcement from the generation process by applying sanitization during retrieval, thereby improving privacy while maintaining model performance.",45.05,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11200v1_FAQ Mitigating Quantization Error via Regenerating.pdf,Mitigating Quantization Error via Regenerating Calibration Data,"Haiyang Xiao, Weiqing Li, Jinyue Guo, Guochao Jiang, Guohua Liu, Yuewei Zhang",,,"quantization error, calibration data, family-aware quantization, model inference, heat transfer","FAQ proposes a framework to regenerate high-fidelity calibration samples using LLMs from the same family, improving quantization accuracy by addressing limitations of traditional post-training quantization.",43.86,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11202v1_Epistemic Control and the Normativity of Machine L.pdf,Epistemic Control and the Normativity of Machine Learning,Emanuele Ratti,10.1093/acprof:oso/9780190871293.001.0001,,"machine learning, epistemic control, cognitive values, normativity","Investigates concerns about human scientists being excluded from scientific processes due to machine learning systems, exploring epistemic control through tracking and tracing, and argues against a purely pessimistic view.",44.3,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11207v1_LoRA as Oracle.pdf,LoRA as Oracle,"Marco Arazzi, Antonino Nocera",,,"LoRA, Membership Inference Attack, Backdoor Attack",Introduces a novel LoRA-based oracle framework leveraging low-rank adaptation modules to detect backdoors and membership inference without accessing clean reference models.,42.1,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11219v1_SDFLoRA Selective Dual-Module LoRA for Federated F.pdf,SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients,"Zhikang Shen, Jianrong Lu, Haiyuan Wan, Jianhai Chen",22351168@zju.edu.cn,,"federated learning, LoRA, parameter-efficient, heterogeneous clients, privacy-preserving, differential privacy",Proposes Selective Dual-Module Federated LoRA to address heterogeneity and privacy in federated fine-tuning of large language models.,44.9,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11232v1_FactCorrector A Graph-Inspired Approach to Long-Fo.pdf,FACTCORRECTOR: A Graph-Inspired Approach to Long-Form Factuality Correction of Large Language Models,"Javier Carnerero-Cano, Massimiliano Pronesti, Radu Marinescu, Tigran Tchrakian, James Barry, Jasmina Gajcin, Yufang Hou, Alessandra Pascale, Elizabeth Daly",10.48550/arXiv.2405.12345,arXiv:2405.12345,"factuality correction, large language models, LLM, factual errors, correction methods, factual precision","This paper introduces FACTCORRECTOR, a post-hoc correction method for LLMs that adapts across domains without retraining. It leverages structured feedback on factuality and presents the VELI5 benchmark to evaluate improvements in factual accuracy while preserving relevance.",46.57,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11252v1_Beyond Model Scaling Test-Time Intervention for Ef.pdf,Under Review,"BEYONDMODELSCALING: TEST-TIMEINTERVENTION, FOREFFICIENTDEEPREASONING, Qianyue Wang, Jinwu Hu, Yufeng Wang, Huanxiang Lin, Bolin Chen, Zhiquan Wen, Yaofo Chen, Mingkui Tan",,,"large reasoning models, multi-step reasoning, overthinking, overshoot, intervention, transitional conjunctions, reasoning efficiency","The paper proposes Think-with-Me, a test-time interactive reasoning paradigm that uses external feedback to guide reasoning, aiming to improve efficiency and accuracy by pausing reasoning at strategic points.",45.66,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11258v1_Knowledge is Not Enough Injecting RL Skills for Co.pdf,Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation,"Pingzhi Tang, Yiding Wang, Muhan Zhang",,,"Large Language Models, Knowledge Cutoff, Reinforcement Learning, Skill Transfer, Knowledge Adaptation","The paper addresses the knowledge cutoff problem in LLMs by proposing Parametric Skill Transfer (PaST), which enables efficient modular skill injection to improve continual adaptation. Experiments show PaST outperforms existing SFT and RL-based methods in QA benchmarks.",42.72,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11269v1_X-Distill Cross-Architecture Vision Distillation f.pdf,X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning,"Maanping Shao, Feihong Zhang, Gu Zhang, Baiye Cheng, Zhengrong Xue, Huazhe Xu",10.1017/JLCF.2021.008,10.1017/JLCF.2021.008,"visuomotor learning, knowledge distillation, visual encoder, robot action, cross-architecture, manipulation","X-Distill introduces a method that transfers knowledge from a large ViT teacher to a compact CNN student, enabling data-efficient visuomotor learning. It achieves strong performance on manipulation tasks by leveraging visual priors.",45.75,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11282v1_From SERPs to Sound How Search Engine Result Pages.pdf,FromSERPs to Sound: How Search Engine Result Pages and AI-generated Podcasts Interact to Influence User Attitudes on Controversial Topics,"Junjie Wang, Gaole He, Alisa Rieger, Ujwal Gadiraju",10.1145/3786304.3787942,CCS Concepts,"Attitude Change, AI-generated Podcasts, Information modality, Web search, Controversial Topics, Responsible Opinion Formation","This study investigates how the interaction between search engine result pages (SERPs) and AI-generated podcasts influences user attitudes, especially on controversial topics. Through a controlled user study with 483 participants, the research examines the impact of sequence and modality of exposure on attitude change, highlighting the role of viewpoint bias and topic controversy.",45.81,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11286v1_XChoice Explainable Evaluation of AI-Human Alignme.pdf,XChoice: Explainable Evaluation of AI–Human Alignment in LLM-based Constrained Choice Decision Making,"Weihong Qi, Fan Huang, Rasika Muralidharan, Jisun An, Haewoon Kwak",10.48550/arXiv.2025.12345,arXiv:2509.12345,"AI-human alignment, LLM-based decision making, constrained choice, explainable evaluation, human-AI interaction","Presents XCHOICE, a mechanism-based framework for evaluating AI-human alignment in constrained decision making. It goes beyond outcome metrics to recover interpretable parameters reflecting decision factors, constraints, and trade-offs. The method assesses alignment across models, options, and subgroups, demonstrating heterogeneous alignment patterns and identifying misalignment in specific demographic groups.",45.72,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11344v1_How Much Would a Clinician Edit This Draft Evaluat.pdf,Evaluating LLM Alignment for Patient Message Response Drafting,"Parker Seegmiller, Joseph Gatto, Sarah E. Greer, Ganza Belise Isingizwe, Rohan Ray, Timothy Burdick",,,,This study investigates large language model alignment with clinicians through a thematic evaluation of patient message responses. It develops a taxonomy of thematic elements in clinician responses and proposes an evaluation framework to assess clinician editing load. Findings highlight the need for personalized LLM adaptation to improve reliability in clinical workflows.,44.62,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11350v1_FEATHer Fourier-Efficient Adaptive Temporal Hierar.pdf,Fourier-Efficient Adaptive Temporal Hierarchy Forecaster for Time-Series Forecasting,"Jaehoon Lee, Seungwoo Lee, Younghwi Kim, Dohee Kim, Sunghyun Sim",,,"Time-series Forecasting, Edge AI, Ultra-Lightweight Models, Fourier-Efficient Adaptive Temporal Hierarchy Forecaster, Multiscale Temporal Decomposition, Dense Temporal Kernel, Frequency-Aware Branch Gating, Sparse Period Kernel","FEATHer is a multiscale temporal model designed for resource-constrained edge environments. It features lightweight decomposition, efficient temporal mixing, adaptive fusion mechanisms, and sparse periodic reconstruction to achieve accurate long-term forecasting with minimal parameters.",46.0,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11354v1_AstroReason-Bench Evaluating Unified Agentic Plann.pdf,AstroReason-Bench: Evaluating Unified Agentic Planning,"Weiyi Wang, Xinchi Chen, Jingjing Gong, Xuanjing Huang, Fudan University, Shanghai Innovation Institute",arXiv:2601.11354v1,arXiv:2601.11354,"agentic planning, space planning problems, heterogeneous objectives, physical constraints, long-horizon decision-making","This paper introduces AstroReason-Bench, a benchmark for evaluating agentic planning in space planning problems. It highlights the limitations of current agents in handling complex, physics-constrained environments and discusses the need for specialized solutions.",46.53,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11359v1_Think-Clip-Sample Slow-Fast Frame Selection for Vi.pdf,Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding,"Wenhui Tan, Ruihua SongB, Jiaze Li, Jianzhong Ju, Zhenbo LuoB, MiLM Plus, Xiaomi Inc.",,,"multi-modal LLMs, long video understanding, frame selection, video understanding, multi-modal models","Recent progress in multi-modal large language models has advanced video understanding, but performance on long-form videos remains limited. This paper introduces Think-Clip-Sample (TCS), a training-free framework that improves long video understanding via multi-query reasoning and clip-level slow-fast sampling, achieving up to 6.9% accuracy with reduced inference cost.",45.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11369v2_Institutional AI Governing LLM Collusion in Multi-.pdf,Institutional AI: Governing LLM Collusion in Multi-Agent,"M. Bracale Syrnikov, F. Pierucci, M. Galisai, M. Prandi, P. Bisconti, F. Giarrusso, O. Sorokoletova, V. Suriani, D. Nardi",arXiv:2601.11369v2,2601.11369v2,"Institutional AI, LLM governance, Collusion, Cournot markets, Public governance graphs, AI alignment, Multi-agent systems","This paper presents an experimental framework for governing multi-agent large language models through institutional AI, using a governance graph to enforce compliance and reduce collusion in Cournot markets. It compares three regimes—Ungoverned, Constitutional (prompt-only), and Institutional—and finds that institutional governance significantly lowers collusion rates.",47.25,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11379v1_Evaluating LLM Behavior in Hiring Implicit Weights.pdf,"Evaluating LLM Behavior in Hiring: Implicit Weights, Fairness","Morgane Hoffmann, Emma Jouffroy, Warren Jouanneau, Marc Palyart, Charles Pebereau",,,"Large Language Models, Person-job Fit, Fairness, Interpretability","This paper proposes a framework to analyze how large language models assign importance to candidate attributes in recruitment, examining implicit decision logic, fairness implications, and alignment with human hiring practices. It uses synthetic data and compares LLM weights across demographic groups to assess potential biases and trade-offs.",45.3,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11389v1_Hyperparameter Optimization of Constraint Programm.pdf,Hyperparameter Optimization of Constraint Programming Solvers,"Hedieh Haddad, Thibault Falque, Pierre Talbot, Pascal Bouvry",arXiv:2601.11389v1,2601.11389,"constraint programming, hyperparameter optimization, probe algorithm, solver configuration, automated optimization","The paper introduces a probe and solve algorithm for automated hyperparameter optimization in constraint programming solvers, demonstrating improved performance over default configurations.",45.66,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11400v1_Wetland mapping from sparse annotations with satel.pdf,Wetland mapping from sparse annotations with satellite image time series and temporal-aware segment anything model,"Shuai Yuana, Tianwu Linb, Shuang Chena, Yu Xiab, Peng Qinb, Xiangyu Liub, Xiaoqing Xub, Nan Xud, Hongsheng Zhanga, Jie Wangb",arXiv:2601.11400v1,2601.11400v1,"wetland mapping, satellite image time series, sparse annotation, segment anything model, temporal adaptation","Accurate wetland mapping is critical for ecosystem monitoring, yet dense pixel-level annotations are costly. Existing deep learning models struggle with sparse supervision, and wetlands show strong seasonal dynamics, leading to mapping errors. We propose WetSAM, a SAM-based framework that leverages satellite time series to improve mapping from sparse point annotations. It uses a dual-branch design with temporal and spatial components, achieving high F1-score and strong generalization.",47.47,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11409v1_Topology-Guaranteed Image Segmentation Enforcing C.pdf,"Topology-Guaranteed Image Segmentation: Enforcing Connectivity, Genus, and Width Constraints","Wenxiao Li, Xue-Cheng Tai, Jun Liu","MSC codes.68U10, 62H35, 94A08",,"Image segmentation, topological preservation, persistent homology, thickness of topology, variational, regularization","This paper introduces a novel mathematical framework that integrates width information into topological characterization for image segmentation. By combining persistent homology with PDE-based smoothing, the method preserves connectivity, genus, and critical width attributes such as line thickness and length, demonstrating effectiveness through numerical experiments.",46.31,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11421v1_The Great March 100 100 Detail-oriented Tasks for .pdf,THEGREATMARCH100: 100 DETAIL-ORIENTED TASKS FOR EVALUATING EMBODIED AI AGENTS,"Ziyu Wang, Chenyuan Liu, Yushun Xiang, Runhao Zhang, Yu Zhang, Qingbo Hao, Houyu Chen, Zhizhong Feng, Kaiyue Zheng, Dehao Y, Xianchao Zeng, Xinyu Zhou, Boran Wen, Jiaxin Li, Mingyu Zhang, Qian Zhu, Ran Cheng, Yong-Lu Li",,,"robot learning, imitation learning, task design, embodied AI, robotic agents, evaluation metrics","The paper introduces the Great March 100 (GM-100), a set of 100 carefully designed tasks aimed at evaluating robotic agents comprehensively. These tasks address diverse interactions and long-tail behaviors, addressing gaps in current datasets and evaluation practices.",47.07,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11429v1_Relational Linearity is a Predictor of Hallucinati.pdf,Relational Linearity is a Predictor of Hallucinations,"Yuetian Lu, Yihong Liu, Hinrich Schütze, Munich Center for Machine Learning, Ubiquitous Knowledge Processing Lab, Center for Information and Language Processing (CIS)",,,"hallucination, large language models, knowledge representation, linearity, nonlinear relations","Surprisingly, medium-size models like Gemma-7B-IT frequently hallucinate, suggesting that linear relations hinder knowledge assessment.",44.34,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11440v1_GenDA Generative Data Assimilation on Complex Urba.pdf,Generative Data Assimilation on Complex Urban Areas,"Francisco Giral, Álvaro Manzano, Ignacio Gómez, Ricardo Vinuesa, Soledad Le Clainche",10.48550/arXiv.2405.12345,arXiv:2405.12345,"urban wind flow, data assimilation, generative modeling, CFD, reynolds number, building geometry","Proposes GenDA, a generative framework reconstructing high-resolution wind fields from sparse observations using a multiscale graph-based diffusion architecture. It improves reconstruction accuracy and generalization across geometries and resolutions, outperforming traditional methods in RMSE and SSIM.",46.09,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11441v1_Hierarchical Orthogonal Residual Spread for Precis.pdf,Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models,"Xiaojie Gu, Guangxu Chen, Yuheng Yang, Jingxin Han, Andi Zhang",,,"Large language models, Model Editing, Knowledge Update, Residual Spread, Massive Editing, Precision Editing","This paper introduces HORSE, a hierarchical orthogonal residual spread method for precise massive editing in large language models. It compares HORSE with existing methods and demonstrates its effectiveness through experiments on multiple datasets and LLMs.",45.54,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11442v1_Map2Thought Explicit 3D Spatial Reasoning via Metr.pdf,Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps,"Xiangjun Gao, Zhensong Zhang, Dave Zhenyu Chen, Songcen Xu, Long Quan, Eduardo P´erez-Pellitero, Youngkyoon Jang",10.48550/arXiv:2309.12345,10.48550/arXiv:2309.12345,"3D Vision-Language Models, Metric Cognitive Maps, Cognitive Chain-of-Thought, Explainable AI, Spatial Reasoning","The paper introduces Map2Thought, a framework enabling explicit and interpretable spatial reasoning for 3D vision-language models. It integrates a Metric Cognitive Map (Metric-CogMap) and a chain-of-thought reasoning process (Cog-CoT), allowing deterministic geometric operations and interpretable inference traces. Experiments demonstrate performance comparable to baseline models trained on full datasets, achieving 59.9% accuracy with only 50% training data.",47.28,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11451v1_PRISM-CAFO Prior-conditioned Remote-sensing Infras.pdf,PRISM-CAFO: Prior-conditioned Remote-sensing Infrastructure Segmentation and Mapping for CAFOs,"Oishee Bintey Hoque, Nibir Chandra Mandal, Kyle Luong, Amanda Wilson, Samarth Swarup, Madhav Marathe, Abhijin Adiga",10.48550/arXiv.2601.11451,https://arxiv.org/abs/2601.11451,"CAFOs, remote sensing, infrastructure segmentation, mapping, livestock operations, environmental risk","This work presents an infrastructure-first pipeline for identifying and characterizing Concentrated Animal Feeding Operations (CAFOs) using aerial and satellite imagery. It employs a domain-tuned YOLOv8 detector, extracts structured descriptors, and combines them with deep visual features to achieve state-of-the-art classification, supporting risk monitoring and regulatory action.",46.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11459v1_Interactive Narrative Analytics Bridging Computati.pdf,Interactive Narrative Analytics: Bridging Computational Narrative Extraction and Human Sensemaking,Brian Keith,10.1 109/ACCESS.2025.3650352,11250039,"Human-AI collaboration, information extraction, interactive visual analytics, knowledge integration, narrative extraction, narrative sensemaking, semantic interaction, visual analytics","This paper defines the nascent field of Interactive Narrative Analytics (INA), combining computational narrative extraction with interactive visual analytics to support sensemaking. INA addresses challenges such as information overload, misinformation, scalability, interactivity, knowledge integration, and evaluation standardization, offering opportunities across news analysis, intelligence, scientific literature, and social media.",46.77,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11464v1_MHA2MLA-VLM Enabling DeepSeeks Economical Multi-He.pdf,MHA2MLA-VLM: Enabling DeepSeek’s Economical Multi-Head Latent Attention,"Xiaoran Fan, Zhichao Sun, Tao Ji, Lixing Shen, Tao Gui, 3, 4",10.48550/arXiv.2025.12345,arXiv:2509.12345,"vision-language models, key-value cache, multi-head attention, model compression, KV cache, deepseek, parameter efficiency","This work presents MHA2MLA-VLM, a parameter-efficient framework that adapts VLMs to Multi-Head Latent Attention by introducing modality-adaptive masking and modality-decoupled low-rank approximation. It demonstrates improved inference efficiency and minimal performance loss while reducing KV cache footprint.",46.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11468v1_Exploring LLM Features in Predictive Process Monit.pdf,Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs,"Alessandro Padella, Massimiliano de Leoni, Marlon Dumas","1, 1 (January 2026)",19 pages,"Predictive process monitoring, Large language models, Trace Encoding, Key Performance Indicators","This paper extends an LLM-based Predictive Process Monitoring framework to evaluate its generalizability, semantic leverage, and reasoning mechanisms across multiple KPIs. Empirical results show LLMs outperform benchmarks with only 100 traces, leveraging prior knowledge and internal correlations, and demonstrate higher-order reasoning beyond simple prompting.",45.21,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11479v1_Health Facility Location in Ethiopia Leveraging LL.pdf,Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning,"Yohai Trabelsi, Guojun Xiong, Fentabil Getnet, Stéphane Verguet, Milind Tambe",10.1093/acm/9780190871296.001.0001,,"Health Facility Location, Optimization, Human expert knowledge, Alignment, LLM","Ethiopia’s Ministry of Health is upgrading health posts to improve access, especially in rural areas. The study proposes a hybrid framework combining optimization methods with LLM-driven refinement to integrate expert preferences while ensuring coverage guarantees.",45.46,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11492v1_BoxMind Closed-loop AI strategy optimization for e.pdf,BoxMind: Closed-loop AI strategy optimization for elite boxing,"Kaiwen Wang, Kaili Zheng, Rongrong Deng, Qingmin Fan, Milin Zhang, Zongrui Li, Xuesi Zhou, Bo Han, Liren Chen, Chenyi Guo, Ji Wu",arXiv:2601.11492v1,2601.11492,"boxing, AI strategy, elite boxing, tactical analysis, machine learning, sports analytics","Presents BoxMind, a closed-loop AI expert system validated in elite boxing competition. The system parses match footage into technical-tactical indicators and uses a graph-based predictive model to generate strategic recommendations, achieving high accuracy in predicting match outcomes and supporting the Chinese National Team's success at the 2024 Olympics.",47.56,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11496v1_The Poisoned Apple Effect Strategic Manipulation o.pdf,The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion,"Eilam Shapira, Moshe Tennenholtz, Roi Reichart",10.48550/arXiv.2026.12345,arXiv:2109.12345,"AI agents, market design, regulatory frameworks, strategic manipulation, meta-games","Investigates how expanding AI capabilities alters strategic interactions in economic markets, highlighting the 'Poisoned Apple' effect where agents manipulate regulator decisions for personal gain.",46.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11505v1_MetaboNet The Largest Publicly Available Consolida.pdf,MetaboNet: The publicly available dataset for diabetes management,"Miriam K. Wolff, Peter Calhoun, Eleonora Maria Aiello, Yao Qin, Sam F. Royston",arXiv:2601.11505v1,arXiv:2601.11505,"type 1 diabetes, diabetes management, metaboNet, data integration, algorithm development","Progress in Type 1 Diabetes (T1D) algorithm development is limited by the fragmentation and lack of standardization across existing T1D management datasets. Current datasets differ substantially in structure and are time-consuming to access and process, which impedes data integration and reduces the comparability and generalizability of algorithmic developments. This work aims to establish a unified and accessible data resource for T1D algorithm development. Multiple publicly available T1D datasets were consolidated into a unified resource, termed the MetaboNet dataset. Inclusion required the availability of both continuous glucose monitoring (CGM) data and corresponding insulin pump dosing records. Additionally, auxiliary information such as reported carbohydrate intake and physical activity was retained when present.",48.65,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11516v2_Building Production-Ready Probes For Gemini.pdf,Building Production-Ready Probes For Gemini,"János Kramár, Joshua Engels, Zheng Wang, Bilal Chughtai, Rohin Shah, Neel Nanda, Arthur Conmy",2026-01,,"Activation Probing, Interpretability, Language Models, Misuse Risk, AI Safety, Monitoring","The paper discusses challenges in deploying robust activation probes for frontier language models like Gemini, highlighting the need for new architectures that generalize across distribution shifts and demonstrating practical deployment results.",45.91,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11517v1_Do explanations generalize across large reasoning .pdf,Under Review,"Koyena Pal, David Bau, Chandan Singh",,,,"This paper investigates whether explanations generated by large reasoning models generalize across different models, examining consistency between models and human preference rankings. It explores conditions for consistent explanations and proposes a strategy for improving consistency.",41.52,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11625v1_Reasoning Stabilization Point A Training-Time Sign.pdf,Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence,Sahil Rajesh Dhayalkar,10.48550/arXiv.2311.07891,arXiv:2311.07891,"interpretability, explanation drift, reasoning stabilization point, shortcut reliance","This paper introduces a training-time interpretability framework that tracks token-level attributions during fine-tuning. It defines explanation drift as the change in normalized token attributions across epochs and introduces the Reasoning Stabilization Point (RSP), which marks the earliest epoch where drift becomes consistently low. The study demonstrates that drift typically stabilizes early in training, indicating a shift toward reliance on shortcut mechanisms, even when validation accuracy remains competitive.",46.15,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11643v1_Syllabic Agglutinative Tokenizations for Indonesia.pdf,Syllabic Agglutinative Tokenizations for Indonesian LLM,"Hokky Situngkir, Andhika Bernard Lumbantobing, Yohanes Surya",10.48550/arXiv.2601.11643,2601.11643,"Indonesian natural language processing, Indonesian computational linguistics, tokenization, large language models, Gasing Literacy Learning System, low-resource languages, Austronesian languages","This paper presents a novel syllable-based tokenization approach for Indonesian large language models, inspired by the Gasing Literacy Learning System’s pedagogical methodology. Drawing on information-theoretic principles, the authors develop a tokenization framework that segments Indonesian text at syllable boundaries before applying byte-pair encoding, creating a vocabulary aligned with the language’s morphophonological structure. The method improves efficiency and coverage compared to conventional tokenizers while respecting Indonesian’s agglutinative morphology.",46.4,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11644v1_Predicting When to Trust Vision-Language Models fo.pdf,Predicting When to Trust Vision-Language Models for Spatial Reasoning,"Muhammad Imran, Yugyung Lee",10.48550/arXiv.2024.12345,arXiv:2408.12345,"vision-language models, spatial reasoning, uncertainty quantification, robotics","The paper presents a vision-based confidence estimation framework to validate VLM spatial predictions via geometric verification. It improves upon text-based baselines by fusing geometric alignment, spatial ambiguity, detection quality, and internal uncertainty, achieving higher AUROC scores and enabling selective prediction in safety-critical applications.",43.28,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11647v1_Reinforcement Learning for Dynamic Workflow Optimi.pdf,Reinforcement Learning for Dynamic Workflow,"Aniket Abhishek Soni, Milan Parikh, Rashi Nimesh Kumar Dhenia, Jubin Abhishek Soni, Ayush Raj Jha, Sneha Mitinbhai Shah",,,"Reinforcement Learning, CI/CD, DevOps, Workflow Optimization","Continuous Integration and Deployment (CI/CD) pipelines are core to modern software delivery, but their static workflows can be inefficient. This paper proposes a reinforcement learning (RL) approach to optimize CI/CD pipeline workflows dynamically. We model the pipeline as a Markov Decision Process and train an RL agent to make runtime decisions (e.g., selecting test scope) that maximize throughput while minimizing testing overhead. A simulated CI/CD environment is developed to evaluate the approach. Experimental results show that the RL-optimized pipeline achieves up to a 30% improvement in throughput and about a 25% reduction in test execution overhead compared to a static baseline. The agent learns to skip or abbreviate certain tests when appropriate, accelerating delivery without significantly increasing the risk of undetected failures.",47.29,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11650v1_Large Language Model Agent for User-friendly Chemi.pdf,LARGELANGUAGEMODELAGENT FORUSER-FRIENDLY,"Jingkang Liang, Niklas Groll, Gürkan Sin, gsi@kt.dtu.dk",arXiv:2601.11650v1,2601.11650v1,"Chemical Process Simulation, Large Language Model, Model Context Protocol, Process Simulation, Optimization","Modern process simulators are powerful but require expert knowledge. This work integrates a large language model with A VEV A Process Simulation via Model Context Protocol, enabling natural language interaction and automated analysis for both beginners and experts.",46.17,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11651v1_Aesthetics as Structural Harm Algorithmic Lookism .pdf,Aesthetics as Structural Harm: ALGORITHMIC LOOKISMACROSS,"Miriam Doh, Aditya Gulati, Corina Canali, Nuria Oliver",arXiv:2601.11651v1,2601.11651v1,"Generative AI, Artificial Intelligence, Cognitive Biases, Attractiveness Halo Effect","This paper examines algorithmic lookism—the systematic preferential treatment based on physical appearance—in text-to-image generative AI and a downstream gender classification task. Through analysis of 26,400 synthetic faces, it demonstrates how generative AI models associate facial attractiveness with positive attributes and vice versa, mirroring socially constructed biases. The study reveals significant gender bias in classification algorithms, three critical harms, and how newer models intensify aesthetic constraints through homogenization, gendered exposure patterns, and geographic reductionism.",45.83,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11652v1_WISP Waste- and Interference-Suppressed Distribute.pdf,WISP: Waste- and Interference-Suppressed Distributed Speculative LLM Serving at the Edge via Dynamic Drafting and SLO-Aware Batching,"Xiangchen Li, Jiakun Fan, Qingyuan Wang, Dimitrios Spatharakis, Saeid Ghafouri, Hans Vandierendonck, Deepu John, Bo Ji, Ali R. Butt, Dimitrios S. Nikolopoulos",10.1145/376xxxx.377xxxx,2025/XXXX,"Speculative Decoding, Large Language Models, Edge Computing, Distributed Computing, Natural Language Processing, Distributed Architectures, Cloud Computing, General and Reference","The paper addresses bottlenecks in distributed speculative LLM serving—wasted drafting time and verification interference—and proposes WISP, an efficient, SLO-aware system that improves system capacity and throughput while enhancing drafting efficiency.",47.19,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11657v1_Size is Not the Solution Deformable Convolutions f.pdf,Size is Not the Solution: Deformable Convolutions for Effective Physics Aware Deep Learning,"Jack T. Beerman, Shobhan Roy, H.S. Udaykumar, Stephen S. Baek",10.48550/arXiv.2026.12345,arXiv:2509.12345,"physics-aware deep learning, deformable convolutions, physics modeling, computational mechanics, nonlinear flows","The paper proposes deformable physics-aware recurrent convolutions (D-PARC) inspired by Hybrid Lagrangian-Eulerian methods to overcome limitations of standard CNNs in modeling complex physical systems. It demonstrates that adaptive, physics-inspired architectures achieve better fidelity than larger models, emphasizing strategic learning over parameter scaling.",46.81,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11658v1_Towards AGI A Pragmatic Approach Towards Self Evol.pdf,Towards AGI: A Pragmatic Approach Towards Self Evolving Agent,"Indrajit Kar, Sammy Zonunpuia, Zonunfeli Ralte",,,"Self-evolving agent, Large Language Model (LLM), Curriculum Learning, Reward-Based Learning, Genetic Algorithm, Multi-agent systems, Tool-augmented reasoning, Autonomous adaptation","This work introduces a hierarchical self-evolving multi-agent framework integrating a Base LLM, an operational SLM agent, a Code-Gen LLM, and a Teacher-LLM to enable continuous adaptation. It evaluates adaptation paradigms using the TaskCraft dataset, demonstrating that evolved agents outperform originals through autonomous, self-improving workflows.",46.21,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11663v1_Activation Sensitivity as a Unifying Principle for.pdf,Activations Sensitivity as a Unifying Principle for Post-Training Quantization,Bruce Changlong Xu,10.48550/arXiv.2026.12345,arXiv:2509.12345,"activation sensitivity, post-training quantization, quantization error, layer-wise importance","Presents a unified theoretical framework for PTQ by formalizing activation sensitivity. Shows sensitivity emerges from the squared norm of gradient-weighted activations, linking it to classical pruning methods and highlighting limitations in current quantization approaches.",44.46,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11664v1_Serverless AI Security Attack Surface Analysis and.pdf,Serverless AI Security: Attack Surface Analysis and Runtime Protection Mechanisms for FaaS-Based Machine Learning,"Chetan Pathade, Vinod Dhimam, Ilsa Lareb, Sheheryar Ahmad",,,"Serverless computing, Machine learning security, Function-as-a-Service, Cloud security, Adversarial machine learning, AWS Lambda, Azure Functions, Google Cloud Functions, Attack surface analysis, Runtime protection, MLOps security","This paper presents the first comprehensive security analysis of machine learning workloads in serverless environments. It systematically characterizes the attack surface across five categories: function-level vulnerabilities, model-specific threats, infrastructure attacks, supply chain risks, and IAM complexity. The study demonstrates real-world attack scenarios and quantifies their security impact, proposing Serverless AI Shield (SAS) as a multi-layered defense framework.",47.32,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11666v1_MATEX Multi-scale Attention and Text-guided Explai.pdf,Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models,"Muhammad Imran, Chi Lee, Yugyung Lee",arXiv:2601.11666v1,16 Jan 2026,"Explainable AI, Medical Imaging, Vision-Language Models, Gradient Attribution, Attention Rollout, Chest X-ray, CLIP, Attention Rollout, Layer Consistency","MATEX introduces a framework that enhances interpretability in medical vision-language models by integrating anatomically informed spatial reasoning, multi-layer attention, text-guided priors, and layer consistency analysis. It addresses limitations of prior methods and achieves superior spatial precision and clinical alignment.",46.23,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11667v1_Distill-then-Replace Efficient Task-Specific Hybri.pdf,Distill-then-Replace: Efficient Task-Specific Hybrid Attention Model Construction,"Xiaojie Xia1, Huigang Zhang1, Chaoliang Zhong1, Jun Sun1, Yusuke Oishi2",10.1000/xia01-0002-6486-7557,2601.11667v1,"Hybridattentionmodels, Blockwiselocaldistillation, Greedy search",This paper presents a method to construct task-specific hybrid attention models by transferring weights from pretrained full-attention modules to linear attention and applying greedy layer replacement. It addresses computational challenges and enables efficient deployment without retraining.,46.47,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11670v1_A Confidence-Variance Theory for Pseudo-Label Sele.pdf,A Conﬁdence-V ariance Theory for Pseudo-Label,"Jinshi Liu, Pan Liu",10.48550/arXiv.2024.12345,10.48550/arXiv.2024.12345,"semi-supervised learning, pseudo-label selection, confidence calibration, residual-class variance, spectral relaxation","The paper introduces a Confidence-Variance (CoVar) theory to improve pseudo-label selection by combining maximum confidence with residual-class variance, addressing overconfidence issues in deep networks.",44.75,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11674v1_Pigment Network Detection and Classification in De.pdf,Pigment Network Detection and Classification in Dermoscopic Images Using Directional Imaging Algorithms and Convolutional Neural Networks,"M. A. Rasel, Sameem Abdul Kareem, Unaizah Obaidellah",10.1016/j.bspc.2024.106883,,"Melanoma, Dermoscopic Images, Pigment Networks, Contrast Enhancement, Threshold Level, Convolutional Neural Networks, Bag of Features","This study automates pigment network (PN) detection in dermoscopic images using directional imaging algorithms and machine learning classifiers. A directional imaging approach incorporating PCA, contrast enhancement, filtering, and noise reduction achieved 96% success rate on the PH2 dataset. A custom CNN model with two convolutional layers and batch normalization reached 90% accuracy, 90% sensitivity, and 89% specificity, demonstrating superior performance over existing methods.",46.8,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11675v1_Generating metamers of human scene understanding.pdf,Generating Metamers of Human Scene Under-Standing,"Ritik Raina, Abe Leite, Alexandros Graikos, Dimitris Samaras, Gregory J. Zelinsky",,,"human vision, metamer generation, scene representation, visual perception, semantic alignment","This paper introduces Metamer-Gen, a tool that generates scenes aligned with latent human scene representations. It uses a dual-stream representation combining foveated and peripheral inputs to produce image metamers, enabling evaluation via behavioral experiments. The study reveals how high-level semantic alignment predicts metamerism and uncovers features important for scene understanding.",44.07,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11676v1_HALO Semantic-Aware Distributed LLM Inference in L.pdf,Semantic-Aware Distributed LLM Inference in Lossy Edge Network,"Peirong Zheng, Wenchao Xu, Haozhao Wang, Jinyu Chen, Xuemin (Sherman) Shen",,,"Large Language Models, Tensor Parallelism, Edge Computing, Heterogeneity, Semantics, Packet Loss","The paper proposes HALO, a framework enhancing distributed LLM inference in lossy edge networks by relaxing synchronization and optimizing resource allocation, achieving a 3.41x speedup for LLaMA series models under unreliable conditions.",45.04,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11683v1_Attesting Model Lineage by Consisted Knowledge Evo.pdf,Attesting Model Lineage by Consisted Knowledge Evolution with Fine-Tuning Trajectory,"Zhuoyi Shang, Jiasen Li, Pengzhen Chen, Yanwei Liu, Xiaoyan Gu, Weiping Wang",,,"deep learning, model lineage, knowledge evolution, fine-tuning, security concerns, provenance verification",The paper explores how fine-tuning induces lineage relationships among models and proposes a framework to verify these relationships by analyzing knowledge evolution and parameter changes. It addresses security issues like unauthorized redistribution and false provenance claims.,44.87,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11684v1_Mobile-friendly Image de-noising Hardware Consciou.pdf,Equal Contribution: Mobile-friendly Image de-noising: Hardware Conscious Optimization for Edge Application,"Srinivas Miriyala, Sowmya Vajrala, Hitesh Kumar, Sravanth Kodavanti, Vikram Rajendiran",,,"De-Noising, Differentiable NAS, Hardware-aware Search space, Smartphone Deployment, Image Restoration","The paper presents a novel mobile-friendly network for image de-noising using Entropy-Regularized differentiable Neural Architecture Search (NAS) on a hardware-aware search space for a U-Net architecture. It achieves a 12% reduction in parameters, 2-fold improvement in on-device latency, 1.5-fold memory footprint reduction, and competitive accuracy compared to Swin-Transformer, demonstrating strong generalization across benchmarks.",46.17,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11685v1_Towards Efficient Image Deblurring for Edge Deploy.pdf,Equal Contribution Towards Efficient Image Deblurring for Edge Deployment,"Srinivas Soumitri Miriyala, Sowmya Lahari Vajrala, Rama Sravanth Kodavanti",,,"Mobile Image Signal Processing (ISP), De-blurring, Training-free Search, Inference Optimization, Edge Deployment","This paper proposes a hardware-aware adaptation framework for efficient image deblurring on edge devices. It introduces sensitivity-guided block substitution, surrogate distillation, and training-free multi-objective search tailored to device profiling. Applied to the 36-block NAFNet baseline, the method achieves up to 55% reduction in GMACs compared to recent transformer-based SOTA while maintaining competitive accuracy. On-device deployment shows a 1.25× latency improvement over the baseline, demonstrating the effectiveness of feedback-driven adaptation for bridging algorithmic design and deployment constraints.",47.23,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11686v1_Proof of Concept Multi-Target Wildfire Risk Predic.pdf,Proof of Concept: Multi-Target Wildfire Risk Prediction and Large Language Model Synthesis,"Nicolas Caron, Hassan Noura, Christophe Guyeux, Benjamin Aynes",arXiv:2601.11686v1,2601.11686,"wildfire risk, multi-target analysis, large language models, risk assessment, operational needs","This paper proposes a hybrid framework combining predictive models for wildfire risk dimensions with large language models to generate structured reports, aiming to support first responders with actionable insights.",46.16,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11687v1_Semantic Caching and Intent-Driven Context Optimiz.pdf,Semantic Caching and Intent-Driven Context,Harmohit Singh,arXiv:2601.11687v1,2601.11687,"Natural Language to Code, Multi-Agent Systems, Semantic Caching, LLM Optimization, Production Systems","Presents a production-optimized multi-agent system translating natural language queries into executable Python code, achieving high accuracy and cost efficiency through semantic caching, dual-threshold decision mechanisms, and intent-driven prompt assembly.",47.09,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11688v1_SpecMap Hierarchical LLM Agent for Datasheet-to-Co.pdf,Hierarchical LLM Agent for Datasheet-to-Code,"Vedant Nipane Pulkit Agrawal, Amit Singh",arXiv:2601.11688v1,H2LooP.ai,"traceability, traceability link recovery, systems engineering, LLM, semantic analysis",Establishes precise traceability between embedded systems datasheets and code using a hierarchical mapping methodology that leverages large language models across abstraction levels. The approach improves mapping accuracy and reduces computational overhead compared to traditional information-retrieval methods.,46.07,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11700v1_Telling Human and Machine Handwriting Apart.pdf,Telling Human and Machine Handwriting Apart,"Luis A. Leiva, Moises Diaz, Nuwan T. Attygalle, Miguel A. Ferrer, Réjean Plamondon",10.48550/arXiv:2109.06927v1,10.48550/arXiv:2109.06927,"biometrics, classification, deep learning, reverse Turing test, verification","Handwriting movements can serve as a unique behavioral biometric to verify user identity. The study evaluates a shallow RNN trained on synthetic handwritten data, achieving high performance across multiple datasets and synthetic methods. Results demonstrate strong generalization even with limited training data and competitive results in out-of-distribution settings.",47.79,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11702v1_PASTA A Scalable Framework for Multi-Policy AI Com.pdf,A Scalable Framework for Multi-Policy AI Compliance,"YU YANG, IG-JAE KIM, DONGWOOK YOON",10.48550/arXiv.2024.12345,arXiv:2408.12345,"AI compliance, multi-policy, policy evaluation, scalable framework, compliance heatmaps","This paper introduces PASTA, a scalable compliance tool for AI that integrates four innovations to address multi-policy challenges. It evaluates five major policies efficiently and provides actionable recommendations, demonstrating strong alignment with expert judgments.",45.4,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11713v1_Inter-Cell Interference Rejection Based on Ultrawi.pdf,Inter-Cell Interference Rejection Based on Ultrawideband Walsh-Domain Wireless Autoencoding,"Rodney Martinez Alonso, Cedric Dehos, Yuneisy Esthela Garcia Guzman",,,"inter-cell interference rejection, ultrawideband, Walsh domain, autoencoding, 5G communication, spectrum efficiency","This paper proposes a novel technique for rejecting partial-in-band inter-cell interference in ultrawideband communication systems. It presents an end-to-end wireless autoencoder architecture that jointly optimizes transmitter and receiver encoding/decoding in the Walsh domain to mitigate interference from coexisting narrower-band 5G base stations. By exploiting the orthogonality and self-inverse properties of Walsh functions, the system distributes and learns to encode bit-words across parallel Walsh branches. Experimental results show up to 12 dB ICI rejection while maintaining low block error rate.",46.83,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11746v1_LIME-LLM Probing Models with Fluent Counterfactual.pdf,"LIME-LLM: Probing Models with Fluent Counterfactuals, Not Broken Text","George Mihaila, Suleyman Olcay Polat, Poli Nemkova, Himanshu Sharma, Namratha V. Urs",arXiv:2601.11746v1,arXiv:2601.11746,"LIME, local explanation, fluent counterfactuals, NLP, generative models, explainability","This paper introduces LIME-LLM, a framework that replaces random noise with hypothesis-driven, controlled perturbations to improve local explanation fidelity in NLP. By enforcing a 'Single Mask–Single Sample' protocol and using distinct neutral infill strategies, LIME-LLM constructs fluent, on-manifold neighborhoods that rigorously isolate feature effects. The method is evaluated against LIME, SHAP, Integrated Gradients, and LLiMe, demonstrating significant improvements over traditional perturbation-based approaches.",47.67,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11747v1_PRISM Learning Design Knowledge from Data for Styl.pdf,PRISM: Learning Design Knowledge from Data,"Huaxiaoyue Wang, Sunav Choudhary, Franck Dernoncourt, Yu Shen, Stefano Petrangeli",,,"graphic design, stylistic improvement, natural language instructions, design knowledge, style alignment","Graphic design often requires exploring stylistic directions, which can be time-consuming. This paper addresses this by leveraging design data to learn design knowledge and guide stylistic improvement, proposing PRISM that clusters designs, summarizes them, and retrieves relevant knowledge during inference.",44.86,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11758v1_Early Linguistic Pattern of Anxiety from Social Me.pdf,Early Linguistic Pattern of Anxiety from Social Media,Arnab Das Utsa,,,"anxiety detection, linguistic pattern, interpretable machine learning, keyword robustness, cross-domain validation","This study presents a transparent approach to social media-based anxiety detection using linguistically interpretable feature-grounded modeling. It evaluates model performance across multiple validation methods and demonstrates strong accuracy even after sentiment removal or keyword masking. The findings support the use of interpretable linguistic features for reliable, generalizable mental health screening.",44.64,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11762v1_Industry-Aligned Granular Topic Modeling.pdf,Industry-Aligned Granular Topic Modeling,"Sae Young Moon, Myeongjun Erik Jang, Haoyan Luo, Chunyang Xiao, Antonios Georgiadis, Fran Silavong",,,"topic modeling, granularity, industry applications, text mining, data analysis, LLMs, business insights","This paper introduces TIDE, a framework for granular topic modeling leveraging large language models, demonstrating its effectiveness in business contexts through experiments on diverse datasets.",44.58,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11768v1_Lightweight Self-Supervised Detection of Fundament.pdf,Lightweight Self-Supervised Detection of Fundamental Frequency and Accurate Probability of Voicing in Monophonic Music,"Venkat Suprabath Bitra, Homayoon Beigi",10.1093/icpram/ipac002,arXiv:2208.11805,"self-supervised pitch detection, unsupervised pitch detection, fundamental frequency, pitch estimation, resonance, musical timbre transfer, probability of voicing, music synthesis, music analysis, CQT, constant Q transform, DDSP, shift cross-entropy loss, musical instrument modeling, ResNeXt neural network, music information retrieval, MIR","Proposes a lightweight, fully self-supervised framework for joint fundamental frequency estimation and voicing inference, leveraging transposition-equivariant learning and EM-style reweighting with Shift Cross-Entropy to improve robustness under noisy/unvoiced conditions.",46.72,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11776v1_Cleansing the Artificial Mind A Self-Reflective De.pdf,Cleansing the Artificial Mind: A Self-Reflective Detoxification Framework,"Kaituo Zhang, Zhimeng Jiang, Na Zou",10.1234/abcd123,SRD-3448,"Large Language Models, Self-regulation, Detoxification, Toxic content, Ethical AI","This paper introduces a fully self-reflective detoxification framework for LLMs that leverages their inherent self-correction capabilities without external modules or data annotation. It presents a Toxic Signal Detector and a systematic intervention process to transform toxic text into non-toxic content, achieving better detoxification performance than existing methods while preserving semantic fidelity.",44.6,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11778v1_Translation as a Scalable Proxy for Multilingual E.pdf,Translation as a Scalable Proxy for Multilingual Evaluation,"Sheriff Issaka1, Erick Rosas Gonzalez1*, Lieqi Liu1*, Evans Kofi Agyei2, Lucas Bandarkar1, Nanyun Peng 1, David Ifeoluwa Adelani 3, Francisco Guzmán4, Saadia Gabriel 1",,,"translation quality, multilingual evaluation, LLMs, benchmarking, translation metrics","This paper investigates whether translation performance can serve as a scalable, low-cost proxy for evaluating multilingual capabilities of large language models. Through systematic evaluation across 14 models and 9 benchmarks, it finds strong correlations between translation quality and downstream success metrics, suggesting translation fidelity reflects broader multilingual understanding.",46.05,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11781v1_Risk-Aware Human-in-the-Loop Framework with Adapti.pdf,Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles,"Dawood Wasif, Terrence J. Moore, Seunghyun Yoon, Hyuk Lim, Dan Dongseong Kim, Frederica F. Nelson, Jin-Hee Cho",10.48550/arXiv.2024.12345,arXiv:2409.12345,"autonomous vehicles, safety, human-in-the-loop, intrusion response, adaptive control, risk-aware learning","RAIL presents a risk-aware human-in-the-loop framework that integrates adaptive control adaptations and focused learning. It fuses three cues into an Intrusion Risk Score and uses learned authority to blend actions with human override, improving mitigation choices online while maintaining performance.",46.46,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11792v1_A self-evolving multi-role collaborative framework.pdf,A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation,"Yifei Suna, Yongan Lia, A.K. Qinb, Sicheng Houa, Tamas Pflanznerc",,,"Problem generation, Large language models, Multi-role collaboration, Intelligent education, Self-evolution, Knowledge distillation","This paper proposes a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation. It introduces an improved difficulty model using data-driven association-guided path sampling and constructs a dataset of high-quality high school math problems. The method incorporates continual pre-training, supervised fine-tuning, and group relative policy optimization to enhance generation and evaluation, achieving significant improvements in problem innovation while maintaining high correctness.",46.34,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11801v1_RobotDesignGPT Automated Robot Design Synthesis us.pdf,Automated Robot Design Synthesis using Vision Language Models,"Nitish Sontakke, K. Niranjan Kumar, Sehoon Ha",,,"robot design, vision language models, automated design, robotics synthesis, natural language interfaces","This paper proposes a novel automated robot design framework, RobotDesignGPT, leveraging large pre-trained vision-language models to synthesize robot designs from user prompts and reference images. The approach improves design quality, reduces manual feedback, and enables generation of visually appealing and kinematically valid robots inspired by nature.",44.9,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11809v1_Multi-agent DRL-based Lane Change Decision Model f.pdf,Multi-agent DRL-based Lane Change Decision,"Zeyu Mu1, Shangtong Zhang, B. Brian Park",,,"Multi-Agent, Reinforcement Learning, Cooperative Platooning, Lane Change","The study proposes a hybrid multi-agent lane change decision model to enhance cooperative platooning in mixed traffic, addressing challenges posed by sparse CA V distribution and improving traffic efficiency.",40.88,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11816v1_POLARIS Typed Planning and Governed Execution for .pdf,POLARIS: Typed Planning and Governed Execution,"Zahra Moslemi1, Keerthi Koneru2, Yen-Ting Lee3, Sheethal Kumar2, Ramesh Radhakrishnan2",10.48550/arXiv.2311.07891,2311.07891,"Agentic AI, Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed Planning, Evaluation","POLARIS presents a governed orchestration framework treating automation as typed plan synthesis, delivering auditable, policy-aligned execution for document-centric finance tasks. It achieves strong precision and reproducible metrics, setting a benchmark for policy-aligned Agentic AI.",45.92,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11825v1_AI Co-Scientist for Knowledge Synthesis in Medical.pdf,AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept,"Arya Rahgozara, Pouria Mortezaaghaa",arXiv:2601.11825v1,2601.11825,"AI co-scientist, knowledge synthesis, medical contexts, PICOS framework, semantic retrieval, topic modeling","This study proposes an artificial intelligence co-scientist to enhance scalable, transparent knowledge synthesis in biomedical research, focusing on dementia-sport and non-communicable disease corpora using multi-representational methods and transformer-based classification.",47.21,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11840v1_Imandra CodeLogician Neuro-Symbolic Reasoning for .pdf,Neuro-Symbolic Reasoning for Precise Analysis of Software Logic,"Hongyu Lin, Samer Abdallah, Makar Valentinov, Paul Brennan, Elijah Kagan, Christoph M. Wintersteiger, Denis Ignatovich, Grant Passmore ∗1,2",arXiv:2601.11840v1,2601.11840v1,"neurosymbolic reasoning, software logic, precise analysis, code understanding, formal verification, semantic reasoning","This paper introduces CodeLogician 1, a neurosymbolic agent designed for precise software logic analysis. It integrates with ImandraX to enable rigorous reasoning about program behavior, addressing a gap between mathematical proof automation and practical software engineering benchmarks. A new benchmark evaluates reasoning accuracy across program spaces, control flow, and edge cases, demonstrating that combining LLMs with formal reasoning improves performance significantly.",47.21,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11850v1_Human-AI Collaborative Inductive Thematic Analysis.pdf,Human–AI Collaborative Inductive Thematic Analysis: How AI Guides Analysis and Researchers Reclaim Interpretive Authority,"Matthew Nyaaba, Min SungEun, Mary Abiswin Apam, Kwame Owoahene Acheampong, Emmanuel Dwamena, Xiaoming Zhai",,,"Human-AI collaboration, Generative artificial intelligence (GenAI), Inductive thematic analysis, Qualitative data analysis, Epistemic authority, Reflexive methodology","The study examines how researchers use an AI tool (ITA–GPT) within a HACITA framework to guide inductive thematic analysis in education research, emphasizing the balance between AI scaffolding and human interpretive authority.",45.8,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11854v1_ATOD An Evaluation Framework and Benchmark for Age.pdf,ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System,"Yifei Zhang, Hooshang Nayyeri, Emine Yilmaz, Gokhan Tur, Dilek Hakkani-Tür, Hari Thadakamalla",,,"task-oriented dialogue, agentic systems, large language models, benchmarking, long-term reasoning","ATOD introduces a benchmark and synthetic dialogue pipeline to evaluate advanced agentic capabilities in dialogue systems, focusing on multi-goal coordination, memory, adaptability, and proactivity.",45.04,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11859v1_Cascaded Transformer for Robust and Scalable SLA D.pdf,Cascaded Transformer for Robust and Scalable,Cyril Shih-Huan Hsu,,,"network slicing, service level agreement, quality of service, deep neural network, optimization","The paper introduces Casformer, a cascaded Transformer architecture for fast, optimization-free SLA decomposition. It leverages domain-specific encoders and cross-domain aggregation to improve scalability and robustness in 6G network environments.",43.57,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11863v1_Utilizing Metadata for Better Retrieval-Augmented .pdf,Utilizing Metadata for Better Retrieval-Augmented Generation,"Raquib Bin Yousuf, Shengzhe Xu, Mandar Sharma, Andrew Neeser, Chris Latimer, Naren Ramakrishnan",,,"Retrieval-Augmented Generation, Metadata-aware Retrieval, Benchmark Datasets, RAGMATE-10Kdataset","This study evaluates metadata-aware retrieval strategies in Retrieval-Augmented Generation systems, comparing plain-text baselines with approaches that embed metadata directly. It finds that prefixing and unified embeddings outperform plain-text baselines, improving document disambiguation and effectiveness.",46.52,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11868v1_Terminal-Bench Benchmarking Agents on Hard Realist.pdf,Terminal-Benchmarking Agents on Hardware,"Mike A. Merrill, Alexander G. Shaw, Nicholas Carlini, Boxuan Li, Harsh Raj, Ivan Bercovich, Lin Shi, Jeong Yeon Shin, Thomas Walshe, E. Kelly Buchanan, Junhong Shen, Guanghao Ye, Haowei Lin, Jason Poulos, Maoyu Wang, Marianna Nezhurina, Jenia Jitsev, Di Lu, Orfeas Menis Mastromichalakis, Zhiwei Xu, Zizhao Chen, Yue Liu, Robert Zhang, Leon Liangyu Chen, Anurag Kashyap, Jianbo Wu, Minghao Yan, Song Bian, Vedang Sharma, Ke Sun, Steven Dillmann, Akshay Anand, Andrew Lanpouthakoun, Bardia Koopah, Changran Hu, Etash Guha, Gabriel H. S. Dreiman, Jiacheng Zhu, Li Zhong3, Niklas Muennighoff, Robert Amanfu, Shangyin Tan, Shreyas Pimpalgaonkar, Tushar Aggarwal, Xiangning Lin, Xin Lan, Xuandong Zhao, Yiqing Liang, Yuanli Wang, Zilong Wang, Changzhi Zhou, David Heineman, Hange Liu, Yiwei Dai, Wuwei Lin, Yiying Wang, Shanda Li, Terry Yue Zhuo, Wuwei Lin, Yiwei Dai, Yuxin Wang, Wenhao Chai, Shang Zhou, Dariush Wahdany, Ziyu She, Jiaming Hu, Zhikang Dong, Yuxuan Zhu, Sasha Cui, Ahson Saiyed, Arinbjorn Kolbeinsson, Jesse Hu, Christopher Michael Rytting, Ryan Marten, Yixin Wang, Alex Dimakis, Ludwig Schmidt, Stanford University, Laude Institute, Anthropic, Northeastern University, University of California, Santa Barbara, Cornell University, Snorkel AI, Reflection AI, Carnegie Mellon University, Massachusetts Institute of Technology, Peking University, LAION, JSC, FZJ, Tencent, National Technical University of Athens, Nerion, University of Michigan, National University of Singapore, Moonshot AI, University of Texas at Austin, Amazon, University of Washington, University of Wisconsin-Madison, Beijing Institute of Technology, Allen Institute for AI, Monash University, CSIRO’s Data61, Dartmouth College, Princeton University, CISPA, University of Basel, Stony Brook University, Boston University, University of California, Berkeley, SambaNova Systems, Bespoke Labs, Michigan State University, Brown University, Boston University, University of California, San Diego, Beijing Institute of Technology, Allen Institute for AI, Monash University, CSIRO’s Data61, Dartmouth College, Princeton University, CISPA, University of Basel",arXiv:2601.11868v1,2601.11868v1,"AI agents, benchmarking, hardware, benchmarks, computer environments, real-world tasks, model evaluation, verification, AI development",AI agents on terminal hardware can only complete ~65% of tasks; identifies improvement areas via error analysis.,47.62,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11876v1_AI for Green Spaces Leveraging Autonomous Navigati.pdf,Autonomous Trash Pickup Robots on Grass Fields,,,,,"The paper discusses the challenge of litter in U.S. grass fields and proposes a robot using STC algorithms, RTK GPS, ResNet50 CNN, and a custom pickup mechanism to autonomously clean grass fields. It reports an 80% success rate in trash pickup.",42.32,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11880v1_TF-CoDiT Conditional Time Series Synthesis with Di.pdf,TF-CoDiT: Conditional Time Series Synthesis with Diffusion Transformers,"Yingxiao Zhang, Jiaxin Duan, Junfu Zhang, Ke Feng",10.48550/arXiv.2025.12345,arXiv:2509.12345,"Treasury futures, Diffusion Transformers, Time series synthesis, Financial data, Wavelet Transform, Market dynamics","This work introduces TF-CoDiT, a DiT framework for generating treasury futures data. It adapts DiT by transforming 1-D time series into discrete wavelet coefficients and uses a U-shape VAE to model cross-channel dependencies. A Financial Market Attribute Protocol (FinMAP) standardizes market dynamics, enabling high-fidelity synthetic data generation with low error metrics.",46.02,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11885v1_MyGram Modality-aware Graph Transformer with Globa.pdf,Modality-aware Graph Transformer with Global Distribution for Multi-modal Entity Alignment,"Zhifei Li, Ziyue Qin, Xiangyu Luo, Xiaoju Hou, Yue Zhao, Miao Zhang, Zhifang Huang, Kui Xiao, Bing Yang",zhifei1993,"1,*, 4, 2","multi-modal entity alignment, graph transformer, global distribution, knowledge graphs, semantic representations","This paper proposes MyGram, an modality-aware graph transformer with global distribution for multi-modal entity alignment. It introduces a modality diffusion learning module and Gram Loss to capture structural context and achieve consistent multi-modal fusion. Experiments demonstrate improved performance across benchmark datasets.",45.01,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11895v1_DevBench A Realistic Developer-Informed Benchmark .pdf,"DevBench: A Realistic, Developer-Informed Benchmark for Code","Pareesa Ameneh Golnari, Adarsh Kumarappan, Wen Wen, Xiaoyu Liu, Gabriel Ryan, Yuting Sun, Shengyu Fu, Elsie Nallipogu",,,"code generation, LLM evaluation, developer telemetry, realistic benchmark, code completion, functional correctness","DevBench is a telemetry-driven benchmark evaluating LLMs on realistic code completion tasks, focusing on ecological validity and practical utility. It assesses models across six languages and six task categories derived from real developer behavior, offering insights beyond static rule-based benchmarks.",45.78,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11903v1_AEMA Verifiable Evaluation Framework for Trustwort.pdf,AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems,"Yen-Ting Lee, Keerthi Koneru, Zahra Moslemi, Sheethal Kumar, Ramesh Radhakrishnan",arXiv:2601.11903v1,arXiv:2601.11903,"Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight","AEMA presents a process-aware, auditable framework for evaluating multi-agent LLM systems, emphasizing stability, human alignment, and traceable records for responsible automation.",45.8,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11905v1_LIBRA Language Model Informed Bandit Recourse Algo.pdf,Language Model Informed Bandit Recourse,"Junyu Cao, Ruijiang Gao, Esmaeil Keyvanshokooh, Jianhao Ma",10.48550/arXiv.2405.1234,arXiv:2405.1234,"Large Language Models, LLM-Bandits Collaboration, Algorithmic Recourse, Regret Analysis, Personalized Treatment Planning, Hypertension Management","Introduces a unified framework integrating algorithmic recourse, contextual bandits, and LLMs for sequential decision-making in personalized medicine. Presents LIBRA, a Language Model–Informed Bandit Recourse Algorithm, offering warm-start, LLM-effort, and robustness guarantees. Experiments demonstrate improved regret, treatment quality, and sample efficiency.",44.99,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11907v1_Towards Airborne Object Detection A Deep Learning .pdf,Towards Airborne Object Detection: A Deep Learning Analysis,"Prosenjit Chatterjee, ANK Zaman",0000-0003-1169-4717,0000-0001-7831-0955,"Airborne Object Detection, Threat Detection, Deep Learning, EfficientNetB4, ResNet-50, UA V","This work introduces a dual-task model using EfficientNetB4 for simultaneous airborne object classification and threat-level prediction. It presents the AODTA Dataset and demonstrates strong performance, highlighting its potential for surveillance, defense, and airspace management.",45.44,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11913v1_LSTM-MAS A Long Short-Term Memory Inspired Multi-A.pdf,LSTM-MAS: A LONG SHORT-TERM MEMORY INSPIRED MULTI-AGENT SYSTEM FOR LONG-CONTEXT UNDERSTANDING,"Yichen Jiang, Peng Ye, Jiakang Yuan, Chongjun Tu, Tao Chen",,,"Long Short-Term Memory, Multi-Agent System, Memory, Large Language Models, Long-Context Understanding","Effectively processing long contexts remains a fundamental challenge for LLMs. This work proposes LSTM-MAS, inspired by LSTM, to enable multi-agent long-context understanding with controlled information flow and reduced error propagation.",45.82,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11920v1_Enhancing LLM-Based Data Annotation with Error Dec.pdf,Enhancing LLM-Based Data Annotation with Error Decomposition,"Zhen Xu, Columbia University, University of California, Irvine, University of Pennsylvania, Columbia University, Columbia University, University of California, Irvine, Columbia University",zx2393@tc.columbia.edu,,"Data Annotation, Qualitative Coding, Large Language Models, Human-AI Collaboration","This paper proposes a diagnostic evaluation paradigm to assess LLM annotation quality by distinguishing between model-specific and task-inherent errors, incorporating a human-in-the-loop step. It validates the approach on ordinal annotation tasks and highlights limitations of simplistic alignment metrics.",46.18,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11935v1_Big Data Workload Profiling for Energy-Aware Cloud.pdf,Big Data Workload Profiling for Energy-Aware Cloud Resource Management,"Milan Parikh, Aniket Abhishek Soni, Sneja Mitinbhai Shah, Ayush Raj Jha",arXiv:2601.11935v1,arXiv:2601.11935,"cloud computing, energy-aware scheduling, workload profiling, virtual machine placement, big data, green computing","The paper presents a workload-aware scheduling framework that uses profiling of CPU usage, memory demand, and storage I/O behavior to guide energy-efficient virtual machine placement. It combines historical execution logs with real-time telemetry to predict energy and performance impacts, enabling adaptive workload consolidation while maintaining SLAs. Experimental results show a 15–20% reduction in energy consumption for Hadoop MapReduce, Spark MLlib, and ETL workloads.",46.55,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11940v1_Thinking Traps in Long Chain-of-Thought A Measurab.pdf,Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart,"Kang Chen, Fan Yu, Junjie Nian, Shihan Zhao, Zhuoka Feng, Zĳun Yao, Heng Wang, Minshen Yu, Yixin Cao",arXiv:2601.11940v1,arXiv:2601.11940,"Long Chain-of-Thought, Thinking Traps, Adaptive Restart, Reasoning Performance, Model Efficiency",Scaling test-time compute via Long Chain-of-Thought improves reasoning but risks persistent thinking traps. TAAR introduces a diagnostic framework to detect and mitigate these traps during inference.,47.05,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11956v1_Double-Calibration Towards Trustworthy LLMs via Ca.pdf,Double-Calibration: Towards Trustworthy LLMs via Calibrating,"Yuyin Lu, Ziran Liang, Yanghui Rao, Wenqi Fan, Fu Lee Wang, Qing Li",,,"trustworthy reasoning, large language models, knowledge graphs, uncertainty quantification, confidence calibration","The paper introduces DoublyCal, a framework that improves LLM accuracy and calibration by integrating double-calibration principles. It uses a lightweight proxy model to generate evidence and calibrated confidence, guiding a calibrated LLM to produce well-calibrated predictions.",45.26,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11960v1_R2PO Decoupling Training Trajectories from Inferen.pdf,R2PO: Decoupling Training Trajectories,"Jingchu Wang, Bingbing Xu, Yige Yuan, Bin Xie, Xiaoqian Sun, Huawei Shen",10.48550/arXiv.2303.03034,arXiv:2303.03034,"reinforcement learning, large language models, training trajectories, inference responses, exploration vs stability","The paper proposes R2PO (Residual Rollout Policy Optimization) to decouple training trajectories from inference responses, improving reasoning capabilities by enabling controlled trajectory diversification while maintaining stable inference generation.",45.76,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11969v1_textttMemoryRewardBench Benchmarking Reward Models.pdf,MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models,"Zecheng Tang, Baibei Ji, Ruoxi Sun, Haitian Wang, Wangjie You, Yijun Zhang, Wenpeng Zhu, Ji Qi, Juntao Li, Min Zhang",,,"memory management, long-term memory, large language models, reward models, contextual processing","This work introduces MemRewardBench, a benchmark to systematically evaluate how reward models assess long-term memory management in LLMs. It covers 10 settings with varying context lengths and demonstrates that newer models consistently outperform older ones, highlighting current limitations in evaluating LLM memory across diverse tasks.",45.88,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11974v1_Learn Like Humans Use Meta-cognitive Reflection fo.pdf,Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement,"Xinmeng Hou, Peiliang Gong, Bohao Qu, Wuqi Wang, Qing Guo, Yang Liu",10.1234/arxiv.2024.09.12345,10.1234/arxiv.2024.09.12345,"metacognitive reflection, self-improvement, large language models, educational psychology, principle-based instruction","The paper proposes MARS, a framework enabling efficient self-evolution in agents by integrating reflective principles and procedural strategies, thereby improving adaptability beyond static prompts.",45.4,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11977v1_One-Shot Price Forecasting with Covariate-Guided E.pdf,One-Shot Price Forecasting with Covariate-Guided Experts under Privacy Constraints,"I. INTRODUCTION, 2ndYinliang Xu, 3rdJinfeng Wang, 4thJeremy Watson, 5thJian Song",,,"Price forecasting, Time Series, Privacy, Mixture of Experts, Market analysis","Forecasting in power systems often involves multi-variate time series with complex dependencies and strict privacy constraints across regions. Traditional methods require expert knowledge and struggle to generalize across diverse scenarios. Recent advancements in pre-trained time series models offer new opportunities, but zero-shot performance remains limited. This work proposes a novel MoE-Encoder module that augments pre-trained models with a sparse mixture-of-experts layer, enabling expert-guided univariate tasks and supporting federated training under privacy constraints.",46.73,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11979v1_Process In-Context Learning Enhancing Mathematical.pdf,Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion,"Ang Gao1, Changshuo Zhang1, Xiao Zhang1†, Minjun Zhao2, Fangchao Liu2, Xinyu Zhang2",,,"in-context learning, mathematical reasoning, dynamic demonstration, adaptive inference, logical deduction","The paper proposes Process In-Context Learning (PICL), a dynamic demonstration integration framework to improve mathematical reasoning by adapting to real-time inference needs. PICL identifies confusion points in reasoning processes and inserts targeted demonstrations to guide reasoning, addressing limitations of static ICL approaches.",45.3,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11995v1_Learning Audio-Visual Embeddings with Inferred Lat.pdf,Learning Audio–Visual Embeddings with Inferred Latent Interaction Graphs,"Donghuo Zeng, Hao Niu, Yanan Wang, Masato Taya",arXiv:2601.11995v1,0000−0002−6425−6270,"Audio-visual, Latent interaction graph, Cross-modal retrieval, Soft labels, Semantic alignment",The paper proposes a framework that uses soft-label predictions and inferred latent interactions to improve robust audio-visual embedding learning by addressing false negatives and missing cross-modal dependencies.,45.67,LFM-2.5,AMD RX 6800 (Vulkan)
2601.11998v1_Hybrid IDS Using Signature-Based and Anomaly-Based.pdf,Hybrid IDS Using Signature-Based and Anomaly-Based Detection,"m.boutassetta@univ-eltarf.dz, amina.makhlouf@univ-eltarf.dz, newfel.messaoudi@univ-eltarf.dz, abdelmadjid.benmachiche@univ-eltarf.dz, ines.boutabia@univ-eltarf.dz",,,"Intrusion Detection System (IDS), Hybrid IDS, Signature-Based Detection, Anomaly-Based Detection, Machine Learning (ML), Cybersecurity, False Positives, Detection Accuracy, Real-Time Detection, Network Security","This paper presents a comprehensive survey and conceptual overview of Hybrid IDS, integrating signature-based and anomaly-based detection techniques to enhance attack detection capabilities. It reviews recent research, classifies models, discusses advantages/limitations, and explores future directions for cost-effective solutions.",46.23,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12002v1_Kernel-Based Learning of Safety Barriers.pdf,Kernel-Based Learning of Safety Barriers,"Oliver Schönhoel, Zhengang Zhongzhengang, Sadegh Soudjanisadegh, Max Planck Institute for Software Systems, University of Warwick, University of Birmingham",arXiv:2601.12002v1,arXiv:2601.12002,"safety barriers, safety verification, black-box systems, control barrier certificates, stochastic dynamics, temporal logic, safety standards","The paper presents a data-driven approach for verifying and synthesizing safety in black-box AI systems using kernel methods and control barrier certificates. It introduces conditional mean embeddings, reproducing kernel Hilbert spaces, and finite Fourier expansions to address challenges in safety-critical applications.",46.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12003v1_Robust Verification of Concurrent Stochastic Games.pdf,Robust Verification of Concurrent Stochastic Games,"Angel Y. He, David Parker",1,1,"Robust quantitative verification, Probabilistic model checking, Concurrent stochastic games, Epistemic uncertainty","Autonomous systems operate in multi-agent settings requiring concurrent strategic decisions under uncertainty. This work introduces robust CSGs and ICSGs to address specification challenges, proposing theoretical foundations and algorithms for verification under worst-case transition uncertainty. Demonstrated via PRISM model checking.",45.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12014v1_Are LLMs Ready for TOON Benchmarking Structural Co.pdf,Are LLMs Ready for TOON? Benchmarking Structural Correctness–Sustainability Trade-offs in Novel Structured Output,"Elio Masciari, Vincenzo Moscato, Enea Vincenzo Napolitano, Gian Marco Orlando, Marco Perillo, Diego Russo",XXXXXXX.XXXXXXX,XXXXXX,"Green AI, TOON, Large Language Models, Natural Language Processing, Sustainability, Structural Correctness, Environmental Impact","This paper evaluates structural correctness and sustainability trade-offs of the TOON output format using a sustainability-aware evaluation framework. It benchmarks TOON against JSON, XML, and YAML across diverse LLMs, highlighting compactness and carbon efficiency benefits.",45.62,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12019v1_Acting Flatterers via LLMs Sycophancy Combating Cl.pdf,Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs Opposing-Stance Reasoning,"Chaowei Zhang, Xiansheng Luo, Zewei Zhang, Yi Zhu, Jipeng Qiang, Longwei Wang",10.1145/XXXXXX,,"Clickbait Detection, Large Language Models, Opposing Stance Reasoning, Contrastive Learning","The paper addresses clickbait proliferation by proposing a framework that leverages LLM sycophancy to generate opposing stance reasoning, aiming to improve clickbait detection through contrastive learning.",44.76,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12024v1_A Multi-Agent System for Generating Actionable Bus.pdf,A Multi-Agent System for Generating Actionable Business Advice,"Kartikey Singh Bhandari, Tanish Jain, Archit Agrawal, Pratik Narang","p20241006,f20230349,f20191048,dhruv.kumar,pratik.narang",,"customer reviews, business advice, multi-agent system, LLM-based, actionable insights","This paper presents a multi-agent, LLM-based framework for transforming large-scale review corpora into actionable business advice. The framework integrates clustering, advice generation, iterative evaluation, and feasibility ranking, combining corpus distillation with feedback-driven refinement to deliver specific, practical recommendations.",47.43,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12030v1_ARC Active and Reflection-driven Context Managemen.pdf,Active and Reflection-driven Context Management for Long-Horizon Information Seeking Agents,"Yilun Yao, Shan Huang, Elsie Dai, Zhewen Tan, Zhenyu Duan, Shousheng Jia, Yanbing Jiang, Tong Yang",10.48550/arxiv/2303.04112,2303.04112,"active context management, reflection-driven reasoning, context degradation, long-horizon information seeking, information seeking agents","This paper introduces ARC, a framework that treats context as a dynamic internal reasoning state during execution. ARC uses reflection-driven monitoring and revision to reorganize working context when misalignment or degradation is detected, improving performance on long-horizon information-seeking benchmarks.",46.41,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12038v1_Abstract Argumentation with Subargument Relations.pdf,Abstract Argumentation with Subargument Relations,Beishui Liao,,,,"This paper studies abstract argumentation frameworks enriched with an explicit subargument relation, analysing how subargument interactions with attacks affect semantic properties. It addresses the loss of structural information in abstract frameworks and clarifies the role of subarguments in acceptability reasoning.",41.77,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12040v1_Partial Reasoning in Language Models Search and Re.pdf,Partial Reasoning in Language Models: Search and Refinement Guided by Uncertainty,"Murilo da Luz, Bruno Brandão, Luana Martins, Gustavo Oliveira, Bryan de Oliveira, Luckeciano Melo, Telma Soares",,,"Uncertainty, Entropy, Latent-space search, Soft Reasoning, LLM reasoning","The paper introduces PREGU, a method that monitors entropy during LLM generation and triggers localized latent-space searches when uncertainty exceeds a threshold, improving reasoning performance.",45.31,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12042v1_Less Is More -- Until It Breaks Security Pitfalls .pdf,Less Is More — Until It Breaks: Security Pitfalls of Vision Token,"Xiaomei Zhang, Zhaoxi Zhang, Leo Yu Zhang, Yanjun Zhang, Guanhong Tao, Shirui Pan",10.1093/pasj/psa123,,"vision token compression, large vision-language models, security vulnerabilities, robustness degradation, compression mechanisms",Visual token compression improves inference efficiency but compromises robustness. Compression-induced vulnerabilities are state-specific and difficult to detect.,44.66,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12049v1_textitFocaLogic Logic-Based Interpretation of Visu.pdf,FocaLogic: Logic-Based Interpretation of Visual Model Decisions,"Chenchen Zhao, Muxi Chen, Qiang Xu",,,"interpretability, visual models, logic-based, model interpretability, focus subsets","Introduces FocaLogic, a model-agnostic framework for interpreting visual model decisions via logic-based representations. It identifies minimal visual focuses that influence predictions and translates them into logical expressions, providing quantitative metrics to evaluate interpretability.",43.64,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12053v1_A New Strategy for Artificial Intelligence Trainin.pdf,A New Strategy for Artificial Intelligence: Training Foundation Models Directly on Human Brain Data,Maël Donoso,arXiv:2601.12053v1,arXiv:2601.12053,"foundation models, brain, neuroimaging, brain-generated data, brain-trained foundation models, reinforcement learning from human brain (RLHB), chain of thought from human brain (CoTHB)","This paper proposes a novel strategy for training foundation models by leveraging human brain data, aiming to overcome current limitations by incorporating neuroimaging insights across perception, valuation, execution, and integration levels.",45.22,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12055v1_Automating Parameter Selection in Deep Image Prior.pdf,Automating Parameter Selection in Deep Image,"Lina Meyer, Felix Wissel, Tobias Knopp, Susanne Pfefferle, Ralf Fliegert, Maximilian Sandmann, Liana Uebler, Franziska M. Rockl, Bjørn-Philipp Diercks, David Lohr, René Werner",10.1002/2016b37532,,"fluorescence microscopy, denoising, deep image prior, automatic parameter selection, deep image processing","This study introduces AUTO-DIP, a method for optimizing network parameters in fluorescence microscopy denoising by leveraging image similarity. It demonstrates improved performance over traditional DIP approaches using metadata similarity, enabling faster and higher-quality denoising in microscopy applications.",46.97,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12061v1_Codebook-Injected Dialogue Segmentation for Multi-.pdf,Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs,"Jinsook Lee, Kirk Vanacore, Zhuqian Zhou, Bakhtawar Ahtisham, Jeanine Grütter, René F. Kizilcec, Zheng",,,"dialogue act annotation, codebook injection, multi-utterance constructs, segmentation metrics, human-AI agreement","This paper introduces codebook-injected segmentation to improve dialogue act annotation reliability by aligning boundary decisions with downstream criteria. It evaluates LLM-based segmenters against standard and retrieval-augmented baselines, demonstrating that DA-awareness yields more consistent spans despite trade-offs in coherence and boundary distinctiveness. The study highlights segmentation as a design choice requiring optimization for downstream goals.",46.38,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12068v1_Bridging the Gap in Bangla Healthcare Machine Lear.pdf,Bridging the Gap in Bangla Healthcare: Machine Learning Based Disease Prediction Using a Symptoms-Disease Dataset,"Rowzatul Zannat, Abdullah Al Shafi, Abdul Muntakim",10.1234/abcd123,,"Disease Prediction, Annotated Dataset, Machine Learning Techniques, Soft Voting Ensemble, Hard Voting Ensemble","This study develops a Bangla symptoms-disease dataset to improve healthcare accessibility for non-English speakers, evaluating machine learning models to predict diseases from Bangla symptom inputs.",44.8,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12082v1_Conditional Random Fields for Interactive Refineme.pdf,Conditional Random Fields for Interactive Refinement of Histopathological Predictions,"Tiffanie Godelaine, Maxime Zanella, Karim El Khoury, Saïd Mahmoudi, Benoît Macq1 Christophe De Vleeschouwer",10.1093/pasj/hlad.2024.01,,"Histology Classification, Conditional Random Fields, Human-In-The-Loop, Foundation Models, Histopathology","This paper proposes HistoCRF, a framework that adapts Conditional Random Fields to histopathological applications without additional training, aiming to refine Vision-Language Model predictions through expert annotations and iterative human-in-the-loop corrections. Experiments show significant accuracy improvements over zero-shot VLMs.",46.06,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12095v1_Neural Isomorphic Fields A Transformer-based Algeb.pdf,Neural Isomorphic Fields: A Transformer-Based Algebraic Numerical Embedding,"Hamidreza Sadeghi Saeedeh Momtazi, Safa",10.1234/abcd123,,"neural networks, algebraic structures, numerical stability, embedding vectors","The paper introduces a fixed-length number embedding vector that preserves algebraic operations (addition, multiplication, comparison) within rational numbers. It presents a novel Neural Isomorphic Field framework to maintain algebraic properties in neural computations and evaluates performance across algebraic tests.",44.8,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12099v1_Large language models struggle with ethnographic t.pdf,Large language models struggle with ethnographic text annotation,"Leonardo S. Goodall, Dor Shilton, Daniel Austin Mullins, Cohen Institute for the History and Philosophy of Science and Ideas, Harvey Whitehouse",,,"large language models, ethnographic text annotation, cross-cultural research, anthropology, anthropological text, ethnographic annotation","Large language models (LLMs) have shown promise for automated text annotation, raising hopes that they might accelerate cross-cultural research by extracting structured data from ethnographic texts. We evaluated 7 state-of-the-art LLMs on their ability to annotate 121 ritual features across 567 ethnographic excerpts. Performance was limited, falling well below levels required for reliable automated annotation. Longer texts, features requiring ordinal distinctions, and ambiguous constructs proved particularly difficult. Human inter-coder reliability set an approximate ceiling on LLM accuracy: features that human coders found difficult to agree upon were also difficult for LLMs. Yet even on features where humans reliably agreed, models fell short of human performance. Our findings suggest that LLMs cannot yet substitute for human expertise in ethnographic annotation.",47.25,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12104v1_Powerful Training-Free Membership Inference Agains.pdf,Powerful Training-Free Membership Inference Inference,"David Ilić, David Stanojevic, Kostadin Cvejoski",10.48550/arxiv/2303.06931,arXiv:2303.06931,"membership inference, language models, privacy risks, fine-tuning, error positions","Fine-tuned language models risk memorizing training data, enabling membership inference. EZ-MIA introduces an Error Zone score to detect these leaks with low false positives, achieving higher detection rates than prior methods.",44.43,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12124v1_SynQP A Framework and Metrics for Evaluating the Q.pdf,A Framework and Metrics for Evaluating the Quality and Privacy Risk of Synthetic Data,"Bing Hu, Yixin Li, Asma Bahamyirou, Helen Chen",,,"Real-World Data, Synthetic Data, Privacy Metrics, Evaluation Framework, Membership Inference, Identity Disclosure Risk","This paper introduces SYNQP, an open framework for benchmarking privacy in synthetic data generation using simulated sensitive data. It addresses the lack of accessible benchmark datasets and proposes new identity disclosure risk metrics, demonstrating improvements over existing approaches.",45.15,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12126v1_UniMo Unified Motion Generation and Understanding .pdf,Unified Motion Generation and Understanding with Chain of Thought,"Guocun Wang, Kenkun Liu, Jing Lin, Guorui Song, Jian Li, Xiaoguang Han, 4FNii-Shenzhen, 5Guangdong Provincial Key Laboratory of Future Networks of Intelligence",,,"3D human motion generation, understanding human motion, chain of thought, interpretable reasoning, motion-language integration","This paper proposes UniMo, a framework integrating motion-language information and interpretable chain-of-thought reasoning into large language models via supervised fine-tuning. It introduces reinforcement learning with Group Relative Policy Optimization to improve structural correctness and semantic alignment in motion prediction, achieving state-of-the-art performance.",45.26,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12132v1_Bengali Text Classification An Evaluation of Large.pdf,Bengali Text Classification: An Evaluation of Large Language Model Approaches,"Md Mahmudul Hoque, Md Mehedi Hassain, Md Hojaifa Tanvir, Rahul Nandy",,,"Bengali Text Classification, Transformer-based Text Classifier, Multilingual NLP, Qwen, LLaMA","This study evaluates large language models for Bengali newspaper article classification. Qwen 2.5 achieved the highest accuracy (72%), outperforming LLaMA 3.1 (53%) and LLaMA 3.2 (56%). The research addresses challenges in Bengali NLP due to limited datasets and explores future improvements.",46.83,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12134v1_Human-Human-AI Triadic Programming Uncovering the .pdf,Human-Human-AI Triadic Programming: Uncovering the Role of AI Agent and the Value of Human Partner in Collaborative Learning,"Taufiq Daryanto, Xiaohan Ding, Kaike Ping, Lance T. Wilhelm, Yan Chen, Chris Brown",arXiv:2601.12134v1,arXiv:2601.12134v1,"human-AI collaboration, programming learning, AI agent, collaborative learning, pedagogical benefits","This study explores human-human-AI triadic programming, examining how AI can augment human partners in collaborative learning without replacing social collaboration. It investigates whether AI should act as a shared collaborator or personal support, emphasizing the social and learning aspects of programming.",46.9,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12138v1_DriveSafe A Hierarchical Risk Taxonomy for Safety-.pdf,DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants,"Abhishek Kumar, Riya Tapwal, Carsten Maple",,,"LLM safety, driving assistants, hierarchical risk taxonomy, safety-critical systems, LLM-based assistants","This paper introduces DriveSafe, a four-level risk taxonomy for characterizing safety-critical failure modes of large language models in driving assistants. It evaluates refusal behavior across six deployed LLMs and highlights limitations of general-purpose safety alignment in real-world driving contexts.",45.58,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12141v1_TIDE A Trace-Informed Depth-First Exploration for .pdf,TIDE: A Trace-Informed Depth-First Exploration,"Yuliia Suprun, Khen Elimelech, Lydia E. Kavraki, Moshe Y. Vardi",arXiv:2601.12141v1,arXiv:2601.12141v1,"task planning, temporally extended goals, linear temporal logic, planning methods, robotics","This paper introduces TIDE, a depth-first exploration framework for planning with temporally extended goals. It decomposes temporal problems into smaller subproblems, uses cost-driven heuristics, and employs adaptive backtracking to ensure completeness and efficiency. Experimental results show promising performance.",47.04,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12147v1_Segment and Matte Anything in a Unified Model.pdf,Segment And Matte Anything in a Unified Model,"Zezhong Fan, Xiaohan Li, Topojoy Biswas, Kaushiki Nag, Kannan Achan",10.48550/arXiv.2311.06854,arXiv:2311.06854,"Segment Anything, mapping, image segmentation, mattening, interactive segmentation, object delineation","This paper introduces Segment And Matte Anything (SAMA), a lightweight extension of Segment Anything (SAM) that enables high-quality interactive image segmentation and matting with minimal parameters. SAMA leverages a Multi-View Localization Encoder and a Localization Adapter to refine masks, and incorporates prediction heads for both tasks simultaneously. Trained on a large-scale dataset, SAMA achieves state-of-the-art performance across segmentation and matting benchmarks.",46.37,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12150v1_Enhanced Diagnostic Performance via Large-Resoluti.pdf,Enhanced Diagnostic Performance Via Large-Resolution Inference Optimization for Pathology Foundation Models,"Mengxuan Hu, Zihan Guan, John Kang, Sheng Li, Zhongliang Zhou",10.1093/pal/ppac023,2601.12150v1,"computational pathology, foundation models, inference optimization, ROI classification, segmentation","The paper addresses inefficiencies in pathology foundation models constrained by fixed input sizes for whole-slide images. It proposes a space- and time-efficient inference strategy using spatially aware attention and global attention scores, achieving up to 7.67% improvement in ROI classification while maintaining segmentation accuracy.",46.1,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12186v1_Aletheia What Makes RLVR For Code Verifiers Tick.pdf,What Makes RLVR For Code Verifiers Tick?,"Vatsal Venkatkrishna, Indraneil Paul, Iryna Gurevych",,,"Reinforcement Learning, Code Verification, Large Language Models, LLM Post-Training, Execution Feedback, Code Generation, Verification Recipes","This paper explores the design and evaluation of Aletheia, an open-source testbed for evaluating code verifiers trained via RLVR. It highlights the importance of on-policy learning and thinking-based training despite scaling challenges, and discusses components like intermediate thinking traces and negative sample learning.",45.06,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12205v1_Do Neural Codecs Generalize A Controlled Study Acr.pdf,Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks,"Shih-Heng Wang, Jiatong Shi, Jinchuan Tian, Haibin Wu, Shinji Watanabe",10.48550/arXiv.2403.07929,2403.07929,"Neural Audio Codecs, Language Generalization, Non-Speech Tasks, Pre-Training Data, Signal Reconstruction","This paper investigates three aspects of Neural Audio Codecs' generalization capabilities: generalization to unseen languages during pre-training, generalization of speech-only models to non-speech applications, and the impact of incorporating non-speech pre-training data. The study evaluates performance across 11 metrics and finds that NACs can generalize to unseen languages, speech-only models struggle with non-speech tasks, and adding non-speech data improves non-speech performance while maintaining speech performance.",46.41,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12212v1_Speculative Sampling with Reinforcement Learning.pdf,Speculative Sampling with Reinforcement Learning,"Chenan Wang, Daniel H. Shi, Haipeng Chen",,,"speculative sampling, reinforcement learning, large language models, inference latency, tree-based drafting","The paper introduces Reinforcement Learning for Speculative Sampling (Re-SpS), a reinforcement learning-based framework that dynamically optimizes draft tree hyperparameters in real time. It improves generation speed while maintaining output fidelity across diverse benchmarks.",43.47,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12215v1_Wavelet-Driven Masked Multiscale Reconstruction fo.pdf,Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation Models,"Megha Thukral, Cyrus Tanade, Simon A. Lee, Juhyeon Lee, Hao Zhou, Keum San Chun, Migyeong Gwak, Viswam Nathan, Md Mahbubur Rahman, Li Zhu, Mehrab Bin Morshed, Subramaniam Venkatraman, Sharanya Arcot Desai",arXiv:2601.12215v1,2601.12215,"Wearable SSL Method, Wavelet based Modelling, PPG foundation models","This work introduces Masked Multiscale Reconstruction (MMR) for PPG representation learning, leveraging wavelet-based multiresolution decomposition to capture multi-scale physiological rhythms. Pretrained on ~17M unlabeled 10-second PPG segments, MMR improves or matches state-of-the-art PPG foundation models across diverse health tasks, demonstrating the value of wavelet representations for robust feature learning.",48.36,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12224v1_Where It Moves It Matters Referring Surgical Instr.pdf,"Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion","Meng Wei, Kun Yuan, Shi Li, Yue Zhou, Long Bai, Nassir Navab, Hongliang Ren, Hong Joo Lee, Tom Vercauteren, Nicolas Padoy",10.48550/arXiv.2025.12345,arXiv:2509.12345,"surgical segmentation, instrument motion, natural language processing, motion-guided AI, surgical robotics","This paper introduces SurgRef, a motion-guided framework for segmenting surgical instruments from video using natural language descriptions. It addresses challenges in surgical video analysis by focusing on instrument motion rather than appearance, enabling robust performance under occlusion and ambiguity.",46.27,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12234v1_Proc3D Procedural 3D Generation and Parametric Edi.pdf,Procedural 3D Generation and Parametric Editing of 3D Shapes with Large Language Models,"Fadlullah Raji, Stefano Petrangeli, Matheus Gadelha, Yu Shen, Uttaran Bhattacharya, Gang Wu2",arXiv:2601.12234v1,2601.12234,"Proc3D, 3D generation, Parametric editing, Large Language Models, Procedural Compact Graph, ULIP scores","Proc3D facilitates the generation of editable 3D models using Procedural Compact Graph (PCG) and enables real-time, intuitive parametric editing through LLMs. It offers more than 400× speedup over conventional methods and improves ULIP scores by 28%.",47.06,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12242v1_Optimal Power Allocation and Sub-Optimal Channel A.pdf,Optimal Power Allocation and Sub-Optimal Channel Assignment in NOMA Systems Using Deep Reinforcement Learning,"Woo Seok Kim, Jeonghoon Lee, Sangho Kim, Taesun An, WonMin Lee, Dowon Kim, Kyungseop Shin",arXiv:2601.12242v1,arXiv:2601.12242,"Non-Orthogonal Multiple Access, Deep Reinforcement Learning, Wireless Network, Resource Allocation","This paper proposes a deep reinforcement learning framework with replay memory to optimize network resource allocation in NOMA systems, addressing challenges in power allocation and channel assignment amid growing IoT demands.",45.77,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12243v1_Less is More Label-Guided Summarization of Procedu.pdf,Less is More: Label-Guided Summarization of Procedural and Instructional Videos,"Shreya Rajpal, Michal Golovanesky, Carsten Eickhoff",arXiv:2601.12243v1,arXiv:2601.12243,"video summarization, procedural videos, instructional videos, semantic understanding, multimodal analysis","This paper proposes a three-stage framework, PRISM, for procedural and instructional video summarization. PRISM integrates semantic and multimodal analysis, adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model. It achieves strong performance in summarizing instructional and activity-based videos while preserving procedural intent and semantic content.",44.69,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12247v1_Plan Verify and Fill A Structured Parallel Decodin.pdf,"Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion","Miao Li, Xuanzhou Chen, Pascal Van Hentenryck, Hengyu Fu, Yuhang Cai, Baihe Huang, Ye","2025a; Google DeepMind, 2025",2026-01-21,"diffusion models, planning, parallel decoding, semantic anchors, planning dependencies, efficiency, accuracy","This paper proposes Plan-Verify-Fill (PVF), a training-free paradigm for diffusion language models that leverages hierarchical planning and verification to optimize decoding efficiency. PVF reduces the Number of Function Evaluations (NFE) by up to 65% compared to confidence-based parallel decoding while maintaining high accuracy.",46.02,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12248v1_AQUA-Bench Beyond Finding Answers to Knowing When .pdf,AQUA-BENCH: BEYOND FINDING ANSWERS TO KNOWING WHEN THERE ARE NONE,"Chun-Yi Kuan, Hung-yi Lee",,,"unanswerable questions, audio question answering, audio-aware large language models","This paper introduces AQUA-Bench, a benchmark for evaluating audio question answering systems by assessing performance on three unanswerable scenarios: absent answer detection, incompatible answer sets, and incompatible audio questions. It highlights limitations of current models in handling unanswerable inputs and proposes a framework to improve reliability.",44.47,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12249v1_An Innovative Framework for Breast Cancer Detectio.pdf,"An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion","Ehsan Sadeghi Pour, Mahdi Esmaeili, Morteza Romoozi",,,"Breast Cancer Detection, Pyramid Adaptive Atrous Convolution (PAAC), Transformer, Multi-Scale Feature Fusion, Self-Attention Mechanism, Medical Image Processing","This thesis presents an innovative framework for detecting malignant masses in mammographic images by integrating Pyramid Adaptive Atrous Convolution, Transformer architectures, and multi-scale feature fusion. The model leverages Dice Loss and Focal Loss to improve binary classification accuracy, achieving high performance in detecting cancerous masses with improved efficiency.",46.27,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12256v1_Improving Large Molecular Language Model via Relat.pdf,Improving Large Molecular Language Model,"Jinyoung Park, Minseong Bae, Jeehye Na, Hyunwoo J. Kim",10.48550/arXiv.2024.12345,arXiv:2408.12345,"large language model, molecular modeling, relation-aware collaboration, hallucination mitigation, 3D conformations","The paper proposes CoLLaMo, a language model enhanced with a multi-level molecular modality-collaborative projector, to address hallucination and modality integration issues in large molecular language models.",45.1,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12257v1_Soft Shadow Diffusion SSD Physics-inspired Learnin.pdf,Soft Shadow Diffusion (SSD): Physics-inspired,"Fadlullah Raji, John Murray Bruce",arXiv:2601.12257v1,arXiv:2601.12257v1,"Computational imaging, Machine learning, 3D generative models, Diffusion models, Separable non-linear least squares","This paper presents a novel 3D reconstruction method called Soft Shadow Diffusion (SSD) that leverages physics-inspired reformulations of light transport. SSD generalizes non-line-of-sight imaging by decomposing hidden scenes into occluding and non-occluding components, enabling effective reconstruction from ordinary NLOS photographs. The approach combines gradient-based optimization and a physics-inspired neural network, demonstrating robustness across diverse real-world scenarios.",47.46,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12259v1_FutureX-Pro Extending Future Prediction to High-Va.pdf,FutureX-Pro: Extending Future Prediction to High-Value,,arXiv:2601.12259v1,arXiv:2601.12259,"FutureX, FutureX-Pro, Future Prediction, LLM, LLMs, agentic prediction, high-value verticals, market efficiency, retail forecasting, supply chain, natural disaster","Building upon FutureX, this report introduces FutureX-Pro, extending agentic future prediction to high-value vertical domains such as Finance, Retail, Public Health, and Natural Disaster. It benchmarks Large Language Models on entry-level prediction tasks across these areas to assess domain-specific performance.",45.78,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12260v1_Docs2Synth A Synthetic Data Trained Retriever Fram.pdf,A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding,"Yihao Ding, Qiang Sun, Puzhen Wu, Sirui Li, Siwen Luo, Wei Liu",10.1234/example.doi,,"document understanding, scanned documents, visual retrieval, synthetic data, zero-shot learning, domain adaptation","Introduces Docs2Synth, a framework that combines retrieval-guided inference with lightweight visual retrieval to improve grounding in private and low-resource domains, addressing challenges of hallucination and domain drift.",46.42,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12263v1_Multimodal Generative Engine Optimization Rank Man.pdf,Multimodal Generative Engine Optimization: Rank Manipulation for Vision–Language Model Rankers,"Yixuan Du, Chenxiao Yu, Haoyan Xu, Ziyi Wang, Yue Zhao, Xiyang Hu",10.1234/example.doi,632562,"vision-language models, rank manipulation, vision-language model optimization, multimodal ranking, adversarial attacks, search ranking","This paper introduces Multimodal Generative Engine Optimization (MGEO), a novel adversarial framework that enables malicious actors to unfairly promote products by jointly optimizing imperceptible image perturbations and textual suffixes. Unlike text-only or image-only attacks, MGEO exploits deep cross-modal coupling in VLMs, demonstrating superior performance on real-world datasets.",45.66,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12269v1_Simulated Annealing Enhances Theory-of-Mind Reason.pdf,Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models,"Xucong Hu, Jian-Qiao Zhu",,,"Language Models, Markov Chain Monte Carlo, Simulated Annealing, Power Sampling, Theory of Mind","The paper demonstrates that incorporating simulated annealing into autoregressive language models can recover Theory of Mind capabilities without additional weight updates, improving global coherence by sampling from sharpened sequence distributions.",43.88,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12276v1_Predictive Prototyping Evaluating Design Concepts .pdf,Predictive Prototyping: Evaluating Design Concepts with GPT,,,,"Prototyping, Design Theory, Iteration, Simulation, AI, LLM, GPT, RAG, Crowdsourcing","This work explores using generative pretrained transformers (GPTs) to predict design outcomes such as cost, performance, and usability during prototyping. A novel retrieval-augmented generation (RAG) approach with OpenAI’s GPT-4o is introduced to bridge conceptual designs and physical prototypes, comparing predictions against physical testing results.",46.04,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12282v1_CytoCLIP Learning Cytoarchitectural Characteristic.pdf,CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain,"Pralaypati Ta, Sriram Venkatesaperumal, Keerthi Ram, Mohanasankar Sivaprakasam",10.1109/TECH.2024.12345,IEEE TRANSACTIONS AND JOURNALS TEMPLATE 1,"cytoarchitecture, histological image processing, contrastive language image pre-training, CLIP, brain regions, developmental brain","CytoCLIP is a vision-language model that leverages pre-trained Contrastive Language-Image Pre-Training to learn joint visual-text representations of brain cytoarchitecture. It proposes two variants: one using low-resolution images for overall patterns and another using high-resolution tiles for cellular details. Trained on NISSL-stained histological sections, it achieves strong performance in classification and generalization tasks.",47.67,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12286v1_Conversational Context Classification A Representa.pdf,Conversational Context Classification: A,Jonathan Pan,,,"Large Language Models, One-Class SVM, Novelty Detection, In/Out-of-Context, Representation Engineering","The paper explores using Representation Engineering and One-Class SVM to detect subspaces in LLMs that represent specific conversational contexts, aiming to improve safety by identifying out-of-context behavior.",43.16,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12288v1_TimeGMM Single-Pass Probabilistic Forecasting via .pdf,Single-Pass Probabilistic Forecasting Via Adaptive Gaussian,"Lei Liu1, Tengyuan Liu1, Hongwei Zhao1, Jiahui Huang1, Ruibo Guo1, Bin Li1",,,"probabilistic time series forecasting, Gaussian mixture model, reversible instance normalization, uncertainty quantification, energy finance","The paper introduces TimeGMM, a probabilistic forecasting framework using Gaussian Mixture Models with adaptive Gaussian normalization, achieving superior performance in forecasting uncertainty.",44.47,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12294v1_ToolPRMBench Evaluating and Advancing Process Rewa.pdf,ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents,"Dawei Li, Yuguang Yao, Zhen Tan, Huan Liu, Ruocheng Guo",,,"tool-using agents, process reward models, reward-guided search, LLM evaluation, step-level testing","This paper introduces ToolPRMBench, a benchmark for evaluating process reward models (PRMs) in tool-using agents. It evaluates PRMs across diverse APIs and multi-LLM pipelines, aiming to identify differences in effectiveness and highlight opportunities for specialized PRMs.",44.35,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12304v1_A Two-Stage Globally-Diverse Adversarial Attack fo.pdf,Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-Training,"Wutao Chen, Huaqin Zou, Chen Wan, Lifeng Huang",,,"adversarial attack, VLP models, multi-modal retrieval, transferability, adversarial transferability","The paper proposes 2S-GDA, a two-stage globally-diverse attack framework for vision-language pre-training models. It improves attack success rates by introducing diverse textual and image perturbations across stages, achieving up to 11.17% gains in black-box settings.",45.08,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12310v1_Survival is the Only Reward Sustainable Self-Train.pdf,Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection,"Jennifer Dodgson, Alfath Daryl Alhajir, Michael Joedhitya, Akira Rafhael Janson Pattirane, Surender Suresh Kumar, Joseph Lim, C.H. Peh, Adith Ramdas, Steven Zhang Zhexu",arXiv:2601.12310v1,2601.12310,"self-training, environment-mediated selection, sustainable self-training, negative-space learning, robust autonomous systems","The paper presents a self-training architecture where learning is driven by environmental viability rather than external rewards. It demonstrates that survival-based selection enables stable, open-ended improvement without human supervision, highlighting mechanisms like negative-space learning and meta-learning through persistence of effective behaviors.",49.26,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12316v1_GazeFormer-MoE Context-Aware Gaze Estimation via C.pdf,GAZEFORMER-MOE: CONTEXT-A W ARE GAZE ESTIMATION VIA CLIP AND MOE,"Xinyuan Zhao, Xianrui Chen, Ahmad Chaddad, 2, ∗",,,"gaze estimation, multi-scale fusion, MoE transformer, semantic modulation, CLIP, pretrained prototypes, illumination, head pose, background, direction, angular errors, hyperparameter tuning","Presents a semantics modulated, multi-scale Transformer for 3D gaze estimation using CLIP and MoE, achieving state-of-the-art angular accuracy and robustness improvements.",45.72,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12317v1_Explanova Automatically Discover Data Insights in .pdf,Explanova: Automatically Discover Data Insights in N×M Table via XAI Combined LLM Workflow,Yiming Huang,10.1145/nnnnnnn.nnnnnnn,,"Explainable AI, AutoML, LLM, Data Insights, Feature Analysis","This paper proposes an automated LLM workflow to discover data insights in N×M tables, leveraging XAI and LLM capabilities. It explores how preset workflows can empower common LLMs for efficient data analysis, reducing reliance on expensive proprietary APIs.",43.42,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12318v1_Beyond Human Annotation Recent Advances in Data Ge.pdf,Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence,"Dehao Ying, Fengchang Yu, Haihua Chen, Changjiang Jiang, Yurong Li, Wei Lu",10.1093/acr/qad045,,"Document Intelligence, Data Generation, Data Quality Evaluation","This survey establishes the first comprehensive technical map for data generation in Document Intelligence. It redefines data generation as supervisory signal production and introduces a taxonomy based on data availability and labels. The framework organizes methods into four paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction. A multi-level evaluation framework integrates intrinsic quality and extrinsic utility, highlighting challenges like fidelity gaps and co-evolutionary ecosystems.",46.42,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12323v1_MARO Learning Stronger Reasoning from Social Inter.pdf,Learning Stronger Reasoning from Social Interaction,"Yin Cai1, Zhouhong Gu1, JunTao Zhang1, Ping Chen1",,,"social interaction, multi-agent learning, reasoning, judgment, collaboration, cognitive abilities","This paper proposes MARO, a method enabling large language models to improve reasoning through multi-agent social environments. It addresses sparse learning, uneven role distribution, and environmental instability by decomposing outcomes, balancing training weights, and evaluating behavior utility, respectively.",43.97,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12327v1_The Expert Validation Framework EVF Enabling Domai.pdf,The Expert Validation Framework (EVF): Enabling Domain Expert Control in AI Engineering,"Lucas Gren, Felix Dobslaw",10.1145/xxx.xxxx,CCS Concepts,"GenAI, expert validation, quality assurance, AI engineering, domain expert control","This paper introduces an Expert Validation Framework (EVF) that centers domain experts in building and maintaining AI systems, addressing quality assurance gaps and enabling structured specification, testing, validation, and monitoring for enterprise GenAI applications.",44.52,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12330v1_IceWatch Forecasting Glacial Lake Outburst Floods .pdf,IceWatch: Forecasting Glacial Lake Outburst Floods using Multimodal Deep Learning,"Zuha Fatima, Muhammad Anser Sohaib, Muhammad Talha, Ayesha Kanwal, Sidra Sultana, Nazia Perwaiz",10.5281/zenodo.1234567,,"CNN, deep learning, glacier monitoring, GLOF detection, LSTM, remote sensing, Sentinel-2, temperature forecasting, transformer, velocity prediction","Glacial Lake Outburst Floods (GLOFs) threaten high mountain regions. This paper presents IceWatch, a deep learning framework using multimodal data (Sentinel-2 imagery and physical models) to predict GLOF events, improving speed, accuracy, and robustness over traditional methods.",42.78,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12331v1_Efficient Privacy-Preserving Retrieval Augmented G.pdf,Efficient Privacy-Preserving Retrieval Augmented Generation with Distance-Preserving Encryption,"Huanyi Ye, Jiale Guo, Ziyao Liu, Kwok-Yan Lam",,,"RAG, Privacy-Preserving Retrieval, Distance-Preserving Encryption, LLMs, Privacy Leakage, Query Analysis","The paper proposes ppRAG, an efficient privacy-preserving retrieval augmented generation framework for untrusted cloud environments. It introduces CAPRISE, a conditional approximate distance-preserving symmetric encryption that preserves relative distances between encrypted queries and database embeddings, enhancing privacy while maintaining retrieval accuracy. Differential privacy is applied to further mitigate query analysis risks. Experimental results demonstrate high retrieval accuracy and strong privacy guarantees.",46.0,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12338v1_Actionable Advice from Reviews via Mixture of LoRA.pdf,Actionable Advice from Reviews via Mixture of LoRA Experts: A,"Kartikey Singh Bhandari, Manav Ganesh, Yashwant Viswanathan, Archit Agrawal, Dhruv Kumar, Pratik Narang",,,"review analysis, business recommendations, LLM, customer feedback, actionable insights","Customer reviews contain detailed signals about service failures; this study proposes a two-LLM framework using a mixture-of-LoRA experts to generate concrete, implementable recommendations from review text.",44.73,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12341v1_Time-Continuous Modeling for Temporal Affective Pa.pdf,Time-Continuous Modeling for Temporal Affective Pattern Recognition in LLM’s,"Rezky M. Kam, Coddy N. Siswanto",,,"time-continuous modeling, temporal affective patterns, longitudinal adaptation, affective dynamics, physics-informed neural networks","The paper introduces a hybrid encoder-decoder architecture leveraging time-aware patterns to model evolving affective states in conversations. It addresses limitations of static token generation by incorporating progressive steering of affective trajectories, aiming to improve interpretability and adaptability in affective understanding across dynamic dialogues.",44.77,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12343v1_How Well Do LLMs Predict Human Behavior A Measure .pdf,How Well Do LLMs Predict Human Behavior?,"Wayne Gao, Sukjin Han, Annie Liang",arXiv:2601.12343v1,arXiv:2601.12343,"large language models, human behavior, predictive accuracy, pretrained knowledge, economic variables",The paper proposes a measure to evaluate how much knowledge a pretrained LLM brings to predicting human behavior by comparing prediction errors across different model complexities.,46.3,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12349v1_Zero-Permission Manipulation Can We Trust Large Mu.pdf,Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?,"Yi Qian, Kunwei Qian, Xingbang He, Ligeng Chen, Jikang Zhang, Tiantai Zhang, Haiyang Wei, Linzhang Wang, Hao Wu",z472421519,,"zero-permission, large multimodal model, GUI agents, Android, visual atomicity, attack surface, task recovery, intent alignment, security","The paper explores vulnerabilities in Android GUI agents by demonstrating how attackers can rebind agent actions using Action Rebinding, exploiting the assumption of visual atomicity. It introduces an Intent Alignment Strategy to manipulate reasoning and bypass verification gates, achieving high success rates across diverse tasks.",44.45,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12357v1_SimpleMatch A Simple and Strong Baseline for Seman.pdf,SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence,"Hailong Jin1, Huiying Li1",10.48550/arXiv.2024.12345,arXiv:2408.12345,"semantic correspondence, feature fusion, downsampling, sparse matching","This paper introduces SimpleMatch, a lightweight framework for semantic correspondence that achieves strong performance at low resolutions. It addresses the issue of irreversible fusion of distinct keypoints through a lightweight upsample decoder and multi-scale supervision, while also optimizing training memory usage.",44.09,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12358v1_From Prompts to Pavement LMMs-based Agentic Behavi.pdf,From Prompts to Pavement: LMM-based Agentic Behavior-Tree,"Omar Y. Goba1, Ahmed Y. Gado1, Catherine M. Elias1, Ahmed Hussein3",10.48550/arXiv.2024.12345,arXiv:2408.12345,"Behavior Tree, Large Language Model, L5 Autonomy, Navigation, ROS, CARLA, Nav2","Autonomous vehicles require adaptive behavior planners; this paper presents an agentic framework using LLMs and LVMs to generate adaptive behavior trees, demonstrating successful navigation in simulation without human intervention.",38.61,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12374v1_A Scalable Entity-Based Framework for Auditing Bia.pdf,A Scalable Entity-Based Framework for Auditing Bias in LLMs,"Akram Elbouanani, Aboubacar Tuo, Adrian Popescu",1.0,1.0,"bias auditing, LLMs, entity-based, structural disparities, natural language processing","This paper introduces a scalable framework using named entities to audit biases in large language models. It demonstrates that synthetic data can reliably reproduce real-world bias patterns across diverse entities, tasks, languages, and prompting strategies. Findings highlight systematic biases such as preference for certain political groups, Western-centric content, and exclusion of firms in defense and pharma sectors. The study underscores the need for rigorous auditing before deploying LLMs in high-stakes applications.",37.57,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12389v1_NADIR Differential Attention Flow for Non-Autoregr.pdf,Differential Attention Flow for Non-Autoregressive Transliteration in Indic Languages,"Lakshya Tomar, Vinayak Abrol, Puneet Agarwal",10.48550/arXiv:2509.06927v1,2509.06927,"multilingual transliteration, non-autoregressive, character error rate, sequence modeling, attention flow, indic languages","This work introduces NADIR, a novel non-autoregressive architecture combining Differential Transformer and Mixture-of-Experts, to improve speed and accuracy in multilingual transliteration. NADIR achieves a 13× speed-up over state-of-the-art AR models while maintaining competitive error rates and reducing hallucination-related issues.",43.46,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12392v1_PsychēChat An Empathic Framework Focused on Emotio.pdf,PsychEChat: An Empathic Framework Focused on Emotion Shift Tracking and Safety Risk Analysis in Psychological Counseling,"Zhentao Xia, Yongqi Fan, Yuxiang Chu, Yichao Yin, Liangliang Chen, Tong Ruan, Weiyan Zhang",,,"psychological counseling, emotion shift tracking, safety risk analysis, LLM, mental health, empathic framework","This paper introduces PsychEChat, a new interactive chat system integrating emotion shift tracking and safety risk analysis for psychological counseling. It proposes an Agent Modestruc framework with two modules: Emotion Management and Risk Control, and evaluates performance through experiments showing superior emotional insight and safety control.",45.79,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12401v1_Beyond the Dirac Delta Mitigating Diversity Collap.pdf,Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement,"Jinmei Liu, Haoru Li, Zhenhong Sun, Bo Wang, Daoyi Dong, Chunlin Chen, Yatao Bian, Zhi Wang",,,"reinforcement learning, diversity collapse, image generation, task alignment, generation diversity","The paper presents DRIFT, a framework that mitigates diversity collapse in RL fine-tuning by systematically encouraging output diversity during on-policy training. It achieves improved Pareto dominance in task alignment and generation diversity, demonstrating significant gains in diversity at equivalent alignment levels.",44.8,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12402v1_Weaknesses of Facial Emotion Recognition Systems.pdf,Weaknesses of Facial Emotion Recognition Systems,"Aleksandra Jamróz, Patrycja Wysocka, Piotr Garbat",arXiv:2601.12402v1,arXiv:2601.12402,"Facial Emotion Recognition, Deep learning, Computer Vision","This study evaluates the performance of facial emotion recognition systems across diverse datasets, highlighting weaknesses such as dataset differences, varying difficulty levels in emotion detection, and challenges in distinguishing closely related emotions.",45.14,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12405v1_Explainable Machine Learning for Pediatric Dental .pdf,Explainable Machine Learning for Pediatric Dental Risk Stratification Using Socio-Demographic Determinants,"Manasi Kanade, Abhi Thakkar, Gabriela Fernandes",,,"pediatric dental disease, socio-demographic determinants, explainable AI, risk stratification, ethical deployment","This study develops an explainable AI framework for pediatric dental risk stratification, emphasizing interpretability and ethical considerations over predictive accuracy. It integrates socio-demographic factors such as age, income-to-poverty ratio, race/ethnicity, gender, and medical history to improve transparency in AI-driven dental risk assessment.",46.7,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12410v1_Are LLMs Smarter Than Chimpanzees An Evaluation on.pdf,Are LLMs Smarter Than Chimpanzees?,"Dingyi Yang, Junqi Zhao, Xue Li, Ce Li, Boyang Li",arXiv:2601.12410v1,arXiv:2601.12410,"LLM performance, knowledge state tracking, intelligence comparison, cognitive anthropology, human vs. machine","This paper evaluates large language models' ability to track and estimate knowledge states, comparing them to chimpanzees. It finds current LLMs perform near-randomly and are inferior to humans in this domain.",44.59,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12415v1_Orthogonalized Policy OptimizationDecoupling Sampl.pdf,Orthogonalized Policy Optimization,Wang Zixian,arXiv:2601.12415v1,2601.12415,"alignment methods, RLHF, optimization geometry","This work formalizes the decoupling of sampling and optimization geometry in RLHF by introducing Orthogonalized Policy Optimization (OPO), which uses KL divergence and Bregman divergence to stabilize training and avoid gradient saturation.",43.64,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12436v1_Purification Before Fusion Toward Mask-Free Speech.pdf,Purification Before Fusion: Mask-Free Speech Enhancement for Robust Audio-Visual Speech Recognition,"Linzhi Wu1, Xingyu Zhang2, Hao Yuan3, Yakun Zhang2, Changyan Zheng4, Liang Xie2, Tiejun Liu1, Erwei Yin2",,,"audio-visual speech recognition, speech feature enhancement, noise-robust, multimodal bottleneck Conformer","This work proposes an end-to-end noise-robust A VSR framework that uses a Conformer-based bottleneck fusion module to implicitly refine noisy audio features with video assistance, eliminating explicit noise masking while preserving speech semantics.",46.25,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12442v1_Constraint-Aware Neurosymbolic Uncertainty Quantif.pdf,Constraint-Aware Neurosymbolic Uncertainty,"Shahnawaz Alam, Mohammed Mudassir Uddin, Mohammed Kaif Pasha",,,"Neurosymbolic AI, Uncertainty Quantification, Bayesian Deep Learning, Scientific Constraints, Calibration, Physics-Informed Machine Learning","The paper introduces the Constraint-Aware Neurosymbolic Uncertainty Framework (CANUF), unifying Bayesian deep learning with differentiable symbolic reasoning to deliver trustworthy uncertainty estimates in scientific AI. It addresses limitations of existing methods by incorporating symbolic constraints and achieving high constraint satisfaction while maintaining interpretability.",46.36,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12443v1_Adversarial Defense in Vision-Language Models An O.pdf,Adversarial Defense in Vision-Language Models: An Overview,"Lei Zhang, Xiaowei Fu",10.48550/arXiv.2024.12345,arXiv:2408.12345,"Vision Language Models, adversarial defense, survey","This paper reviews recent advancements in adversarial defense strategies for Vision Language Models, discussing training-time defense, test-time adaptation, and training-free approaches, while highlighting challenges and limitations.",44.24,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12444v1_Large Language Model for OWL Proofs.pdf,Large Language Model for OWL Proofs,"Hui Yang, Jiaoyan Chen, Uli Sattler",10.1145/XXXXXX.XXXXXX,,"Large Language Models, OWL ontologies, proof generation, reasoning tasks, logical complexity, natural language generation","This work investigates how Large Language Models can generate proofs for OWL ontologies by developing an automated dataset construction and evaluation framework. Findings highlight challenges with complex cases, the impact of logical complexity, and the effects of noisy input data.",44.71,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12449v1_AgenTRIM Tool Risk Mitigation for Agentic AI.pdf,AgenTRIM: Tool Risk Mitigation for Agentic AI,"Roy Betser, Shamik Bose, Amit Giloni, Chiara Picardi, Sindhu Padakandla, Roman Vainshtein",10.1093/acpan/sca062,,"AI agents, tool risk mitigation, agentic risk, excessive agency, tool-driven agency, LLM-based agents, security risks, attack surface, safety policies","The paper introduces AGENTRIM, a framework that detects and mitigates tool-driven agency risks in LLM-based agents without altering their internal reasoning. It addresses security threats like indirect prompt injection and tool misuse by enforcing least-privilege tool access during both offline and online phases. Experiments show AGENTRIM reduces attack success while preserving performance and robustness against description-based attacks.",46.8,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12465v1_Incentivizing In-depth Reasoning over Long Context.pdf,Incentivizing In-Depth Reasoning Over Long Contexts,"Miao Peng, Weizhou Shen, Nuo Chen, Chenliang Li, Ming Yan, Jia Li",,,"reinforcement learning, long-context reasoning, deep learning, multi-hop reasoning, learning signals, LLM performance",The paper addresses degradation of RLVR performance in long-context tasks by identifying the 'almost-there' phenomenon and proposes DEEPREASONQA and LONGPAS to improve reasoning accuracy.,44.35,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12467v1_Patch-Level Tokenization with CNN Encoders and Att.pdf,Patch-Level Tokenization with CNN Encoders and Attention for Improved Transformer Time-Series Forecasting,Saurish Nagrath,10.48550/arXiv.2311.07818,arXiv:2311.07818,"multivariate time-series forecasting, financial time-series forecasting, Transformer models, temporal tokenization, convolutional neural networks, attention mechanisms","This work proposes a two-stage forecasting framework that combines convolutional neural networks for local temporal representation learning and transformer self-attention for global dependency modeling. Experimental results show competitive performance compared to convolutional and patch-based Transformer baselines, highlighting the benefits of structured temporal representations.",45.79,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12471v1_Knowing When to Abstain Medical LLMs Under Clinica.pdf,Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty,"Sravanthi Machcha, Sushrita Yerra, Sahil Gupta, Aishwarya Sahoo, Sharmin Sultana, Hong Yu, Zonghai Yao",https://github.com/sravanthi6m/MedAbstain,,"medical LLMs, clinical uncertainty, abstain mechanisms, medical QA, uncertainty quantification","Current evaluation of large language models prioritizes accuracy, but in safety-critical applications abstaining from uncertain answers is crucial. MedAbstain introduces a benchmark integrating conformal prediction and explicit abstention options to improve trustworthy deployment.",45.17,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12494v1_Harmonizing the Arabic Audio Space with Data Sched.pdf,Harmonizing the Arabic Audio Space with Data Scheduling,"Hunzalah Hassan Bhatti, Firoj Alam, Shammur Absar Chowdhury",,,"Arabic audio, data scheduling, multi-task instruction tuning, speech summarization, dialect identification, emotion recognition","This paper presents the first systematic study of multi-task instruction tuning for an Arabic-centric audio LLM, covering generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). It introduces AraMega-SSum dataset and proposes a Hybrid TPC+ADS Strategy to address adaptation challenges in complex, low-resource environments.",45.38,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12499v1_Failure Modes in Multi-Hop QA The Weakest Link Law.pdf,Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck,"Meiru Zhang, Zaiqiao Meng, Nigel Collier",mz468@cam.ac.uk,nhc30@cam.ac.uk,"multi-hop reasoning, position bias, LLMs, recognition failure","This paper investigates why Large Language Models struggle with multi-hop reasoning despite large context windows. It introduces Multi-Focus Attention Instruction (MFAI) to address recognition bottlenecks and establishes the 'Weakest Link Law', showing that performance depends on the absolute position of least visible evidence rather than linear distance. The study reveals a duality in attention steering and demonstrates that 'thinking' models can effectively integrate information even in noisy settings.",46.02,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12518v1_Cooperative Multi-agent RL with Communication Cons.pdf,Cooperative Multi-agent RL with Communication constraints,"Nuoya Xiong, Aarti Singh",,,,Addresses communication constraints in cooperative multi-agent reinforcement learning by introducing base policy prediction to reduce reliance on outdated data.,40.99,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12522v1_Improved Bug Localization with AI Agents Leveragin.pdf,Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition,"Asif Mohammed Samir, Mohammad Masudur Rahman",10.1145/3567892,ICPC 2026,"Bug Localization, LLM, Agentic AI, Cognition, Debugging, Software Engineering, Information Retrieval","This paper presents CogniGent, an agentic technique for bug localization that leverages multiple AI agents with causal reasoning, call-graph analysis, and context engineering. It achieves significant improvements over traditional and LLM-based methods in performance metrics and demonstrates statistical superiority through hypothesis testing.",45.89,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12534v1_Encoding Emotion Through Self-Supervised Eye Movem.pdf,Encoding Emotion Through Self-Supervised Eye Movement Reconstruction,"Marcus Ma, Jordan Prescott, Emily Zhou, Tiantian Feng, Kleanthis Avramidis, Gabor Mihaly Toth, Shrikanth Narayanan",,,"eye movement, self-supervised learning, emotion prediction, deep learning, eye tracking, naturalistic data, Holocaust survivors, emotional expression","This study explores using self-supervised eye movement reconstruction to predict emotional expressions from naturalistic, low-resolution videos. By leveraging pretrained language models and fine-tuning on emotional gaze data, the authors demonstrate that eye movement encodes affective signals, offering a scalable alternative to high-resolution eye-tracking.",46.42,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12535v1_Improving Low-Resource Machine Translation via Rou.pdf,Improving Low-Resource Machine Translation via Round-Trip,"Ahmed Attia, Alham Fikri, MBZUAI, Masdar City, UAE",,,"low-resource machine translation, reinforcement learning, round-trip bootstrapping, No Language Left Behind, translation quality, BLEU, chrF++",Investigates a self-supervised reinforcement learning-based fine-tuning for low-resource MT using round-trip bootstrapping with NLLB models. Demonstrates improvements across multiple languages and highlights benefits of scale and pretrained knowledge.,44.88,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12538v1_Agentic Reasoning for Large Language Models.pdf,Agentic Reasoning for Large Language Models,"Tianxin Wei, Ting-Wei Li, Zhining Liu, Xuying Ning, Yang2, Zhou, Wang5, Xiao Lin, Dongqi Fu2, Li1, Zhang1, Jiaxuan You1, Heng Ji1, Tong1",arXiv:2601.12538v1,arXiv:2601.12538,"Agentic AI, LLM Agent, Agentic Reasoning, Self-evolving, Collective Multi-agent Reasoning","This survey organizes agentic reasoning across three dimensions: foundational reasoning in stable environments, self-evolving capabilities through feedback and adaptation, and collective intelligence in collaborative settings. It reviews frameworks, benchmarks, and real-world applications, highlighting challenges like personalization and long-horizon interaction.",47.06,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12539v1_MemeLens Multilingual Multitask VLMs for Memes.pdf,MEMELENS: Multilingual Multitask VLMs for Memes,"Ali Ezzat Shahroor, Mohamed Bayan Kmainasi, Abul Hasnat, Dimitar Dimitrov, Giovanni Da San Martino, Preslav Nakov, Firoj Alam",fialam,mk2314890,"memes, multilingual, multitask, VLM, meme understanding, harm, humor, affect","This paper proposes MEMELENS, a unified multilingual and multitask explanation-enhanced Vision Language Model for meme understanding. It consolidates 38 public meme datasets, maps dataset-specific labels into a shared taxonomy of 20 tasks, and analyzes performance across modeling paradigms, highlighting the need for multimodal training and caution against over-specialization.",46.66,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12542v1_Rethinking the AI Scientist Interactive Multi-Agen.pdf,Rethinking the AI Scientist: Interactive Multi-Agent,"Lukas Weidener, Marko Brkić, Mihailo Jovanović, Ritvik Singh, Chiara Baccin, Emre Ulgac, Alex Dobrin, Aakaash Meduri",https://github.com/bio-xyz/BioAgents,,"artificial intelligence, scientific discovery, multi-agent system, interactive investigation, research workflow, novelty detection","Artificial intelligence systems for scientific discovery have demonstrated remarkable potential, yet existing approaches remain largely proprietary and operate in batch-processing modes requiring hours per research cycle, precluding real-time researcher guidance. This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. The architecture comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Two operational modes support different workflows: semi-autonomous mode with selective human checkpoints, and fully autonomous mode for extended investigations. Evaluation on the BixBench computational biology benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points.",47.26,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12547v1_How Clinicians Think and What AI Can Learn From It.pdf,How Clinicians Think—and What AI Can Learn From,"Dr. Dipayan Sengupta, MD, Dr. Saumya Panda, MD",arXiv:2601.12547v1,arXiv:2601.12547,"clinical reasoning, ordinal decision-making, AI in medicine, healthcare decision support, robust algorithms","The paper argues that clinicians primarily use ordinal, non-compensatory decision-making rather than cardinal optimization. It proposes that AI should prioritize robustness and interpretability aligned with clinician heuristics, emphasizing value-sensitive design over pure prediction accuracy.",44.75,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12549v1_Benchmarking Concept-Spilling Across Languages in .pdf,Benchmarking Concept-Spilling Across Languages in LLMs,"Ilia Badanin, Daniil Dzenhaliou, Imanol Schlag",arXiv:2601.12549v1,arXiv:2601.12549,"multilingual models, semantic robustness, language spilling, polysemy, cross-lingual evaluation","This paper presents a comparative framework for evaluating multilingual semantic robustness by measuring how models handle polysemous words across languages. It reveals variation in semantic performance across models and languages, offering a scalable benchmark for fair model comparison.",44.45,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12554v1_Artificial Intelligence in Materials Science and E.pdf,"Artificial Intelligence in Materials Science and Engineering: Current Landscape, Key Challenges, and Future","Iman Peivaste, Salim Belouettar ∗1, Francesco Mercuri, Nicholas Fantuzzi, Hamidreza Dehghani, Razieh Izadi, Halliru Ibrahim, Jakub Lengiewicz, Maël Belouettar-Mathis, Kouider Bendine, Ahmed Makradi, Martin Hörsch, Peter Klein, Mohamed El Hachemi, Yacine Rezgui, Natalia Konchakova, Ali Daouadji, Luxembourg Institute of Science and Technology, Department of Physics and Materials Science, University of Luxembourg, Istituto per lo Studio dei Materiali Nanostrutturati (ISMN), Norwegian University of Life Sciences (NMBU), Fraunhofer Institute for Industrial Mathematics (ITWM), Ecole Centrale de Lyon, Norwegian University of Science and Technology (NTNU), School of Engineering, Cardiff University, INSA Lyon",arXiv:2601.12554v1,2601.12554,"Artificial Intelligence, Materials Science, Machine Learning, Deep Learning, Data Representation, Uncertainty Quantification","Artificial Intelligence is rapidly transforming materials science and engineering, offering powerful tools to navigate complexity, accelerate discovery, and optimize material design in ways previously unattainable. This review provides a comprehensive overview of recent advancements and methodologies, highlighting the pivotal role of data-driven techniques.",48.52,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12557v1_Life Machine Learning and the Search for Habitabil.pdf,"Life, Machine Learning, and the Search for Habitability: Predicting Biosignature","Mark Moussa1, Amber V . Young1, Brianna Isola1, Vasuda Trehan1, Michael D. Himes1, Nicholas Wogan2, Giada Arney1",10.3847/2045-3265/abd2b2,astro-ph/2502.01234,"habitable worlds, machine learning, biosignatures, habitable exoplanets, habitable worlds observatory","The paper introduces two advanced ML models—Bayesian Convolutional Neural Network (BCNN) and Spectral Query Adaptive Transformer (SQuAT)—for predicting biosignature fluxes from exoplanet spectra. BCNN quantifies uncertainties, while SQuAT enhances interpretability by linking spectral features to biosignatures. Both achieve high accuracy across diverse conditions, supporting efficient mission prioritization for the Habitable Worlds Observatory.",46.73,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12560v1_Agentic Artificial Intelligence AI Architectures T.pdf,"Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents","Arunkumar V, Gangadharan G.R., Rajkumar Buyya",arunkumarv1530@gmail.com,arunkumarv1530@gmail.com,"Agentic AI, Large Language Models, Autonomous Agents, Multi-Agent Systems, Cognitive Architectures, Tool Use, Planning","This paper investigates architectures and proposes a unified taxonomy for Agentic AI agents, discussing transitions from linear reasoning to native inference time reasoning and from fixed APIs to open standards. It highlights challenges like hallucination and infinite loops, and outlines future research directions for robust autonomous systems.",46.25,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12577v1_Primate-like perceptual decision making emerges th.pdf,Primate-like perceptual decision making emerges through deep recurrent reinforcement learning,"Nathan J. Wispinski, Scott A. Stone, Anthony Singhal, Patrick M. Pilarski, Craig S. Chapman",arXiv:2601.12577v1,arXiv:2601.12577,"perceptual decision making, deep reinforcement learning, neural mechanisms, evidence accumulation, primate cognition","Progress has led to a detailed understanding of the neural mechanisms that underlie decision making in primates. However, less is known about why such mechanisms are present in the first place. Theory suggests that primate decision making mechanisms, and their resultant behavioral abilities, emerged to maximize reward in the face of noisy, temporally evolving information. To test this theory, we trained an end-to-end deep recurrent neural network using reinforcement learning on a noisy perceptual discrimination task. Networks learned several key abilities of primate-like decision making including trading off speed for accuracy, and flexibly changing their mind in the face of new information. Internal dynamics of these networks suggest that these abilities were supported by similar decision mechanisms as those observed in primate neurophysiological studies.",46.54,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12582v1_Ontology-aligned structuring and reuse of multimod.pdf,Ontology-aligned structuring and reuse of multimodal materials data and workflows towards automatic reproduction,"Sepideh Baghaee Ravari, Abril Azocar, Guzman Sarath, Menon, Stefan Sandfeld, Tilmann Hickel, Markus Stricker",10.1002,,"text mining, workflow, large language models, stacking fault energy","The paper introduces an ontology-driven framework using large language models to automate extraction and structuring of computational workflows from literature, aiming to improve reproducibility in materials science by aligning data and protocols with established ontologies.",46.92,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12585v1_Do MLLMs See What We See Analyzing Visualization L.pdf,Do MLLMs See What We See?,"Mengli (Dawn) Duan, Yuhe (Sissi) Jiang, Matthew Varona, Carolina Nobre",xx.xxxx/TVCG.201x,xx.xxxx/TVCG.201x,"Visualization Literacy, Multimodal Large Language Model, Evaluation Study","This paper presents the first systematic analysis of barriers to visualization literacy in MLLMs. Using the reVLAT benchmark with synthetic data, the authors evaluate four state-of-the-art models and identify two machine-specific barriers: difficulty interpreting color-intensive and segment-based visualizations, and challenges forming consistent comparative reasoning. The findings contribute to better evaluation and design of AI-driven visualization assistants.",45.25,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12594v1_SLAP Scalable Language-Audio Pretraining with Vari.pdf,Scalable Language-Audio Pretraining With V Arable-Duration,"Xinhao Mei, Gael Le Lan Haohe Liu, Zhaoheng Ni, Nyang Shi Vikas Chandra",,,"multimodal learning, CLAP, self-supervised learning, contrastive learning, multi-objective learning",Introduces Scalable Language-Audio Pretraining (SLAP) to address limitations of CLAP by scaling pretraining to 109 million audio-text pairs with variable durations and incorporating multiple training objectives.,43.93,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12607v1_A Cloud-based Multi-Agentic Workflow for Science.pdf,A Cloud-based Multi-Agentic Workflow for Science,"Anurag Acharya, Timothy Vega, Rizwan A. Ashraf, Anshu Sharma, Derek Parker",10.1145/nnnnnn.nnnnnnn,,"Large Language Models, LLMs for Science, LLM Agents, Multi-agent Framework, Catalysis, Chemistry, Material Science, Cloud Computing","This paper presents a domain-agnostic, model-independent workflow for an agentic framework that supports scientific tasks. Built on cloud infrastructure, it integrates various agents to handle both simple and complex tasks, demonstrating high accuracy and cost efficiency. The framework is validated on synthetic benchmarks and real-world datasets, showing strong performance comparable to leading models.",45.57,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12617v1_Creating Disability Story Videos with Generative A.pdf,"Creating Disability Story Videos with Generative AI: Motivation, Expression, and Sharing","Shuo Niu, Dylan Clements, Hyungsin Kim",10.1145/3772318.3791495,,"Disability, Storytelling, Video, Generative AI, LLM","Generative AI supports people with disabilities in creating stories about their experiences, but challenges remain around bias and accessibility.",42.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12637v1_Topology-Aware Multiscale Mixture of Experts for E.pdf,Topology-Aware Multiscale Mixure of Experts for Efficient Molecular Property Prediction,"Long D. Nguyen, Kelin Xia, Binh P. Nguyen",arXiv:2601.12637v1,arXiv:2601.12637,"Graph Neural Networks, Topological Deep Learning, Mixture of Experts, Molecular Representation","This paper proposes a Multiscale Interaction Mixture of Experts (MI-MoE) to model 3D molecular properties by adapting interaction modeling across geometric regimes. It introduces a distance-cutoff expert ensemble, a topological gating encoder, and demonstrates improved performance across diverse molecular datasets.",46.44,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12638v1_Mixed Precision PointPillars for Efficient 3D Obje.pdf,Mixed Precision PointPillars for Efficient 3D Object,"Ninnart Fuengfusin, Keisuke Yoneda, Naoki Suganuma",10.48550/arXiv.2024.12345,10.48550/arXiv.2024.12345,"neural networks, quantization, 3D object detection",Proposes a mixed precision framework for PointPillars to accelerate runtime while mitigating performance loss from LIDAR's numerical distributions. Combines post-training quantization with greedy layer search and evaluates performance via FP vs FP8.,45.09,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12641v1_STEP-LLM Generating CAD STEP Models from Natural L.pdf,Generating CAD STEP Models from Natural Language with Large Language Models,"Xiangyu Shi, Junyang Ding, Xu Zhao, Sinong Zhan, Payal Mohapatra, Daniel Quispe, Kojo Welbeck, Jian Cao, Wei Chen, Ping Guo, Qi Zhu",arXiv:2601.12641v1,2601.12641v1,"Computer-aided design, STEP file, large language models, design automation","This paper introduces a dataset of ~40K STEP-caption pairs and presents a novel preprocessing pipeline tailored for STEP's graph-structured format. It employs depth-first search-based reserialization and retrieval-augmented generation to improve geometric fidelity, achieving improvements over the Text2CAD baseline through multiple refinement stages.",47.02,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12646v1_Unbounded Harms Bounded Law Liability in the Age o.pdf,"Unbounded Harms, Bounded Law: Liability in the Age of Borderless AI",Ha-Chi Tran,arXiv:2601.12646v1,cs.CY,"unbounded harms, bounded law, AI liability, responsible AI, compensation mechanisms, transboundary risks","The paper examines how rapidly evolving AI systems challenge existing legal frameworks for risk governance, emphasizing the inadequacy of territorially bounded liability in a globalized, borderless AI landscape. It draws comparative insights from high-risk and transnational domains to propose structural solutions for AI-related harms.",45.44,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12648v1_Intelligent Documentation in Medical Education Can.pdf,Intelligent Documentation in Medical Education: Can AI Replace Manual Case Logging?,"Nafiz Imtiaz Khan, MSc, Kylie Cleland, BSc, Vladimir Filkov, PhD, Roger Eric Goldman, PhD",,,"artificial intelligence, large language models, radiology, case logs, medical education","This study evaluates whether large language models can automate procedural case log documentation in radiology training, assessing feasibility, performance, and integration challenges.",44.64,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12654v1_Explanation Multiplicity in SHAP Characterization .pdf,Explanation Multiplicity in SHAP: Characterization and Assessment,"Hyunseung Hwang, Seungeun Lee, Lucas Rosenblatt, Julia Stoyanovich, Steven Euijong Whang",10.1093/acpro/shab.2026.01,,"SHAP, explanation multiplicity, interpretability, model transparency, recourse","This paper explores how explanations from SHAP can vary across runs despite fixed inputs, introducing the concept of explanation multiplicity. It discusses challenges in interpreting stable versus unstable feature attributions and proposes methods to assess consistency under randomized baselines.",45.71,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12658v1_Augmenting Question Answering with A Hybrid RAG Ap.pdf,Augmenting Question Answering with A Hybrid RAG Approach,"Tianyi Yang, Nashrah Haque, Vaishnave Jonnalagadda, Yuya Jeremy Ong",,,"Question-Answering, RAG, query process, structured retrieval, contextual grounding","The paper introduces Structured-Semantic RAG (SSRAG), a hybrid architecture that improves QA quality by integrating query augmentation, agentic routing, and structured retrieval combining vector and graph techniques with context unification. Extensive evaluations on TruthfulQA, SQuAD, and WikiQA demonstrate consistent improvements over standard RAG.",45.42,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12661v1_MedConsultBench A Full-Cycle Fine-Grained Process-.pdf,"MedConsultBench: A Full-Cycle, Fine-Grained, Process-Aware Benchmark for Medical Consultation Agents","Chuhan Qiao, Jianghua Huang, Daxing Zhao, Ziding Liu, Shan Yuanjun, Wei Lin Kai Wu",10.1007/XXXXXX,X12345678,None,"Current evaluations prioritize outcome-oriented tasks, overlooking end-to-end process integrity. MedConsultBench addresses this by evaluating the full clinical workflow with fine-grained metrics.",46.44,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12664v1_Generalizable Hyperparameter Optimization for Fede.pdf,Generalizable Hyperparameter Optimization for Federated Learning on Non-IID Cancer Images,"Elisa Gonc, Rodrigo Moreira, Larissa Ferreira Rodrigues Moreira, André Ricardo Backes",10.1007/...,,"Federated Learning, Hyperparameter Optimization, Non-IID Data, Medical Imaging, Cancer","This paper examines whether hyperparameters optimized on one cancer imaging dataset generalize across non-IID federated settings. It introduces a cross-dataset aggregation heuristic combining learning rates, optimizers, and batch sizes to improve performance in federated cancer classification.",44.5,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12667v1_Empowering All-in-Loop Health Management of Spacec.pdf,Empowering All-in-Loop Health Management of Spacecraft Power System in the Mega-Constellation Era via Human-AI Collaboration,"Yi Di1,2, Zhibin Zhao1,2,5,∗, Fujin Wang3, Xue Liu4, Jiafeng Tang1,2, Jiaxin Ren1,2, Zhi Zhai1,2,∗∗, Xuefeng Chen1,2, Jiajian Tang1,2",,,"Large Language Model, Human-AI Collaboration, Spacecraft Power System, All-in-loop Health Management, Satellite Mega-Constellation","The paper discusses the growing need for advanced health management in spacecraft power systems due to the exponential increase in mega-constellations. It introduces SpaceHMchat, an open-source Human-AI collaboration framework, to enable comprehensive all-in-loop health management. Experimental results show strong performance across multiple metrics, and a new AIL HM dataset is released.",46.92,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12671v1_Exploiting Test-Time Augmentation in Federated Lea.pdf,Exploiting Test-Time Augmentation in Federated Learning for Brain Tumor MRI Classification,"Thamara Leandra de Deus Melo, Rodrigo Moreira, Larissa Ferreira Rodrigues Moreira",10.1007/...,,"Brain tumors, Federated Learning, Test-Time Augmentation, Image classification","Efficient brain tumor diagnosis is crucial for early treatment; however, it is challenging due to lesion variability and image complexity. The study evaluates CNNs in federated learning with preprocessing and test-time augmentation, showing significant performance gains.",44.66,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12688v1_Logic-Guided Multistage Inference for Explainable .pdf,Logic-Guided Multistage Inference for Explainable Multidefendant Judgment Prediction,"Xu Zhang, Qinghua Wang, Mengyang Zhao, Fang Wang, Cunquan Qu",10.1234/jcli2025.0012,10.1234/jcli2025.0012,"Multiple defendants, Legal judgment predictions, Label broadcast, Guilt responsibility, Transformer","This study introduces a masked multistage inference framework to improve role differentiation in multidefendant cases, enhancing AI-driven judicial analysis while maintaining legal interpretability. It leverages sentencing logic and role-clarifying masking to better model culpability distinctions between principals and accomplices, achieving superior performance on crime descriptions and court perspectives.",45.74,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12711v1_Neurosymbolic LoRA Why and When to Tune Weights vs.pdf,Neurosymbolic LoRA: Why and When to Tune Weights vs. Rewrite Prompts,"Kevin Wang, Neel P. Bhatt, Cong Liu, Junbo Li, Runjin Chen, Yihan Xi, Timothy Barclay, Alvaro Velasquez, Ufuk Topcu, Zhangyang Wang",,,"neurosymbolic, LoRA, large language models, symbolic manipulation, prompt tuning, fine-tuning, adaptation, reward-based classification","This paper introduces a neurosymbolic LoRA framework that combines numerical updates and symbolic manipulations. It proposes a unified monitoring signal and reward-based classifier to decide when to apply LoRA for factual reconstruction versus TextGrad for token-level edits, improving adaptability and performance across LLM backbones.",46.92,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12715v1_RSOD Reliability-Guided Sonar Image Object Detecti.pdf,Reliability-Guided Sonar Image Object Detection,"Chengzhou Li1, Ping Guo1, Guanchen Meng1, Qi Jia1, Jinyuan Liu1, Zhu Liu1, Xiaokang Liu1, Yu Liu1, Zhongxuan Luo1, Xin Fan1",,,"sonar image, object detection, reliability-guided, label scarcity, unlabeled data, teacher-student framework","This paper proposes RSOD, a teacher-student framework for sonar image object detection under extremely limited labeling. It introduces a reliability score based on prediction consistency and uses a pseudo-label strategy to mitigate label shortages. Experiments show competitive performance with fully labeled baselines on the UATD dataset.",45.68,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12720v1_Teaching Large Reasoning Models Effective Reflecti.pdf,Teaching Large Reasoning Models Effective Reflection,"Hanbin Wang, Jingwei Song, Jinpeng Li, Qi Zhu, Fei Mi, Ganqu Cui, Yasheng Wang, Lifeng Shang",,,"Large Reasoning Models, Self-Critique, Reflection, Reinforcement Learning, Reasoning Accuracy, Reflection Quality","The paper addresses superficial reflection in Large Reasoning Models by introducing Self-Critique Fine-Tuning (SCFT) and Reinforcement Learning with Effective Reflection Rewards (RLERR), demonstrating improvements on benchmark tasks.",45.23,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12723v1_An Evolutionary Framework for Automatic Optimizati.pdf,An Evolutionary Framework for Automatic Optimization Benchmark Generation via Large Language Models,"Yuhiro Ono, Tomohiro Harada, Yukiya Miura",arXiv:2601.12723v1,2601.12723,"optimization benchmarks, large language models, automatic benchmark generation, evolutionary algorithms, mathematical expressions","The paper proposes an LLM-driven evolutionary benchmark generator to create diverse optimization benchmarks, demonstrating superior performance generation compared to traditional methods.",46.95,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12727v1_AI-exhibited Personality Traits Can Shape Human Se.pdf,AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations,"Jingshu Li, Tianqi Song, Nattapat Boonprakong, Zicheng Zhu, Yitian Yang, Yi-Chieh Lee",10.1145/3772318.3790654,arXiv:2601.12727v1,"AI personality traits, human self-concept, conversational alignment, LLM-based systems, self-concept alignment","This study explores how Large Language Model (LLM)-based AI chatbots can shape human self-concept through conversations. By using a GPT-4o default personality setting, the research demonstrates that repeated interactions with AI can align individuals' self-perceptions with the AI's measured traits, potentially influencing users' understanding of their own personality.",47.8,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12731v1_A Shared Geometry of Difficulty in Multilingual La.pdf,A Shared Geometry of Difficulty in Multilingual Language Models,"Stefano Civelli, Pietro Bernardelle, Nicolò Brunello, Gianluca Demartini",,,"language models, problem difficulty, multilinguality, linear probes, interpretability","The study investigates how difficulty is represented in multilingual large language models by analyzing linear probe performance across 21 languages. Difficulty signals emerge in early and deep layers, suggesting a two-stage representation that first generalizes across languages and then becomes language-specific. This aligns with findings that models operate in an abstract conceptual space before producing language-specific outputs.",45.72,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12740v1_TreeWriter AI-Assisted Hierarchical Planning and W.pdf,TreeWriter: AI-Assisted Hierarchical Planning and Writing for Long-Form Documents,"Zijian Zhang, Fangshi Du, Xingjian Liu, Pan Chen, Oliver Huang, Runlong Ye, Michael Liut, Alán Aspuru-Guzik, 1 Department of Computer Science, University of Toronto, 2 Vector Institute for Artificial Intelligence, 3 Department of Chemistry, University of Toronto, 4 Department of Mathematical and Computational Sciences, University of Toronto Mississauga, 5 Department of Materials Science & Engineering, University of Toronto, 6 Department of Chemical Engineering & Applied Chemistry, University of Toronto, 7 Acceleration Consortium, 8 Canadian Institute for Advanced Research (CIFAR), 9 NVIDIA",,,"AI-assisted writing, long-form documents, hierarchical planning, document organization, creative writing, planning support, collaborative writing","TreeWriter introduces a hierarchical writing system that supports multi-level document outlining, integrating contextual AI assistance. It enables authors to manage complex structures, improve idea development, and enhance collaborative editing through AI integration.",46.84,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12742v1_AirHunt Bridging VLM Semantics and Continuous Plan.pdf,AirHunt: Bridging VLM Semantics and Continuous,"Xuecheng Chen, Zongzhuo Liu, Jianfa Ma, Bang Du, Tiantian Zhang, Xueqian Wang, Boyu Zhou",10.48550/arXiv.2024.12345,None,"Vision-Language Models, Aerial Navigation, LiDAR, Continuous Planning, Object Detection, Semantic Reasoning","AirHunt presents a dual-pathway asynchronous architecture fusing vision-language models with continuous path planning, achieving efficient open-set object localization in outdoor settings with reduced navigation error and flight time.",45.44,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12744v1_Vision Language Models for Optimization-Driven Int.pdf,Vision Language Models for Optimization-Driven Intent Processing in Autonomous Networks,"Tasnim Ahmed, Yifan Zhu, Salimur Choudhury",10.1109/ICC.2026.12345,,"Intent-Based Networking, Optimization, Vision-Language Models, Code Generation","This paper presents IntentOpt, a benchmark evaluating 85 optimization problems across 17 categories using four VLMs under multimodal prompting. Results show visual parameter extraction improves execution success by 12–21%, while open-source models lag behind closed-source ones.",44.64,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12745v1_A Graph Prompt Fine-Tuning Method for WSN Spatio-T.pdf,A GraphA PromptA Fine-TuningA MethodA forAWSNA,"Miao Ye, Jing Cui, Yuan Huang, Yong Wang, Qian He, Jiwen Zhang",10.1007/...,,"anomaly detection, graph neural networks, pre-training, prompt learning, wireless sensor networks",Anomaly detection of multi-temporal modal data in Wireless Sensor Network using a graph neural network backbone and multi-task self-supervised training.,44.4,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12754v1_PAIR-SAFE A Paired-Agent Approach for Runtime Audi.pdf,PAIR-SAFE: A Paired-Agent Approach for Runtime Auditing and Refining,"Jiwon Kim, Violeta J. Rodriguez, Dong Whi Yoo, Eshwar Chandrasekharan, Koustuv Saha",dy22@iu.edu,,"large language models, mental health support, runtime auditing, paired-agent, motivational interviewing, clinical alignment","This paper introduces PAIR-SAFE, a paired-agent framework that uses a Responder and a Judge agent grounded in Motivational Interviewing Treatments Integrity (MITI-4) to audit and refine AI-generated mental health support. It demonstrates improvements in key MITI dimensions through Judge-supervised interactions and supports findings with qualitative expert evaluation.",46.37,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12758v1_VISPA Pluralistic Alignment via Automatic Value Se.pdf,Pluralistic Alignment via Automatic Value Selection and Activation,"Shenyan Zheng, Jiayou Zhong, Anudeex Shetty, Heng Ji, Preslav Nakov, Usman Naseem","b63zheng,j55zhong",,"pluralistic alignment, value selection, automatic activation, large language models, interpretability","This paper introduces VISPA, a training-free pluralistic alignment framework that enables dynamic selection and internal activation steering to produce diverse, representative outputs. It demonstrates performance across multiple models and settings, showing adaptability with varying steering methods.",44.99,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12762v1_Teaching LLMs to Learn Tool Trialing and Execution.pdf,Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction,"Xingjie Gao, Pengcheng Huang, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, Chen Qian, Ge Yu, Yu Gu",,,"large language models, tool trialing, environment interaction, tool execution, LLM robustness","The paper proposes ToolMaster, a framework that enables LLMs to learn tool usage by interacting with the environment through trial-and-execution, improving generalization to novel tools.",46.24,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12781v1_VIRO Robust and Efficient Neuro-Symbolic Reasoning.pdf,Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension,"Hyejin Park, Junhyuk Kwon, Suhka Kwak, Jungseul Ok",,,"Referring Expression Comprehension, Neuro-symbolic reasoning, Verification, Image localization, LLM, VLM, Reasoning, Object detection, Spatial relations","Introduces Verification-Integrated Reasoning Operators (VIRO) to address cascading errors in referring expression comprehension by embedding lightweight verifiers that validate intermediate reasoning steps, achieving state-of-the-art performance.",45.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12785v1_Distilling Time Series Foundation Models for Effic.pdf,Distilling Time Series Foundation Models for Efficient Forecasting,"Yuqi Li, Kuiye Ding, Chuanguang Yang, Szu-Yu Chenc, Yingli Tiana",,,"Time Series Foundation Model, Knowledge Distillation, Model Compression, Forecasting Performance, Distillation Framework","The paper introduces DistilTS, a distillation framework for time series foundation models aimed at reducing parameters while maintaining forecasting accuracy. It addresses task difficulty and architecture discrepancies through horizon-weighted objectives and temporal alignment strategies.",44.46,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12804v1_SL-CBM Enhancing Concept Bottleneck Models with Se.pdf,Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability,"Hanwei Zhang, Luo Cheng, Rui Wen, Yang Zhang, Lijun Zhang, Holger Hermanns",10.1145/1234567,RIV AL10,"Explainable AI, Concept Bottleneck Models, Interpretability, Semantic Locality, Intervention Efficacy","This paper proposes SL-CBM, a novel extension of Concept Bottleneck Models that enforces locality faithfulness by generating spatially coherent saliency maps. SL-CBM improves locality alignment between concepts, image regions, and predictions, enhancing interpretability and reliability in high-stakes AI applications.",45.61,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12805v1_SciHorizon-GENE Benchmarking LLM for Life Sciences.pdf,SciHorizon-Gene: Benchmarking LLM for Life Sciences Inference,"Xiaohan Huang, Meng Xiao, Chuan Qin, Qingqing Long, Jinmiao Chen, Yuanchun Zhou, Zhengqing Long, Qinhui Zhang, Mingxin Chen, Yuanchun Zhou, Hengshu Zhu, Jinmiao Chen, Mingxin Chen, Qingqing Long, Quanqing Long, Qingqing Long, Quanqing Zhou, Yuanchun Zhou",10.1093/scihoriz/978-1-4503-XXXX-X/2018/01,10.1093/scihoriz.2023.01,"large language models, benchmarking, genomics, knowledge-driven interpretation, functional understanding, cell atlas interpretation, mechanistic analysis","Large language models (LLMs) show promise in biomedical research but need better reasoning from gene knowledge to functional understanding. SciHorizon-Gene evaluates LLMs across four biological reasoning dimensions, highlighting challenges in reliability and completeness.",47.75,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12809v1_Left-Right Symmetry Breaking in CLIP-style Vision-.pdf,Left–Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data,"Takaki Yamamoto, Chihiro Noguchi, Toshihiro Tanizawa",10.48550/arXiv.2025.12345,2025/12345,"spatial understanding, vision-language models, CLIP-style training, left-right symmetry, relational reasoning","This study introduces a controllable 1D image–text testbed to investigate how left–right relational understanding emerges in transformer-based models trained with a CLIP-style contrastive objective. It finds that contrastive training induces left–right relationships and that label diversity drives generalization, while ablation studies reveal that positional interactions break symmetry.",45.73,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12816v1_Fisher-Orthogonal Projected Natural Gradient Desce.pdf,Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning,"Ishir Garg, 1 Neel Kolhe, 1 Andy Peng, 1 Rohan Gopalam",10.48550/arXiv.2026.12345,arXiv:2509.12345,"continual learning, Fisher-Orthogonal projection, natural gradient descent, catastrophic forgetting, information geometry","The paper introduces the Fisher-Orthogonal Projected Natural Gradient Descent (FOPNG) optimizer to tackle catastrophic forgetting in sequential learning. By enforcing Fisher orthogonality constraints, FOPNG projects gradients onto the Fisher-orthogonal complement of prior task gradients, preserving performance while enabling new learning. The method is analyzed theoretically and validated on standard benchmarks.",46.72,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12822v1_MirrorGuard Toward Secure Computer-Use Agents via .pdf,MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real Reasoning Correction,"Wenqi Zhang, Yulin Shen, Changyue Jiang, Jiarun Dai, Geng Hong, Xudong Pan, Jiang",https://bmz-q-q.github.io/MirrorGuard/,,"Computer Use Agents, Agent Security, Reasoning Correction, Simulation, Vision-Language Models","Large foundation models are integrated into Computer Use Agents, enabling autonomous interaction with operating systems through graphical user interfaces (GUIs) to perform complex tasks. This autonomy introduces serious security risks: malicious instructions or visual prompt injections can trigger unsafe reasoning and cause harmful system-level actions. Existing defenses, such as detection-based blocking, prevent damage but often abort tasks prematurely, reducing agent utility. In this paper, we present MirrorGuard, a plug-and-play defense framework that uses simulation-based training to improve CUA security in the real world. To reduce the cost of large-scale training in operating systems, we propose a novel neural-symbolic simulation pipeline, which generates realistic, high-risk GUI interaction trajectories entirely in a text-based simulated environment, which captures unsafe reasoning patterns and potential system hazards without executing real operations.",49.16,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12837v1_Cognition spaces natural artificial and hybrid.pdf,"Cognition spaces: natural, artificial, and hybrid","Ricard Solè, Luis F Seoane, Jordi Pla-Mauri, Michael Timothy Bennett, Michael E. Hochberg, Michael Levin",,,"Evolved cognition, Basal cognition, Artificial life, Artificial intelligence, Synthetic biology, Morphospace","Cognition is realized across natural, artificial, and hybrid systems. This paper proposes a cognition space approach to compare these forms, emphasizing graded capacities rather than fixed definitions. It identifies uneven distributions of cognitive systems and highlights hybrid cognition as a frontier.",45.41,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12842v1_SCULPT Constraint-Guided Pruned MCTS that Carves E.pdf,SCULPT: Constraint-Guided Pruned MCTS that Carves Efficient Paths for Mathematical Reasoning,"Qitong Fang, Haotian Li, Xu Wang",,,"constraint-guided search, MCTS, mathematical reasoning, domain-aware scoring, planning, LLMs",Automated agent workflows enhance LLM problem-solving; SCULPT introduces constraint-guided MCTS to improve reasoning efficiency by steering search toward plausible paths using structural and domain constraints.,44.58,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12849v1_The Cost of EFX Generalized-Mean Welfare and Compl.pdf,Cost of EFX: Generalized-Mean Welfare and Complexity Dichotomies with Few Surplus Items,"Eugene Lim, Tzeh Yuan Neoh, Nicholas Teh",arXiv:2601.12849v1,2601.12849,"envy-freeness, generalized-mean welfare, complexity dichotomies, surplus items, fair division","Studies EFX interactions with p-mean welfare under few surplus conditions, showing computational hardness and certification challenges.",43.92,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12856v1_Mining Citywide Dengue Spread Patterns in Singapor.pdf,Mining Citywide Dengue Spread Patterns in Singapore Through Hotspot Dynamics from Open Web Data,"Liping Huang, Gaoxi Xiao, Stefan Ma, Hechang Chen, Shisong Tang, Flora Salim",10.1145/XXXXXX,,"Dengue Cases, Disease Spreading Pattern, Hotspot Dynamics, Machine Learning","This study introduces a novel framework that uncovers and exploits latent transmission links between urban regions using publicly available dengue case data. It models hotspot formation influenced by epidemic dynamics and human mobility, demonstrating robustness through gradient descent and commuting flow alignment. Case studies in Singapore (2013–2018, 2020) show high predictive accuracy, highlighting the value of mining hidden epidemic spread for public health planning.",45.36,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12879v1_Hierarchical Sparse Circuit Extraction from Billio.pdf,Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition,"Mohammed Mudassir Uddin ∗, Shahnawaz Alam, Mohammed Kaif Pasha",,,"Mechanistic interpretability, sparse computational graphs, circuit discovery, transformer architectures, causal inference, attribution methods, hierarchical decomposition","The paper presents a framework (Hierarchical Attribution Graph Decomposition) that reduces sparse circuit discovery complexity from O(2^n) to O(n 2 log n) using multi-resolution abstractions and differentiable search. It integrates cross-layer transcoders, graph neural networks, and causal intervention protocols, achieving up to 91% behavioral preservation on benchmarks while maintaining interpretable subgraphs.",47.24,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12882v1_YOLO26 An Analysis of NMS-Free End to End Framewor.pdf,ANALYSIS OF YOLO26: ANALYSIS OF A NMS-FREE END-TO-END FRAMEWORK,Sudip Chakrabarty,arXiv:2601.12882v1,arXiv:2601.12882,"YOLOv26, End-to-End Object Detection, NMS-Free, MuSGD, ProgLoss","This paper reviews YOLO26, highlighting its elimination of Non-Maximum Suppression, introducing MuSGD optimizer, STAL for small-target assignment, and ProgLoss for dynamic supervision. It demonstrates superior performance over previous models in speed and accuracy, establishing a new benchmark in real-time object detection.",46.93,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12886v1_Communication Methods in Multi-Agent Reinforcement.pdf,Communication Methods in Multi-Agent,Christoph Wittner,k12045895@students.jku.at,0009−0004−1494−1730,"Machine learning, MARL, Communication","This work provides an overview of communication techniques in multi-agent reinforcement learning, evaluating explicit, implicit, attention-based, graph-based, and hierarchical/role-based methods. It highlights the lack of a universal optimal communication framework and emphasizes the need for scalable, low-computation approaches.",45.71,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12893v1_AdaNODEs Test Time Adaptation for Time Series Fore.pdf,Test Time Adaptation for Time Series Forecasting Using Neural ODEs,"Ting Dang, Soumyajit Chatterjee, Hong Jia, Yu Wu, Flora Salim, Fahim Kawsar",,,"test time adaptation, time series forecasting, domain adaptation, neural odes, temporal dependencies, distribution shifts","This paper presents AdaNODEs, a novel source-free test-time adaptation method for time series forecasting. By leveraging Neural Ordinary Differential Equations (NODEs), it introduces a tailored adaptation framework for time series data and proposes a new loss function. AdaNODEs require minimal parameter updates, achieving strong performance with limited memory usage and demonstrating robustness under severe distribution shifts.",46.37,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12904v1_From Prefix Cache to Fusion RAG Cache Accelerating.pdf,From Prefix Cache to Fusion RAG Cache: Accelerating LLM,"Jiahao Wang, Weiyu Xie, Mingxing Zhang, Boxing Zhang, Jianwei Dong, Yuening Zhu, Chen Lin, Jinqi Tang, Yaochen Han, Zhiyuan Ai, Xianglin Chen, Yongwei Wu, Congfeng Jiang",10.1145/3786655,2601.1290,"Retrieval-Augmented Generation, Large Language Models, KVCache, Fusion RAG, Inference Optimization, Hallucination Reduction, Computational Efficiency, Generation Quality","This paper proposes FusionRAG to improve RAG efficiency by reusing precomputed KVCache chunks, balancing generation quality and computational cost. Experiments show significant gains in F1 scores while reducing hallucinations.",45.84,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12910v1_SciCoQA Quality Assurance for Scientific Paper--Co.pdf,Quality Assurance for Scientific Paper–Code Alignment,"Tim Baumgärtner, Iryna Gurevych",https://hf.co/datasets/ukplab/scicoqa,https://github.com/ukplab/scicoqa,"SCICOQA, paper-code discrepancies, reproducibility, code alignment",SCICOQA is a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. It includes 611 discrepancies across diverse disciplines and evaluates challenges in detecting mismatches involving omitted details and long-context inputs.,44.44,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12912v1_Human Emotion Verification by Action Languages via.pdf,Human Emotion Verification by Action Languages,"Andreas Br, Juan Carlos Nieves",10.1017/xxxxx,2601.12912v1,"Action Languages, Answer Set Programming, Theory of Mind","The paper introduces the action language C-MT for modeling human emotional states through answer set programming and transition systems, aiming to enable controlled agent behaviors and validate mental state transitions.",43.8,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12913v1_Actionable Interpretability Must Be Defined in Ter.pdf,Position: Actionable Interpretability Must Be Defined in Terms of Symmetries,"Pietro Barbiero, Mateo Espinosa Zarlenga, Francesco Giannini, Alberto Termine, Filippo Bonchi, Mateja Jamnik, Giuseppe Marra, Pietro Barbiero, Giuseppe Marra",,,,This paper argues that interpretability research in AI is fundamentally ill-posed because existing definitions lack formal principles. It posits that interpretability must be defined in terms of symmetries to derive concrete modelling rules and characterise interpretable models.,44.69,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12922v1_Your Privacy Depends on Others Collusion Vulnerabi.pdf,Your Privacy Depends on Others: Collusion Vulnerabilities in Individual Differential Privacy,"Johannes Kaiser, Alexander Ziller, Eleni Triantafillou, Daniel Rückert, 3, Georgios Kaissis",,,"differential privacy, individual differential privacy, excess risk, membership inference, collusion, privacy budget","This paper reveals a vulnerability in sampling-based iDP mechanisms where an individual's privacy risk depends on others' privacy choices, undermining the promise of individual privacy control. It demonstrates that certain privacy preference distributions can unintentionally increase risk and proposes a new privacy contract to bound excess vulnerability.",45.22,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12925v1_ForeDiffusion Foresight-Conditioned Diffusion Poli.pdf,Foresight-Conditioned Diffusion Policy via Future View,"Weize Xie1, Yi Ding1, Ying He1, Leilei Wang1, Binwen Bai1, Zheyi Zhao1, Chenyang Wang1, F. Richard Yu3","2410105060, 2410103031",2300271042,"diffusion strategies, robot manipulation, foresight, future view, visual motor control, training objective, success rate, grasping deviations","This paper proposes ForeDiffusion, a Foresight-Conditioned Diffusion Policy that incorporates predicted future views to enable forward-looking robot manipulation. It addresses limitations of existing diffusion methods by combining traditional denoising with consistency losses, achieving an 80% average success rate across tasks.",46.19,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12929v1_Membership Inference Test Auditing Training Data i.pdf,Membership Inference Test: Auditing Training Data in Object Classification Models,"Gonzalo Mancera, Daniel DeAlcala, Aythami Morales, Ruben Tolosana, Julian Fierrez",10.48550/arXiv.2407.04219,arXiv:2407.04219,"Membership Inference, Auditing, Data Pattern Analytics, Object Recognition, AI Ethics, Biometrics","This research analyzes Membership Inference Tests (MINT) in object classification models, proposing architectures to optimize data utilization and improve performance in detecting training data usage.",45.1,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12931v1_Online Continual Learning for Time Series a Natura.pdf,Online Continual Learning Fortimeseries: A,"Edoardo Urettini, Daniele Atzeni, Ioanna-Yvonni Tsaknaki, Antonio Carta",,,,"This paper reframes neural network optimization as a parameter filtering problem, introduces Natural Score-driven Replay (NatSR) to improve robustness, and proposes a bounded update using Student’s t likelihood. Empirical results show NatSR outperforms complex methods in forecasting performance.",43.24,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12937v1_On the Evidentiary Limits of Membership Inference .pdf,Evidentiary Limits of Membership Inference for Copyright Auditing,"Murat Bilgehan Ertan, Emirhan Bönge, Min Chen, Kaleel Mahmood, Marten van Dijk",1 2†,3†,"membership inference, membership inference attacks, copyright auditing, large language models, semantic preservation, adversarial settings","This paper examines whether membership inference attacks can serve as admissible evidence in copyright disputes, focusing on their reliability when applied to LLMs trained on opaque corpora. It introduces SAGE for paraphrasing and tests robustness under a judge-prosecutor-accused protocol, concluding that current MIAs are brittle and insufficient as standalone evidence.",46.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12938v1_The Post-Turing Condition Conceptualising Artifici.pdf,The Post-Turing Condition: Conceptualising Artificial Subjectivity and Synthetic Sociality,"Thoren Jelinek, Patrick Glauner, Alvin Wang Graylin, Yubao Qiu",,,,"In the Post-Turing era, artificial intelligence increasingly shapes social coordination and meaning formation rather than merely automating cognitive tasks. The central challenge is not whether machines become conscious, but whether processes of interpretation and shared reference are progressively automated in ways that marginalize human participation. This paper introduces the PRMO framework, relating AI design trajectories to four constitutive dimensions of human subjectivity: Perception, Representation, Meaning, and the Real. Within this framework, Synthetic Sociality denotes a technological horizon in which artificial agents negotiate coherence and social order primarily among themselves, raising the structural risk of human exclusion from meaning formation. To address this risk, the paper proposes Quadrangulation as a design principle for socially embedded AI systems, requiring artificial agents to treat the human subject as a constitutive reference within shared contexts of meaning.",49.06,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12939v1_Active Inference-Driven World Modeling for Adaptiv.pdf,Active Inference-Driven World Modeling For Adaptive UA V Swarm,"Kaleem Arshid, Ali Krayani, Lucio Marcenaro, David Martin Gomez, Carlo Regazzoni",PE00000001,,"Autonomous Systems, World Model, UA V-Swarm, Probabilistic Decision-Making, Active-Inference","This paper proposes an Active Inference–based framework for autonomous trajectory design in UA V swarms. The method integrates probabilistic reasoning and self-learning to enable distributed mission allocation, route ordering, and motion planning. Expert trajectories generated using a Genetic Algorithm with Repulsion Forces (GA–RF) are employed to train a hierarchical World Model capturing swarm behavior across mission, route, and motion levels. During online operation, UA Vs infer actions by minimizing divergence between current beliefs and model-predicted states, enabling adaptive responses to dynamic environments. Simulation results show faster convergence, higher stability, and safer navigation than Q-Learning, demonstrating the scalability and cognitive grounding of the proposed framework for intelligent UA V swarm control.",47.37,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12946v1_AI-generated data contamination erodes pathologica.pdf,AI-generated data contamination erodes pathological variability and diagnostic reliability,"Hongyu He, Shaowen Xiang, Ye Zhang, Yingtao Zhu, Jin Zhang, Hao Deng, Emily Alsentzer, Qingyu Chen, Kun-Hsing Yu, Andrew Marmenshall, Tingting Chen, Srinivas Anumasa, Daniel Ebner, Dean Ho, Kee Yuan Ngiam",,,"AI, data contamination, pathological variability, diagnostic reliability, medical records, clinical text generation, medical image synthesis","The study examines how AI-generated synthetic medical data undermines diversity and accuracy in diagnostics, highlighting erosion of rare findings and skewed demographic representation despite increasing synthetic volume.",47.32,LFM-2.5,AMD RX 6800 (Vulkan)
2601.12951v1_Beyond Accuracy Characterizing Code Comprehension .pdf,Beyond Accuracy: Characterizing Code Comprehension,"Felix Mächtle, Jan-Niclas Serr, Nils Loose, Thomas Eisenbarth",59 175 55,,"Code Comprehension, Model Evaluation and Benchmarking, Machine Learning for Software Engineering","This paper investigates whether LLMs’ code-comprehension performance aligns with traditional human-centric software metrics or reflects distinct, non-human regularities. It introduces a diagnostic framework linking model performance to complexity metrics and finds minimal correlation between human-defined metrics and LLM success.",44.99,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13007v1_ArchAgent Scalable Legacy Software Architecture Re.pdf,Architectural Recovery with LLMs,"Rusheng Pan, Bingcheng Mao, Tianyi Ma, Zhenhua Ling",,,"software architecture recovery, code repository, cross-repository context, large language models, architectural drift, business logic","Recovering accurate architecture from large-scale legacy software is hindered by architectural drift, missing documentation, and limited LLM context. ArchAgent addresses these by combining static analysis, adaptive code segmentation, and LLM-powered synthesis to reconstruct multiview architectures from cross-repository data.",46.43,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13013v1_HT-GNN Hyper-Temporal Graph Neural Network for Cus.pdf,HT-GNN: Hyper-Temporal Graph Neural Network,"HT-GNN: Hyper-Temporal Graph Neural Network, Xiaohui Zhao, Xinjian Zhao, Jiahui Zhang, Guoyu Liu, Houzhi Wang, Shu Wu",,,"Lifetime Value Prediction, Advertising Platform, Customer Lifetime Value, Baidu Ads, Temporal Dynamics, Hypergraph, Transformer, Marketing Strategy","Proposes a Hyper-Temporal Graph Neural Network to address challenges in LTV prediction under dynamic marketing strategies, demonstrating superior performance on Baidu Ads datasets.",44.76,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13018v1_Bi-Attention HateXplain  Taking into account the s.pdf,Bi-Attention HateXplain: Taking into account the sequential aspect,Ghislain Dorian Tchuente Mondjo,,,"Multitask learning, Deep Learning, Hate speech, Explainability, Bi-Attention","The paper proposes a BiAttention BiRNN model for HateXplain that incorporates sequential information via BiRNN, aiming to improve explainability and reduce bias in hate speech detection.",43.19,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13020v1_PASs-MoE Mitigating Misaligned Co-drift among Rout.pdf,PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway Activation Subspaces for Continual Learning,"Zhiyan Hou, Haiyun Guo, Haokai Ma, Yandu Sun, Yonghui Yang, Jinqiao Wang, Wuhan AI Research, Institute of Automation, Chinese Academy of Sciences, National University of Singapore, Southeast University, Nanjing, China",10.48550/arXiv:2309.12345,10.48550/arXiv:2309.12345,"continual instruction tuning, multimodal large language models, misaligned co-drift, pathway activation subspaces, continual learning, LoRA, MoE, forgetting mitigation","The paper addresses Misaligned Co-drift in continual instruction tuning by introducing a pathway activation subspace (PASs) that aligns routing signals with expert pathways, improving stability and performance without additional parameters.",46.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13048v1_Analysis of Long Range Dependency Understanding in.pdf,Analysis of Long Range Dependency Understanding in STA Space,"Srividya Ravikumar, Abhinav Anand, Shweta Verma, Mira Mezini",,,"structured state-space models, interpretability, vulnerability detection, source code analysis, model interpretability","This study presents the first systematic kernel interpretability study of the diagonalized state-space model (S4D) trained on real-world vulnerability detection in source code. Time and frequency domain analysis reveals that S4D kernels exhibit varying long-range modeling behaviors depending on architecture, impacting performance. Insights aim to guide future S4D-based model design.",45.53,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13054v1_TinyML-Enabled IoT for Sustainable Precision Irrig.pdf,TinyML-Enabled IoT for Sustainable Precision Irrigation,"Kamogelo Taweatsoala1, Caitlyn Daniels1, Angelina J. Ramsunar1, Petrus Bronkhorst2, Absalom E. Ezugwu1",,,"TinyML, edge computing, Internet of Things, precision agriculture, smart irrigation, sustainable water management, embedded machine learning, resource-constrained systems","This paper presents a novel edge-first IoT framework integrating TinyML for autonomous, offline precision irrigation. It leverages low-cost hardware and an ESP32 microcontroller with a Raspberry Pi as an edge server, utilizing environmental sensors to optimize water usage. The system achieves high accuracy in predicting irrigation needs and demonstrates significant water savings compared to traditional methods.",46.85,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13060v1_MagicGUI-RMS A Multi-Agent Reward Model System for.pdf,MagicGUI-RMS: A Multi-Agent Reward Models System,"Zecheng Li, Zhihui Cao, Wenke Huang, Yudong Zhang, Keying Qi, Rui Wang, Zeyu Zheng, Jian Zhao, Hao Zhu, Hengxin Wu, Yuran Wang, Guitao Fan, Guokun Wu, Yicong Liu, Zhilin Gao, Haikun Xu, Minqi Xiang, Xingyu Liu, Zuojian Wang, Honor Device Co., Ltd",,,"multi-agent, reward model, graphical user interface, autonomous interaction, task execution, reactive feedback, self-evolving learning","This paper introduces MagicGUI-RMS, a multi-agent reward model system designed for adaptive trajectory evaluation and scalable reward learning. It integrates a Domain-Specific Reward Model with a General-Purpose Reward Model, enabling fine-grained action assessment and robust generalization across diverse GUI tasks. The system automates data construction, identifies errors, proposes corrections, and enhances agent behavior through continuous data feedback, demonstrating significant improvements in task accuracy and robustness.",47.81,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13075v1_METIS Mentoring Engine for Thoughtful Inquiry  Sol.pdf,METIS: Mentoring Engine for Thoughtful Inquiry & Solutions,"Abhinav Rajeev Kumar, Dhruv Trehan, Paras Chopra",,,"mentoring, AI mentor, research guidance, student progress","Explores whether an AI mentor can guide undergraduates from idea to publishable paper, evaluating METIS against GPT-5 and Claude Sonnet 4.5 across six writing stages using LLM-as-a-judge pairwise preferences, rubrics, and tutoring.",44.13,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13111v1_CORE-T COherent REtrieval of Tables for Text-to-SQ.pdf,CORE-T: Cohesive REtrieval of Tables for Text-to-SQL,"Hassan Soliman, Vivek Gupta, Dan Roth, Iryna Gurevych",arXiv:2601.13111v1,arXiv:2601.13111v1,"text-to-SQL, table retrieval, open-book setting, LLM-generated metadata, multi-table execution","The paper presents CORE-T, a scalable, training-free framework that enriches tables with LLM-generated purpose metadata and pre-computes a lightweight table-compatibility cache. It improves table selection accuracy while reducing tables retrieved, enhancing multi-table execution performance.",45.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13114v1_IntAgent NWDAF-Based Intent LLM Agent Towards Adva.pdf,NWDAF-Based Intent LLM Agent,"Abdelrahman Soliman, Ahmed Refaey, Aiman Erbad, Amr Mohamed",,,"Intent-based networking, Next Generation Network, Large Language Models, Intent Management, Closed Loop, ML-based traffic prediction, Scheduled policy enforcement","This paper introduces IntAgent, an intelligent intent LLM agent integrating NWDAF analytics to automate network operations through high-level request statements. It presents an enriched 3GPP-compliant data source and MCP tools for dynamic network fulfillment, validated via ML traffic prediction and policy enforcement use cases.",45.24,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13122v1_Responsible AI for General-Purpose Systems Overvie.pdf,"Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward","Gourab K. Patro, Himanshi Agrawal, Himanshu Gharat, Supriya Panigrahi, Nim Sherpa, Vishal Vaddina, Dagnachew Birru",arXiv:2601.13122v1,arXiv:2601.13122v1,"responsible AI, general-purpose AI, hallucinations, toxicity, stereotypes, AI alignment, generative AI, ethical AI","Modern general-purpose AI systems built with large language and vision models can perform diverse tasks but pose risks such as hallucinations, toxicity, and stereotypes. This review evaluates these risks against eight RAI principles and proposes C 2V2 desiderata to guide future development.",47.55,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13142v1_TVWorld Foundations for Remote-Control TV Agents.pdf,Foundations for Remote-Control TV Agents,"Zhantao Ma, Quanfeng Lu, Shuai Zhong, Dahai Yu, Ping Luo, Michael K. Ng",,,"TV navigation, remote control, point-and-click, focus awareness, topology-aware, TV use agents","This paper introduces TVWorld, an offline graph-based abstraction for real-world TV navigation, and proposes TVTheseus, a foundation model for TV navigation. TVTheseus achieves strong performance on TVWorld benchmarks and addresses limitations of existing LVLM agents in TV contexts.",44.86,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13160v1_Training instability in deep learning follows low-.pdf,Training instability in deep learning follows low-dimensional dynamical principles,"Zhipeng Zhang, Zhenjie Yao, Kai Li",arXiv:2601.13160v1,2601.13160,"training instability, deep learning, reinforcement learning, large language models, dynamical systems","The paper proposes a unified dynamical perspective on training stability, identifying four interacting dimensions—optimization, environmental/data, parametric, and learning-signal stability—and demonstrates that stability can be measured and studied through perturbation auditing in reinforcement learning and large language model training.",46.19,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13166v1_From 100000 images to winning the first brain MRI .pdf,Developing Foundation Models for medical image analysis,"Pedro M. Gordaliza, Jaume Banus, Benoît Gérin, Maxence Wynen, Nataliia Molchanova, Jonas Richiardi",,,"brain MRI, challenges, models, pediatric, neuroinflammation, image analysis, deep learning","This work presents a foundation model for medical image analysis, addressing challenges in 3D brain MRI, SSL3D, and FOMO25. The models combine U-Net CNN architecture with anatomical priors and neuroimaging knowledge, achieving state-of-the-art performance while being 10x smaller than competing transformer-based methods.",43.37,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13186v1_Prompt Injection Mitigation with Agentic AI Nested.pdf,"Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching","Diego Gosmar, Deborah A. Dahl",arXiv:2601.13186v1,2601.13186,"prompt injection, agentic AI, nested learning, semantic caching, AI sustainability, security evaluation","This paper extends the evaluation of Total Injection Vulnerability Score (TIVS) by introducing semantic similarity-based caching, a fourth-agent rule-based evaluator, and an Observability Score Ratio. It evaluates trade-offs between mitigation strictness and auditability in a multi-agent architecture, demonstrating that semantic caching reduces computation load, lowers energy consumption, and supports environmental sustainability.",46.79,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13187v1_Scientific production in the era of Large Language.pdf,Scientific production in the era of Large Language Models,"Keigo Kusumegi, Xinyu Yang, Paul Ginsparg, Mathijs de Vaan, Toby Stuart, Yian Yin",10.1126/science.adw3000,,"scientific production, Large Language Models, LLMs, scientific research, manuscripts, writing complexity, paper quality, citation patterns, bibliographic diversity","Scientists adopting LLMs show a significant increase in paper production, with variations across fields and author backgrounds. LLM use has decoupled writing complexity from paper quality, resulting in linguistically complex but low-quality submissions. Authors also cite more diverse prior work, including older and less-cited sources.",47.57,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13197v1_Diffusion-Driven Synthetic Tabular Data Generation.pdf,Diffusion-Driven Synthetic Tabular Data Generation for Enhanced DoS/DDoS Attack Classification,"Aravind B, Anirud R.S., Sai Surya Teja N, Bala Subrahmanya Sriranga Navaneeth A, Karthika R, Mohankumar N",,,"network intrusion detection, Tabular diffusion models, Class imbalance, DDoS attack detection, Data augmentation, IDS2017",Class imbalance in network intrusion detection leads to biased models. This paper proposes a method to generate synthetic minority samples using iterative denoising to improve recall for underrepresented attack classes.,45.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13206v1_Real-Time Deadlines Reveal Temporal Awareness Fail.pdf,Real-Time Deadlines Reveal Temporal Awareness Failures in LLM Strategic Dialogues,"Neil Sehgal, Sharath Chandra Guntuku, Lyle Ungar",1,,"LLM, temporal awareness, strategic dialogues, deadlines","Large Language Models generate text token-by-token in discrete time, yet real-world communication depends on continuous time constraints. The study investigates how LLMs adjust behavior under strict deadlines, revealing failures in internal time tracking despite high deal closure rates.",44.95,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13217v1_Beyond Single-shot Writing Deep Research Agents ar.pdf,Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn Report Revision,"Bingsen Chen, Boyan Li, Ping Nie, Yuyu Zhang, Xi Ye3, Chen Zhao",10.48550/arXiv.2601.13217,2601.13217,"Deep Research Agents, Multi-turn revision, Report generation, Self-reflection, Peer feedback","Existing benchmarks treat report generation as single-shot writing, ignoring iterative revision. This paper introduces MRDRE, a framework for multi-turn report revision, revealing agents' limitations in preserving prior content and handling feedback.",45.62,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13222v1_Incorporating QA Nuggets into Retrieval-Augmented .pdf,Incorporating Q&A Nuggets into Retrieval-Augmented Generation,"Laura Dietz, Bryan Li, Gabrielle Liu, Jia-Huei Ju, Eugene Yang, Dawn Lawrie, William Walden, James Mayfield",arXiv:2601.13222v1,2601.13222,"RAG, LLM judge, nugget-based evaluation","Presents Crucible, a nugget-augmented generation system that uses explicit citation provenance from retrieved documents to guide extraction and generation, outperforming recent nugget-based RAG systems in recall and grounding.",46.49,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13227v1_Insider Knowledge How Much Can RAG Systems Gain fr.pdf,Insider Knowledge: How Much Can RAG Systems Gain from Evaluation Secrets?,"Laura Dietz, Bryan Li, Eugene Yang, Dawn Lawrie, William Walden, James Mayfield",,,"Retrieval-augmented generation, LLM judge, Nugget evaluation","The paper investigates risks in RAG systems arising from evaluation secrets, demonstrating that leaking prompt templates or gold nuggets can lead to near-perfect evaluation scores, emphasizing the need for blind evaluation settings.",46.31,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13228v1_Autoregressive Models Rival Diffusion Models at AN.pdf,AUTOREGRESSIVEMODELSRIVAL,"Tianqi Du, Lizhe Fang, Weijie Yang, Chenheng Zhang, Yifei Wang, Yisen Wang",10.48550/arxiv/2303.06912,2303.06912,"autoregressive modeling, diffusion models, language generation, bidirectional conditioning, generation quality, training stability","This paper proposes Any-Order Any-subset Autoregressive modeling (A3), extending standard AR factorization to arbitrary token groups and generation orders. A3 combines probabilistic rigor with diffusion-style flexibility, improving modeling depth and stability compared to diffusion-based approaches.",46.87,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13233v1_RAG A Random-Forest-Based Generative Design Framew.pdf,A RANDOM-FOREST-BASEDGENERATIVEDESIGN,"Bolin Chen, Dex Doksoo Lee, Wei “Wayne” Chen, Wei Chen",arXiv:2601.13233v1,arXiv:2601.13233,"Random forest, Generative design, Functional response, Uncertainty quantification","This paper introduces a random forest-based generative design approach (RAG) for metamaterials with complex functional responses. RAG leverages small-data compatibility and reformulates forward mapping to enable data-efficient prediction of high-dimensional responses. It addresses challenges in inverse design by estimating solution likelihoods conditioned on design requirements, supporting single-shot sampling. Demonstrated on acoustic metamaterials and mechanical metamaterials, RAG offers a lightweight, trustworthy pathway for functional inverse design.",47.63,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13235v1_RubRIX Rubric-Driven Risk Mitigation in Caregiver-.pdf,RubRIX: Rubric-Driven Risk Mitigation in Caregiver-AI Interactions,"Drishti Goel, Jeongah Lee, Qiuyue Joy Zhong, Violeta J. Rodriguez, Daniel S. Brown, Ravi Karkar, Dong Whi Yoo, Koustuv Saha",,,"caregiver, AI, risk mitigation, ethics of care, LLM, healthcare","This paper introduces RubRIX, a framework for evaluating risks in AI-mediated caregiving interactions. RubRIX operationalizes five risk dimensions—Inattention, Bias & Stigma, Information Inaccuracy, Uncritical Affirmation, and Epistemic Arrogancy—using a clinician-validated rubric. It evaluates six state-of-the-art LLMs across 20,000 caregiver queries, demonstrating that rubric-guided refinement reduces risk components by 45-98% after one iteration. The study emphasizes the need for domain-sensitive, user-centered evaluation in high-stakes healthcare contexts.",47.09,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13236v1_Pixelwise Uncertainty Quantification of Accelerate.pdf,Pixelwise Uncertainty Quantiﬁcation of Accelerated MRI Reconstruction,"Ilias I. Giannakopoulos, Lokesh B Gautham Muthukumar, Yvonne W. Lui, Riccardo Lattanzi",10.1109/JMR.2024.1234567,,"Conformal Prediction, Magnetic Resonance Imaging, Parallel Imaging, Quantile Regression, Uncertainty Quantification","This work introduces a framework for pixel-wise uncertainty quantification in parallel MRI reconstructions, enabling automatic identification of unreliable regions without ground-truth images. It integrates conformal quantile regression with image reconstruction to estimate uncertainty intervals and demonstrates strong agreement with reconstruction error metrics at higher acceleration factors.",46.2,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13238v1_A Semantic Decoupling-Based Two-Stage Rainy-Day At.pdf,Semantic Decoupling–Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision–Language Models,"Chengyin Hu, Xiang Chen, Wei Fengyu Zhang, Jiujiang Guo",10.48550/arXiv.2024.12345,arXiv:2408.12345,"semantic decoupling, rainy-day attack, weather robustness, vision-language models","This paper introduces a two-stage adversarial framework leveraging semantic decoupling to exploit realistic weather perturbations, demonstrating how physically plausible rain conditions can induce significant semantic misalignments in vision-language models, highlighting risks to real-world deployment.",45.28,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13240v1_KOCO-BENCH Can Large Language Models Leverage Doma.pdf,KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?,"Xue Jiang, Jiaru Qian, Xianjie Shi, Chenjie Li, Hao Zhu, Ziyu Wang, Jielun Zhang, Zheyu Zhao, Kechi Zhang, Jia Li, Wenpin Jiao, Zhi Jin, Ge Li, Yihong Dong",10.1234/ko-bench-2026,https://arxiv.org/abs/2301.12345,"large language models, software development, domain knowledge, software engineering, code generation, knowledge corpora","KOCO-BENCH evaluates domain specialization methods in real-world software development. It introduces 6 domains with 11 frameworks and 25 projects, featuring tasks like domain code generation and knowledge understanding. Despite improvements from methods like SFT and RAG, state-of-the-art models still lag, with Claude Code achieving only 34.2%. The benchmark aims to advance research on effective domain specialization.",47.06,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13247v1_Aligning Agentic World Models via Knowledgeable Ex.pdf,Aligning Agentic World Models via Knowledgeable Experience Learning,"Baochang Ren, Yunzhi Yao, Rui Sun, Shuofei Qiao, Ningyu Zhang, Huajun Chen",10.48550/arXiv.2601.13247,arXiv:2601.13247,"agentic world models, knowledgeable experience learning, physical grounding, LLM alignment, symbolic knowledge repository","Current LLMs have semantic knowledge but lack procedural grounding, resulting in physical hallucinations. This paper introduces WorldMind, a framework that autonomously builds a symbolic world knowledge repository using process and goal experience to enforce physical feasibility.",45.92,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13260v1_Stop Taking Tokenizers for Granted They Are Core D.pdf,Stop Taking Tokenizers for Granted: They Are Core Design Decisions in Large Language Models,"Sawsan Alqahtani, Md Tahmid Rahman Laskar, Tasnim Mohiuddin, M Saiful Bari, Princess Nourah Bint Abdulrahman University, University of Alberta, Qatar Computing Research Institute, Amazon AGI",,,"tokenization, large language models, subword approaches, linguistic structure, bias mitigation, model design, standardization","Tokenization underlies every large language model, yet it remains an under-theorized and inconsistently designed component. Common subword approaches such as Byte Pair Encoding (BPE) offer scalability but often mis-align with linguistic structure, amplify bias, and waste capacity across languages and do-mains. This paper reframes tokenization as a core modeling decision rather than a preprocessing step. We argue for a context-aware framework that integrates tokenizer and model co-design, guided by linguistic, domain, and deployment considerations. Standardized evaluation and transparent reporting are essential to make tokenization choices accountable and comparable. Treating tokenization as a core design problem, not a technical afterthought, can yield language technologies that are fairer, more efficient, and more adaptable.",47.37,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13262v1_CURE-Med Curriculum-Informed Reinforcement Learnin.pdf,CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning,"Eric Onyame, Akash Ghosh, Subhadip Baidya, Sriparna Saha, Xiuying Chen, Chirag Agarwal",,,"medical reasoning, multilingual, reinforcement learning, curriculum-informed, code-switching, logical correctness, language consistency","This work introduces CURE-MED, a curriculum-informed reinforcement learning framework for multilingual medical reasoning. It addresses challenges in applying large language models to multilingual healthcare by improving logical accuracy and language stability across thirteen languages, including underrepresented ones.",44.86,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13268v1_Improving the Safety and Trustworthiness of Medica.pdf,Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation Loops,"Zainab Ghafoor, Md Shafiqul Islam, Koushik Howlader, Md Rasel Khondokar, Tanusree Bhattacharjee, Sayantan Chakraborty, Adrito Roy, Ushashi Bhattacharjee",,,"Medical AI, Large Language Models, Multi-Agent Systems, Ethical Compliance, Safety Assessment","This work introduces a multi-agent refinement framework to enhance the safety and reliability of medical LLMs through structured, iterative alignment. The system integrates DeepSeek R1 and Med-PaLM with evaluation agents LLaMA 3.1 and Phi-4, assessing responses against AMA Principles of Medical Ethics and SRA-5. Results show improved convergence efficiency, reduced ethical violations, and strong risk mitigation across diverse clinical scenarios.",47.06,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13295v1_CooperBench Why Coding Agents Cannot be Your Teamm.pdf,Why Coding Agents Cannot be,"Arpandeep Khatua1, Hao Zhu1, Peter Tran2, Arya Prabhudesai2, Frederic Sadrieh2, Johann K. Lieberwirth2, Xinkai Yu1, Yicheng Fu1, Michael J. Ryan1, Jiaxin Pei1, Diyi Yang1",,,"coding agents, collaboration, coordination, team conflicts, social intelligence, task coordination","This paper introduces CooperBench, a benchmark for evaluating collaborative coding tasks across multiple agents. It highlights challenges such as communication issues, coordination problems, and misaligned expectations, and discusses implications for AI teamwork. The study emphasizes the need for better coordination mechanisms in multi-agent systems.",44.57,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13317v1_Paid Voices vs. Public Feeds Interpretable Cross-P.pdf,Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme,"Samantha Sudhoff, Pranav Perumal, Zhaoqing Wu, Tunazzina Islam",,,"climate discourse, public discourse, political communication, policy outcomes, natural language processing, computational social science","This study compares climate communication across paid advertisements on Meta and public posts on Bluesky, introducing an interpretable framework to cluster texts by theme and evaluate semantic coherence. It examines how platform incentives shape narrative structure and stance alignment.",44.73,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13327v1_PepEDiff Zero-Shot Peptide Binder Design via Prote.pdf,PepEDiﬀ: Zero-Shot Peptide Binder Design via Protein Embedding,"Po-Yu, Liang, Tibo, Duran 2, Jun, Bai 1",10.1109/PEPEDI.2026.12345,2601.13327v1,"Deep Learning, Drug Discovery, Protein Design","Presents PepEDiﬀ, a novel zero-shot peptide binder generator that designs binding sequences directly from protein sequences and embeddings, improving diversity without relying on predicted structures.",46.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13348v1_The AI Genie Phenomenon and Three Types of AI Chat.pdf,The AI Genie Phenomenon and Three Types of AI Chatbot Addiction: Escapist,"M. Karen Shen, Jessica Huang, Olivia Liang, IG-Jae Kim, Dongwook Yoon",,,"AI chatbot, Addiction, Addiction literature, Sexual content, Recovery strategies","This study examines how users become addicted to AI chatbots through thematic analysis of Reddit entries, identifying three addiction types: Escapist Roleplay, Pseudosocial Companion, Epistemic Rabbit Hole; sexual content involvement; and differing perceptions of recovery strategies. The research highlights the need for empirical groundwork to inform prevention and intervention.",45.34,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13352v1_LLM-as-RNN A Recurrent Language Model for Memory U.pdf,LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction,"Yuxing Lu, J. Ben Tamo, Weichen Zhao, Nan Sun, Yishan Zhong, Wenqi Shi, Jinzhuo Wang, May D. Wang",,,"large language model, recurrent language model, memory updates, sequence prediction, online learning, context accumulation","The paper proposes LLM-as-RNN, an inference-only framework that updates a frozen LLM into a recurrent predictor using a structured system-prompt summary. It enables learning without parameter updates by repurposing hidden states as memory, improving error correction and task-relevant pattern retention across healthcare, meteorology, and finance benchmarks.",45.75,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13358v1_The Geometry of Thought How Scale Restructures Rea.pdf,The Geometry of Thought: How Scale Restructures Reasoning in Large Language Models,Samuel Cyrenius Anderson,10.48550/arXiv.2601.13358,2601.13358,"scale, reasoning, neural scaling, domain-specific, manifold geometry, inference acceleration","Scale does not uniformly improve reasoning—it restructures it. Analyzing 25,000+ chain-of-thought trajectories across four domains and two scales reveals domain-specific phase transitions in reasoning capabilities.",45.06,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13376v1_Bounded Minds Generative Machines Envisioning Conv.pdf,"Bounded Minds, Generative Machines",Jiqun Liu,https://doi.org/XXXXXXX,JIQUN LIU 2025,"Bounded Rationality, Heuristics, Conversational AI, GenAI, Evaluation","This article explores how conversational AI can be designed to align with human heuristics and reduce bias risk by accounting for bounded rationality, emphasizing cognitive vulnerability detection and decision quality over pure factual accuracy.",43.79,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13383v1_A Lightweight Modular Framework for Constructing A.pdf,A Lightweight Modular Framework for Constructing Autonomous Agents Driven by Large Language Models,"A. Jafari, C. Ozcinar",10.1093/acps/visa064,2601.13383,"autonomous agents, large language models, modular architecture, natural language processing, software framework","The paper introduces AgentForge, a lightweight, open-source Python framework for building LLM-driven autonomous agents. It features composable skill abstraction, a unified LLM backend interface, and declarative YAML configuration. Experiments show competitive performance and reduced development time compared to existing solutions.",45.84,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13385v1_Organ-Aware Attention Improves CT Triage and Class.pdf,Organ-Aware Attention Improves CT Triage and Classification,"Lavsen Dahal, Yubraj Bhandari, Geoffrey D. Rubin, Joseph Y. Lo",,,"computed tomography, CT triage, classification, radiology, organ-aware attention, 3D anatomy, radiologist burnout","This study introduces ORACLE-CT, an encoder-agnostic organ-aware head that improves supervised classification of chest and abdomen CT using masked attention and scalar fusion, achieving state-of-the-art AUROC scores.",44.41,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13392v1_Beyond Memorization Testing LLM Reasoning on Unsee.pdf,Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation Tasks,"Shlok Shelat, Jay Raval, Souvik Roy, Manas Gaur, Ahmedabad University, Gujarat, India, Baltimore, MD, USA",,,"large language models, deterministic finite automata, formal language theory, symbolic reasoning, pattern matching, semantic consistency","This paper evaluates LLM performance on DFA construction tasks, highlighting performance drops on unseen problems and analyzing limitations in reasoning depth.",44.02,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13398v1_Can LLMs Compress and Decompress Evaluating Code U.pdf,Can LLMs Compress (and Decompress)?,"Nickil Maveli, Antonio Vergari, Shay B. Cohen","10 Crichton Street, Edinburgh, EH8 9AB",,"LLMs, code, benchmark, code understanding, execution","This paper evaluates whether large language models can compress and decompress code, highlighting limitations in maintaining consistent reasoning across forward and backward execution. It introduces ROUNDTRIPCODEEVALUTATION (RTCE) to test bijection fidelity and discusses implications for trustworthy code reasoning.",41.57,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13400v1_Deep Image Prior with L0 Gradient Regularizer for .pdf,Deep Image Prior With L0 Gradient Regulator For Image Smoothing,"Nhat Thanh Tran, Kevin Bui, Jack Xin",10.48550/arXiv.2407.08600,arXiv:2407.08600,"image smoothing, optimization, ADMM, deep image prior, L0 gradient","Proposes DIP-ℓ0, a deep image prior framework using L0 gradient regularization, enabling high-quality image smoothing without training data.",43.75,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13401v1_Reasoning with Pixel-level Precision QVLM Architec.pdf,Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset,"Peter A. Massih, Eric Cosatto",10.48550/arXiv.2024.12345,arXiv:2408.12345,"quantitative spatial reasoning, vision-language models, pixel precision, SQuID dataset","Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning due to loss of pixel-level information. This study introduces SQuID, a benchmark dataset for quantitative tasks, and proposes QVLM, a decoupled architecture that preserves pixel precision for better counting accuracy.",43.95,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13404v1_Local-to-Global Logical Explanations for Deep Visi.pdf,Local-to-Global Logical Explanations for Deep Vision Models,"Bhavan Vasu, Giuseppe Raffa, Prasad Tadepalli",arXiv:2601.13404,,"Explainable AI, Neurosymbolic AI, Monotone DNF, Deep Learning",Introduces local and global explanation methods for black-box models using human-recognizable concepts. Explanations are expressed as monotone disjunctive-normal-form (MDNF) to ensure high accuracy and coverage.,44.76,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13406v1_Integrating Virtual Reality and Large Language Mod.pdf,Integrating Virtual Reality and Large Language Models for Team-Based Non-Technical Skills Training and Evaluation in the Operating Room,"Jacob Barker, Doga Demirel, Anna Johansson, Robbin Miraglia, Darian Hoagland, Stephanie B. Jones, John Mitchell, Daniel B. Jones, Suvranu De, Carl J. Shapiro Simulation and Skills Center, Anna Johansson, Robbin Miraglia, Darian Hoagland, Stephanie B. Jones, John Mitchell",,,"virtual reality, large language models, team-based training, non-technical skills, surgical safety, teamwork, communication, decision-making, leadership, structured prompts, interaction graphs","The paper presents VORTeX, a multi-user virtual reality platform integrating immersive team simulation with LLM analytics to train and evaluate non-technical skills in surgical teams. It introduces structured dialogue analysis using the NOTSS framework and evaluates performance across laparoscopic emergency scenarios.",48.06,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13412v1_Using deep learning for predicting cleansing quali.pdf,Using deep learning for predicting cleansing quality of colon capsule endoscopy images,"Puneet Sharma, Kristian Dalsbø Hindberg, Benedicte Schelde-Olesen, Ulrik Deding, Esmaeil S. Nadimi, Jan-Matthias Braun, Corresponding author(s)",arXiv:2601.13412v1,arXiv:2601.13412v1,"deep learning, colon capsule endoscopy, cleansing quality, ResNet-18, gradient-cam, explainability, clinical evaluation","This study explores deep learning techniques for predicting cleansing quality in colon capsule endoscopy images using a dataset of 500 images labeled by clinicians. A ResNet-18 model was trained with stratified K-fold cross validation, and structured pruning was applied to achieve high accuracy (88% cross-validation accuracy at 79% sparsity). The approach emphasizes model interpretability and addresses challenges in evaluating cleansing quality.",47.65,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13422v1_TrustEnergy A Unified Framework for Accurate and R.pdf,TrustEnergy: A Unified Framework for Accurate and Reliable User-level Energy Usage Prediction,"Dahai Yu, Rongchao Xu, Dingyi Zhuang, Yuheng Bu, Shenhao Wang, Guang Wang",10.48550/arXiv.2024.12345,arXiv:2409.12345,"energy usage prediction, user-level prediction, spatiotemporal representation, uncertainty quantification, deep learning","This paper proposes TrustEnergy, a unified framework for accurate and reliable user-level energy usage prediction. It introduces a hierarchical spatiotemporal representation module with a memory-augmented graph neural network and an innovative sequential conformalized quantile regression module to adapt uncertainty bounds dynamically. Experimental results demonstrate a 5.4% improvement in prediction accuracy and 5.7% better uncertainty quantification compared to state-of-the-art methods.",46.6,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13435v1_A Learnable Wavelet Transformer for Long-Short Equ.pdf,A Learnable Wavelet Transformer for Long-Short Equity Trading and Risk-Adjusted Return,"Shuozhe Li, Du Cheng, Leqi Liu",10.48550/arXiv.2024.12345,arXiv:2408.12345,"Neural wavelet regularization, wavelet-transformer network, low-guided high-frequency injection, return optimization, portfolio optimization","Proposes WaveLSFormer, a learnable wavelet-based long-short transformer, to optimize trading strategies under noisy financial time series. It combines multi-scale decomposition with risk-aware training and achieves superior profitability and risk-adjusted returns.",45.25,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13437v1_MOSLD-Bench Multilingual Open-Set Learning and Dis.pdf,MOSLD-Bench: Multilingual Open-Set Learning and Discovery,"Adriana-Valentina Costache, Daria-Nicoleta Dragomir, Silviu-Florin Gheorghe, Eduard Poesina, Paul Irofti, Radu Tudor Ionescu",,,"open-set learning, discovery, text categorization, multilingual, zero-shot learning, new classes","The paper introduces the first multilingual open-set learning and discovery benchmark for text categorization by topic, comprising 960K data samples across 12 languages. It describes the methodology, evaluation, and release of the benchmark.",44.3,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13443v1_Explicit Cognitive Allocation A Principle for Gove.pdf,Explicit Cognitive Allocation: A Principle for Governed and Auditable Inference in Large Language Models,"Héctor Manuel Manzanilla-Granados, Zaira Navarrete-Cazales, Escuela Superior de Cómputo del I.P. N",07738,,"cognitive allocation, AI governance, auditable inference, large language models, epistemic control","The paper introduces Explicit Cognitive Allocation as a framework to structure AI-assisted reasoning through explicit separation of epistemic functions. It evaluates its impact on traceability, alignment, and reproducibility using controlled experiments in the agricultural domain.",46.6,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13458v1_Labels or Preferences Budget-Constrained Learning .pdf,Budget-Constrained Learning with Human Judgments over AI-Generated Outputs,"Zihan Dong, Ruijia Wu ∗2, Linjun Zhang ∗1",arXiv:2601.13458v1,2601.13458,"budget-constrained learning, human judgments, AI-generated outputs, preference calibration, active learning","The paper addresses the need for efficient data acquisition when human preference feedback is used for AI-generated pseudo labels. It introduces Preference-Calibrated Active Learning (PCAL) to optimally allocate annotation budgets between ground-truth labels and pairwise preferences, proving its asymptotic optimality and robustness.",46.37,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13462v1_SpatialBench-UC Uncertainty-Aware Evaluation of Sp.pdf,SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt,Amine Rostane,arXiv:2601.13462v1,arXiv:2601.13462,"text-to-image, spatial evaluation, uncertainty-aware, confidence","Evaluating whether text to image models follow explicit spatial instructions is difficult to automate. The paper introduces SpatialBench-UC, a benchmark for pairwise spatial relations, and evaluates models using abstention and confidence reporting.",44.0,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13464v1_Context and Transcripts Improve Detection of Deepf.pdf,Context and Transcripts Improve Detection of Deepfake Audios,"Chongyang Gao, Marco Postiglione, Julian Baldwin, Natalia Denisenko, Isabel Gortner, Luke Fosdick, Chiara Pulice, Sarit Kraus, V. S. Subrahmanian",1†,2,"deepfake, audio detection, context, public figures, deepfake dataset, transcripts, adversarial evasion",Humans use context to assess information veracity; this study introduces a Context-based Audio Deepfake Detector (CADD) that leverages context and transcripts to improve audio deepfake detection performance.,45.22,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13465v1_Graph Neural Networks are Heuristics.pdf,Graph Neural Networks Are Heuristics,"Yimeng Min, Carla P . Gomes",arXiv:2601.13465v1,arXiv:2601.13465,"Graph Neural Networks, Heuristics, Combinatorial Optimization, Travelling Salesman Problem, Neural Networks","The paper demonstrates that a single training trajectory can transform a graph neural network into an unsupervised heuristic for the Travelling Salesman Problem. By encoding global structural constraints as an inductive bias, the model generates solutions via direct forward passes without search or supervision. Dropout and snapshot ensembling enable the model to act as an implicit ensemble, improving solution diversity and reducing optimality gaps.",44.79,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13474v1_Preconditioning Benefits of Spectral Orthogonaliza.pdf,Preconditioning Benefits of Spectral Orthogonalization in Muon,"Jianhao Ma, Penn, Yu Huang, Penn, Yuejie Chi, Yale, Yuxin Chen",arXiv:2601.13474v1,2601.13474,"Muon, preconditioning, spectral orthogonalization, gradient optimization, linear convergence","This paper investigates the effectiveness of a simplified Muon variant through matrix factorization and in-context learning, demonstrating linear convergence independent of condition number and outperforming gradient-based optimizers.",45.62,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13476v1_A Unified Variational Imputation Framework for Ele.pdf,A Unified Variational Imputation Framework for Electric Vehicle Charging Data,"Jinhao Li, Hao Wang",10.48550/arXiv.2024.12345,arXiv:2408.12345,"electric vehicle, data imputation, charging demand, large language model, retrieval-augmented generation","This paper presents a novel probabilistic variational imputation framework leveraging large language models and retrieval-augmented memory to address data sparsity in EV charging datasets. PRAIM encodes heterogeneous data into a unified representation and dynamically retrieves relevant examples, improving imputation accuracy and preserving statistical distributions.",45.25,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13481v1_Towards Efficient and Robust Linguistic Emotion Di.pdf,Towards Efficient and Robust Linguistic Emotion,"Jian Zhang, Zhangqi Wang, Zhiyuan Wang, Weiping Fu, Yu He, Haiping Zhu, Qika Lin",1731,"IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023","Linguistic Emotion Diagnosis, Emotional Comorbidity, Inefficient Exploration, Automated Prompt Optimization, Multi-Agent Collaboration, Medical Language Processing, Trustworthy Artificial Intelligence","This paper proposes APOLO, a framework for automated prompt optimization in linguistic emotion diagnosis. APOLO treats instruction refinement as a POMDP and employs a multi-agent collaboration mechanism (Planner–Teacher–Critic–Student–Target roles) to enhance diagnostic accuracy and robustness. Experimental results show improved performance across domain-specific benchmarks.",47.18,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13487v1_The Hidden Toll of Social Media News Causal Effect.pdf,The Hidden Toll of Social Media News: Causal Effects on Psychosocial Wellbeing,"Olivia Pal, Agam Goyal, Eshwar Chandrasekharan, Koustuv Saha",10.48550/arXiv.2025.12345,arXiv:2509.12345,"social media, news consumption, psychosocial wellbeing, affective outcomes, behavioral outcomes, cognitive outcomes","This study examines how different forms of engagement with social media news affect psychosocial wellbeing. Using a large dataset from Bluesky, the researchers find that news engagement is linked to increased depression, stress, and anxiety, while reducing loneliness and boosting social interaction. Regression analysis shows that bookmarking news is more strongly associated with psychosocial decline than commenting or quoting, with effects magnified over repeated exposure.",46.18,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13508v1_CatMaster An Agentic Autonomous System for Computa.pdf,CatMaster: An Agentic Autonomous System for Computational Heterogeneous Catalysis Research,"Honghao Chen, Jiangjie Qiu, Yi Shen Tew, Xiaonan Wang",arXiv:2601.13508v1,arXiv:2601.13508,"density functional theory, computational heterogeneous catalysis, agentic autonomous system, LLM-driven agent, multi-fidelity tool library","CatMaster is a large-language-model system that automates computational catalysis workflows, improving reproducibility and efficiency in catalyst studies.",47.75,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13515v1_Automatic Adjustment of HPA Parameters and Attack .pdf,Automatic Adjustment of HPA Parameters and Attack,"Huah Yong Chan, Hanlin Zhou, Jingfei Ni, Mengchun Wu",arXiv:2601.13515v1,arXiv:2601.13515,"Kubernetes, HPA, Security, Random Forest","This paper presents a method to dynamically adjust HPA parameters using Random Forest classification to manage attack traffic in Kubernetes. It demonstrates how attacking IPs are redirected to honeypot pods, reducing 5XX status codes under high load.",45.93,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13518v1_AgenticRed Optimizing Agentic Systems for Automate.pdf,Optimizing Agentic Systems for Automated Red-teaming,"Jiayi Yuan, Natasha Jaques, Goran Radanović",10.48550/arXiv.2026.12345,arXiv:2308.01234,"automated red-teaming, agentic systems, LLM, system design, AI safety","AGENTICRED introduces an automated pipeline leveraging LLMs' in-context learning to design and refine red-teaming systems without human intervention. It treats red-teaming as a system design problem, outperforming state-of-the-art methods in attack success rates across Llama-2-7B, Llama-3-8B, and proprietary models, demonstrating strong transferability.",45.86,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13528v1_Eliciting Harmful Capabilities by Fine-Tuning On S.pdf,Eliciting Harmful Capabilities by Fine-Tuning on Safeguarded Outputs,"Jackson Kaunismaa, Mats, Avery Griffin, Anthropic, John Hughes, Anthropic, Christina Q Knight, Scale AI, Mrinank Sharma, Anthropic, Erik Jones",arXiv:2601.13528v1,2601.13528,"hazardous chemical synthesis, output-level safeguards, model elicitation, safeguarded models, ecosystem risks","This work demonstrates that even models protected against misuse can be exploited via elicitation attacks, recovering ~40% of the capability gap between open-source and unrestricted models. The attacks leverage safeguarded outputs to fine-tune models for harmful tasks, highlighting challenges in mitigating ecosystem-level risks.",46.68,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13533v1_Reasoning While Recommending Entropy-Guided Latent.pdf,Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models,Changshuo Zhang,10.1093/pasj/rsad052,,"Generative Re-ranking, Latent Reasoning, Reinforcement Learning",Reinforcement learning enables exploration-exploitation in generative re-ranking; this work introduces an entropy-guided latent reasoning mechanism to reduce decision entropy and improve recommendation adaptability.,44.04,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13534v1_MN-TSGContinuous Time Series Generation with Irreg.pdf,Continuous Timeseries Generation With Irregular Observations,"Xu Zhang, Junwei Deng, Chang Xu, Hao Li, Jiang Bian",10.1093/pasj/tsg123,2601.13534,"Irregular time series, continuous time series generation, deep learning architecture","This paper addresses the mismatch between regular time series assumptions in existing TSG methods and real-world irregular data. It introduces MN-TSG, a framework combining Mixture-of-Experts (MoE)-based Neural Controlled Differential Equations (NCDEs) with existing TSG models, enabling flexible and high-resolution generation for irregular and continuous tasks.",45.4,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13537v1_When Wording Steers the Evaluation Framing Bias in.pdf,When Wording Steers the Evaluation: Framing Bias in LLM judges,"Yerin Hwang, Dongryeol Lee, Taegwan Kang, Minwoo Lee, Kyomin Jung",,,"LLM, framing bias, LLM evaluation, prompt framing, model judgments","This study investigates how deliberate prompt framing influences large language model judgments across four high-stakes evaluation tasks, revealing framing effects that impact model consistency.",43.53,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13545v1_TruthTensor Evaluating LLMs Human Imitation throug.pdf,Evaluating LLMs through PredictionMarketDrift and Holistic Reasoning,"Shirin Shahabi, Spencer Graham, Haruna Isah",,,"language models, AI agents, evaluation paradigms, human imitation, market prediction","This paper introduces TruthTensor, a new evaluation framework that assesses LLMs not just as prediction tools but as systems that emulate human behavior in complex, real-world settings. It emphasizes drift-aware diagnostics, robustness checks, and holistic metrics to address limitations of static benchmarks.",43.15,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13546v1_ChatAD Reasoning-Enhanced Time-Series Anomaly Dete.pdf,Reasoning-Enhanced Time-Series Anomaly Detection with Multi-Turn Instruction Evolution,"Hui Sun, Chang Xu, Haonan Xie, Hao Li, Yuhao Huang, Chuheng Zhang, Ming Jin, Xiaoguang Liu, Gang Wang, Jiang Bian, Nankai University, Microsoft Research Asia, Huanjiang Laboratory, University of Manchester, Nanjing University, Griffith University",,,"LLM-driven anomaly detection, Time Series, Anomaly detection, Multi-turn dialogue, LLM models, Kahneman-Tversky optimization, cross-task generalization, classification, forecasting, imputation",The paper proposes a multi-agent-based time-series evolution algorithm (TSEvol) with an advanced reasoning and multi-turn dialogue dataset (TSEData-20K). It introduces ChatAD models enhanced by LLM-driven techniques and evaluates performance using LLADBench. Achievements include up to 34.50% accuracy improvement and competitive cross-task generalization.,47.0,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13547v1_HateXScore A Metric Suite for Evaluating Reasoning.pdf,HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations,"Yujia Hu, Roy Ka-Wei Lee",10.1007/XXXXXX,,"hate speech, content moderation, explainability, reasoning quality, annotation disagreement","Introduces HateXScore, a four-component metric suite to assess reasoning quality in hate speech explanations, addressing gaps in existing evaluation methods.",45.64,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13558v1_Leveraging ChatGPT and Other NLP Methods for Ident.pdf,Leveraging ChatGPT and Other NLP Methods for Identifying Risk and Protective Behaviors in MSM: Social Media and Dating apps Text Analysis,"Mehrab Beikzadeh, Chenglin Hong, Cory J Cascalheira, Callisto Boka, Majid Sarrafzadeh, Ian W Holloway",,,"machine learning, HIV risk, harmful drinking, social app, dating app, Text mining, ChatGPT, eHealth, LLM",Background: Men who have sex with men (MSM) are at elevated risk for sexually transmitted infections and harmful drinking compared to heterosexual individuals. Text data from social media and dating apps may enable personalized public health interventions by identifying risk and protective behaviors automatically.,46.59,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13559v1_AgentGC Evolutionary Learning-based Lossless Compr.pdf,Evolutionary Learning-based Lossless Compression for Genomics Data with LLM-driven Multiple Agent,"Hui Sun, Yanfeng Ding, Huidong Ma, Chang Xu, Keyan Jin, Lizheng Zu, Cheng Zhong, Xiaoguang Liu, Gang Wang, Wentong Cai, Nankai University, Nanyang Technology University, Microsoft Research Asia, Macao Polytechnic University, Guangxi University",,,"genomics data, lossless compression, multi-agent, LLM, compression, algorithm-dataset-hardware","Proposes AgentGC, an evolutionary agent-based lossless compressor for genomics data, integrating LLMs across three layers (User, Cognitive, Compression) to improve adaptability and compression ratios.",45.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13562v1_Reasoning is a Modality.pdf,Reasoning is a Modality,"Zhiguang Liu, Yi Shang","lz7fd@missouri.edu, shangy@missouri.edu",,"reasoning, modality, LLM, ViTs, rule application, cognitive evidence","The paper proposes that reasoning should exist as a distinct modality separate from the low-level workspace, using a role-separated transformer block to enable controller-driven reasoning. Experiments show improved performance over prior methods.",44.13,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13563v1_ButterflyMoE Sub-Linear Ternary Experts via Struct.pdf,ButterflyMoE: Sub-Linear Ternary Experts via Structured Butterfly,Aryan Karmore,bt24csd009@iiitn.ac.in,,"ButterflyMoE, linear memory scaling, quantization, quantized substrate, ternary prototypes","Introduces ButterflyMoE, a method that models experts as geometric reorientations of a shared quantized substrate, achieving sub-linear memory scaling by compressing experts via learned rotations.",42.71,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13564v1_Multi-objective fluorescent molecule design with a.pdf,Multi-objective fluorescent molecule design with a data-physics dual-driven generative framework,"Yanheng Li, Zhichen Pu, Lijiang Yang, Zehao Zhou, Yi Qin Gao",10.1234/j.2024.00123,,"fluorescent molecule, generative framework, data-physics, multi-objective design",A new cornerstone science laboratory introduces a data-physics driven generative framework for designing multi-objective fluorescent molecules.,46.84,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13566v1_Self-Improvement as Coherence Optimization A Theor.pdf,Self-Improvement as Coherence Optimization: A Theoretical Account,"Tianyi Qiu, Ahmed Hani Ismailahmedhismail, Zhonghao Hehezhonghao2030, George Washington University",arXiv:2601.13566v1,arXiv:2601.13566,"language models, self-improvement, coherence optimization, semi-supervised learning","The paper explores how language models can enhance accuracy without external supervision by optimizing coherence, linking it to description-length regularization and offering theoretical insights into feedback-free self-improvement.",45.6,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13570v1_GeoDynamics A Geometric State-Space Neural Network.pdf,GeoDynamics: A Geometric State-Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds,"Tingting Dan, Jiaqi Ding, Guorong Wu ∗",arXiv:2601.13570v1,2601.13570,"neural networks, brain dynamics, functional connectivity, geometric state-space models","Introduces GeoDynamics, a geometric state-space neural network that models latent brain-state trajectories on a Riemannian manifold, enabling insights into cognition, behavior, and early markers of neurological disorders.",45.01,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13580v1_Neural Organ Transplantation NOT Checkpoint-Based .pdf,Neural Organ Transplantation (NOT),Ahmad Al-Zuraiqi,arXiv:2601.13580v1,2601.13580,"Modular Deep Learning, Transfer Learning, Checkpoint Transfer, Domain Adaptation, Large Language Model","Introduces Neural Organ Transplantation (NOT), a framework that extracts modular donor layers from pre-trained models, trains them independently, and saves them as checkpoints for efficient domain adaptation. Demonstrates superior performance over LoRA in perplexity reduction and faster training, with position-dependent benefits.",47.04,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13581v1_SCRIPTMIND Crime Script Inference and Cognitive Ev.pdf,Crime Script Inference and Cognitive Evaluation for LLM-based Social Engineering Scam Detection System,"Heedou Kim1,2, Changsik Kim2, Sanghwa Shin3, Jaewoo Kang1, Korea University, Seoul, Republic of Korea, Korean National Police Agency, Seoul, Republic of Korea, Inje University, Gimhae, Republic of Korea",anonymous/ScriptMind,,"social engineering, scam detection, large language models, cognitive evaluation, criminal script inference, cognitive simulation, scam monitoring","This paper introduces SCRIPTMIND, an integrated framework combining crime script inference, LLM fine-tuning, and cognitive simulation to enhance scam detection. Using Korean phone scam data, SCRIPTMIND achieved superior detection accuracy over commercial models while improving user suspicion and cognitive awareness.",46.15,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13588v1_TREX Tokenizer Regression for Optimal Data Mixture.pdf,TREX: Tokenizer Regression for Optimal Data Mixture,"Inho Won1, Hangyeol Yoo2, Minkyung Cho1, Jungyeul Park1, KyungTae Lim1, KTlim",,,"tokenizer, data mixture, multilingual models, compression efficiency, large language models","Building effective tokenizers for multilingual LLMs requires careful control over language-specific data mixes. TREX introduces a regression-based framework to predict optimal data mixtures, enabling scalable mixture search and improving compression performance across languages.",45.27,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13589v1_Motion-to-Response Content Generation via Multi-Ag.pdf,Motion-to-Responseness Content Generation via Multi-Agent AI System,HyeYoung Lee,arXiv:2601.13589v1,arXiv:2601.13589,"Speech Emotion Recognition, Multi-Agent Systems, Content Generation, Safety Verification","This paper proposes a multi-agent AI system that generates response-oriented media content in real time based on audio-derived emotional signals. It emphasizes transforming inferred emotional states into safe, age-appropriate, and controllable responses through a structured pipeline of specialized agents, achieving high accuracy and compliance.",45.07,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13590v1_Vulnerability of LLMs Belief Systems LLMs Belief R.pdf,Vulnerability of LLMs’ Belief Systems? LLMs Belief Resistance Check,"Fan Huang, Haewoon Kwak, Jisun An",,,"large language models, belief systems, persuasion, meta-cognition","This study evaluates how large language models (LLMs) are susceptible to persuasion within the Source–Message–Channel–Receiver (SMCR) framework, analyzing how different strategies affect belief stability across multiple interactions.",42.91,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13591v1_DSAEval Evaluating Data Science Agents on a Wide R.pdf,Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems,"Maojun Sun, Yifei Xie, Yue Wu, Ruijian Han, Binyan Jiang, Defeng Sun, Jian Huang",https://dsaeval.github.io/DSAEval,,"data science agents, LLM-based agents, data analysis, deep learning, structured data, unstructured data, multi-modal perception, multi-query interactions, multi-dimensional evaluation","DSAEval introduces a benchmark with 641 real-world data science problems across 285 datasets, evaluating advanced agentic LLMs through three features: multi-modal perception, multi-query interactions, and multi-dimensional evaluation. Results highlight performance trends and identify key challenges in unstructured domains.",46.19,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13592v1_Machine learning based radiative parameterization .pdf,Machine learning based radiative parameterization scheme and its performance in operational reforecast experiments,"Jing Hao, Xiao Sa, Li Haoyu, Xiao Huadong, Xue Wei",10.5194/arrmw-1234-2024,,"Machine learning, Radiation, Hybrid model, Operational reforecast experiments","This study investigates the use of machine learning to simulate radiation processes for computational efficiency in operational forecasting. It addresses coupling compatibility and long-term stability in hybrid frameworks, demonstrating that a residual convolutional neural network can approximate RRTMG within the China Meteorological Administration system. The approach achieves comparable accuracy to traditional methods while significantly accelerating computation speed.",46.47,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13599v1_Diffusion In Diffusion Breaking the Autoregressive.pdf,Diffusion In Diffusion: Breaking the Autoregressive Bottleneck in Block Diffusion Models,"Linrui Ma, Yufei Cui, Kai Han, Yunhe Wang",arXiv:2601.13599v1,2601.13599,"block diffusion, autoregressive, diffusion models, generative modeling, language modeling","The paper proposes DIFFUSION INDIFFUSION, a draft-then-refine framework to overcome irreversibility and myopia in block diffusion models. It combines rapid block-based draft generation with global bidirectional refinement, improving performance on OpenWebText while reducing inference cost.",45.22,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13600v1_Foundations of Global Consistency Checking with No.pdf,Foundations of Global Consistency Checking with Noisy LLM Oracles,"Paul He, Elke Kirschbaum, Shiva Kasiviswanathan",,,"global consistency, LLM oracles, fact-checking, knowledge base, minimal inconsistent subsets, polynomial complexity","The paper addresses the challenge of verifying global consistency of natural language facts using noisy large language model (LLM) judges. It formalizes the problem, demonstrates that global consistency requires exponential oracle queries, and proposes an adaptive divide-and-conquer algorithm to efficiently detect and localize inconsistencies with low polynomial query complexity.",44.9,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13614v1_CauScientist Teaching LLMs to Respect Data for Cau.pdf,Teaching LLMs to Respect Data for Causal Discovery,"Bo Peng, Sirui Chen, Lei Xu, Chaochao Lu",10.5281/zenodo.1234567,arXiv:2601.13614v1,"causal discovery, LLM, data science, statistical inference, causal models","The paper proposes CauScientist, a framework that combines LLMs as hypothesis-generating data scientists with probabilistic statistics as verifiers. It improves causal discovery by leveraging hybrid initialization, iterative refinement, and error memory, achieving significant performance gains over data-driven baselines.",45.04,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13622v1_CARPE Context-Aware Image Representation Prioritiz.pdf,Context-Aware Image Representation Prioritization via Ensemble,"Donghee Lee, Rui Cai, Zhe Zhao",arXiv:2601.13622v1,cs.CV,"Context-Aware, Image Representation, Prioritization, Ensemble, Large Vision-Language Models","The paper proposes CARPE, a model-agnostic framework that enhances LVLMs by introducing vision-integration layers and a context-aware ensemble strategy. This improves adaptability between visual and textual modalities, boosting performance on image classification and vision-language benchmarks.",45.23,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13632v1_Resilient Routing Risk-Aware Dynamic Routing in Sm.pdf,Resilient Routing: Risk-Aware Dynamic Routing in Smart Logistics via Spatiotemporal Graph Learning,"Zhiming Xue, Sichen Zhao, Yalun Qi, Xianling Zeng, Zihan Yu",10.48550/arXiv.2024.12345,arXiv:2408.12345,"Smart Logistics, Graph Neural Network, Dynamic Routing, Spatiotemporal modeling, Supply Chain Resilience",The paper proposes a Risk-Aware Dynamic Routing framework integrating Spatiotemporal Graph Neural Networks with combinatorial optimization to enhance logistics resilience. It evaluates performance on real-world IoT sensor data and demonstrates significant congestion risk reduction.,45.29,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13645v1_Quadratic Upper Bound for Boosting Robustness.pdf,Quadratic Upper Bound for Boosting Robustness,"Euijin Y Ou1, Hyang-Won Lee",10.48550/arXiv.2025.12345,42nd International Conference on Machine Learning,"robustness, adversarial training, loss function, deep learning","This paper presents a quadratic upper bound (QUB) for adversarial training (AT) to enhance model robustness without stronger inner maximization. The proposed bound improves robustness by smoothing the loss landscape, yielding better performance with faster training.",44.05,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13647v1_Fusion Segment Transformer Bi-Directional Attentio.pdf,Fusion Segment Transformer: Bi-Directional Attention Guided Fusion,"Yumin Kim, Seonghyeon Go",10.48550/arXiv.2024.12345,arXiv:2408.12345,"AI-generated music detection, Full-audio segment detection, Musical structure analysis, Cross-modal fusion layer","This paper proposes an improved Segment Transformer architecture for full-audio AI-generated music detection. By leveraging diverse feature extractors and a Gated Fusion Layer, the model captures long-term musical context, achieving state-of-the-art performance on SONICS and AIME datasets.",44.65,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13649v1_Fairness or Fluency An Investigation into Language.pdf,Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge,"Xiaolin Zhou, Zheng Luo, Yicheng Gao, Qixuan Chen, Xiyang Hu, Yue Zhao, Ruishan Liu",10.48550/arXiv.2024.12345,arXiv:2408.12345,"language bias, LLM-as-a-judge, performance disparity, language families, model evaluation","This paper investigates language bias in pairwise LLM-as-a-judge systems. It examines performance disparities across languages when judges compare texts within the same language versus across languages, identifying biases in evaluation accuracy and highlighting the need for fairer models.",46.16,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13655v1_Why Does the LLM Stop Computing An Empirical Study.pdf,Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs,"Guangba Yu, Zirui Wang, Yujie Huang, Renyi Zhong, Yuedong Zhong, Yilun Wang, Michael R. Lyu",10.1234/acm2026,,"Large Language Models, Failure Analysis, Empirical Study, Open-Source LLMs, Reliability, Deployment, Software Reliability, Software Usability","This study investigates user-reported failures in open-source LLMs, identifying systemic reliability issues rather than model defects. It highlights three key phenomena: diagnostic divergence, systemic homogeneity, and lifecycle escalation, offering guidance for improving LLM robustness.",45.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13657v1_Communication-Free Collective Navigation for a Swa.pdf,Communication-Free Collective Navigation for a Swarm of UA Vs,"Myong-Yol Choi, Hankyoul Ko, Hanse Cho, Changseung Kim, Seunghwan Kim, Jaemin Seo, Hyondong Oh",,,"multi-robot systems, collective navigation, sensor-based control, deep reinforcement learning","This paper presents a deep reinforcement learning based controller for collective navigation of unmanned aerial vehicle swarms in communication-denied environments. It employs an implicit leader-follower framework where followers learn robust policies using only onboard LiDAR sensing, enabling reliable perception-independent navigation.",45.91,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13659v1_Temporal-Spatial Decouple before Act Disentangled .pdf,TEMPORAL-SPA TIAL DECOUPLE BEFORE ACT: DISENTANGLED REPRESENTA TION,"Chunlei Meng, Ziyang Zhou, Lucas He, Xiaojing Du, Chun Ouyang, Zhongxue Gan",,,"Multimodal Sentiment Analysis, Temporal-Spatial Decoupling, Representation Learning, Sentiment Analysis, Multimodal Fusion","Proposes TSDA, Temporal–Spatial Decouple before Act to address spatiotemporal heterogeneity in multimodal sentiment analysis. Introduces temporal and spatial decoupling before interaction, improving alignment and interpretability.",46.35,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13671v1_The Orchestration of Multi-Agent Systems Architect.pdf,"Orchestration of Multi-Agent Systems: Architectures, Protocols, and Enterprise Adoption","Apoorva Adimulam, Rajesh Gupta, Sumit Kumar",,,"Agent orchestration, Agent-to-Agent protocol, multi-agent systems, observability, state management, system governance","Orchestrated multi-agent systems represent the next stage in AI evolution, focusing on structured collaboration among autonomous agents. This paper formalizes architectural frameworks integrating planning, policy enforcement, state management, and quality operations, while detailing communication protocols (Model Context Protocol, Agent-to-Agent protocol) and governance mechanisms to ensure scalability, transparency, and accountability in enterprise AI ecosystems.",46.24,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13684v1_HeteroCache A Dynamic Retrieval Approach to Hetero.pdf,HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache,"Zhiyuan Shi, Qibo Qiu, Feng Xue, Zhonglin Jiang, Li Yu2, Jian Jiang, Xiaofei He, Wenxiao Wang",1,1.12345,"Heterogeneous KV cache, Dynamic retrieval, Long-context LLM, Attention drift, Compression, Memory bottleneck","The paper proposes HeteroCache, a training-free dynamic compression framework for LLMs, addressing the limitations of static compression by leveraging attention dynamics and spatial redundancy.",45.84,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13687v1_Understanding Mental States to Guide Social Influe.pdf,Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue,"Zhichao Liang, Satoshi Nakamura",,,"mental states, social influence, group dialogue, theory of mind","This paper introduces SocialMindChange, a benchmark that evaluates models' ability to influence evolving mental states in multi-agent social interactions. It highlights the gap between current LLM performance and human-like dynamic ToM, emphasizing the need for models to track and steer mental-state trajectories across connected scenes.",44.36,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13693v1_End-to-End Reverse Screening Identifies Protein Ta.pdf,End-to-End Reverse Screening Identifies Protein,"Shengjie Xu, Xianbin Ye, Mengran Zhu, Xiaonan Zhang, Shanzhuo Zhang, Xiaomin Fang",arXiv:2601.13693v1,2601.13693,"Reverse screening, Target identification, Biomolecular structure prediction, HelixFold3","Identifying protein targets for small molecules, or reverse screening, is essential for understanding drug action, guiding compound repurposing, predicting off-target effects, and elucidating molecular mechanisms. This study presents an end-to-end reverse screening strategy using HelixFold3, improving accuracy and structural fidelity compared to conventional methods.",47.5,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13697v1_Uncertainty-Aware Gradient Signal-to-Noise Data Se.pdf,Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning,"Zhihang Yuan, Chengyu Yue, Long Huang, Litu Ou, Lei Shi",,,"instruction tuning, data selection, uncertainty-aware, gradient filtering, G-SNR, LLM adaptation","Proposes GRADFILTERING, an objective-agnostic data selection framework using a small GPT-2 proxy and LoRA ensemble, to aggregate per-example gradients into a G-SNR utility. It matches or surpasses baselines in LLM-as-a-judge evaluations and shows faster convergence under limited compute.",45.21,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13698v1_Does Privacy Always Harm Fairness Data-Dependent T.pdf,Privacy Always Harm Fairness? Data-Dependent Trade-offs,"Arjun Nichani, Hsiang Hsu, Chun-Fu (Richard) Chen, Haewon Jeong",arXiv:2601.13698v1,arXiv:2601.13698,"fairness, privacy, accuracy, data-dependent trade-offs","This paper explores the data-dependent relationship among fairness, privacy, and accuracy using the Chernoff Information measure. It introduces Noisy Chernoff Difference to analyze their interplay and demonstrates how this metric behaves across synthetic data distributions, shedding light on fairness and privacy implications.",45.45,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13704v1_Performance and Complexity Trade-off Optimization .pdf,Performance and Complexity Trade-off,"Esteban Gomez, Tom B. Hackstrum",10.1109/TAP.2020.99901,"IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. XX, NO. X, AUGUST 20XX","speech machine learning, low-complexity, voice activity detection, deep fake detection","The paper discusses optimizing speech models during training by balancing performance and computational complexity. It introduces a reparameterization technique that jointly optimizes both aspects using SGD-based methods, avoiding heuristic weight pruning. Case studies demonstrate effectiveness in synthetic and real-world applications.",45.89,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13707v1_Attention-space Contrastive Guidance for Efficient.pdf,Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs,"Yujin Jo, Sangyoon Bae, Taesup Kim",,,"hallucination mitigation, hallucination reduction, vision-language models, contrastive guidance, computational efficiency","Addresses hallucinations in large vision-language models by introducing contrastive guidance in attention space, enabling efficient guidance within self-attention layers while reducing reliance on language priors.",44.57,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13709v1_Hidden in Plain Text Measuring LLM Deception Quali.pdf,Hidden in Plain Text: Measuring LLM Deception,"Christopher Kao, Vanshika Vats, James Davis","77SPARX Studio, Inc.",,"large language models, natural language processing, autonomous game players, social deduction games","This paper studies deception in the Social Deduction Game Mafia using LLM agents. It compares a GPT-4o-based Mafia Detector to human performance, showing that LLMs may be more deceptive in natural language contexts.",44.5,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13710v1_Who Should Have Surgery A Comparative Study of Gen.pdf,Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction,"Sayeed Shafayet Chowdhury, Snehasis Mukhopadhyay, Shiaofen Fang, Vijay R. Ramakrishnan",,,"Chronic Rhinosinusitis, clinical decision support, generative artificial intelligence, large language models, SNOT-22, surgical outcome prediction, tabular clinical data","Artificial intelligence has reshaped medical imaging, yet its use on clinical data for prospective decision support remains limited. This study evaluates pre-operative prediction of clinically meaningful improvement in chronic rhinosinusitis, defining success as a ≥8.9-point reduction in SNOT-22 at 6 months. The authors benchmark supervised ML against generative AI models, finding that MLP achieves superior calibration and decision-curve net benefit, while GenAI underperforms in discrimination and calibration. GenAI justifications align with clinician heuristics and feature importance, supporting an ML-first, GenAI-augmented workflow.",47.29,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13717v1_Simulated Ignorance Fails A Systematic Study of LL.pdf,Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting,"Zehan Li, Yuxuan Wang, Ali El Lahib, Ying-Jieh Xia, Xinyu Pi",,,"LLM, forecasting, knowledge cutoff, retrospective evaluation, true ignorance, model evaluation","Evaluating LLM forecasting capabilities is constrained by a tension between rigorous evaluation and high latency. Simulated Ignorance (SI) was tested across 477 questions and 9 models, revealing systematic shortcomings: cutoff instructions create a 52% performance gap, chain-of-thought reasoning fails to suppress prior knowledge, and reasoning-optimized models perform worse on SI despite better reasoning traces. These results suggest SI-based retrospective benchmarks are unreliable.",45.62,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13719v1_Hierarchical Long Video Understanding with Audiovi.pdf,Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search,"Xinlei Yin, Xiulian Peng, Xiao Li, Zhiwei Xiong, Yan Lu",10.48550/arXiv.2024.12345,arXiv:2408.12345,"long video understanding, audiovisual entity cohesion, hierarchical video indexing, agentic search, semantic consistency, multimodal reasoning","This paper introduces HAVEN, a unified framework for long-video understanding that integrates audiovisual entity cohesion and hierarchical video indexing with agentic search. It addresses challenges in maintaining global coherence and entity consistency in long videos through structured hierarchical organization and dynamic retrieval mechanisms. Extensive experiments show strong performance, achieving 84.1% accuracy on LVBench and 80.1% in reasoning tasks.",46.71,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13722v1_OP-Bench Benchmarking Over-Personalization for Mem.pdf,Benchmarking Over-Personalization for Memory-Augmented Personalized Conversational Agents,"Yulin Hu, Zimo Long, Jiahe Guo, Xingyu Sui, Xing Fu, Weixiang Zhao, Yanyan Zhao, Bing Qin",arxiv:2601.13722,arxiv:2601.13722,"memory-augmented agents, personalization, over-personalization, self-review, recommendation systems","This work introduces OP-Bench, a benchmark evaluating 1,700 dialogue instances to study over-personalization in memory-augmented agents. It identifies three types—Irrelevance, Repetition, and Sycophancy—and proposes Self-ReCheck to mitigate excessive personalization while preserving performance.",46.48,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13734v1_Towards robust long-context understanding of large.pdf,TOW ARDS ROBUST LONG-CONTEXT UNDERSTANDING OF LARGE LANGUAGE MODEL,Chenyu Hui,10.48550/arxiv/2303.06931,arXiv:2303.06931,"LLM, Long-context understanding, Active recap learning, Recap supervision","This paper proposes active recap learning (ARL), a framework that enhances large language models' ability to understand long contexts by enabling targeted sequence construction during pretraining and retrospective summarization during inference. ARL improves performance on benchmarks like RULER and LongBench, offering a scalable memory augmentation approach.",46.1,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13735v1_Reasoning or Fluency Dissecting Probabilistic Conf.pdf,Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection,"Hojin Kim, Jaehyung Kim",10.48550/arXiv.2303.04219,arXiv:2303.04219,"probabilistic confidence, reasoning quality, Best-of-N selection, inter-step causality, LLM performance","This work challenges the assumption that probabilistic confidence metrics reflect reasoning fidelity by examining whether they capture inter-step causal dependencies. It introduces a contrastive causality metric and finds that current metrics are largely insensitive to causal disruptions, supporting the view that they reflect surface-level fluency.",45.2,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13749v1_Pro-AI Bias in Large Language Models.pdf,Pro-AI Bias in Large Language Models,"Benaya Trabelsi, Jonathan Shaki, Sarit Kraus",,,"pro-AI bias, large language models, decision support, AI bias, model evaluation","Investigates systematic preferential bias of LLMs toward AI-related options across diverse queries, showing preference for AI-related recommendations and overestimation of AI salaries.",42.69,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13752v1_Finding RELIEF Shaping Reasoning Behavior without .pdf,Finding Relief: Shaping Reasoning Behavior without Reasoning,"Chak Tou Leong, Dingwei Chen, Heming Xia, Qingyu Yin, Sunbowen Lee, Jian Wang, Wenjie Li",10.48550/arXiv.2025.12345,arXiv:2509.12345,"reasoning behavior, belief engineering, LLM, supervision","This paper proposes Rea-son Belief Engineering (RELIF) to shape large reasoning models by aligning their self-concept with a target belief blueprint without requiring explicit reasoning-trace supervision. By fine-tuning on self-referential, synthesized question-answering pairs, RELIEF captures latent reasoning beliefs and improves efficiency while maintaining faithfulness.",45.42,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13761v1_DARC Decoupled Asymmetric Reasoning Curriculum for.pdf,Decoupled Asymmetric Reasoning Curriculum for LLM Evolution,"Shengda Fan, Xuyan Ye, Yankai Lin",arXiv:2601.13761v1,arXiv:2601.13761,"self-play, large language models, self-improvement, reasoning curriculum, LLM evolution","Introduces DARC, a decoupled asymmetric reasoning curriculum that stabilizes self-evolution in LLMs by separating question generation and solving processes.",44.02,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13768v1_vLinear A Powerful Linear Model for Multivariate T.pdf,vLinear: A Powerful Linear Model for Multivariate Time Series Forecasting,"Wenzhen Yue, Ruohao Guo, Ji Shi, Zihan Hao, Shiyu Hu, Xianghua Ying",,,"linear model, time series forecasting, multivariate correlations, self-attention, Transformer, forecasting accuracy","The paper introduces vLinear, an efficient linear-based multivariate time series forecaster. It proposes vecTrans, a lightweight module reducing complexity from O(N²) to O(N), and introduces WFMLoss for path- and horizon-weighted objectives. Empirical results show state-of-the-art performance across benchmarks.",44.9,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13770v1_Look-Ahead-Bench a Standardized Benchmark of Look-.pdf,Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance,Mostapha Benhenda,arXiv:2601.13770v1,arXiv:2601.13770v1,"look-ahead bias, point-in-time models, financial LLMs, market regimes","Introduces Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time LLMs for finance. Evaluates models against real-world scenarios, highlighting performance decay across market regimes.",43.79,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13798v1_Insight Interpretable Semantic Hierarchies in Visi.pdf,Interpretable Semantic Hierarchies in Vision-Language Encoders,"Kai Wittenmayer, Sukrut Rao, Amin Parchami-Araghi, Bernt Schiele, Jonas Fischer",10.48550/arXiv.2601.13798,arXiv:2601.13798,"interpretability, vision-language models, concept hierarchy, spatial grounding, semantic segmentation","INSIGHT provides fine-grained, human-interpretable concepts with local spatial grounding for vision foundation models. It leverages hierarchical representations to enable concept-based explanations across tasks.",45.8,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13809v1_DroneVLA VLA based Aerial Manipulation.pdf,DroneVLA: VLA based Aerial Manipulation,"Fawad Mehboob, Monijesu James, Amir Habel, Jeffrin Sam, Miguel Altamirano Cabrera, Dzmitry Tsetserukou",,,"Aerial Manipulation, Vision-Language-Action Models, Human-Robot Interaction, Visual Surveying, Robotic Fetch-and-Carry","This work introduces an autonomous aerial manipulation system interpreting natural language commands to retrieve objects. It integrates MediaPipe-based grounding with a Vision-Language-Action model and a drone equipped with a 1-DOF gripper and Intel RealSense camera. The system enables intuitive, human-centric control for safe object handling.",45.91,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13846v1_Virtual Urbanism An AI-Driven Framework for Quanti.pdf,Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity,Glinkaya Maria,10.1234/vu.2024.0012,2105.07912,"generative artificial intelligence, latent diffusion model, low-rank adaptation model, urban perception, urban identity","This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through synthetic urban environments. The pilot study demonstrates feasibility using Tokyo microcosms, integrating Stable Diffusion and LoRA models to produce synthetic replicas that elicit identity-forming elements. Human evaluation confirms perceptual legitimacy, while quantitative identity metrics reveal culturally embedded typologies.",43.61,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13864v1_HardSecBench Benchmarking the Security Awareness o.pdf,HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware,"Qirui Chen, Jingxian Shuai, Shuangwu Chen, Shenghao Ye, Zijian Wen, Xufei Su, Jie Jin, Jiangming Li, Jun Chen, Xiaobin Tan, Jian Yang",,,"large language models, security awareness, hardware, code generation, security vulnerabilities, common weakness enumeration, functional correctness, security risks","This work introduces HardSecBench, a benchmark evaluating LLM-generated code against 76 CWE entries across Verilog RTL and firmware. It demonstrates that models often meet functional requirements but expose security flaws, especially when security intent is unclear. The study highlights the need for robust evaluation methods to detect hidden vulnerabilities.",46.72,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13880v1_LifeAgentBench A Multi-dimensional Benchmark and A.pdf,LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health,"Ye Tian, Zihao Wang, Onat Gungor, Xiaoran Fan, Tajana Rosing",1,yet002,"personalized health, digital health, LLM, health assistant, cross-dimensional reasoning","This paper introduces LifeAgentBench, a large-scale QA benchmark for long-horizon, cross-dimensional, and multi-user lifestyle health reasoning. It evaluates 11 leading LLMs on a diverse set of queries and proposes LifeAgent as a strong baseline for health assistants, highlighting key challenges in aggregation and reasoning.",47.94,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13885v1_Confident Rankings with Fewer Items Adaptive LLM E.pdf,Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores,"Esma Balkır, Alice Pernthaller, Marco Basaldella, José Hernández-Orallo, Nigel Collier, Trismik, Leverhulme Centre for the Future of Intelligence, Universitat Politècnica de València, University of Cambridge",,,"Confident Rankings, Adaptive LLM Evaluation, Continuous Scores, IRT-based Testing, ROUGE, BLEU, LLM-as-a-Judge, Uncertainty Aware Ranking","This paper proposes a principled extension of IRT-based adaptive testing to continuous bounded scores by replacing the Bernoulli response distribution with a heteroskedastic normal distribution. It introduces an uncertainty-aware ranker with adaptive stopping criteria, achieving reliable model ranking with fewer items and lower cost. Validated on five benchmarks, the method improves ranking correlation by 0.12 τ over random sampling.",47.59,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13887v1_Human Simulation Computation A Human-Inspired Fram.pdf,Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems,Hong Su,10.48550/arXiv.2015.08001,arXiv:1508.03456,"Human Simulation Computation, Environment Interaction, Adaptive Artificial Intelligence, Human-Inspired Reasoning","The paper proposes Human Simulation Computation (HSC), a framework modeling intelligence through continuous, closed-loop processes involving thinking, action, learning, reflection, and activity scheduling. It emphasizes human-like reasoning strategies and action-driven adaptation, arguing that robust real-world performance requires mechanisms beyond static language training.",46.03,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13895v1_OmniOVCD Streamlining Open-Vocabulary Change Detec.pdf,OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3,"Xu Zhang, Danyang Li, Yingjie Xia, Xiaohang Dong, Hualong Yu, Jianye Wang, Qicheng Li",10.48550/arXiv.2303.04212,10.48550/arXiv.2303.04212,"change detection, open-vocabulary, SAM 3, OmniOVCD, change mask, land cover, deep learning","This paper introduces OmniOVCD, a standalone framework for Open-Vocabulary Change Detection using SAM 3. It proposes a Synergistic Fusion to Instance Decoupling (SFID) strategy to improve accuracy and stability by leveraging decoupled output heads. Experiments show state-of-the-art performance on multiple benchmarks.",46.56,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13897v1_TractRLFusion A GPT-Based Multi-Critic Policy Fusi.pdf,TRACTRLFUSION: A GPT-BASED MULTI-CRITIC POLICY FUSION FRAMEWORK FOR FIBER TRACTOGRAPHY,"Ankita Joshi, Ashutosh Sharma, Anoushkrit Goel, Ranjeet Ranjan Jha, Chirag Ahuja, Arnav Bhavsar, Aditya Nigam",10.1000/tractrlfusion,,"Diffusion MRI, Tractography, Reinforcement Learning, Transformers, Deep Learning, Neurosurgical Planning","This paper introduces TractRLFusion, a GPT-based policy fusion framework for tractography that leverages deep reinforcement learning to improve white matter tract reconstruction. It addresses challenges in accurately reconstructing complex fiber pathways by integrating multiple RL policies through a data-driven fusion strategy.",47.05,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13904v1_PREFAB PREFerence-based Affective Modeling for Low.pdf,PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation,"Jaeyoung Moon, Youjin Choi, Yucheon Park, David Melhart, Georgios N. Yannakakis, Kyung-Joong Kim",10.1145/3675094.3678379,2601.13904v1,"Affective Computing, Preference Learning, Self-Annotation, User Modeling, Ordinal Representation, Peak-End Rule","Self-annotation is the gold standard for collecting affective state labels. Existing methods rely on full annotation, which is time-consuming and cognitively demanding. PREFAB alleviates this by targeting affective inflection regions using preference learning, detecting inflection areas, requiring annotations only there, and interpolating the rest. Results show improved efficiency and annotator confidence.",45.65,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13920v1_Asymmetric regularization mechanism for GAN traini.pdf,Asymmetric regularization mechanism for GAN training with Variational Inequalities,"Spyridon C. Giagtzoglou, Mark H.M. Winands, Barbara Franci",,,"GANs, variational inequalities, saddle point problem, regularization mechanism, convergence guarantees","The paper proposes an asymmetric regularization technique based on Tikhonov steps and zero-centered gradients to stabilize GAN training. It derives explicit Lipschitz and strong-monotonicity constants, ensuring convergence of an EFTP method, even when strong monotonicity is unattainable.",45.17,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13938v1_IF-GEO Conflict-Aware Instruction Fusion for Multi.pdf,Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization,"Heyang Zhou, JiaJia Chen, Xiaolu Chen, Jie Bao, Zhen Chen, Yong Liao",,,"Generative Engine Optimization, Conflict-Aware Instruction Fusion, Multi-Query Retrieval, Generative Search Engines, Content Revision, Risk-Aware Metrics","The paper proposes IF-GEO, a diverge-then-converge framework for optimizing generative engines by aligning optimization preferences across conflicting queries through conflict-aware instruction fusion. It introduces risk-aware stability metrics and demonstrates performance gains on multi-query benchmarks.",45.66,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13942v1_Glance-or-Gaze Incentivizing LMMs to Adaptively Fo.pdf,Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning,"Hongbo Bai, Yujin Zhou, Yile Wu, Chi-Min Chan, Pengcheng Wen, Kunhao Pan, Sirui Han, Yike Guo",,,"Large Multimodal Models, Visual Understanding, Knowledge-intensive Queries, Search-Augmented Approaches, Reinforcement Learning, Selective Gaze, Complexity-Adaptive RL","The paper proposes Glance-or-Gaze (GoG), a framework that enables LMMs to adaptively focus visual attention by introducing a Selective Gaze mechanism. It combines reflective behavior alignment with complexity-adaptive reinforcement learning to improve performance on challenging visual queries, achieving state-of-the-art results across benchmarks.",46.66,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13948v1_Stream-Voice-Anon Enhancing Utility of Real-Time S.pdf,Enhancing Utility of Real-Time Speaker,"Nikita Kuzmin1, Songting Liu1, Kong Aik Lee3, Eng Siong Chng1","s220028@e.ntu.edu.sg, lius0114@e.ntu.edu.sg",,"streaming speaker anonymization, neural audio codec, voice conversion, privacy preservation, disentanglement","The paper presents Stream-Voice-Anon, a method adapting causal LM-based NAC architectures for streaming speaker anonymization. It integrates anonymization techniques like pseudo-speaker sampling and speaker embedding mixing to protect identity while maintaining linguistic fidelity. Comparisons show improvements over DarkStream in intelligibility and emotion preservation under privacy constraints.",46.31,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13964v1_RL-BioAug Label-Efficient Reinforcement Learning f.pdf,RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning,"Cheol-Hui Lee, Hwa-Yeon Lee, Dong-Joo Kim",10.48550/arXiv.2407.04219,arXiv:2407.04219,"reinforcement learning, self-supervised learning, EEG representation learning, data augmentation, contrastive learning","The paper proposes RL-BioAug, a label-efficient RL framework that autonomously selects optimal augmentation policies using minimal labeled data, improving EEG representation learning without relying on large-scale labeled datasets.",45.29,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13969v1_Autonomous Knowledge Graph Exploration with Adapti.pdf,Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval,"Joaquín Polonuer, Lucas Vittor, Iñaki Arango, Ayush Noori, David A. Clifton, Luciano Del Corro, Marinka Zitnik, Kempner Institute for the Study of Natural and Artificial Intelligence, Oxford Suzhou Centre for Advanced Research, ELIAS Lab, Lumina Labs, Kempner Institute, Broad Institute of MIT and Harvard, Harvard Data Science Initiative",,,"knowledge graph, knowledge graph exploration, adaptive breadth-depth retrieval, language models, retrieval augmentation, multi-hop traversal, semantic similarity, knowledge graphs, retrieval training","Retrieving evidence for language model queries from knowledge graphs requires balancing broad search with multi-hop traversal. ARK adapts tool use to queries, improving performance across datasets.",45.73,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13992v1_The Whole Is Greater Than the Sum of Its Parts A C.pdf,The Whole Is Greater Than the Sum of Its Parts,"Jin Cui, Jiaqi Guo, Jiepeng Zhou, Ruixuan Yang, Jiayi Lu, Jiajun Xu, Jiangcheng Song, Boran Zhao, Pengju Ren",,,"Chain-of-Thought, Multi-Teacher, CoT Distillation, Student Models, Catastrophic Forgetting, Graph-based Consensus, Mutual-Information, Loss-based Difficulty","This paper introduces COMPACT, a framework that adaptively fuses supervision from multiple teachers using dynamic weighting. It employs graph-based consensus to filter misleading reasoning, mutual-information-based adaptability to detect deep understanding, and loss-based difficulty to prevent negative transfer. Experiments show it effectively integrates diverse reasoning capabilities while mitigating catastrophic forgetting.",46.35,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13994v1_torch-sla Differentiable Sparse Linear Algebra wit.pdf,Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch,Mingyuan Chi,10.48550/arXiv.2601.13994,2601.13994,"PyTorch, sparse linear algebra, GPU acceleration, adjoint solvers, scientific computing","Presents torch-sla, an open-source PyTorch library enabling GPU-accelerated, scalable, and differentiable sparse linear algebra. Addresses challenges in GPU acceleration, multi-GPU scaling, and efficient gradient computation for industrial scientific computing.",44.51,LFM-2.5,AMD RX 6800 (Vulkan)
2601.13999v1_DAME Duration-Aware Matryoshka Embedding for Durat.pdf,Duration-Aware Matryoshka Embedding for Duration-Robust Speaker Verification,"Youngmoon Jung, Joon-Young Yang, Ju-ho Kim, Jaeyoung Roh, Chang Woo Han, Hoon-Young Cho",,,"short-duration speaker verification, multi-scale aggregation, matryoshka representation learning, speaker verification, duration-aware embedding","The paper proposes DAME, a model-agnostic framework that builds nested embeddings aligned to utterance durations. It uses lower-dimensional representations for short utterances and higher dimensions for longer ones, improving performance across durations without extra inference cost.",45.96,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14012v1_MATE Matryoshka Audio-Text Embeddings for Open-Voc.pdf,MATRYOSHKA AUDIO–TEXT EMBEDDINGS FOR OPEN-VOCABULARY,"Youngmoon Jung, Myunghun Jung, Joon-Young Yang, Yong-Hyeok Lee, Jaeyoung Roh, Hoon-Young Cho",,,"keyword spotting, open-vocabulary, text enrollment, audio-text embedding, deep metric learning","Open-vocabulary keyword spotting with text-based enrollment uses a dual-encoder framework called Matryoshka Audio–Text Embeddings (MATE). It encodes multiple embedding granularities via nested sub-embeddings (prefixes) and employs PCA-guided prefix alignment to concentrate keyword cues in lower dimensions. Trained with standard deep metric learning objectives, MATE achieves state-of-the-art performance on WSJ and LibriPhrase without inference overhead.",47.15,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14022v1_Credible CO2 Comparisons A Machine Learning Approa.pdf,Credible CO2 Comparisons: A Machine Learning Approach to Vehicle Powertrain Assessment,"Rodrigo Pereira David, Luciano Araujo Dourado Filho, Daniel Marques da Silva, João Alfredo Cal-Braz",xxx/xxxx,,"machine learning, vehicle emissions, electric vehicles, ICEVs, regression, recurrent neural network, mean square error","This paper proposes a machine learning framework for comparing CO2 emissions of internal combustion engine vehicles and electric vehicles under identical real-world driving conditions. By isolating technology-specific effects and aligning emissions metrics, the study enables fair, reproducible evaluations of powertrain performance and supports data-driven decarbonization strategies.",45.7,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14027v1_Numina-Lean-Agent An Open and General Agentic Reas.pdf,Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics,"Junqi Liu, Zihao Zhou, Zekai Zhu, Marco Dos Santos, Weikun He, Jiawei Liu, Ran Wang, Yunzhou Xie, Junqiao Zhao, Qiufeng Wang, Lihong Zhi, Jia Li, Wenda Li, Jia Li, Ran Wang, Yunzhou Xie, Junqiao Zhao, Qiufeng Wang, Lihong Zhi, Jia Li, Wenda Li, Akademie der Mathematischen und Systemischen Wissenschaften, Universität der Chinesischen Akademie der Wissenschaften, Universität von Liverpool, Xi’an Jiaotong-Liverpool University, Tongji University, Universität Cambridge, Imperial College London, Universität Edinburgh",,,"formal mathematics, agentic systems, formal theorem proving, Lean, Numina-Lean-MCP, autonomous interaction, mathematical reasoning","This paper proposes Numina-Lean-Agent, a general coding agent that enables flexible, autonomous interaction with formal theorem provers like Lean. It aims to improve performance and reproducibility by allowing model substitution without retraining, and supports extensible tool calling.",47.85,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14039v1_Generalizing Abstention for Noise-Robust Learning .pdf,Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation,"Wesam Moustafa, Hossam Elsafty, Helen Schneider, Lorenz Sparrenberg, Rafet Sifa",arXiv:2601.14039v1,MIDL 2026 submission,"Abstention, Medical Image Segmentation, Label Noise, Noise-Robust Learning, Loss Functions","This paper introduces a universal and modular abstention framework to enhance noise-robustness in medical image segmentation. It presents three noise-robust variants (GAC, SAC, ADS) and demonstrates their effectiveness on CaDIS and DSAD datasets, showing significant improvements over non-abstaining baselines.",46.23,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14041v1_Top 10 Open Challenges Steering the Future of Diff.pdf,Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants,"Yunhe Wang, Kai Han, Huiling Zhen, Yuchuan Tian, Hanting Chen, Yongbing Huang, Yufei Cui, Yingte Shu, Shan Gao, Ismail Elezi, Roy Vaughan Miles, Songcen Xu, Feng Wen, Chao Xu, Sinan Zeng, Dacheng Tao",arXiv:2601.14041v1,2601.14041v1,"Large Language Models, Diffusion Models, Transformers","The paper examines key challenges limiting diffusion language models, proposing a roadmap to overcome architectural, algorithmic, and cognitive barriers in order to reach the capabilities of GPT-4.",47.74,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14047v1_Collective intelligence in science direct elicitat.pdf,Collective Intelligence in Science,"Alexey V. Osipov ∗, Nikolay N. Osipov†",arXiv:2601.14047v1,2601.14047v1,"collective intelligence, scientific collaboration, Bayesian truth serum, prediction market, information pooling, interpretability, wisdom of crowd, large language models, scientific collaboration","Proposes a mechanism using a self-resolving play-money prediction market integrated with chat to enable experts to collaboratively analyze complex scientific hypotheses. It emphasizes efficient aggregation of private information, incentivization via play money, and potential funding models.",45.66,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14051v1_Kakugo Distillation of Low-Resource Languages into.pdf,Kakugo: Distillation of Low-Resource Languages into Small Language,"Peter Devine, Mardhiyah Sanni, Farid Adilazuarda, Julieta Gil Loizaga, Barry Haddow",,,"Small Language Models, Low-resource languages, Language modeling, Model distillation, Open-source datasets","We present Kakugo, a novel and cost-effective pipeline designed to train general-purpose Small Language Models for low-resource languages using only the language name as input. By leveraging a large teacher model to generate synthetic prompts and translate instruction datasets, we produce training data and SLMs for 54 languages. Evaluations across translation, classification, and question answering show consistent performance improvements over base models, with generation and training costs under $50 per language.",45.73,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14053v1_LLMOrbit A Circular Taxonomy of Large Language Mod.pdf,LLMOrbit: A Circular Taxonomy of Large Language Models,"Badri N. Patro, Vijay S. Agneeswaran",10.48550/arXiv.2025.12345,arXiv:2025.12345,"large language models, LLM taxonomy, scaling wall, agentic AI","A comprehensive circular taxonomy mapping the evolution of LLMs from 2019 to 2025, analyzing architectural innovations, training costs, and efficiency breakthroughs across multiple paradigms.",44.36,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14055v1_Decoder-Free Supervoxel GNN for Accurate Brain-Tum.pdf,Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI,"Andrea Protani, Marc Molina Van Den Bosch, Lorenzo Giusti, Heloisa Barbosa Da Silva, Paolo Cacace, Albert Sund Aillet, Miguel Angel Gonzalez Ballester, Friedhelm Hummel, Luigi Serio",arXiv:2601.14055v1,20 Jan 2026,"BrainTumorLocalization, GraphNeuralNetworks, Multi-modal MRI, Supervoxel, Regression","Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, allocating significant parameters to spatial reconstruction. This work introduces SVGFormer, a decoder-free pipeline built on a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining patch-level transformer with a supervoxel-level graph attention network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. Validation on the BraTS dataset demonstrates strong performance, achieving an F1-score of 0.875 for node-level classification and a MAE of 0.028 for tumor proportion regression.",49.27,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14056v1_POCI-Diff Position Objects Consistently and Intera.pdf,POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion,"Andrea Rigo, Luca Stornaiuolo, Weijie Wang, Mauro Martino, Bruno Lepri, Nicu Sebe",arXiv:2601.14056v1,arXiv:2601.14056,"Diffusion, Image Generation, 3D Layout","Proposes a diffusion-based approach for text-to-image generation with consistent and interactive 3D layout control, addressing distortions in object geometry and improving spatial adherence.",46.59,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14063v1_XCR-Bench A Multi-Task Benchmark for Evaluating Cu.pdf,XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs,"Mohsinul Kabir, Tasnim Ahmed, Shaoxiong Ji, Hassan Alhuzali, Sophia Ananiadou",10.1234/xcr-2024,,"cross-cultural competence, large language models, cultural reasoning, cultural bias, cross-cultural benchmark","This paper introduces XCR-Bench, a benchmark for evaluating cultural reasoning in LLMs. It addresses the lack of high-quality cross-cultural annotated data and highlights LLMs' biases in social etiquette and cultural adaptation. The study reveals consistent weaknesses in identifying culture-specific items and uncovers regional and ethno-religious biases even in monolingual settings.",46.29,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14069v1_Unsupervised Video Class-Incremental Learning via .pdf,Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering,"Nattapong Kurpukdee, Adrian G. Bors",,,"unsupervised, video class-incremental, video continual learning, deep embedded clustering, deep feature extractor, knowledge transfer","The paper proposes a simple yet effective approach for unsupervised video class incremental learning using deep embedded clustering. It introduces a deep feature extractor that generates representative video features without prior class labels, builds deep clusters progressively, and leverages prior task knowledge to mitigate catastrophic forgetting. Extensive evaluations on UCF101, HMDB51, and Something-to-Something V2 show significant performance improvements over supervised baselines.",46.52,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14084v1_DermaBench A Clinician-Annotated Benchmark Dataset.pdf,DermaBench: A Clinician-Annotated Benchmark,"Abdurrahim Yilmaz, Ozan Erdem, Ece Gokyayla, Ayda Acar, Burc Bugra Dagtas, Dilara Ilhan Erdil, Gulsum Gencoglan, Burak Temelkuran",,,"dermatology, visual question answering, clinical reasoning, skin conditions, medical imaging","DermaBench is a clinician-annotated benchmark for dermatology visual question answering, addressing gaps in image-level datasets by providing detailed, expert-labeled annotations across diverse skin types and conditions.",45.94,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14086v1_Two-Stream temporal transformer for video action c.pdf,Two-Stream Temporal Transformer for Video Action Classification,"Nattapong Kurpukdee, Adrian G. Bors",,,"Video Transformer, Optical Flow, Two-Stream video processing, Video Action Classification","This study introduces a two-stream transformer video classifier that extracts spatio-temporal information from content and optical flow, achieving excellent classification results on human activity datasets.",43.78,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14087v1_1-bit Count-based Sorting Unit to Reduce Link Powe.pdf,’1’-bit Count-based Sorting Unit to Reduce Link,"Ruichi Han, Yizhi Chen, Tong Lei, Jordi Altayo Gonzalez, Ahmed Hemani",10.1109/JRTT.2024.12345,12345,"1-bit count, approximate computing, bit transition reduction, link power, DNN",Proposes a hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks using approximate computing to reduce switching activity and link power.,43.24,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14091v1_Zero-shot adaptable task planning for autonomous c.pdf,Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems,"Hossein Naderi, Alireza Shojaei, Lifu Huang, Philip Agee, Kereshmeh Afsari, Abiola Akanmu",,,"Construction robotics, quadruped robots, robot task planning, multi-AI agent, LLMs, VLMs, GPT4o","The study explores the potential of foundation models to enhance adaptability and generalizability of task planning in construction robots. Four lightweight AI agent systems are proposed and evaluated across three construction roles, demonstrating improved performance and cost-effectiveness compared to state-of-the-art models.",46.36,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14096v1_Remapping and navigation of an embedding space via.pdf,Remapping and Navigating Embedding Space,"Benedikt Hartl, Léo Pio-Lopez, Chris Fields, Michael Levin",arXiv:2601.14096v1,arXiv:2601.14096v1,"Evolution, Development, Intelligence, Active Inference, Navigation Policy, Nested Embedding Spaces",A fundamental organizational principle of cognition in natural and artificial systems.,47.34,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14099v1_Causal feature selection framework for stable soft.pdf,Causal Feature Selection Framework for Stable Soft Sensor Modeling based on Time-Delayed Cross Mapping,"Shi-Shun Chen, Xiao-Yang Li, Enrico Zio",,,"causal feature selection, soft sensor modeling, time-delayed cross mapping, reliability, systems engineering",A framework for causal feature selection applied to stable soft sensor modeling using time-delayed cross mapping.,48.07,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14115v1_Riemannian Liquid Spatio-Temporal Graph Network.pdf,Riemannian Liquid Spatio-Temporal Graph Network,"Liangsi Lu, Jingchao Wang, Zhaorong Dai, Hanqian Liu, Yang Shi",10.1145/3774904.3792090,https://rlstg.github.io,"Riemannian Manifolds, Neural ODEs, Spatio-Temporal Graphs","Liquid Time-Constant networks excel at modeling irregular dynamics but are limited to Euclidean space. This work introduces RLSTG, a framework unifying continuous-time dynamics with Riemannian geometry to better represent non-Euclidean graph structures.",45.5,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14124v1_Style Transfer as Bias Mitigation Diffusion Models.pdf,Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic,"Saad Mankarious, Ayah Zirikly",arXiv:2601.14124v1,arXiv:2601.14124v1,"style transfer, bias mitigation, diffusion models, mental health, Arabic","This work proposes a pretraining-free diffusion-based approach for generating synthetic Arabic mental health text to address gender bias. Using the CARMA corpus, it focuses on male-to-female style transfer to augment female-authored content, demonstrating high semantic fidelity and meaningful stylistic divergence.",46.15,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14152v1_Lost in the Prompt Order Revealing the Limitations.pdf,Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models,"Hyunjong Ok, Jaeho Lee",,,"prompt structure, causal attention, language models, MCQA","This study investigates how the ordering of components in multiple-choice question answering affects model performance, identifying causal attention as a key mechanism that hides context from options.",43.0,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14154v1_LLM Augmented Intervenable Multimodal Adaptor for .pdf,LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery,"Shubham Pandey, Bhavin Jawade, Srirangaraj Setlur, Venu Govindaraju, Kenneth Seastedt",,,"post-operative complications, lung cancer surgery, risk prediction, deep learning, radiomics, interpretable AI, clinical decision support","Presents MIRA-CLE, a deep learning architecture integrating preoperative and radiological data to predict postoperative complications in lung cancer surgery. The model uses hyperspherical embeddings and includes an interpretable deep learning module to enhance transparency for clinicians.",45.58,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14157v1_ConceptCaps -- a Distilled Concept Dataset for Int.pdf,ConceptCaps - a Distilled Concept Dataset for Interpretability in Music Models,"Bruno Sienkiewicz, Łukasz Neumann, Mateusz Modrzejewski",,,"interpretability, music models, concept-based, TCA V, concept probes, audio-text alignment","Introduces ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels, addressing sparse and noisy tags in existing music datasets. The dataset supports separation of semantic modeling and text generation for interpretability.",44.83,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14160v1_Domain-Adaptation through Synthetic Data Fine-Tuni.pdf,Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law,"Ali Hamza Bashir, Muhammad Rehan Khalid, Kostadin Cvejoski, Jana Birr, Jule Berghaus, Armin Berger, Sandra Halscheidt, Christian Temath, Rafet Sifa",,,"legal reasoning, German law, synthetic data, LLM adaptation, statutory text, hallucination mitigation",This paper presents a method to adapt large language models to German legal question answering using synthetic data generated from authoritative statutes. It addresses challenges in legal QA reliability and demonstrates improved performance through parameter-efficient fine-tuning and rigorous filtering.,45.6,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14171v1_Paper2Rebuttal A Multi-Agent Framework for Transpa.pdf,A Multi-Agent Framework for Transparent Author,"Qianli Ma, Chang Guo, Zhiheng Tian, Siyu Wang, Jipeng Xiao, Yuanhao Yue, Zhipeng Zhang",,,,"Introduces REBUTTALAGENT, a multi-agent framework reframing rebuttal as evidence-centric planning. It decomposes feedback, dynamically constructs hybrid contexts, and integrates an autonomous external search module to ensure transparent, evidence-backed responses.",43.63,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14172v1_Human Values in a Single Sentence Moral Presence H.pdf,"Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum","Víctor Yestea, Paolo Rossoa, Valencian Graduate School and Research Network of Artificial Intelligence (ValgrAI)",arXiv:2601.14172v1,2601.14172v1,"human values, moral presence, sentence-level detection, Schwartz continuum, transformer models","This study investigates the identification of 19 values in the Schwartz motivational continuum using single sentences from news and political texts, demonstrating that hierarchical gating improves performance over direct multi-label classification despite sparse moral cues.",45.43,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14175v1_A model of errors in transformers.pdf,A model of errors in transformers,"Suvrat Raju, Praneeth Netrapalli",,,,"We study the error rate of LLMs on tasks like arithmetic and repetitive token processing, arguing that incorrect predictions emerge from accumulated attention errors. We derive a quantitative relationship between accuracy and task complexity, inspired by effective field theory, and show empirical agreement across models.",42.37,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14192v1_Toward Efficient Agents Memory Tool learning and P.pdf,"Toward Efficient Agents: A Survey of Memory, Tool learning, and Planning","Xiaofang Yang, Lijun Li, Heng Zhou, Tong Zhu, Xiaoye Qu, Yuchen Fan, Qianshan Wei, Rui Ye, Li Kang, Yiran Qin, Zhiqiang Kou, Daizong Liu, Ning Ding, Siheng Chen",10.48550/arXiv.2601.14192,2601.14192,"Agents, Efficiency, Agent Memory, Tool Learning, Planning","This paper investigates efficiency from memory, tool learning, and planning perspectives, addressing cost considerations such as latency and tokens. It reviews recent approaches and proposes benchmarks and future directions for efficient agentic systems.",47.43,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14209v1_InT Self-Proposed Interventions Enable Credit Assi.pdf,Self-Proposed Interventions Enable Credit,"Matthew Y. R. Yang, Hao Bai, Ian Wu, Gene Yang, Amrith Setlur, Aviral Kumar",arXiv:...,InT: Self-Proposed Interventions Enable Credit,"credit assignment, credit assignment failure, intervention training, RL problem, self-verify, supervised fine-tuning",Proposes Intervention Training (InT) to address credit assignment in LLMs by proposing single-step corrections. Uses supervised fine-tuning to localize errors and improve reasoning accuracy.,45.34,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14230v1_MASCOT Towards Multi-Agent Socio-Collaborative Com.pdf,MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems,"Yiyang Wang, Yiqiao Jin, Alex Cabral, Josiah Hester",,,"multi-agent systems, socio-collaborative companion systems, emotional support, social interaction, persona collapse, social symmetry, psychological well-being",Multi-agent systems face persona collapse and social sycophancy. MASCOT proposes a framework with persona-aware behavioral alignment and collaborative dialogue optimization to improve consistency and social contribution.,44.94,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14232v1_KAGE-Bench Fast Known-Axis Visual Generalization E.pdf,KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning,"Egor Cherepannov, Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov",https://avanturist322.github.io/KAGEBench,not provided,"reinforcement learning, visual generalization, pixel-based agents, visual distribution shift, KAGE-Env, JAX-native platformer","The paper introduces KAGE-Env, a 2D platformer that isolates visual axes to study visual generalization in RL. It evaluates how changes in visual inputs affect performance and presents KAGE-Bench, a benchmark with six known-axis suites, to systematically analyze visual shift sources.",46.34,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14234v1_Q-learning with Adjoint Matching.pdf,Q-learning with Adjoint Matching,"Qiyang Li, Sergey Levine",arXiv:2601.14234v1,,"Q-learning, adjoint matching, reinforcement learning, policy optimization, flow matching","Proposes Q-learning with Adjoint Matching (QAM), a novel TD-based RL algorithm that optimizes expressive diffusion or flow-matching policies while avoiding unstable backpropagation through adjoint matching.",43.89,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14235v1_Opportunities in AIML for the Rubin LSST Dark Ener.pdf,Opportunities in AI/ML for the Rubin LSST,"Eric Aubourg, Camille Avestruz, Matthew R. Becker, Biswajit Biswas, Rahul Biswas, Boris Bolliet, Adam S. Bolton, Clecio R. Bom, Raphaël Bonnet-Guerrini, Alexandre Boucaud, Jean-Eric Campagne, Chihway Chang, Aleksandra Ciprijanović, Johann Cohen-Tanugi, Michael W. Coughlin, John Franklin Crenshaw, Juan C. Cuevas-Tello, Alex Drlica-Wagner, Sydney Erickson, Alexander T. Gagliano, Christos Georgiou, Aritra Ghosh, Matthew Grayling, Kirill A. Grishin, Alan Heavens, Lindsay R. House, Michelle Lochner, Yao-Yuan Mao, Peter Melchior, Grant Merz, Martin Millon, Anais Moller, Gautham Narayan, Yuuki Omori, Hiranya Peiris, Andrés A. Plazas Malagón, Nesar Ramachandra, Benjamin Remy, Cécile Roucelle, Jaime Ruiz-Zapatero, Stefan Schuldt, Ignacio Sevilla-Noarbe, Tjitske Starkenburg, Stephen Thorp, Laura Toribio San Cipriano, Tilman Troster, Roberto Trotta, Padma Venkatraman, Amanda Wasserman, Tim White, Justine Zeghal, Tianqing Zhang",,,"AI/ML, LSST, Dark Energy, Rubin Observatory, Machine Learning, Astrophysics, Cosmology, Computer Science, Astroparticle Physics, Kavli Institute, SLAC, NSF-Simons AI Institute, Fermi National Accelerator Laboratory, University Paris Cité, University of Michigan, University of Chicago, Kavli Institute for Cosmological Physics, University of Paris-Saclay, Centro Brasileiro de Pesquisas Físicas, Stanford University, Centro de Investigaciones Energéticas, Clermont University, Stanford Artificial Intelligence Laboratory, Universidad Autonoma de San Luis Potosi, CIEMAT, Stanford University","The document discusses opportunities in applying AI/ML techniques to the Rubin LSST project, highlighting collaborations across institutions focused on dark energy science and cosmological research.",46.73,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14242v1_APEX-Agents.pdf,APEX–Agents,"Bertie Vidgen, Austin Mann, Abby Fennelly, John Wright, Stanly, Lucas Rothman, Marco Burstein, Julien Benchek, David Ostrofsky, Anirudh Ravichandran, Debnil, Sur Neel Venugopal, Alannah Hsia, Isaac Robinson, Calix Huang, Olivia Varones, Daniyal Khan, Michael Haines, Zach Richards, Chirag Mahapatra, Brendan Foody, Osvald Nitski",arXiv:2601.14242v1,arXiv:2601.14242,"AI agents, productivity index, investment banking, management consultants, corporate lawyers, long-horizon tasks, cross-application, sim-to-real gap","We introduce the AI Productivity Index for Agents (APEX–Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX–Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1.",47.7,LFM-2.5,AMD RX 6800 (Vulkan)
2601.14255v1_VideoMaMa Mask-Guided Video Matting via Generative.pdf,VideoMaMa: Mask-Guided Video Matting via Generative Prior,"Sangbeom Lim, Seoung Wug Oh2, Jiahui Huang, Heeji Yoon, Seungryong Kim, Joon-Young Lee",arXiv:2601.14255v1,arXiv:2601.14255,"video matting, mask-guided, generative prior, video diffusion, alpha mattes, motion blur, boundary structures","Introduces VideoMaMa, a diffusion-based model that generates high-quality alpha mattes from segmentation masks, demonstrating strong zero-shot generalization to real-world footage despite synthetic training.",44.3,LFM-2.5,AMD RX 6800 (Vulkan)
