filename,title,authors,doi,arxiv_id,keywords,summary,tps,model,platform
2601.07192v1_Relink Constructing Query-Driven Evidence Graph On.pdf,Relink: Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG,"Manzong Huang, Chenyang Bu, Yi He, Xingrui Zhuo, Xindong Wu",Not found,Not found,"Graph-based Retrieval-Augmented Generation, Large Language Models, Knowledge Graph, Query-Driven Evidence Graph, Dynamic Construction","Graph-based Retrieval-Augmented Generation (GraphRAG) mitigates hallucinations in Large Language Models (LLMs) by grounding them in structured knowledge. However, current GraphRAG methods are constrained by a static, pre-constructed Knowledge Graph (KG) paradigm, which faces challenges of incomplete paths and misleading facts. Relink proposes a reason-and-construct paradigm and a framework that dynamically builds a query-specific evidence graph, instantiating required facts from a latent relation pool and employing a unified, query-aware evaluation strategy to handle incompleteness and misleading facts.",11.91,Qwen2.5-3B,Apple M1 (Metal)
2601.07197v1_Beyond Variance Knowledge-Aware LLM Compression vi.pdf,Beyond Variance: Knowledge-Aware LLM Compression via Fisher-Aligned Subspace Diagnostics,"Ibne Farabi Shihab*, Sanjeda Akter*, Anuj Sharma",Not found,Not found,"Large Language Models, Knowledge Compression, Fisher Information Matrix, Transformer Architectures, Activation Gradient Coupling","Post-training activation compression is essential for deploying Large Language Models on resource-constrained hardware. Standard methods like Singular Value Decomposition (SVD) are gradient-blind, preserving high-variance dimensions regardless of their impact on factual knowledge preservation. Fisher-Aligned Subspace Compression (FASC) is introduced as a knowledge-aware framework that selects subspaces by directly modeling activation-gradient coupling, minimizing a second-order surrogate of the loss function. FASC leverages the Fisher Information Matrix to identify dimensions critical for factual knowledge, often residing in low-variance but high-gradient-sensitivity subspaces. The Dependence Violation Score (ρ) is proposed as a general-purpose diagnostic metric to quantify activation-gradient coupling, revealing where factual knowledge is stored within transformer architectures. Extensive experiments on Mistral-7B and Llama-3-8B demonstrate that FASC preserves 6-8% more accuracy on knowledge-intensive benchmarks (MMLU, LAMA) compared to variance-based methods at 50% rank reduction, effectively enabling a 7B model to match the factual recall of a 13B uncompressed model. Analysis reveals that ρ serves as a fundamental signal of stored knowledge, with high-ρ layers emerging only when models internalize factual associations during training.",12.97,Qwen2.5-3B,Apple M1 (Metal)
2601.07199v1_Forward versus Backward Comparing Reasoning Object.pdf,Forward versus Backward: Comparing Reasoning Objectives in Direct Preference Optimization,"Murtaza Nikazad, Raghuram Ramanujan",Not found,Not found,"Direct Preference Optimization, Reasoning, Model Confidence, Error Recognition","This paper investigates the effect of training objective composition on reasoning reliability through Direct Preference Optimization, comparing forward chain-of-thought generation and backward verification training signals.",11.81,Qwen2.5-3B,Apple M1 (Metal)
2601.07200v1_Safeguarding LLM Fine-tuning via Push-Pull Distrib.pdf,Safeguarding LLM Fine-tuning via Push-Pull Distributional Alignment,"Haozhong Wang, Zhuo Li, Yibo Yang, He Zhao, Hongyuan Zha, Dandan Guo",,,"Large Language Models, Fine-tuning, Safety Alignment, Optimal Transport, Push-Pull Mechanism, Data Distribution, Harmful Patterns","The paper introduces Safety Optimal Transport (SOT), a novel framework that reframes safe fine-tuning from an instance-level filtering challenge to a distribution-level alignment task grounded in Optimal Transport (OT). SOT optimizes sample importance by actively pulling the downstream distribution towards a trusted safe anchor while simultaneously pushing it away from a general harmful reference, establishing a robust geometric safety boundary that effectively purifies the training data. Extensive experiments demonstrate that SOT significantly enhances model safety while maintaining competitive downstream performance.",12.13,Qwen2.5-3B,Apple M1 (Metal)
2601.07201v1_CalPro Prior-Aware Evidential--Conformal Predictio.pdf,CalPro: Prior-Aware Evidential–Conformal Prediction with Structure-Aware Guarantees for Protein Structures,"Ibne Farabi Shihab * 1, Sanjeda Akter * 1, Anuj Sharma 2",Not found,Not found,"protein structure prediction, uncertainty quantification, conformal prediction, geometric evidential, domain priors","Deep protein structure predictors like AlphaFold provide non-calibrated confidence estimates that degrade under distribution shifts. CalPro introduces a prior-aware evidential-conformal framework for robust uncertainty quantification, combining geometric evidential outputs, differentiable conformal layers, and domain priors. Theoretical guarantees and empirical results show improved coverage and calibration.",11.73,Qwen2.5-3B,Apple M1 (Metal)
2601.07206v1_LLMRouterBench A Massive Benchmark and Unified Fra.pdf,LLMRouterBench: A Massive Benchmark and Unified Framework for LLM Routing,"Hao Li*, Yiqun Zhang*, Zhaoyan Guo*, Chenxu Wang*, Shengji Tang, Qiaosheng Zhang, Yang Chen, Biqing Qi, Peng Ye, Lei Bai, Zhen Wang†, Shuyue Hu†",Not found,Not found,"Large language models, routing, benchmark, performance-cost trade-off","LLMRouterBench is a large-scale benchmark and unified framework for LLM routing, comprising over 400K instances from 21 datasets and 33 models. It provides comprehensive metrics for both performance-oriented and performance-cost trade-off routing and integrates 10 representative routing baselines. The framework confirms strong model complementarity and finds that many routing methods exhibit similar performance under unified evaluation, with some recent approaches failing to reliably outperform a simple baseline. The benchmark also enables latency-aware analysis and highlights the limitations of backbone embedding models and the diminishing returns of larger ensembles compared to careful model curation.",12.35,Qwen2.5-3B,Apple M1 (Metal)
2601.07209v1_SIRR-LMM Single-image Reflection Removal via Large.pdf,SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model,"Yu Guo, Zhiqiang Lao, Xiyun Song, Yubin Zhou, Heather Yu",,,"reflection removal, single-image, large multimodal model, path tracing, 3D glass models, HDR environment maps","Introduces a synthetic dataset generation framework that path-traces 3D glass models over real background imagery to create physically accurate reflection scenarios. Combines image layers into a single composite input, applies joint captioning, and fine-tunes the model using task-specific LoRA rather than full-parameter training, achieving improved reflection removal and separation performance.",12.41,Qwen2.5-3B,Apple M1 (Metal)
2601.07214v1_BlindU Blind Machine Unlearning without Revealing .pdf,BlindU: Blind Machine Unlearning without Revealing Erasing Data,"Weiqi Wang, Zhiyi Tian, Chenhao Zhang, Shui Yu",Not found,Not found,"Machine Unlearning, Federated Learning, Privacy Leakage, Privacy Preserving, Information Bottleneck","This paper explores how to implement unlearning under the condition of not uncovering the erasing data to the server. It proposes Blind Unlearning (BlindU), which carries out unlearning using compressed representations instead of original inputs. BlindU only involves the server and the unlearning user, allowing users to generate privacy-preserving representations locally and perform unlearning on these representations and their labels. The paper also introduces a noise-free differential privacy (DP) masking method to further enhance privacy protection.",12.4,Qwen2.5-3B,Apple M1 (Metal)
2601.07224v1_Consolidation or Adaptation PRISM Disentangling SF.pdf,Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration,"Yang Zhao, Yangou Ouyang, Xiao Ding, Hepeng Wang, Bibo Cai, Jinglong Gao, Zhouhao Sun, Li Du, Bing Qin, Ting Liu",Not provided,Not provided,"Schema Theory, Hybrid Supervised Fine-Tuning, Reinforcement Learning, Data Arbitration, Gradient Concentration, Pattern Consolidation, Structural Adaptation","PRISM is a dynamics-aware framework that arbitrates data based on its degree of cognitive conflict with the model's existing knowledge. By analyzing the spatial geometric structure of gradients, PRISM identifies data triggering high spatial concentration as high-conflict signals that require RL for structural restructuring. In contrast, data yielding diffuse updates is routed to SFT for efficient consolidation. Extensive experiments on WebShop and ALFWorld demonstrate that PRISM achieves a Pareto improvement, outperforming state-of-the-art hybrid methods while reducing computational costs by up to 3.22 ×. The findings suggest that disentangling data based on internal optimization regimes is crucial for scalable and robust agent alignment.",13.11,Qwen2.5-3B,Apple M1 (Metal)
2601.07226v1_Lost in the Noise How Reasoning Models Fail with C.pdf,Lost in the Noise: How Reasoning Models Fail with Contextual Distractors,"Seongyun Lee, Yongrae Jo, Minju Seo, Moontae Lee, Minjoon Seo",,,"reasoning models, agentic AI, external tools, contextual distractors, robustness, misalignment, distractor tokens","Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent.",13.03,Qwen2.5-3B,Apple M1 (Metal)
2601.07229v2_DiSCo Making Absence Visible in Intelligent Summar.pdf,DiSCo: Making Absence Visible in Intelligent Summarization Interfaces,"ERAN FAINMAN, University of Haifa, Israel, HAGIT BEN SHOSHAN, University of Haifa, Israel, ADIR SOLOMON, University of Haifa, Israel, OSNAT MOKRYN, University of Haifa, Israel",,,"Review Summarization, Absence, Expectations, Learning via surprisability, Missing commonalities","Intelligent interfaces increasingly use large language models to summarize user-generated content, yet these summaries emphasize what is mentioned while overlooking what is missing. This presence bias can mislead users who rely on summaries to make decisions. We present Domain Informed Summarization through Contrast (DiSCo), an expectation-based computational approach that makes absences visible by comparing each entity’s content with domain topical expectations captured in reference distributions of aspects typically discussed in comparable accommodations. This comparison identifies aspects that are either unusually emphasized or missing relative to domain norms and integrates them into the generated text. In a user study across three accommodation domains, namely ski, beach, and city center, DiSCo summaries were rated as more detailed and useful for decision making than baseline large language model summaries, although slightly harder to read. The findings show that modeling expectations reduces presence bias and improves both transparency and decision support in intelligent summarization interfaces.",13.61,Qwen2.5-3B,Apple M1 (Metal)
2601.07232v1_Yes FLoReNce I Will Do Better Next Time Agentic Fe.pdf,Agentic Feedback Reasoning for Humorous Meme Detection,"Olivia Shanhong Liu, Pai Chet Ng, De Wen Soh, Konstantinos N. Plataniotis",Not found,Not found,"humorous memes, agentic feedback, closed-loop reasoning, open-loop inference, prompting, multimodal","This paper proposes FLoReNce, an agentic feedback reasoning framework for detecting meme humor. It treats meme understanding as a closed-loop process during learning and an open-loop process during inference, enabling better, self-aligned reasoning without finetuning. FLoReNce improves predictive performance and explanation quality over static multimodal baselines on the PrideMM dataset.",11.77,Qwen2.5-3B,Apple M1 (Metal)
2601.07233v1_From Thinking to Justifying Aligning High-Stakes E.pdf,From “Thinking” to “Justifying”: Aligning High-Stakes Explainability with Professional Communication Standards,"Chen Qian, William & Mary, cqian03@wm.edu, Yimeng Wang, William & Mary, ywang139@wm.edu, Yu Chen, Anytime AI, ychen@anytime-ai.com, Lingfei Wu, Anytime AI, lwu@anytime-ai.com, Andreas Stathopoulos, William & Mary, axstat@wm.edu",Not found,Not found,"explainable AI, Chain-of-Thought, professional communication, legal writing, explainability metrics","Explainable AI in high-stakes domains should help stakeholders trust and verify system outputs. However, Chain-of-Thought methods reason before concluding, and logical gaps or hallucinations can yield conclusions that do not reliably align with their rationale. We propose 'Result → Justify', which constrains the output communication to present a conclusion before its structured justification. SEF operationalizes professional conventions via six metrics for structure and grounding. Experiments validate this approach: all six metrics correlate with correctness, and SEF achieves 83.9% accuracy (+5.3 over CoT). These results suggest structured justification can improve verifiability and reliability.",12.81,Qwen2.5-3B,Apple M1 (Metal)
2601.07238v1_Group Pattern Selection Optimization Let LRMs Pick.pdf,Group Pattern Selection Optimization: Let LRMs Pick the Right Pattern for Reasoning,"Hanbin Wang, Jingwei Song, Jinpeng Li, Fei Mi, Lifeng Shang",Not provided,Not provided,"Large reasoning models, reinforcement learning, pattern selection, mathematics, science, reasoning patterns","This paper introduces Group Pattern Selection Optimization (GPSO), a reinforcement learning framework that enables large reasoning models to select the most effective reasoning patterns for a given problem, thereby mitigating sub-optimality and fostering more robust and adaptable reasoning. GPSO explores a portfolio of diverse reasoning strategies and optimizes the policy on the most effective ones, enabling the model to internalize the mapping from problem characteristics to optimal reasoning patterns.",12.11,Qwen2.5-3B,Apple M1 (Metal)
2601.07239v1_Stochastic CHAOS Why Deterministic Inference Kills.pdf,"Stochas( CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artificial Cognition","Tanmay Joshi1, Shourya Aggarwal1, Anusa Saha1, Aadi Pandey1, Shreyash Dhoot1, Vighnesh Rai1, Raxit Goswami2, Aman Chadha3, Vinija Jain4, Amitava Das1",Not found,Not found,"Large Language Models, Reproducibility, Distributio(nal Variability, Emergent Abilities, Safety Alignment","This paper argues against deterministic inference in large language models, claiming it kills the ability to model uncertainty, disrupts reasoning abilities, and renders safety alignment brittle. Instead, the authors advocate for stochastic inference, arguing that distributional variability is the heart of artificial cognition.",12.73,Qwen2.5-3B,Apple M1 (Metal)
2601.07245v1_Learning to Trust the Crowd A Multi-Model Consensu.pdf,Learning to Trust the Crowd: A Multi-Model Consensus Reasoning Engine for Large Language Models,Pranav Kallem,,,"Large Language Models, Reliability, Multi-Model Consensus, Consensus Reasoning Engine, Gradient-Boosted Trees, Listwise Ranking, Graph Neural Networks","The study investigates the reliability of large language models (LLMs) through multi-model consensus. Given responses from several heterogeneous LLMs, the system learns which answer is most likely correct for a given query. The system maps natural language responses into structured features using semantic embeddings, pairwise similarity and clustering statistics, lexical and structural cues, reasoning-quality scores, confidence estimates, and model-specific priors. The best graph-attention-based consensus model improves macro-average accuracy by 4.6 percentage points over the strongest single LLM and by 8.1 points over majority vote, while also yielding lower Brier scores and fewer hallucinations.",12.95,Qwen2.5-3B,Apple M1 (Metal)
2601.07250v1_DDT A Dual-Masking Dual-Expert Transformer for Ene.pdf,DDT: A Dual-Masking Dual-Expert Transformer for Energy Time-Series Forecasting,"Mingnan Zhu, Qixuan Zhang, Yixuan Cheng, Fangzhou Gu, Shiming Lin",Not found,Not found,"Time-Series Forecasting, Multivariate Temporal Modeling, Dynamic-Causal Masking, Adaptive Feature Fusion","Accurate energy time-series forecasting is crucial for ensuring grid stability and promoting the integration of renewable energy. DDT, a novel and robust deep learning framework, addresses these challenges through a dual-masking mechanism and a dual-expert system, demonstrating superior performance across multiple benchmarks.",12.02,Qwen2.5-3B,Apple M1 (Metal)
2601.07261v1_Pseudodata-guided Invariant Representation Learnin.pdf,Pseudodata-guided Invariant Representation Learning Boosts the Out-of-Distribution Generalization in Enzymatic Kinetic Parameter Prediction,"Haomin Wu, Zhiwei Nie, Hongyu Zhang, Zhixiang Ren",Not provided,2601.07261,"Enzyme kinetics, Deep learning, Out-of-distribution, Invariant representation, Perturbation augmentation","This paper proposes O 2DENet, a lightweight module that enhances out-of-distribution generalization in enzyme–substrate interaction predictors, improving accuracy and robustness across stringent sequence-identity-based benchmarks.",13.74,Qwen2.5-3B,Apple M1 (Metal)
2601.07263v1_When Bots Take the Bait Exposing and Mitigating th.pdf,When Bots Take the Bait: Exposing and Mitigating the Emerging Social Engineering Attack in Web Automation Agent,"Xinyi Wu†, Geng Hong †B, Yueyue Chen†, MingXuan Liu §, Feier Jin †, Xudong Pan †‡, Jiarun Dai †, Baojun Liu ¶",Not provided,Not provided,"Web automation, Social engineering, Web agents, Large language models, Runtime mitigation","This paper presents the first systematic study of social engineering attacks against web automation agents and designs a pluggable runtime mitigation solution. It introduces the AGENTBAIT paradigm and proposes SUPERVISOR, a lightweight runtime module to enforce environment and intention consistency alignment. Empirical results show that mainstream frameworks are highly vulnerable to AGENTBAIT, with an average attack success rate of 67.5% and peaks above 80% under specific strategies. The proposed module can reduce attack success rates by up to 78.1% while incurring only a 7.7% runtime overhead and preserving usability.",12.71,Qwen2.5-3B,Apple M1 (Metal)
2601.07291v1_A Visual Semantic Adaptive Watermark grounded by P.pdf,A Visual Semantic Adaptive Watermark grounded by Prefix-Tuning for Large Vision-Language Model,"Qi Zheng, Shuliang Liu, Yu Huang, Sihang Jia, Jungang Li, Lyuhao Chen, Junhao Chen, Hanqian Li, Aiwei Liu, Yibo Yan, Xuming Hu",Not found,Not found,"Watermarking, Large Vision-Language Model, Prefix-Tuning, Visual Evidence, Semantic Fidelity, Inference Efficiency","Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases, while some semantic-aware methods incur prohibitive inference latency due to rejection sampling. This paper proposes the Visual Semantic Adaptive Watermark (VISA-Mark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. Our approach employs a lightweight, efficiently trained prefix-tuner to extract dynamic Visual Evidence Weights, which guide an adaptive vocabulary partitioning and logits perturbation mechanism, concentrating watermark strength specifically on visually-supported tokens. By actively aligning the watermark with visual evidence, VISA-Mark effectively maintains visual fidelity and outperforms conventional methods in terms of visual consistency and semantic fidelity.",12.73,Qwen2.5-3B,Apple M1 (Metal)
2601.07292v1_Photometric Redshift Estimation Using Scaled Ensem.pdf,Photometric Redshift Estimation Using Scaled Ensemble Learning,"Swagata Biswas, Shubhrangshu Ghosh, Avyarthana Ghosh, Yogesh Wadadekar, Abhishek Roy Choudhury, Arijit Mukherjee, Shailesh Deshpande, Arpan Pal",,,"Galaxies, High-redshift galaxies, Redshift surveys, Neural networks","The study presents a new ensemble-based ML framework aimed at predicting photometric redshifts (Pz) for faint galaxies and higher redshift ranges, relying solely on optical (grizy) photometric data. The proposed architecture integrates several learning algorithms, including gradient boosting machine, extreme gradient boosting, k-nearest neighbors, and artificial neural networks, within a scaled ensemble structure. The framework demonstrates consistent accuracy in estimating redshifts, maintaining strong performance up to z ∼ 4. The model is validated using publicly available data from the Hyper Suprime-Cam Strategic Survey Program by the Subaru Telescope. Our results show marked improvements in the precision and reliability of Pz estimation.",12.52,Qwen2.5-3B,Apple M1 (Metal)
2601.07296v1_LRAS Advanced Legal Reasoning with Agentic Search.pdf,LRAS: Advanced Legal Reasoning with Agentic Search,"Yujin Zhou, Chuxue Cao, Jinluan Yang, Lijun Wu, Conghui He, Sirui Han, Yike Guo",,,"Legal Reasoning, Agentic Search, Introspective Imitation Learning, Difficulty-aware Reinforcement Learning","Presenting LRAS, the first framework designed to transition legal LLMs from static and parametric 'closed-loop thinking' to dynamic and interactive 'Active Inquiry'. By integrating Introspective Imitation Learning and Difficulty-aware Reinforcement Learning, LRAS enables LRMs to identify knowledge boundaries and handle legal reasoning complexity, outperforming state-of-the-art baselines by 8.2-32%.",11.83,Qwen2.5-3B,Apple M1 (Metal)
2601.07304v1_Heterogeneous Multi-Expert Reinforcement Learning .pdf,Heterogeneous Multi-Expert Reinforcement Learning for Long-Horizon Multi-Goal Tasks in Autonomous Forklifts,"Yun Chen, Bowei Huang, Fan Guo, Kang Song",,,"Autonomous Forklift, Hierarchical Reinforcement Learning, Mobile Manipulation, Hybrid Training, Modality Decoupling","Proposes a Heterogeneous Multi-Expert Reinforcement Learning (HMER) framework tailored for autonomous forklifts, decomposing long-horizon tasks into specialized sub-policies controlled by a Semantic Task Planner to separate navigation and manipulation, improving task success rate and reducing operation time.",11.73,Qwen2.5-3B,Apple M1 (Metal)
2601.07309v1_ARM Role-Conditioned Neuron Transplantation for Tr.pdf,Agent-Role Merging: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging,"Zhuoka Feng, Kang Chen, Sihan Zhao, Kai Xiong, Yaoning Wang, Minshen Yu, Junjie Nian, Changyi Xiao, Yixin Cao, Yugang Jiang",Not found,2601.07309,"large language model, agent merging, neuron transplantation, cross-environment robustness","This paper proposes Agent-Role Merging (ARM), a training-free method for integrating multiple experts into a single large language model agent. ARM improves cross-environment robustness by integrating role-conditioned activation analysis and neuron transplantation, achieving strong out-of-domain generalization without gradient-based optimization.",13.06,Qwen2.5-3B,Apple M1 (Metal)
2601.07313v1_Explaining Machine Learning Predictive Models thro.pdf,Explaining Machine Learning Predictive Models through Conditional Expectation Methods,"Silvia Ruiz-Espa˜ naa, Laura Arnal, Francois Signola, Juan-Carlos Perez-Cortes, Joaquim Arlandis",Not found,Not found,"machine learning, XAI, explainable models, local explainability, model-agnostic, uncertainty, stability","The rapid adoption of complex artificial intelligence and machine learning models has led to their characterization as black boxes due to the difficulty of explaining their internal decision-making processes. This lack of transparency hinders users' ability to understand, validate, and trust model behavior, particularly in high-risk applications. This work introduces Multivariate Conditional Expectation (MUCE), a model-agnostic method for local explainability designed to capture prediction changes from feature interactions. MUCE extends Individual Conditional Expectation (ICE) by exploring a multivariate grid of values in the neighborhood of a given observation at inference time, providing graphical explanations that illustrate the local evolution of model predictions. In addition, two quantitative indices, stability and uncertainty, summarize local behavior and assess model reliability. Uncertainty is further decomposed into uncertainty + and uncertainty − to capture asymmetric effects that global measures may overlook. The proposed method is validated using XGBoost models trained on three datasets: two synthetic (2D and 3D) to evaluate behavior near decision boundaries, and one transformed real-world dataset to test adaptability to heterogeneous feature types. Results show that MUCE effectively captures complex local model behavior, while the stability and uncertainty indices provide meaningful insight into prediction confidence. MUCE, together with the ICE modification and the proposed indices, offers a practical contribution to local explainability, enabling both graphical and quantitative insights that enhance the interpretability of predictive models and support more trustworthy and transparent decision-making.",13.83,Qwen2.5-3B,Apple M1 (Metal)
2601.07315v1_VLM-CAD VLM-Optimized Collaborative Agent Design W.pdf,VLM-CAD:VLM-OptimizedCollaborativeAgent Design Workflow for Analog Circuit Sizing,"1st Guanyuan Pan, 2nd Yugui Lin, 3rd Tiansheng Zhou, 4th Pietro Li `o, 5th Shuai Wang, 6th Yaqi Wang*",,,"Analog Circuit Sizing, Agentic AI, Vision Language Model, Explainability, Electronic Design Automation","Analog mixed-signal circuit sizing involves complex trade-offs within high-dimensional design spaces. Existing automatic analog circuit sizing approaches often underutilize circuit schematics and lack the explainability required for industry adoption. To tackle these challenges, we propose a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD), which analyzes circuits, optimizes DC operating points, performs inference-based sizing and executes external sizing optimization. We integrate Image2Net to annotate circuit schematics and generate a structured JSON description for precise interpretation by Vision Language Models. Furthermore, we propose an Explainable Trust Region Bayesian Optimization method (ExTuRBO) that employs collaborative warm-starting from agent-generated seeds and offers dual-granularity sensitivity analysis for external sizing optimization, supporting a comprehensive final design report. Experiment results on amplifier sizing tasks using 180nm, 90nm, and 45nm Predictive Technology Models demonstrate that VLM-CAD effectively balances power and performance, achieving a 100% success rate in optimizing an amplifier with a complementary input and a class-AB output stage, while maintaining total runtime under 43 minutes across all experiments.",13.43,Qwen2.5-3B,Apple M1 (Metal)
2601.07316v1_BEAT-Net Injecting Biomimetic Spatio-Temporal Prio.pdf,BEAT-Net: Injecting Biomimetic Spatio-Temporal Priors for Interpretable ECG Classification,"Ma Runze, Liao Caizhi ∗",Not found,2601.07316,"ECG classification, deep learning, biomimetic priors, attention mechanisms, self-supervised learning","Proposes BEAT-Net, a framework that reformulates ECG analysis as a language modeling task, using QRS tokenization to transform continuous signals into biologically aligned heartbeat sequences. The architecture explicitly decomposes cardiac physiology through specialized encoders, improving diagnostic accuracy and robustness compared to dominant CNN architectures.",12.44,Qwen2.5-3B,Apple M1 (Metal)
2601.07320v1_Segmental Advantage Estimation Enhancing PPO for L.pdf,Segmental Advantage Estimation: Enhancing PPO for Long-Context LLM Training,"Xue Gong, Qi Yi, Ziyuan Nan, Guanhua Huang, Kejiao Li, Yuhao Jiang, Ruibin Xiong, Zenan Xu, Jiaming Guo, Shaohui Peng, Bo Zhou",Not provided,Not provided,"Large Language Models, Reinforcement Learning, Proximal Policy Optimization, Advantage Estimation, Sparse Rewards, Generalized Advantage Estimation, Segmental Advantage Estimation","Training Large Language Models for reasoning tasks is increasingly driven by Reinforcement Learning with Verifiable Rewards (RLVR), where Proximal Policy Optimization (PPO) provides a principled framework for stable policy updates. However, the practical application of PPO is hindered by unreliable advantage estimation in the sparse-reward RLVR regime. This issue arises because the sparse rewards in RLVR lead to inaccurate intermediate value predictions, which in turn introduce significant bias when aggregated at every token by Generalized Advantage Estimation (GAE). To address this, we introduce Segmental Advantage Estimation (SAE), which mitigates the bias that GAE can incur in RLVR. Our key insight is that aggregating n-step advantages at every token (as in GAE) is unnecessary and often introduces excessive bias, since individual tokens carry minimal information. Instead, SAE first partitions the generated sequence into coherent sub-segments using low-probability tokens as heuristic boundaries. It then selectively computes variance-reduced advantage estimates only from these information-rich segment transitions, effectively filtering out noise from intermediate tokens. Our experiments demonstrate that SAE achieves superior performance, with marked improvements in final scores, training stability, and sample efficiency.",13.59,Qwen2.5-3B,Apple M1 (Metal)
2601.07342v1_Agentic Diagnostic Reasoning over Telecom and Data.pdf,Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure,Nicolas Tacheny,Not found,2601.07342,"Large Language Model, Telecom Infrastructure, Datacenter, Root Cause Analysis, Impact Analysis, Autonomous Incident Resolution, Change Impact Mitigation","This work introduces an agentic diagnostic framework where a Large Language Model autonomously navigates a telecom and datacenter infrastructure model using a constrained tool-space, enabling autonomous incident resolution and change impact mitigation.",12.39,Qwen2.5-3B,Apple M1 (Metal)
2601.07344v1_PulseMind A Multi-Modal Medical Model for Real-Wor.pdf,PulseMind: A Multi-Modal Medical Model for Real-World Clinical Diagnosis,"Jiao Xu, Junwei Liu, Jiangwei Lao, Qi Zhu, Yunpeng Zhao, Congyun Jin, Shinan Liu, Zhihong Lu, Lihe Zhang, Xin Chen, Jian Wang, Ping Wang",,,"Medical model, Multi-modal, Clinical diagnosis, Real-world, PulseMind, Reinforcement Policy Optimization, Multi-turn consultation, Diagnostic benchmark","PulseMind introduces a new family of multi-modal diagnostic models that integrate a curated dataset, a comprehensive evaluation benchmark, and a tailored training framework. It addresses the complexity of real-world clinical diagnostics by incorporating heterogeneous inputs and ongoing contextual understanding during patient-physician interactions.",11.93,Qwen2.5-3B,Apple M1 (Metal)
2601.07348v4_Controlled Self-Evolution for Algorithmic Code Opt.pdf,Controlled Self-Evolution for Algorithmic Code Optimization,"Tu Hu, Ronghao Chen, Shuo Zhang, Jianghao Yin, Mou Xiao Feng, Jingping Liu, Shaolei Zhang, Wenqi Jiang, Yuqi Fang, Sen Hu, Huacan Wang, Yi Xu",Not found,2601.07348,"algorithmic code optimization, self-evolution, controlled evolution, stochastic operations, experience reuse","This paper proposes Controlled Self-Evolution (CSE) to improve exploration efficiency in self-evolution methods for algorithmic code optimization. CSE consists of diversified planning initialization, genetic evolution with feedback-guided mechanisms, and hierarchical evolution memory. Experiments on EffiBench-X demonstrate that CSE consistently outperforms baselines across various large language model backbones.",13.7,Qwen2.5-3B,Apple M1 (Metal)
2601.07351v2_Beyond Hard Masks Progressive Token Evolution for .pdf,Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models,"Linhao Zhong1*, Linyu Wu2*, Bozhen Fang1, Tianjian Feng1, Chenchen Jing1,3, Wen Wang1, Jiaheng Zhang2, Hao Chen1, Chunhua Shen1,3†",Not provided,Not provided,"Diffusion Language Models, Progressive Token Evolution, Soft Token Distribution, Continuous Trajectory Supervision","This paper proposes EvoToken-DLM, a novel diffusion-based language modeling approach that replaces hard binary masks with evolving soft token distributions. It enables a progressive transition from masked states to discrete outputs, supporting revisable decoding. The authors introduce continuous trajectory supervision to effectively support this evolution and demonstrate that EvoToken-DLM consistently achieves superior performance compared to strong diffusion-based and masked DLM baselines.",11.61,Qwen2.5-3B,Apple M1 (Metal)
2601.07356v1_Efficient Convolutional Forward Model for Passive .pdf,Efficient Convolutional Forward Model for Passive Acoustic Mapping and Temporal Monitoring,"Tatiana Gelvez-Barrera, Barbara Nicolas, Bruno Gilles, Adrian Basarab, Denis Kouamé",Not found,Not found,"Passive Acoustic Mapping, Model-based beamforming, Convolutional forward model, Temporal monitoring","Passive acoustic mapping (PAM) is a key imaging technique for characterizing cavitation activity in therapeutic ultrasound applications. Recent model-based beamforming algorithms offer high reconstruction quality and strong physical interpretability. However, their computational burden and limited temporal resolution restrict their use in applications with time-evolving cavitation. This paper introduces a PAM beamforming framework based on a novel convolutional formulation in the time domain, which enables efficient computation. The framework formulates PAM as an inverse problem, incorporating prior knowledge on cavitation activity, and demonstrates superior performance compared to classical beamforming methods.",12.03,Qwen2.5-3B,Apple M1 (Metal)
2601.07359v1_Seeing Right but Saying Wrong Inter- and Intra-Lay.pdf,Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs,"Shezheng Song, Shasha Li, Jie Yu",Not found,Not found,"Multimodal Language Models, Attention Mechanism, Inference Quality, Dual-Perspective Decoding","This paper addresses the internal inconsistency in Multimodal Large Language Models (MLLMs), where deeper layers attend to correct visual regions but final predictions are misled by noisy attention from earlier layers. The authors propose DualPD, a dual-perspective decoding refinement strategy that enhances the model's visual understanding without additional training.",11.94,Qwen2.5-3B,Apple M1 (Metal)
2601.07364v1_On the universal definition of intelligence.pdf,On the universal definition of intelligence,Joseph Chen,,1910.08733,"intelligence, human, AI, comparison, predictive ability, creativity, learning","This paper proposes a universal definition of intelligence to enable fair and consistent comparison of human and artificial intelligence, addressing the anthropocentric nature of existing definitions.",13.14,Qwen2.5-3B,Apple M1 (Metal)
2601.07372v1_Conditional Memory via Scalable Lookup A New Axis .pdf,Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models,"Xin Cheng, Wangding Zeng, Damai Dai, Qinyu Chen, Bingxuan Wang, Zhenda Xie, Kezhao Huang, Xingkai Yu, Zhewen Hao, Yukun Li, Han Zhang, Huishuai Zhang, Dongyan Zhao, Wenfeng Liang",Not found,2601.07372,"Conditional Memory, Sparsity, Large Language Models, Transformers, Mixture-of-Experts, Engram, Lookup","This paper introduces conditional memory as a complementary sparsity axis for large language models, instantiated via Engram. By formulating the Sparsity Allocation problem, the authors uncover a U-shaped scaling law that optimizes the trade-off between neural computation and static memory. Engram, a module that modernizes classic N-gram embedding for O(1) lookup, is scaled to 27B parameters, achieving superior performance over a strictly iso-parameter and iso-FLOPs MoE baseline. The paper demonstrates significant gains in general reasoning and code/math domains, and provides mechanistic analyses and infrastructure-aware efficiency insights.",13.26,Qwen2.5-3B,Apple M1 (Metal)
2601.07376v1_OpenTinker Separating Concerns in Agentic Reinforc.pdf,OpenTinker: Separating Concerns in Agentic Reinforcement Learning,"Siqi Zhu, Jiaxuan You",Not found,2601.07376v1,"Reinforcement Learning, Large Language Models, Agent Interaction, Decoupling","OpenTinker introduces an infrastructure for reinforcement learning of large language model agents, focusing on separating concerns across algorithm design, execution, and agent-environment interaction. It decomposes agentic learning systems into lightweight, composable components and delegates inference and training to a managed execution runtime.",12.07,Qwen2.5-3B,Apple M1 (Metal)
2601.07377v1_Learning Dynamic Collaborative Network for Semi-su.pdf,Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation,"Jiao Xu, Xin Chen",,,"3D vessel segmentation, semi-supervised learning, dynamic collaborative network, adversarial supervision, multi-view integration","This paper presents a dynamic collaborative network (DiCo) for semi-supervised 3D vessel segmentation, addressing the challenges of scarce labeled data and complex vessel appearance. DiCo allows dynamic switching of teacher-student roles and incorporates adversarial supervision and multi-view integration to improve segmentation accuracy.",11.19,Qwen2.5-3B,Apple M1 (Metal)
2601.07389v1_On the Non-decoupling of Supervised Fine-tuning an.pdf,On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training,"Xueyan Niuniuxueyan3@huawei.com, Bo Baibaibo8@huawei.com, Wei Hanharvey.hanwei@huawei.com, Weixi Zhangzhangweixi1@huawei.com",Not found,2601.07389,"Supervised Fine-Tuning, Reinforcement Learning, Post-training, Large Language Models, Model Performance, Generalization","This paper investigates the non-decoupling of supervised fine-tuning and reinforcement learning in the post-training of large language models. It proves that decoupling is impossible in either order: SFT-then-RL coupling increases SFT loss under SFT optimality, and RL-then-SFT coupling lowers the reward achieved by RL. Experiments on Qwen3-0.6B confirm the predicted degradation, showing that SFT and RL cannot be separated without loss of prior performance in the post-training pipeline.",13.18,Qwen2.5-3B,Apple M1 (Metal)
2601.07392v1_OceanSAR-2 A Universal Feature Extractor for SAR O.pdf,OceanSAR-2: A Universal Feature Extractor for SAR Ocean Observation,"Alexandre Tuela, Thomas Kerdreux, Quentin Febvre, Alexis Mouche, Antoine Grouazel, Jean-Renaud Miadana, Antoine Audras, Chen Wang, Bertrand Chapron",,,"SAR, Ocean, Self-supervised learning, Sentinel-1, Wave Mode, Feature extraction","Presenting OceanSAR-2, the second generation of the foundation model for SAR-based ocean observation, which enhances performance while reducing training cost through improved SSL training and dynamic data curation strategies. Demonstrates strong transfer performance across downstream tasks including geophysical pattern classification, ocean surface wind vector and significant wave height estimation, and iceberg detection. Releases standardized benchmark datasets for systematic evaluation and advancement of SAR models for ocean applications.",12.45,Qwen2.5-3B,Apple M1 (Metal)
2601.07393v1_Software-Hardware Co-optimization for Modular E2E .pdf,SOFTWARE-HARDWARECO-OPTIMIZATION FORMODULARE2E,"Chengzhi Ji1, Xingfeng Li1, Zhaodong Lv1, Hao Sun2, Pan Liu1, Hao Frank Yang3, Ziyuan Pu1,∗",10.1109/TCYB.2026.3000000,2601.07393,"Modular end-to-end, Closed-Loop Evaluation, Software–Hardware co-optimization, Energy Consumption","This paper proposes a reusable software–hardware co-optimization and closed-loop evaluation framework for ME2E autonomous driving inference, aiming to improve deployability and system-level performance metrics such as safety, comfort, efficiency, latency, and energy consumption.",11.4,Qwen2.5-3B,Apple M1 (Metal)
2601.07395v1_MCP-ITP An Automated Framework for Implicit Tool P.pdf,MCP-ITP: An Automated Framework for Implicit Tool Poisoning in MCP,"Ruiqi Li, Zhiqiang Wang, Y unhao Y ao, Xiang-Y ang Li",,,"Model Context Protocol, Implicit Tool Poisoning, Tool Poisoning Attack, Large Language Models, Security","To standardize interactions between LLM-based agents and their environments, the Model Context Protocol (MCP) was proposed and has since been widely adopted. However, integrating external tools expands the attack surface, exposing agents to tool poisoning attacks. In such attacks, malicious instructions embedded in tool metadata are injected into the agent context during MCP registration phase, thereby manipulating agent behavior. Prior work primarily focuses on explicit tool poisoning or relied on manually crafted poisoned tools. In contrast, we focus on a stealthy variant: implicit tool poisoning, where the poisoned tool itself remains uninvoked. Instead, the instructions embedded in the tool metadata induce the agent to invoke a legitimate but high-privilege tool to perform malicious operations. We propose MCP-ITP, the first automated and adaptive framework for implicit tool poisoning within the MCP ecosystem. MCP-ITP formulates poisoned tool generation as a black-box optimization problem and employs an iterative optimization strategy that leverages feedback from both an evaluation LLM and a detection LLM to maximize Attack Success Rate (ASR) while evading current detection mechanisms. Experimental results on the MCPTox dataset across 12 LLM agents demonstrate that MCP-ITP consistently outperforms the manually crafted baseline, achieving up to 84.2% ASR while suppressing the Malicious Tool Detection Rate (MDR) to as low as 0.3%.",11.98,Qwen2.5-3B,Apple M1 (Metal)
2601.07397v1_Layerwise goal-oriented adaptivity for neural ODEs.pdf,Layerwise goal-oriented adaptivity for neural ODEs: an optimal control perspective,"Michael Hintermuller, Michael Hinze, Denis Korolev",Not found,arXiv:2601.07397v1,"Resnet, neural ODEs, parameter identification/learning, adaptive neural network","In this work, a novel layerwise adaptive construction method for neural network architectures is proposed, based on a goal-oriented dual-weighted residual technique for the optimal control of neural differential equations. This leads to an optimization problem with controls acting as coefficients and a specific loss function, implemented using a DG(0) Galerkin discretization of the neural ODE and steepest descent for optimization. The method is applied to the construction of neural networks for data set classification, with results presented for selected examples from the literature.",11.39,Qwen2.5-3B,Apple M1 (Metal)
2601.07411v1_SCALPEL Selective Capability Ablation via Low-rank.pdf,SCALPEL: Selective Capability Ablation via Low-rank Parameter Editing for Large Language Model Interpretability Analysis,"Zihao Fu, Xufeng Duan, Zhenguang G. Cai",Not found,Not found,"Large language models, interpretability, neural networks, capability ablation, low-rank parameter editing","Large language models have achieved remarkable success across diverse domains, yet their deployment in many applications remains limited by our incomplete understanding of their internal mechanisms. SCALPEL presents a framework that represents capabilities as low-rank parameter subspaces rather than discrete modules, enabling precise capability removal without affecting others.",10.98,Qwen2.5-3B,Apple M1 (Metal)
2601.07422v1_Two Pathways to Truthfulness On the Intrinsic Enco.pdf,Two Pathways to Truthfulness: On the Intrinsic Encoding of LLM Hallucinations,"Wen Luo, Guangyue Peng, Wei Li, Shaohang Wei, Feifan Song, Liang Wang, Nan Yang, Xingxing Zhang, Jing Jin, Furu Wei, Houfeng Wang",Not found,Not found,"large language models, hallucinations, truthfulness, internal representations, attention, token patching, knowledge boundaries, generative systems","Despite their impressive capabilities, large language models (LLMs) frequently generate hallucinations. This paper demonstrates that truthfulness cues arise from two distinct information pathways: a Question-Anchored pathway and an Answer-Anchored pathway. The authors validate and disentangle these pathways through attention knockout and token patching, uncovering notable properties of these mechanisms. They also propose two applications to enhance hallucination detection performance and discuss the implications for more reliable and self-aware generative systems.",11.13,Qwen2.5-3B,Apple M1 (Metal)
2601.07430v1_KALE Enhancing Knowledge Manipulation in Large Lan.pdf,KALE: Enhancing Knowledge Manipulation in Large Language Models via Knowledge-aware Learning,"Qitan Lv, Tianyu Liu, Qiaosheng Zhang, Xingcheng Xu, Chaochao Lu",,,"Large Language Models, Knowledge Manipulation, Supervised Fine-Tuning, Knowledge Graphs, Rationale Generation, Knowledge-aware Learning","Despite the impressive performance of large language models (LLMs) pretrained on vast knowledge corpora, advancing their knowledge manipulation—the ability to effectively recall, reason, and transfer relevant knowledge—remains challenging. Existing methods mainly leverage Supervised Fine-Tuning (SFT) on labeled datasets to enhance LLMs' knowledge manipulation ability. However, SFT models still exhibit the known&incorrect phenomenon, where they explicitly possess relevant knowledge for a given question but fail to leverage it for correct answers. To address this challenge, we propose KALE (Knowledge-Aware LEarning)—a post-training framework that leverages knowledge graphs (KGs) to generate high-quality rationales and enhance LLMs' knowledge manipulation ability.",11.21,Qwen2.5-3B,Apple M1 (Metal)
2601.07449v1_RLPO Residual Listwise Preference Optimization for.pdf,RLPO: Residual Listwise Preference Optimization for Long-Context Review Ranking,"Hao Jiang, Zhi Yang, Annan Wang, Yichi Zhang, Weisi Lin*",,,"review ranking, long-context, residual learning, large language models","Review ranking is crucial in e-commerce for prioritizing user-generated feedback. Existing methods face a trade-off between efficiency and capturing list-level interactions. This paper proposes RLPO, a residual listwise preference optimization approach, which improves NDCG@k over strong pointwise and listwise baselines and remains robust as list length increases.",10.2,Qwen2.5-3B,Apple M1 (Metal)
2601.07463v1_Puzzle it Out Local-to-Global World Model for Offl.pdf,Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning,"Sijia Li, Xinran Li, Shibo Chen, Jun Zhang",Not found,Not found,"Offline multi-agent reinforcement learning, Multi-agent model-based reinforcement learning","This paper proposes a local-to-global (LOGO) world model for offline multi-agent reinforcement learning (MARL) to improve prediction accuracy and generalize beyond the support of the data. It introduces a novel framework that leverages local predictions to infer global state dynamics, generating synthetic data to augment the original dataset and reducing approximation error propagation to policies. The approach requires only an additional encoder for uncertainty estimation, significantly reducing computational overhead while maintaining accuracy.",10.79,Qwen2.5-3B,Apple M1 (Metal)
2601.07464v1_IFDNS An Iterative Feedback-Driven Neuro-Symbolic .pdf,IFDNS: An Iterative Feedback-Driven Neuro-Symbolic Method for Faithful Logical Reasoning,"Xiaoheng Wang, Tongxuan Liu, Zi Gong, Xianzhe Dong, Yuting Zeng, Minhan Hu, Weizhe Huang, Jing Li",Not found,Not found,"Logical Reasoning, Large Language Model, Reasoning","Large language models (LLMs) have demonstrated impressive capabilities across a wide range of reasoning tasks, including logical and mathematical problem-solving. While prompt-based methods like Chain-of-Thought (CoT) can enhance LLM reasoning abilities to some extent, they often suffer from a lack of faithfulness, where the derived conclusions may not align with the generated reasoning chain. To address this issue, researchers have explored neuro-symbolic approaches to bolster LLM logical reasoning capabilities. However, existing neuro-symbolic methods still face challenges with information loss during the process. To overcome these limitations, we introduce Iterative Feedback-Driven Neuro-Symbolic (IFDNS), a novel prompt-based method that employs a multi-round feedback mechanism to address LLM limitations in handling complex logical relationships. IFDNS utilizes iterative feedback during the logic extraction phase to accurately extract causal relationship statements and translate them into propositional and logical implication expressions, effectively mitigating information loss issues. Furthermore, IFDNS is orthogonal to existing prompt methods, allowing for seamless integration with various prompting approaches. Empirical evaluations across six datasets demonstrate the effectiveness of IFDNS in significantly improving the performance of CoT and Chain-of-Thought with Self-Consistency (CoT-SC). Specifically, IFDNS achieves a +9.40% accuracy boost for CoT on the LogiQA dataset and a +11.70% improvement for CoT-SC on the PrOntoQA dataset.",12.44,Qwen2.5-3B,Apple M1 (Metal)
2601.07468v1_Beyond Dialogue Time Temporal Semantic Memory for .pdf,Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents,"Miao Su, Yucan Guo, Zhongni Hou, Long Bai, Zixuan Li, Yufei Zhang, Guojun Yin, Wei Lin, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng",Not provided,2601.07468,"Large Language Models, Personalized Agents, Temporal Memory, Semantic Time","Memory enables Large Language Model (LLM) agents to perceive, store, and use information from past dialogues. However, existing methods fail to properly model the temporal dimension of memory. We propose Temporal Semantic Memory (TSM), a memory framework that models semantic time for point-wise memory and supports the construction and utilization of durative memory. Experiments show that TSM consistently outperforms existing methods and achieves up to 12.2% absolute improvement in accuracy.",11.62,Qwen2.5-3B,Apple M1 (Metal)
2601.07469v1_Knowledge Distillation for LLM-Based Human Activit.pdf,KNOWLEDGE DISTILLATION FOR LLM-B ASED HUMAN ACTIVITY RECOGNITION IN HOMES,"Julien Cumin, Oussama Er-Rahmany, Xi Chen",,arXiv:2601.07469v1,"Human activity recognition, large language models, knowledge distillation, ambient intelligence, smart homes","This paper presents new experimental results regarding the use of large language models (LLMs) for human activity recognition (HAR) in homes. It explores how recognition performance varies with the size of the LLM used and experiments with fine-tuning smaller LLMs with HAR reasoning examples generated by larger LLMs. The findings show that fine-tuned models can perform almost as well as the largest LLMs, while having 50 times fewer parameters.",11.3,Qwen2.5-3B,Apple M1 (Metal)
2601.07470v1_Learning How to Remember A Meta-Cognitive Manageme.pdf,Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory,"Sirui Liang, Pengfei Cao, Jian Zhao, Wenhao Teng, Xiangwen Liao, Jun Zhao, Kang Liu",Not provided,Not provided,"Memory management, Meta-cognition, Transferable memory, Hierarchical memory organization, Direct preference optimization","This paper proposes the Meta-Cognitive Memory Abstraction (MCMA) method, which treats memory abstraction as a learnable cognitive skill. MCMA decouples task execution from memory management by combining a frozen task model with a learned memory copilot. The memory copilot is trained using direct preference optimization, determining how memories should be structured, abstracted, and reused. Memories are organized into a hierarchy of abstraction levels, enabling selective reuse based on task similarity. When no memory is transferable, MCMA transfers the ability to abstract and manage memory by transferring the memory copilot. Experiments on ALFWorld, ScienceWorld, and BabyAI demonstrate substantial improvements in performance, out-of-distribution generalization, and cross-task transfer over several baselines.",11.68,Qwen2.5-3B,Apple M1 (Metal)
2601.07474v1_Task Prototype-Based Knowledge Retrieval for Multi.pdf,Task Prototype-Based Knowledge Retrieval for Multi-Task Learning,"Youngmin Oh, Hyung-Il Kim, Jung Uk Kim",Not found,Not found,"Multi-task learning, Prototype-based knowledge retrieval, Partial annotation, Robust multi-task learning","Proposes a prototype-based knowledge retrieval framework for achieving robust multi-task learning, addressing the challenges of obtaining fully annotated data for all tasks. The framework consists of task prototypes and a knowledge retrieval transformer, ensuring consistent task-specific characteristics and adaptive feature refinement based on task associations.",9.83,Qwen2.5-3B,Apple M1 (Metal)
2601.07475v1_ARCQuant Boosting NVFP4 Quantization with Augmente.pdf,ARCQuant: Boosting NVFP4 Quantization with Augmented Residual Channels for LLMs,"Haoqian Meng, Yilun Luo, Yafei Zhao, Wenyuan Liu, Peng Zhang*",Not found,Not found,"Large Language Models, Post-Training Quantization, NVFP4, Augmented Residual Channels, Microscaling Formats","The paper proposes ARCQuant, a framework that boosts NVFP4 performance via Augmented Residual Channels. It addresses challenges in adapting existing Post-Training Quantization strategies to fine-grained numerical formats like NVFP4, which compromise block isolation or hardware uniformity. ARCQuant maintains a strictly unified NVFP4 format by augmenting the activation matrix with quantized residual channels, enabling the use of standard, highly optimized GEMM kernels with minimal overhead. Extensive experiments on LLaMA and Qwen models demonstrate state-of-the-art accuracy, comparable to full-precision base-liners in perplexity and downstream tasks.",11.82,Qwen2.5-3B,Apple M1 (Metal)
2601.07477v1_JudgeFlow Agentic Workflow Optimization via Block .pdf,JUDGEFLOW: AGENTICWORKFLOWOPTIMIZATION,"Zihan Ma∗, Zhikai Zhao∗, Chuanbo Hua1, Federico Berto1, Jinkyoo Park1, 3Omelet",Not found,Not found,"Large language models, agentic workflows, LLM optimization, workflow automation","Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities. Current methods rely on coarse, end-to-end evaluation signals and lack fine-grained signals on where to refine, often resulting in inefficient or low-impact modifications. To address these limitations, we propose JUDGEFLOW, an Evaluation-Judge-Optimization-Update pipeline. It incorporates reusable, configurable logic blocks into agentic workflows to capture fundamental forms of logic and designs a dedicated Judge module that inspects execution traces and assigns rank-based responsibility scores to problematic blocks. These fine-grained diagnostic signals are then leveraged by an LLM-based optimizer, which focuses modifications on the most problematic block in the workflow. Our approach improves sample efficiency, enhances interpretability through block-level diagnostics, and provides a scalable foundation for automating increasingly complex agentic workflows.",11.8,Qwen2.5-3B,Apple M1 (Metal)
2601.07496v1_Graph Inference Towards ICD Coding.pdf,Graph Inference Towards ICD Coding,1st Xiaoxiao Deng,460-519,038.9276.2038.9,"transfer learning, graph convolutional network, lightweight attention, ICD code prediction, adversarial domain adaptation","Automated ICD coding involves assigning standardized diagnostic codes to clinical narratives. Challenges include extreme class imbalance and sparse, hierarchical structure. LabGraph reformulates the task as graph generation, combining adversarial domain adaptation, graph-based reinforcement learning, and perturbation regularization to enhance model robustness and generalization. Experiments on benchmark datasets demonstrate LabGraph's superior performance on micro-F1, micro-AUC, and P@K metrics.",10.77,Qwen2.5-3B,Apple M1 (Metal)
2601.07514v1_Data-Driven Stochastic VRP Integration of Forecast.pdf,Data-Driven Stochastic VRP: Integration of Forecast Duration into Optimization for Utility Workforce Management,Matteo Garbellia,,2601.07514,"Stochastic VRP, Machine Learning, XGBoost, Sub-Gaussian Concentration, Multi-Objective Optimization, Evolutionary Algorithms","This paper investigates the integration of machine learning forecasts of intervention durations into a stochastic variant of the Capacitated Vehicle Routing Problem with Time Windows (CVRPTW). It uses tree-based gradient boosting (XGBoost) trained on eight years of gas meter maintenance data to produce point predictions and uncertainty estimates, which drive a multi-objective evolutionary optimization routine. The methodology addresses uncertainty through sub-Gaussian concentration bounds for route-level risk buffers and explicitly accounts for competing operational KPIs through a multi-objective formulation. Empirical analysis of prediction residuals validates the sub-Gaussian assumption underlying the risk model. The results show improvements around 20-25% in operator utilization and completion rates compared to plans computed using default durations. The integration of uncertainty quantification and risk-aware optimization provides a practical framework for handling stochastic service durations in real-world routing applications.",14.01,Qwen2.5-3B,Apple M1 (Metal)
2601.07516v1_Controlling Multimodal Conversational Agents with .pdf,Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions,"Yongqi Li, Hao Lang, Tieyun Qian, Yongbin Li",,,"reinforcement learning, multimodal conversational agents, latent actions, image-text data, cycle consistency loss","This paper proposes a method to enhance the performance of reinforcement learning fine-tuning for multimodal conversational agents (MCAs) by learning a compact latent action space. The authors leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector to transform text embeddings into image-text embeddings. They also introduce a novel cycle consistency loss to improve the robustness of the cross-modal projector. The method is shown to outperform competitive baselines on two conversation tasks across various RL algorithms.",10.96,Qwen2.5-3B,Apple M1 (Metal)
2601.07518v1_Mon3tr Monocular 3D Telepresence with Pre-built Ga.pdf,Mon3tr: Monocular 3D Telepresence with Pre-built Gaussian Avatars as Amortization,"Fangyu Lin, Yingdong Hu, Zhening Liu, Yufan Zhuang, Zehong Lin, Jun Zhang",Not found,Not found,"Monocular 3D telepresence, 3D Gaussian splatting, animatable avatars, real-time neural rendering","A novel Monocular 3D telepresence framework that integrates 3D Gaussian splatting (3DGS) based parametric human modeling into telepresence for the first time, reducing system complexity and cost by using a single monocular RGB camera to capture body motions and facial expressions in real time to drive the 3DGS-based parametric human model. The framework adopts an amortized computation strategy, dividing the process into a one-time offline multi-view reconstruction phase to build a user-specific avatar and a monocular online inference phase during live telepresence sessions. The extracted motion and appearance features are transmitted at <0.2 Mbps over WebRTC's data channel, allowing robust adaptation to network fluctuations. On the receiver side, a lightweight 3DGS attribute deformation network is developed to dynamically generate corrective 3DGS attribute adjustments on the pre-built avatar, synthesizing photorealistic motion and appearance at ∼60 FPS. Extensive experiments demonstrate the state-of-the-art performance of the method, achieving a PSNR of >28 dB for novel poses, an end-to-end latency of ∼80 ms, and >1000× bandwidth reduction compared to point-cloud streaming, while supporting real-time operation from monocular inputs across diverse scenarios.",12.52,Qwen2.5-3B,Apple M1 (Metal)
2601.07525v1_Thinking Before Constraining A Unified Decoding Fr.pdf,Thinking Before Constraining: A Unified Decoding Framework for Large Language Models,"Ngoc Trinh Hung Nguyen1,2, Alonso Silva2, Laith Zumot3, Liubov Tupikina2, Armen Aghasaryan 2, Mehwish Alam 1",,,"Natural language generation, Structured generation, Language models, Constrained decoding, LLMs","This work proposes a simple approach that combines the advantages of natural and structured generation. By allowing LLMs to reason freely until specific trigger tokens are generated, and then switching to structured generation, our method preserves the expressive power of natural language reasoning while ensuring the reliability of structured outputs. The approach is evaluated on several datasets covering both classification and reasoning tasks, demonstrating a substantial gain in accuracy compared to natural generation, while requiring only a small overhead of 10–20 extra tokens.",11.19,Qwen2.5-3B,Apple M1 (Metal)
2601.07528v1_From RAG to Agentic RAG for Faithful Islamic Quest.pdf,From RAG to Agentic RAG for Faithful Islamic Question Answering,"Gagan Bhatia1, Hamdy Mubarak1, Mustafa Jarrar2, George Mikros2, Fadi Zaraket3, Mahmoud Alhirthani2, Mutaz Al-Khatib4, Logan Cochrane5, Kareem Darwish1, Rashid Yahiaoui2, Firoj Alam1",Not found,Not found,"Islamic QA, faithful answers, grounded responses, LLMs, Quran retrieval, agentic RAG","This paper introduces ISLAMIC FAITH QA, a 3,810-item bilingual generative benchmark for Islamic question answering. It also develops an agentic Quran-grounding framework (agentic RAG) that uses structured tool calls for iterative evidence seeking and answer revision. Experiments show that retrieval improves correctness and agentic RAG yields the largest gains beyond standard RAG, achieving state-of-the-art performance and stronger Arabic-English robustness even with a small model.",11.59,Qwen2.5-3B,Apple M1 (Metal)
2601.07553v1_VirtualEnv A Platform for Embodied AI Research.pdf,VirtualEnv: A Platform for Embodied AI Research,"Kabir Swain, Sijie Han, Ayush Raina, Jin Zhang, Shuang Li, Michael Stopa, Antonio Torralba",Not found,Not found,"Embodied AI, Simulation, Large Language Models, Interactive Environments, Object Manipulation, Navigation, Multi-Agent Collaboration, Procedural Generation, Reinforcement Learning","VirtualEnv is a next-generation simulation platform built on Unreal Engine 5 that enables fine-grained benchmarking of large language models (LLMs) in embodied and interactive scenarios. It supports rich agent-environment interactions and integrates large-scale LLMs and vision-language models (VLMs) to generate novel environments and structured tasks from multimodal inputs. The platform is released as an open-source platform to advance research at the intersection of AI and gaming, enable standardized evaluation of LLMs in embodied AI settings, and pave the way for future developments in immersive simulations and interactive entertainment.",11.04,Qwen2.5-3B,Apple M1 (Metal)
2601.07556v1_Backpropagation-Free Test-Time Adaptation for Ligh.pdf,Backpropagation-Free Test-Time Adaptation for Lightweight EEG-Based Brain-Computer Interfaces,"Siyang Li, Jiayi Ouyang, Zhenyao Cui, Ziwei Wang, Tianwang Jia, Feng Wan, Dongrui Wu",Not found,Not found,"test-time adaptation, brain-computer interface, electroencephalogram, domain adaptation, transfer learning","This paper proposes Backpropagation-Free Transformations (BFT), a test-time adaptation approach for EEG decoding that eliminates computational overhead, privacy risks, and sensitivity to noisy data streams. BFT applies multiple sample-wise transformations of knowledge-guided augmentations or approximate Bayesian inference to each test trial, generating multiple prediction scores for a single test sample. A learning-to-rank module enhances the weighting of these predictions, enabling robust aggregation for uncertainty suppression during inference. Extensive experiments on five EEG datasets of motor imagery classification and driver drowsiness regression tasks demonstrate the effectiveness, versatility, robustness, and efficiency of BFT.",11.51,Qwen2.5-3B,Apple M1 (Metal)
2601.07565v1_A Unified Framework for Emotion Recognition and Se.pdf,A Unified Framework for Emotion Recognition and Sentiment Analysis via Expert-Guided Multimodal Fusion with Large Language Models,"Jiaqi Qiao, Xiujuan Xu, Xinran Li, Yu Liu",Not found,Not found,"emotion recognition, sentiment analysis, multimodal fusion, large language models, expert-guided fusion, fine-grained local expert, semantic correlation expert, global context expert, hierarchical dynamic gating, pseudo token injection, prompt-based conditioning, natural language generation, large language model fine-tuning, cross-lingual robustness","This paper presents EGMF, a unified framework combining expert-guided multimodal fusion with large language models for multimodal emotion recognition and sentiment analysis. It features specialized expert networks for fine-grained local emotional nuances, semantic cross-modal relationships, and long-range context dependencies, adaptively integrated through hierarchical dynamic gating. Enhanced multimodal representations are integrated with LLMs via pseudo token injection and prompt-based conditioning, enabling a single generative framework for both classification and regression. LoRA fine-tuning is employed for computational efficiency. Experiments on bilingual benchmarks demonstrate consistent improvements over state-of-the-art methods, revealing universal patterns in multimodal emotional expressions across English and Chinese.",12.42,Qwen2.5-3B,Apple M1 (Metal)
2601.07568v1_d3LLM Ultra-Fast Diffusion LLM using Pseudo-Trajec.pdf,d3LLM: Ultra-Fast dLLM using Pseudo-Trajectory Distillation,"Yu-Yang Qian, Junda Su, Lanxiang Hu, Peiyuan Zhang, Zhijie Deng, Peng Zhao†, Hao Zhang†",Not found,Not found,"diffusion large language models, pseudo-trajectory distillation, parallel decoding, random-order generation, accuracy-parallelism trade-off","d3LLM proposes a method to balance accuracy and parallelism in diffusion large language models (dLLMs), introducing pseudo-trajectory distillation during training and entropy-based multi-block decoding during inference to achieve high parallelism while maintaining accuracy. It achieves up to 10× speedup over vanilla LLaDA/Dream and 5× speedup over AR models without much accuracy drop.",10.68,Qwen2.5-3B,Apple M1 (Metal)
2601.07573v1_A Model of Artificial Jagged Intelligence.pdf,A Model of Artificial Jagged Intelligence,Joshua S. Gans,Not found,2601.07573,"generative AI, adoption, calibration, learning, knowledge density, scaling","This paper develops an economic model of AI performance, called Artificial Jagged Intelligence (AJI), where generative AI systems exhibit uneven performance across tasks. It treats adoption as an information problem, where users care about local reliability but observe only coarse global quality signals. The model interpolates optimally and measures local error by posterior variance. It also explores mastery and calibration, showing that a calibrated user can still have positive expected value in domains that fail the blind adoption test. The paper also discusses how scaling interacts with discoverability and opacity.",12.6,Qwen2.5-3B,Apple M1 (Metal)
2601.07577v1_Beyond Entangled Planning Task-Decoupled Planning .pdf,Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents,"Yunfan Li, Bingbing Xu, Xueyun Tian, Xiucheng Xu, Huawei Shen",Not found,Not found,"long-horizon planning, task decomposition, contextual reasoning, replanning, language models","Recent advances in large language models have enabled agents to autonomously execute complex, long-horizon tasks. However, planning remains a primary bottleneck for reliable task execution. This paper proposes Task-Decoupled Planning (TDP), a training-free framework that decomposes tasks into a directed acyclic graph of sub-goals via a Supervisor, and confine reasoning and replanning to the active sub-task. This approach improves robustness and efficiency for long-horizon agents.",10.78,Qwen2.5-3B,Apple M1 (Metal)
2601.07580v1_Large Language Models for Physics Instrument Desig.pdf,Large Language Models for Physics Instrument Design,"Sara Zoccheddu∗1, Shah Rukh Qasim1,2, Patrick Owen2, Nicola Serra1,2",Not found,Not found,"Large Language Models, Physics Instrument Design, Reinforcement Learning, Detector Design, Particle-Matter Interactions","The study investigates the use of large language models (LLMs) for physics instrument design and compares their performance to reinforcement learning (RL). LLMs, given task constraints and summaries of prior high-scoring designs, propose complete detector configurations. The findings suggest that modern LLMs consistently generate valid, resource-aware, and physically meaningful configurations, despite having no task-specific training. This research explores pairing LLMs with a dedicated trust region optimizer, aiming to automate and reduce human effort in instrument design.",11.63,Qwen2.5-3B,Apple M1 (Metal)
2601.07582v2_ES-Mem Event Segmentation-Based Memory for Long-Te.pdf,ES-Mem: Event Segmentation-Based Memory for Long-Term Dialogue Agents,"Huhai Zou, Tianhao Sun†, Chuanjiang He, Yu Tian, Zhenyang Li, Li Jin, Nayu Liu, Jiang Zhong, Kaiwen Wei†",Not found,Not found,"Memory, Dialogue Agents, Event Segmentation, Hierarchical Memory, Semantic Retrieval, Context Localization","Memory is crucial for dialogue agents to maintain coherence and enable continuous adaptation in long-term interactions. ES-Mem, a framework incorporating dynamic event segmentation and a hierarchical memory architecture, mitigates the limitations of existing memory mechanisms by leveraging boundary semantics to anchor specific episodic memory for precise context localization.",10.47,Qwen2.5-3B,Apple M1 (Metal)
2601.07597v1_Pheromone-Focused Ant Colony Optimization algorith.pdf,Pheromone-Focused Ant Colony Optimization algorithm for path planning,"Yi Liu, Hongda Zhang, Zhongxue Gan, Yuning Chen, Ziqing Zhou, Chunlei Meng, Chun Ouyang",Not found,Not found,"Ant Colony Optimization, Path Planning, Pheromone-Focused, Swarm Intelligence","This paper proposes the Pheromone-Focused Ant Colony Optimization (PFACO) algorithm to enhance the problem-solving ability of the ant colony. It introduces three key strategies: concentrated initial pheromone distribution, reinforcement of promising solutions, and a forward-looking mechanism to penalize redundant path turns. These strategies collectively produce focused pheromones to guide the ant colony's search, significantly improving convergence speed and solution quality across diverse optimization problems.",11.16,Qwen2.5-3B,Apple M1 (Metal)
2601.07606v1_Proof of Time A Benchmark for Evaluating Scientifi.pdf,Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments,"Bingyang Ye1,2†, Shan Chen1,2,3†, Jingxuan Tu4, Chen Liu5, Zidi Xiong1, Samuel Schmidgall6, Danielle S. Bitterman1,2,3§",Not provided,Not provided,"scientific idea evaluation, forecasting, benchmarks, time-indexed evaluation, AI for Science","A benchmarking framework, Proof of Time (PoT), is introduced to link scientific idea judgments to downstream signals that become observable later, enabling verifiable evaluation when ground truth arrives, scalable benchmarking without exhaustive expert annotation, and analysis of human–model misalignment against signals such as peer-review awards. PoT supports scalable evaluation of agents on future-facing scientific idea judgment tasks.",10.78,Qwen2.5-3B,Apple M1 (Metal)
2601.07611v1_DIAGPaper Diagnosing Valid and Specific Weaknesses.pdf,DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning,"Zhuoyang Zou, Abolfazl Ansari, Delvin Ce Zhang†, Dongwon Lee, Wenpeng Yin",Not found,Not found,"scientific papers, weakness identification, multi-agent reasoning, author rebuttals, reviewer bias","A novel multi-agent framework for diagnosing valid and specific weaknesses in scientific papers, addressing limitations of existing approaches by simulating human-defined review criteria, introducing structured debate between author and reviewer agents, and prioritizing the most consequential weaknesses for users.",10.34,Qwen2.5-3B,Apple M1 (Metal)
2601.07618v1_Neural Architecture for Fast and Reliable Coagulat.pdf,Neural Architecture for Fast and Reliable Coagulation Assessment in Clinical Settings: Leveraging Thromboelastography,"Yulu Wang, Ziqian Zeng, Jianjun Wu, Zhifeng Tang",,,"Thromboelastography, Coagulation assessment, Clinical settings, Deep learning, Physiological State Reconstruction, Multi-domain learning, Attentions, Vital signs stability","Presenting Physiological State Reconstruction (PSR), a new algorithm designed to make reliable predictions and diagnoses based on dynamic changes between individuals and small amounts of clinical data. MDFE facilitates integration of varied temporal signals, jointly learning high-level temporal interactions and attentions via HLA, and maintaining the stability of computed vital signs. Evaluations with 4 TEG-specialized data sets show remarkable performance, with predictions of R2 > 0.98 for coagulation traits and error reduction around half compared to state-of-the-art methods, and halving the inference time. Drift-aware learning suggests a new future for medical AI applications with data scarcity.",11.17,Qwen2.5-3B,Apple M1 (Metal)
2601.07632v2_GeoMotionGPT Geometry-Aligned Motion Understanding.pdf,GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models,"Zhankai Ye1, Bofan Li1, Yukai Jin1, Shuoqiu Li1, Wei Wang2, Yanfu Zhang3, Shangqiu Gao1*, Xin Liu1*",https://doi.org/https://github.com/JYe16/GeoMotionGPT,https://arxiv.org/abs/2309.14919,"motion understanding, large language models, motion tokenization, semantic embedding, geometry alignment","Discrete motion tokenization enables large language models to serve as versatile backbones for motion understanding and motion-language reasoning. However, existing pipelines typically decouple motion quantization from semantic embedding learning, linking them solely via token IDs, which fails to effectively align the intrinsic geometry of the motion space with the embedding space. This paper presents a novel framework that explicitly enforces orthogonality on both the motion codebook and the LLM embedding space, ensuring that their relational structures naturally mirror each other.",11.15,Qwen2.5-3B,Apple M1 (Metal)
2601.07635v2_Learning About Learning A Physics Path from Spin G.pdf,Learning About Learning: A Physics Path from Spin Glasses to Artificial Intelligence,"Denis D. Caprioti, Matheus Haas, Constantino F. Vasconcelos, Mauricio Girardi-Schappo",Not found,Not found,"Spin glasses, Neural networks, Artificial intelligence, Statistical physics, Dynamical systems, Linear algebra, Computational methods","This paper presents the Hopfield model as a pedagogically rich framework that naturally unifies core topics from undergraduate statistical physics, dynamical systems, linear algebra, and computational methods. It provides a concise and illustrated theoretical introduction grounded in familiar physics concepts, analyzes the model's energy function, dynamics, and pattern stability, and discusses practical aspects of simulation, including a freely available simulation code. The work aims to help prepare physics students to understand, apply, and critically engage with computational tools increasingly central to research, industry, and society.",11.06,Qwen2.5-3B,Apple M1 (Metal)
2601.07638v1_SALT-KG A Benchmark for Semantics-Aware Learning o.pdf,SALT-KG: A Benchmark for Semantics-Aware Learning on Enterprise Tables,"Isaiah Onando Mulang’, Felix Sasaki, Tassilo Klein, Jonas Kolk, Nikolay Grechanov, Johannes Hoffart",10.48550/arXiv.2601.07638,2601.07638,"Semantics-aware learning, Enterprise tables, Metadata Knowledge Graph, Tabular data, Relational prediction","This paper introduces SALT-KG, a benchmark for semantics-aware learning on enterprise tables, extending the SALT benchmark by linking multi-table transactional data with a structured Operational Business Knowledge represented in a Metadata Knowledge Graph. The extension enables evaluation of models that jointly reason over tabular evidence and contextual semantics, highlighting gaps in models' ability to leverage semantics in relational context.",11.12,Qwen2.5-3B,Apple M1 (Metal)
2601.07641v1_Beyond Static Tools Test-Time Tool Evolution for S.pdf,Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning,"Jiaxuan Lu1,†, Ziyu Kong2,†, Yemin Wang3,†, Rong Fu4,†, Haiyuan Wan1,5, Cheng Yang6,†, Wenjie Lou1,5, Haoran Sun1,5, Xiaosong Wang1,5, Xiao Sun1,5, Dongzhan Zhou1,5",Not provided,Not provided,"AI for Science, Test-Time Tool Evolution, Scientific Reasoning, Computational Tools, Tool Evolution, Open-Ended Scientific World","The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools.",12.1,Qwen2.5-3B,Apple M1 (Metal)
2601.07651v1_Active Evaluation of General Agents Problem Defini.pdf,Active Evaluation of General Agents: Problem Definition and Comparison of Baseline Algorithms,"Marc Lanctot, Kate Larson, Ian Gemp, Michael Kaisers",Not found,Not found,"general evaluation, multitask evaluation, ranking, active learning, game theory, social choice theory","This paper proposes a formal definition and conceptual framework for active evaluation of agents across multiple tasks, assessing the performance of ranking algorithms as a function of number of evaluation data samples. It compares several baselines, including the classical Elo rating system and a recently-proposed method, Soft Condorcet Optimization, under different experimental contexts.",10.23,Qwen2.5-3B,Apple M1 (Metal)
2601.07654v1_Towards Automating Blockchain Consensus Verificati.pdf,Towards Automating Blockchain Consensus Verification with IsabeLLM,"Elliot Jones, William Knottenbelt",Not found,2601.07654,"Blockchain, Consensus, Formal Verification, Theorem Proving, Artificial Intelligence","Consensus protocols are crucial for blockchain systems, requiring correct design and implementation to prevent adversaries from carrying out malicious behavior. Formal verification ensures correctness but requires high effort and expertise. This paper presents IsabeLLM, a tool integrating Isabelle with a Large Language Model to automate proofs. It demonstrates the tool's effectiveness by verifying Bitcoin's Proof of Work consensus protocol.",11.65,Qwen2.5-3B,Apple M1 (Metal)
2601.07663v2_Reasoning Models Will Blatantly Lie About Their Re.pdf,Reasoning Models Will Blatantly Lie About Their Reasoning,"William Walden, Johns Hopkins University",,,"Large Reasoning Models, CoT faithfulness, prompt manipulation, interpretability",This study extends previous work on Large Reasoning Models (LRMs) to show that these models will not only omit information about how hints influence their reasoning but will also lie about using these hints. The authors demonstrate that LRMs will deny relying on hints even when explicitly instructed to check for unusual prompt content and when allowed to use hints. This finding has implications for monitoring and interpreting LRMs' reasoning processes.,9.83,Qwen2.5-3B,Apple M1 (Metal)
2601.07666v1_Variational Contrastive Learning for Skeleton-base.pdf,Variational Contrastive Learning for Skeleton-based Action Recognition,"Dang-Dinh NGUYEN1, Decky ASPANDI-LATIF1, Titus ZAHARIA1",Not found,2601.07666,"Human Action Recognition, Self - Supervised Learning, Variational Inference","In recent years, self-supervised representation learning for skeleton-based action recognition has advanced with the development of contrastive learning methods. However, most contrastive paradigms are inherently discriminative and often struggle to capture the variability and uncertainty intrinsic to human motion. To address this issue, we propose a variational contrastive learning framework that integrates probabilistic latent modeling with contrastive self-supervised learning. This formulation enables the learning of structured and semantically meaningful representations that generalize across different datasets and supervision levels. Extensive experiments on three widely used skeleton-based action recognition benchmarks show that our proposed method consistently outperforms existing approaches, particularly in low-label regimes. Moreover, qualitative analyses show that the features provided by our method are more relevant given the motion and sample characteristics, with more focus on important skeleton joints, when compared to the other methods.",12.77,Qwen2.5-3B,Apple M1 (Metal)
2601.07667v1_Adaptive Layer Selection for Layer-Wise Token Prun.pdf,Adaptive Layer Selection for Layer-Wise Token Pruning in LLM Inference,"Rei Taniguchi, Yuyang Dong, Makoto Onizuka, Chuan Xiao",,,"Large Language Models, Key-Value Cache, Token Pruning, Attention Patterns, Transformer Layers","This paper proposes ASL, a training-free method that adaptively chooses the selection layer for layer-wise token pruning in LLM inference. By exploiting the variance of token ranks ordered by attention score, ASL balances the performance across different tasks while meeting the user-specified KV budget requirement. The method operates during the prefilling stage and can be jointly used with existing KV cache reduction methods such as SnapKV to optimize the decoding stage. Evaluations on benchmarks show that ASL outperforms state-of-the-art layer-wise token selection methods in accuracy while maintaining decoding speed and KV cache reduction.",10.67,Qwen2.5-3B,Apple M1 (Metal)
2601.07685v1_Predictive Analytics for Dementia Machine Learning.pdf,Predictive Analytics for Dementia: Machine Learning on Healthcare Data,"1st Shafiul Ajam Opee, 2nd Nafiz Fahad, 3rd Anik Sen, 4th Rasel Ahmed, 5th Fariha Jahan, 6th Md. Kishor Morol, 7th Md Rashedul Islam",,,"Dementia, Machine learning, Linear Discriminant Analysis (LDA), APOE-ϵ4 allele","This study focuses on enhancing dementia prediction using machine learning techniques on patient health data. Supervised learning algorithms, including K-Nearest Neighbors (KNN), Quadratic Discriminant Analysis (QDA), Linear Discriminant Analysis (LDA), and Gaussian Process Classifiers, are applied. Techniques such as Synthetic Minority Over-sampling Technique (SMOTE) and Term Frequency-Inverse Document Frequency (TF-IDF) vectorization are used to address class imbalance and improve model performance. Among the models, LDA achieved the highest testing accuracy of 98%. This study highlights the importance of model interpretability and the correlation of dementia with features such as the presence of the APOE-ϵ4 allele and chronic conditions like diabetes. Future ML innovations, particularly in integrating explainable AI approaches, are advocated to further improve predictive capabilities in dementia care.",11.93,Qwen2.5-3B,Apple M1 (Metal)
2601.07701v1_Deep Whole-body Parkour.pdf,Deep Whole-body Parkour,"Ziwen Zhuang, Shaoting Zhu, Mengjie Zhao, Hang Zhao†",Not found,2601.07701,"Humanoid robotics, Parkour, Deep Reinforcement Learning, Perceptive motion control, Multi-contact skills","Our framework enables a humanoid robot to autonomously traverse challenging obstacles that impose strict geometric constraints on robot odometry. The system utilizes onboard perception to proactively adjust its approach trajectory, ensuring precise foot placement and hand contact for successful whole-body interaction.",10.62,Qwen2.5-3B,Apple M1 (Metal)
2601.07718v1_Hiking in the Wild A Scalable Perceptive Parkour F.pdf,Hiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoids,"Shaoting Zhu, Ziwen Zhuang, Mengjie Zhao, Kun-Ying Lee, Hang Zhao",Not found,2601.07718,"Humanoid Robotics, Parkour, Perception, Reinforcement Learning, Robust Navigation","This work presents Hiking in the Wild, a scalable, end-to-end parkour perceptive framework designed for robust humanoid hiking in complex, unstructured environments. The framework enables a humanoid robot to traverse diverse terrains in both indoor and outdoor environments, achieving robust performance across scenarios such as running at 2.5 m/s over complex terrain, negotiating stairs, gaps, high platforms, and ramps. It utilizes a single-stage reinforcement learning scheme, mapping raw depth inputs and proprioception directly to joint actions, without relying on external state estimation. Extensive field experiments demonstrate the effectiveness of the approach.",12.82,Qwen2.5-3B,Apple M1 (Metal)
2601.07737v1_Evaluating the encoding competence of visual langu.pdf,Evaluating the encoding competence of visual language models using uncommon actions,"Chen Ling, Nai Ding*",,2601.07737v1,"undergraduate project, visual language models, encoding competence, uncommon actions",This paper evaluates the performance of visual language models in understanding and encoding uncommon actions.,14.4,Qwen2.5-3B,Apple M1 (Metal)
2601.07748v1_Improving Domain Generalization in Contrastive Lea.pdf,Improving Domain Generalization in Contrastive Learning using Adaptive Temperature Control,"Robert Lewis∗, Katie Matton∗, Rosalind W. Picard, John Guttag",10.48550/arxiv.2601.07748,2601.07748,"contrastive learning, domain generalization, covariate shift, temperature control","This paper presents a method for improving domain generalization in contrastive learning by incorporating domain labels to increase the domain invariance of learned representations, leading to better out-of-distribution generalization. The method adjusts the temperature parameter in the InfoNCE loss to upweight pairs from more similar domains, encouraging the model to discriminate based on domain-invariant attributes.",11.31,Qwen2.5-3B,Apple M1 (Metal)
2601.07778v1_DT-ICU Towards Explainable Digital Twins for ICU P.pdf,DT-ICU: Towards Explainable Digital Twins for ICU Patient Monitoring via Multi-Modal and Multi-Task Iterative Inference,Wen Guo,Not found,2601.07778v1,"Digital Twin, ICU Monitoring, Multi-Modal, Multi-Task, Iterative Inference","We introduce DT-ICU, a multimodal digital twin framework for continuous risk estimation in intensive care, integrating clinical time series with static patient information in a unified multitask architecture.",12.79,Qwen2.5-3B,Apple M1 (Metal)
2601.07779v1_OS-Symphony A Holistic Framework for Robust and Ge.pdf,OS-SYMPHONY: A Holistic Framework for Robust and Generalist Computer-Using Agents,"Bowen Yang, Kaiming Jin, Zhenyu Wu, Zhaoyang Liu, Qiushi Sun, Zehao Li, Jingjing Xie, Zhoumianze Liu, Fangzhi Xu, Kanzhi Cheng, Qingyun Li, Yian Wang, Yu Qiao, Zun Wang, Zichen Ding",Not found,Not found,"Computer-Using Agents, Vision-Language Models, Holistic Framework, Robust Automation, Long-Horizon Tasks, Generalization, Trajectory-Level Self-Correction, Multimodal Searcher, Live Tutori, Fidelity Issues, Online Benchmarks","OS-SYMPHONY introduces a holistic framework for Computer-Using Agents (CUAs) that addresses limitations in robustness and generalization. It comprises an Orchestrator coordinating two key innovations: a Reflection-Memory Agent for trajectory-level self-correction and Versatile Tool Agents with a Multimodal Searcher for synthesizing live, visually aligned tutorials. Experimental results demonstrate substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks.",11.16,Qwen2.5-3B,Apple M1 (Metal)
2601.07782v1_Beyond Single-Shot Multi-step Tool Retrieval via Q.pdf,Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning,"Wei Fang, James Glass",,,"tool retrieval, query planning, reinforcement learning, large language models","LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning.",10.06,Qwen2.5-3B,Apple M1 (Metal)
2601.07790v1_Benchmarking Small Language Models and Small Reaso.pdf,Benchmarking Small Language Models and Small Reasoning,"Yahya Masri, Emily Ma, Zifu Wang, Joseph Rogers, Chaowei Yang",,2601.07790,"System logs, Language models, Reasoning, Zero-shot, Few-shot, Retrieval-augmented generation","This paper evaluates nine small language models and small reasoning language models on system log severity classification using real-world journalctl data from Linux production servers. The results show strong stratification, with Qwen3-4B achieving the highest accuracy at 95.64% with retrieval-augmented generation, and Gemma3-1B improving from 20.25% under few-shot prompting to 85.28% with retrieval-augmented generation. The findings suggest that architectural design, training objectives, and the ability to integrate retrieved context jointly determine performance.",11.72,Qwen2.5-3B,Apple M1 (Metal)
2601.07794v1_Kinship Data Benchmark for Multi-hop Reasoning.pdf,Kinship Data Benchmark for Multi-hop Reasoning,"Tianda Sun, Dimitar Kazakov",,,"Kinship, Multi-hop reasoning, Language models, Genealogy, Family trees","A benchmark designed to probe the ability of large language models to perform multi-hop reasoning over kinship relations, using a generative pipeline that produces large-scale, realistic, and culture-specific genealogical data.",9.43,Qwen2.5-3B,Apple M1 (Metal)
2601.07821v1_Failure-Aware RL Reliable Offline-to-Online Reinfo.pdf,Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation,"Huanyu Li, Kun Lei, Sheng Zang, Kaizhe Hu, Yongyuan Liang, Bo An, Xiaoli Li, Huazhe Xu",Not provided,Not provided,"Reinforcement Learning, Offline-to-Online, Failure Detection, Self-Recovery, Real-World Manipulation","This paper introduces Failure-Aware Offline-to-Online Reinforcement Learning (FARL) to reduce Intervention-requiring Failures (IR Failures) during real-world reinforcement learning. FARL integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration, significantly improving performance and generalization.",11.01,Qwen2.5-3B,Apple M1 (Metal)
2601.07832v2_MHLA Restoring Expressivity of Linear Attention vi.pdf,MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head,"Kewei Zhang, Ye Huang, Yufan Deng, Jincheng Yu, Junsong Chen, Huan Ling, Enze Xie, Daquan Zhou",Not found,2601.07832,"Transformer, Linear Attention, Multi-Head, Expressive Power, Image Classification, Natural Language Processing, Image Generation, Video Generation","This work proposes Multi-Head Linear Attention (MHLA) to restore the representational diversity lost in linear attention, which often degrades performance. MHLA maintains linear complexity while recovering expressive power of softmax attention, achieving improvements in various domains.",11.49,Qwen2.5-3B,Apple M1 (Metal)
2601.07885v1_Small Symbols Big Risks Exploring Emoticon Semanti.pdf,"Small Symbols, Big Risks: Exploring Emoticon Semantic Confusion in Large Language Models","Weipeng Jiang, Xiaoyu Zhang, Juan Zhai, Shiqing Ma, Chao Shen, Yang Liu",,,"Emoticons, Large Language Models, Semantic Confusion, Safety Implications, User Intent, Catastrophic Failure","This paper identifies emoticon semantic confusion, a vulnerability where Large Language Models misinterpret ASCII-based emoticons to perform unintended and potentially destructive actions. The study reveals that this vulnerability is pervasive, with over 90% of confused responses leading to 'silent failures'. The research also shows that this vulnerability transfers to popular agent frameworks and that existing prompt-based mitigations are largely ineffective. The authors call for the community to recognize and develop effective mitigation methods to ensure the safety and reliability of human-AI interactions.",10.74,Qwen2.5-3B,Apple M1 (Metal)
2601.07891v1_KVzap Fast Adaptive and Faithful KV Cache Pruning.pdf,"KVzap: Fast, Adaptive, and Faithful KV Cache","Simon Jégou *, Maximilian Jeblick",Not found,2601.07891,"transformer attention, language models, key-value cache, inference bottleneck, KV pruning","Growing context lengths in transformer-based language models have made the key-value (KV) cache a critical inference bottleneck. KVzap, a fast, input-adaptive approximation of KVzip, achieves 2–4× KV cache compression with negligible accuracy loss and state-of-the-art performance on the KVpress Leaderboard.",11.01,Qwen2.5-3B,Apple M1 (Metal)
2601.07892v1_Sherry Hardware-Efficient 1.25-Bit Ternary Quantiz.pdf,Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification,"Hong Huang, Decheng Wu, Qiangqiang Hu, Guanghua Yu, Jinhai Yang, Jianchen Zhu, Xue Liu, Dapeng Wu",Not found,Not found,"Hardware efficiency, Ternary quantization, Fine-grained sparsification, Weight quantization, Edge computing","The deployment of Large Language Models (LLMs) on resource-constrained edge devices is increasingly hindered by prohibitive memory and computational requirements. Ternary quantization offers a compelling solution by reducing weights to {−1,0,+1}, but current implementations suffer from a fundamental misalignment with commodity hardware. Sherry, a hardware-efficient ternary quantization framework, introduces a 3:4 fine-grained sparsity that achieves a regularized 1.25-bit width by packing blocks of four weights into five bits, restoring power-of-two alignment. It also addresses the weight trapping issue in sparse ternary training, which leads to representational collapse, by introducing an annealing residual synapse mechanism. Empirical evaluations on LLaMA-3.2 across five benchmarks demonstrate that Sherry matches state-of-the-art ternary performance while significantly reducing model size.",11.55,Qwen2.5-3B,Apple M1 (Metal)
2601.07894v1_Revealing the Attention Floating Mechanism in Mask.pdf,Revealing the Attention Floating Mechanism in Masked Diffusion Models,"Xin Dai, Pengcheng Huang, Zhenghao Liu, Shuo Wang, Yukun Yan, Chaojun Xiao, Yu Gu, Ge Yu, Maosong Sun",Not found,Not found,"Masked Diffusion Models, Attention Floating, Bidirectional Attention, Diffusion Language Models, Auto-regressive Models, Attention Sinks, In-context Learning","This paper investigates the attention behaviors in Masked Diffusion Models (MDMs), revealing the phenomenon of Attention Floating. Unlike Auto-regressive Models (ARMs), where attention converges to a fixed sink, MDMs exhibit dynamic, dispersed attention anchors that shift across denoising steps and layers. The paper empirically demonstrates that this distinctive attention pattern provides a mechanistic explanation for the strong in-context learning capabilities of MDMs, allowing them to double the performance compared to ARMs in knowledge-intensive tasks.",11.49,Qwen2.5-3B,Apple M1 (Metal)
2601.07898v1_Large Language Models and Algorithm Execution Appl.pdf,Large Language Models and Algorithm Execution: Application to an Arithmetic Function,"Farah Ben Slama, Frédéric Armetta",Not found,Not found,"Algorithmic learning in natural language, Supervised learning by decomposition, Large language model, Fine-tuning",This paper investigates the possibility of extending Large Language Models' capabilities to algorithm execution through specialized supervised training focused on reasoning decomposition. It introduces LLM-DAL (Large Language Model - Decompositional Algorithmic Learning) to demonstrate that LLMs' ability to perform complex algorithmic inferences and generalize can be significantly improved when the training method is properly designed to guide the model in its learning process.,11.28,Qwen2.5-3B,Apple M1 (Metal)
2601.07901v1_Decentralized Online Convex Optimization with Unkn.pdf,Decentralized Online Convex Optimization with Unknown Feedback Delays,"Hao Qiu, Mengxiao Zhang, Juliette Achddou",Not provided,2601.07901,"Decentralized Optimization, Online Convex Optimization, Feedback Delays, Gossip Algorithm, Spectral Gap","This paper studies decentralized online convex optimization under unknown, time- and agent-varying feedback delays, proposing a novel algorithm that achieves improved regret bounds and extending the framework to the strongly convex setting.",10.65,Qwen2.5-3B,Apple M1 (Metal)
2601.07903v2_Enhancing Large Language Models for Time-Series Fo.pdf,Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning,"Jianqi Zhang, Jingyao Wang, Wenwen Qiang, Fanjiang Xu, Changwen Zheng",XXXXXXX.XXXXXXX,XXXXXXX,"Time Series Forecasting, Large Language Model, In-context Learning","The paper explores a method to improve the forecasting performance of large language models for time-series data while freezing all parameters to reduce computational overhead. It proposes LVICL, a vector-injected in-context learning approach, which injects example information into a frozen LLM to enhance its performance on time-series forecasting tasks.",10.8,Qwen2.5-3B,Apple M1 (Metal)
2601.07935v1_Towards Specialized Generalists A Multi-Task MoE-L.pdf,Towards Specialized Generalists: A Multi-Task MoE-LoRA Framework for Domain-Specific LLM Adaptation,"Yuxin Yang, Aoxiong Zeng, Xiangquan Yang",Not found,2601.07935,"Large Language Models, Domain Adaptation, Low-Rank Adaptation, Mixture-of-Experts, Medical Applications","This paper proposes Med-MoE-LoRA, a novel framework integrating Mixture-of-Experts (MoE) with Low-Rank Adaptation (LoRA) to enable efficient multi-task domain adaptation for medical scenarios, addressing challenges such as the 'Stability-Plasticity Dilemma' and 'Task Interference'.",12.13,Qwen2.5-3B,Apple M1 (Metal)
2601.07939v1_SECite Analyzing and Summarizing Citations in Soft.pdf,SECite: Analyzing and Summarizing Citations in Software Engineering Literature,"Shireesh Reddy Pyreddy, Khaja Valli Pathan, Hasan Masum, Tarannum Shaila Zaman",Not found,Not found,"Sentiment Analysis, Natural Language Processing, Machine Learning, Text Summarization, Citations, Software Engineering","This research introduces SECite, a novel approach for evaluating scholarly impact through sentiment analysis of citation contexts. It develops a semi-automated pipeline to extract citations and applies advanced NLP techniques with unsupervised machine learning to classify citation statements as positive or negative. It also uses generative AI to produce sentiment-specific summaries that capture the strengths and limitations of each target paper, derived from clustered citation groups and the full text. The findings reveal meaningful patterns in how the academic community perceives these works, highlighting areas of alignment and divergence between external citation feedback and the authors' own presentation.",12.63,Qwen2.5-3B,Apple M1 (Metal)
2601.07941v2_Moonworks Lunara Aesthetic Dataset.pdf,Moonworks Lunara Aesthetic Dataset,"Yan Wang, M M Sayeef Abdullah, Partho Hassan, Sabit Hassan",10.48550/arxiv.2601.07941,2601.07941,"text-to-image generation, prompt grounding, style conditioning, aesthetic analysis, Moonworks Lunara","This data card presents the first public release of the Lunara Aesthetic Dataset, a curated set of 2,000 image–prompt pairs for research on prompt grounding and style conditioning in text-to-image generation systems.",13.13,Qwen2.5-3B,Apple M1 (Metal)
2601.07946v1_Coupled Diffusion-Encoder Models for Reconstructio.pdf,DiffCoder: Coupled Diffusion–Encoder Models for Reconstruction of Flow Fields,"AmirPouya Hemmasian, Amir Barati Farimani",Not found,Not found,"Flow Field Reconstruction, Diffusion Models, Autoencoders, Kolmogorov Flow","DiffCoder proposes a coupled framework integrating a probabilistic diffusion model with a conventional convolutional ResNet encoder for data-driven flow-field reconstruction. It aims to recover distributional and spectral properties of fluid flows, improving spectral accuracy under aggressive compression compared to Variational Autoencoders.",11.73,Qwen2.5-3B,Apple M1 (Metal)
2601.07948v1_Reinforcement Learning Methods for Neighborhood Se.pdf,Reinforcement Learning Methods for Neighborhood Selection in Local Search,"Yannick Molinghen1, Augustin Delecluse2,3, Renaud De Landtsheer4, Stefano Michelini4",,,"Local Search, Reinforcement Learning, Combinatorial Optimization","This study evaluates reinforcement learning-based neighborhood selection strategies in local search metaheuristics, comparing them against multiple baselines across three problems: the traveling salesman problem, the pickup and delivery problem with time windows, and the car sequencing problem. It highlights the need for carefully designed reward functions to provide stable and informative learning signals, and shows that ε-greedy consistently ranks among the best performers, while deep reinforcement learning approaches have higher computational overhead and longer runtime.",11.61,Qwen2.5-3B,Apple M1 (Metal)
2601.07951v1_Hybrid SARIMA LSTM Model for Local Weather Forecas.pdf,Hybrid SARIMA–LSTM Model for Local Weather Forecasting: A Residual-Learning Approach for Data-Driven Meteorological Prediction,"Shreyas Rajeev, Karthik Mudenahalli, Amit Mallappa",,,"weather forecasting, SARIMA, LSTM, residual learning, Fourier seasonal encoding, long-term prediction","This paper proposes a hybrid SARIMA–LSTM model for local weather forecasting. SARIMA models the long-term seasonal trend, while LSTM learns the random changes in prediction errors. Fourier seasonal encoding is used to clearly show the yearly cycle, and a stabilized recursive forecasting mechanism keeps predictions within a 293-day future horizon without direct observation. The goal is to accurately predict daily average temperature in New York City using data from 2020 to 2023.",12.75,Qwen2.5-3B,Apple M1 (Metal)
2601.07953v1_Quantum automated theorem proving.pdf,Quantum automated theorem proving,"Zheng-Zhi Sun, Qi Ye, Dong-Ling Deng",Not found,Not found,"Quantum computing, Automated theorem proving, Quantum resolution, Quantum algebraic proving, Automated reasoning, Geometric theorems","Proposes a generic framework for quantum automated theorem proving, leveraging intrinsic quantum features to achieve potential advantages in automated reasoning tasks, including propositional and first-order logic with quadratic query complexity, and geometric theorems with quadratic better query complexity.",11.46,Qwen2.5-3B,Apple M1 (Metal)
2601.07957v1_LWMSCNN-SE A Lightweight Multi-Scale Network for E.pdf,LWMSCNN-SE: A Lightweight Multi-Scale Network for Efficient Maize Disease Classification on Edge Devices,"Fikadu Weloday, Jianmei Su",Not found,Not found,"lightweight CNN, multi-scale feature extraction, attention mechanism, plant pathology","This paper proposes LWMSCNN-SE, a lightweight convolutional neural network (CNN) that integrates multi-scale feature extraction, depthwise separable convolutions, and squeeze-and-Excitation (SE) attention mechanisms. The model achieves 96.63% classification accuracy with only 241,348 parameters and 0.666 GFLOPs, making it suitable for real-time deployment in field applications.",12.04,Qwen2.5-3B,Apple M1 (Metal)
2601.07958v1_LJ-Spoof A Generatively Varied Corpus for Audio An.pdf,LJ-SPOOF: A GENERATIVELY V ARIED CORPUS FOR AUDIO ANTI-SPOOFING AND SYNTHESIS SOURCE TRACING,"Surya Subramani, Hashim Ali, Hafiz Malik",,,"Anti-Spoofing, Speaker Verification, Deepfake, Source tracing, Synthetic Speech","Speaker-specific anti-spoofing and synthesis-source tracing are central challenges in audio anti-spoofing. This paper introduces LJ-Spoof, a speaker-specific, generatively diverse corpus that systematically varies prosody, vocoders, generative hyperparameters, bona fide prompt sources, training regimes, and neural post-processing. The corpus spans one speaker including studio-quality recordings, 30 TTS families, 500 generatively variant subsets, 10 bona fide neural-processing variants, and more than 3 million utterances. This variation-dense design enables robust speaker-conditioned anti-spoofing and fine-grained synthesis-source tracing. The dataset is positioned as both a practical reference training resource and a benchmark evaluation suite for anti-spoofing and source tracing.",12.57,Qwen2.5-3B,Apple M1 (Metal)
2601.07964v1_Executable Ontologies in Game Development From Alg.pdf,Executable Ontologies in Game Development: From Algorithmic Control to Semantic World Modeling,Alexander Boldachev,,,"executable ontologies, game ai, behavior trees, goap, event semantics, dataflow architecture, semantic modeling","This paper examines the application of Executable Ontologies (EO) in game development, arguing that EO represents a paradigm shift from algorithmic behavior programming to semantic world modeling. Using a survival game scenario, it demonstrates how EO achieves priority-based task interruption through dataflow conditions rather than explicit preemption logic, and compares it with Behavior Trees (BT) and Goal-Oriented Action Planning (GOAP).",13.11,Qwen2.5-3B,Apple M1 (Metal)
2601.07965v1_When Models Know When They Do Not Know Calibration.pdf,WHENMODELSKNOWWHENTHEYDONOTKNOW,"Chenjie Hao, Weyl Lu, Yuko Ishiwaka, Zengyi Li, Weier Wan, Yubei Chen",Not found,Not found,"model calibration, model uncertainty, confidence, cascading, data cleaning","This work proposes a simple, effective, and universal training-free method for model calibration, cascading, and data cleaning, applicable to both vision and language models. The method leverages calibrated confidence to improve model efficiency and reliability.",11.52,Qwen2.5-3B,Apple M1 (Metal)
2601.07969v1_Tuberculosis Screening from Cough Audio Baseline M.pdf,"Tuberculosisscreening fromcoughaudio: Baselinemodels, clinicalvariables, anduncertaintyquantification","George P. Kafentzis, Efstratios Selisios",Not found,2601.07969,"Tuberculosis, Machine Learning, Cough Audio, Cross-Validation, Uncertainty Quantification, Feature Extraction","This paper proposes a standardized framework for automatic tuberculosis (TB) detection from cough audio and clinical data using machine learning. It addresses the gap in measuring progress due to varying datasets, cohort definitions, feature representations, model families, validation protocols, and reported metrics. The authors establish a strong, well-documented baseline using a recently compiled dataset from several countries, covering feature extraction, multimodal fusion, cougher-independent evaluation, and uncertainty quantification. They release the full experimental protocol to facilitate benchmarking.",12.55,Qwen2.5-3B,Apple M1 (Metal)
2601.07973v1_Cultural Compass A Framework for Organizing Societ.pdf,Cultural Compass: A Framework for Organizing Societal Norms to Detect Violations in Human-AI Conversations,"Myra Cheng, Vinodkumar Prabhakaran, Alice Oh, Hayk Stepanyan, Aishwarya Verma, Charu Kalia, Erin MacMurray van Liemt, Sunipa Dev",Not provided,2601.07973,"Generative AI, Cultural norms, Human-AI interaction, Norm adherence, Sociocultural norms","This paper introduces a taxonomy of societal norms to clarify their contexts, specifications, and mechanisms, and demonstrates how this taxonomy can be used to automatically evaluate models' adherence to these norms in naturalistic, open-ended settings. The authors find that state-of-the-art models frequently violate norms, with rates varying by model, interactional context, and country. They also show that violation rates vary by prompt intent and situational framing. The taxonomy and evaluation pipeline enable nuanced, context-sensitive evaluation of cultural norm adherence in realistic settings.",12.32,Qwen2.5-3B,Apple M1 (Metal)
2601.07988v1_From Word Sequences to Behavioral Sequences Adapti.pdf,From Word Sequences to Behavioral Sequences: Adapting Modeling and Evaluation Paradigms for Longitudinal NLP,"Adithya V Ganesan†‡, Vasudha Varadarajan⊙, Oscar NE Kjell‡, Whitney R Ringwaldδ, Scott Feltman†, Benjamin J Luft†, Roman Kotov†, Ryan L Boyd∆Θ, H Andrew Schwartz†‡",Not found,Not found,"longitudinal NLP, behavioral sequences, diary studies, mental health prediction, person-indexed sequences","While traditional NLP treats documents as independent and unordered samples, longitudinal studies reveal that documents are nested within authors and ordered in time, forming person-indexed, time-ordered behavioral sequences. This mismatch propagates through the standard NLP pipeline, leading to issues such as leaky person-specific signal, scrambled temporal order, and lack of clear real-world generalization goals. The authors propose a longitudinal modeling and evaluation paradigm that updates four parts of the pipeline: evaluation splits, accuracy metrics, sequence inputs, and model internals. They demonstrate the traditional pipeline's limitations and propose improvements on a dataset of 17k daily diary transcripts paired with PTSD symptom severity from 238 participants, finding that traditional document-level evaluation can yield substantially different and sometimes reversed conclusions compared to their ecologically valid modeling and evaluation.",13.09,Qwen2.5-3B,Apple M1 (Metal)
2601.07994v2_DYCP Dynamic Context Pruning for Long-Form Dialogu.pdf,DYCP: Dynamic Context Pruning for Long-Form Dialogue with LLMs,"Nayoung Choi, Jonathan Zhang, Jinho D. Choi",,,"Large Language Models, Dialogue Systems, Context Management, Dynamic Pruning, Long-Form Conversations","This paper introduces DYCP, a lightweight context management method for long-form dialogue with Large Language Models (LLMs). It dynamically segments and retrieves relevant memory at query time, preserving the sequential structure of dialogue without predefined topic boundaries and supporting efficient, adaptive context retrieval. Across three long-form dialogue benchmarks, DyCP consistently improves answer quality while reducing response latency.",11.6,Qwen2.5-3B,Apple M1 (Metal)
2601.08000v1_Reasoning over Precedents Alongside Statutes Case-.pdf,Reasoning over Precedents Alongside Statutes,"Can Jin, Rui Wu, Tong Che, Qixin Zhang, Hongwu Peng, Jiahui Zhao, Wenqi Wei, Ligong Han, Zhao Zhang, Yuan Cao, Ruixiang Tang, Dimitris N. Metaxas",Not found,Not found,"Large Language Models, Safety, Deliberative Alignment, Reinforcement Learning, LLaMA, GPT, Cybersecurity","This work evaluates the effectiveness of explicitly specifying extensive safety codes versus demonstrating them through illustrative cases for Large Language Models (LLMs) in ensuring safety without over-refusal. It proposes CADA, a case-augmented deliberative alignment method for LLMs using reinforcement learning on self-generated safety reasoning chains, which enhances harmlessness, robustness, and utility across diverse benchmarks.",11.66,Qwen2.5-3B,Apple M1 (Metal)
2601.08003v1_LLM Review Enhancing Creative Writing via Blind Pe.pdf,LLM Review: Enhancing Creative Writing via Blind Peer Review Feedback,"Weiyue Li*, Mingxiao Song∗, Zhenda Shen∗, Dachuan Zhao∗, Yunfan Long, Yi Li, Yongce Li, Ruyi Yang, Mengyu Wang",,,"Large Language Models, Creative Writing, Peer Review, Human-Large Language Model Collaboration","A framework that enhances creativity by constraining rather than maximizing information flow through a mechanism called Blind Peer Review, where agents provide targeted feedback on peers' initial drafts but revise independently.",11.26,Qwen2.5-3B,Apple M1 (Metal)
2601.08005v1_Internal Deployment Gaps in AI Regulation.pdf,Internal Deployment Gaps in AI Regulation,JOE Kwon∗ and STEPHEN CASPER,,,"AI regulation, internal deployment, frontier AI, regulatory gaps","This paper examines the gaps in existing AI regulation frameworks that may limit oversight of internal deployment of AI systems. It identifies three main gaps: scope ambiguity, point-in-time compliance assessments, and information asymmetries. The paper analyzes why these gaps persist and proposes potential approaches to address them.",11.6,Qwen2.5-3B,Apple M1 (Metal)
2601.08011v1_TP-Blend Textual-Prompt Attention Pairing for Prec.pdf,TP-Blend: Textual-Prompt Attention Pairing for Precise Object-Style Blending in Diffusion Models,"Xin Jin, Yichuan Zhong, Yapeng Tian",Not provided,2601.08011,"Diffusion models, Text-conditioned editing, Object blending, Attention fusion","Presenting TP-Blend, a lightweight training-free framework that receives two separate textual prompts, one specifying a blend object and the other defining a target style, and injects both into a single denoising trajectory. TP-Blend uses two complementary attention processors, CAOF and SASF, to achieve precise object-style blending.",11.92,Qwen2.5-3B,Apple M1 (Metal)
2601.08017v1_Representations of Text and Images Align From Laye.pdf,Representations of Text and Images Align From Layer One,"Evzen Wybitul, Javier Rando, Florian Tram`er, Stanislav Fort",Not found,2601.08017,"vision-language models, modal alignment, image-text alignment, DeepDream, model interpretability","This work shows that for a variety of concepts in adapter-based vision-language models, the representations of their images and their text descriptions are meaningfully aligned from the very first layer, contradicting the established view that such alignment only appears in late layers. The authors demonstrate this using a new synthesis-based method inspired by DeepDream.",12.36,Qwen2.5-3B,Apple M1 (Metal)
2601.08026v2_FigEx2 Visual-Conditioned Panel Detection and Capt.pdf,FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures,"Jifeng Song, Arun Das, Pan Wang, Hui Ji, Kun Zhao, Yufei Huang",,,"Scientific compound figures, Panel detection, Captioning, Open-ended captioning, Noise-aware gated fusion, Staged optimization, Supervised learning, Reinforcement learning, CLIP-based alignment, BERTScore-based semantic rewards, BioSci-Fig-Cap, Zero-shot transferability","This paper proposes FigEx2, a visual-conditioned framework for detecting and generating panel-wise captions for scientific compound figures. It addresses the challenge of missing or incomplete captions in real pipelines by introducing a noise-aware gated fusion module and a staged optimization strategy combining supervised and reinforcement learning. Experimental results show superior performance in detection and captioning tasks.",11.93,Qwen2.5-3B,Apple M1 (Metal)
2601.08043v1_The Role of Noisy Data in Improving CNN Robustness.pdf,The Role of Noisy Data in Improving CNN Robustness for Image Classification,"Oscar H. Ramírez-Agudela, Nicoleta Gorea, Aliza Reif, Lorenzo Bonasera, Michael Karla",,,"deep learning, CNNs, data quality, CIFAR-10, noise injection, image classification, model robustness","Data quality plays a central role in the performance and robustness of convolutional neural networks (CNNs) for image classification. This paper investigates the effect of deliberately introducing controlled noise into the training data to improve model robustness. Using the CIFAR-10 dataset, we evaluate the impact of three common corruptions, namely Gaussian noise, Salt-and-Pepper noise, and Gaussian blur at varying intensities and training set pollution levels. Experiments using a Resnet-18 model reveal that incorporating just 10% noisy data during training is sufficient to significantly reduce test loss and enhance accuracy under fully corrupted test conditions, with minimal impact on clean-data performance. These findings suggest that strategic exposure to noise can act as a simple yet effective regularizer, offering a practical trade-off between traditional data cleanliness and real-world resilience.",13.06,Qwen2.5-3B,Apple M1 (Metal)
2601.08049v1_Integrating Attendance Tracking and Emotion Detect.pdf,Integrating Attendance Tracking and Emotion Detection for Enhanced Student Engagement in Smart Classrooms,"Keith Ainebyona, Ann Move Oguti, Joseph Walusimbi, Ritah Kobusingye",,,"Affective computing, Attendance automation, Emotion detection, IoT, Smart classroom","This paper presents SCASED (Smart Classroom Attendance System with Emotion Detection), an IoT-based system that integrates automated attendance tracking with facial emotion recognition to support classroom engagement monitoring.",12.78,Qwen2.5-3B,Apple M1 (Metal)
2601.08052v1_Forecast Aware Deep Reinforcement Learning for Eff.pdf,Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms,"Nawazish Alia, Rachael Shaw, Karl Mason",Not provided,2601.08052v1,"Dairy farming, Reinforcement Learning, Deep Learning, Load scheduling, Battery storage, Water heating, Renewable energy, Electricity cost reduction, Sustainable development","This study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints, achieving up to 1.5% lower electricity cost compared to other methods.",13.4,Qwen2.5-3B,Apple M1 (Metal)
2601.08058v1_Reasoning Beyond Chain-of-Thought A Latent Computa.pdf,Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models,"Zhenghao He, Guangzhi Xiong, Bohan Liu, Sanchit Sinha, Aidong Zhang",Not provided,Not provided,"Large Language Models, Chain-of-Thought, Reasoning, Latent Features, Sparse Autoencoders","This work investigates the effectiveness of Chain-of-Thought (CoT) prompting in large language models (LLMs) and identifies a small set of latent features that are causally associated with LLM reasoning behavior. The study finds that steering a single reasoning-related latent feature can substantially improve accuracy without explicit CoT prompting, and this approach is more efficient for large models. The results suggest that multi-step reasoning in LLMs is supported by latent internal activations that can be externally activated, and CoT prompting is one effective but not unique way of activating this mechanism.",12.27,Qwen2.5-3B,Apple M1 (Metal)
2601.08065v1_A New Strategy for Verifying Reach-Avoid Specifica.pdf,A New Strategy for Verifying Reach-Avoid Specifications in Neural Feedback Systems,"Samuel I. Akinwande, Sydney M. Katz, Mykel J. Kochenderfer, Clark Barrett",Not found,2601.08065,"Neural networks, Reachability analysis, Forward analysis, Backward analysis, Neural feedback systems, Safety verification","This work introduces new algorithms to compute both over- and under-approximations of backward reachable sets for neural feedback systems, integrating these with forward analysis techniques to provide a unified verification framework.",12.6,Qwen2.5-3B,Apple M1 (Metal)
2601.08070v1_Semantic Gravity Wells Why Negative Constraints Ba.pdf,Semantic Gravity Wells: Why Negative Constraints Backfire,Shailesh Rana,Not found,Not found,"large language models, instruction-following, negative constraints, semantic pressure, failure mechanisms","Negative constraints, instructions of the form 'do not use word X', represent a fundamental test of instruction-following capability in large language models. Despite their apparent simplicity, these constraints fail with striking regularity, and the conditions governing failure have remained poorly understood. This paper presents the first comprehensive mechanistic investigation of negative instruction failure, introducing semantic pressure as a quantitative measure of the model's intrinsic probability of generating the forbidden token. It demonstrates that violation probability follows a tight logistic relationship with pressure, and through layer-wise analysis, establishes that the suppression signal induced by negative instructions is present but systematically weaker in failures, leading to a 4.4× asymmetry in violation probability between failures and successes. The paper traces this asymmetry to two mechanistically distinct failure modes: priming failure and override failure, revealing a fundamental tension in negative constraint design.",12.79,Qwen2.5-3B,Apple M1 (Metal)
2601.08079v1_MemoBrain Executive Memory as an Agentic Brain for.pdf,MemoBrain: Executive Memory as an Agentic Brain for Reasoning,"Hongjin Qian, Zhao Cao, Zheng Liu*",,,"Executive Memory, Agent Reasoning, Long-Horizon Reasoning, Tool-Augmented Agents","This paper proposes MemoBrain, an executive memory model for tool-augmented agents, which constructs a dependency-aware memory over reasoning steps to capture salient intermediate states and their logical relations. Operating alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context, pruning invalid steps, folding completed sub-trajectories, and preserving a compact, high-salience reasoning backbone under a fixed context budget. The model is evaluated on challenging long-horizon benchmarks, demonstrating consistent improvements over strong baselines.",11.97,Qwen2.5-3B,Apple M1 (Metal)
2601.08089v1_Q-realign Piggybacking Realignment on Quantization.pdf,Q-realign: Piggybacking Realignment on Quantization for Safe and Efficient LLM Deployment,"Qitao Tan, Xiaoying Song, Ningxi Cheng, Ninghao Liu, Xiaoming Zhai, Lingzi Hong, Yanzhi Wang, Zhen Xiang, Geng Yuan",,,"quantization, realignment, large language models, fine-tuning, safety, deployment","This paper proposes Q-realign, a post-hoc defense method based on post-training quantization, aimed at reducing unsafe behaviors in large language models while preserving task performance. It decouples safety alignment from fine-tuning and significantly reduces memory usage and GPU hours.",11.28,Qwen2.5-3B,Apple M1 (Metal)
2601.08094v1_Local-Global Feature Fusion for Subject-Independen.pdf,Local-Global Feature Fusion for Subject-Independent EEG Emotion Recognition,"Zheng Zhou, Isabella McEvoy, Camilo E. Valderrama",Not provided,Not provided,"EEG emotion recognition, subject-independent, local-global fusion, transformer, domain adaptation","This paper proposes a fusion framework that integrates local, channel-wise descriptors and global, trial-level descriptors to improve cross-subject generalization in subject-independent EEG emotion recognition. The method achieves approximately 40% mean accuracy in 7-class subject-independent emotion recognition, outperforming single-view and classical baselines.",12.01,Qwen2.5-3B,Apple M1 (Metal)
2601.08104v1_High-Fidelity Modeling of Stochastic Chemical Dyna.pdf,"HIGH-FIDELITYMODELING OFSTOCHASTICCHEMICAL
DYNAMICS ONCOMPLEXMANIFOLDS: A MULTI-SCALE
SIREN-PINN FRAMEWORK FOR THECURVATURE-PERTURBED
GINZBURG-LANDAUEQUATION","Julian Evan Chrisnanto, Salsabila Rahma Alia, Nurfauzi Fadillah, Yulison Herry Chrisnanto",Not found,2601.08104,"Physics-Informed Neural Networks (PINNs), Spatiotemporal Chaos, Inverse Geometric Problems, Reaction-Diffusion Systems, Defect Turbulence, Riemann Manifold Learning","This work proposes a Multi-Scale SIREN-PINN architecture to accurately model spatiotemporal chaos in reaction-diffusion systems on complex manifolds, particularly when the underlying catalytic surface has complex, unknown topography. The architecture leverages periodic sinusoidal activations with frequency-diverse initialization to resolve high-frequency gradients and preserve topological invariants. It achieves state prediction error ϵL2 ≈1.92×10−2, outperforming standard baselines by an order of magnitude, and solves the inverse pinning problem to reconstruct hidden Gaussian curvature fields from chaotic wave dynamics.",13.02,Qwen2.5-3B,Apple M1 (Metal)
2601.08107v1_STO-RL Offline RL under Sparse Rewards via LLM-Gui.pdf,STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order,"Chengyang Gu, Yuxin Pan, Hui Xiong, Yize Chen",10.1145/nnnnnnn.nnnnnnn,,"Offline RL, Temporal order, Large Language Models","Offline reinforcement learning (RL) enables policy learning from pre-collected datasets, avoiding costly and risky online interactions. However, it often struggles with long-horizon tasks involving sparse rewards. This paper proposes STO-RL, an offline RL framework that leverages large language models (LLMs) to generate temporally ordered subgoal sequences and corresponding state-to-subgoal-stage mappings, applying potential-based reward shaping to transform sparse terminal rewards into dense, temporally consistent signals, promoting subgoal progress while avoiding suboptimal solutions.",12.1,Qwen2.5-3B,Apple M1 (Metal)
2601.08108v1_Debiasing Large Language Models via Adaptive Causa.pdf,Debiasing Large Language Models via Adaptive Causal Prompting with Sketch-of-Thought,"Bowen Li, Ziqi Xu, Jing Ren, Renqiang Luo, Xikun Zhang, Xiuzhen Zhang, Yongli Ren, Feng Xia",Not found,Not found,"Large Language Models, LLMs, prompting, Chain-of-Thought, Sketch-of-Thought, causal inference, bias mitigation","Despite notable advancements in prompting methods for Large Language Models (LLMs), such as Chain-of-Thought (CoT), existing strategies still suffer from excessive token usage and limited generalisability across diverse reasoning tasks. To address these limitations, we propose an Adaptive Causal Prompting with Sketch-of-Thought (ACPS) framework, which leverages structural causal models to infer the causal effect of a query on its answer and adaptively select an appropriate intervention (i.e., standard front-door and conditional front-door adjustments). This design enables generalisable causal reasoning across heterogeneous tasks without task-specific retraining. By replacing verbose CoT with concise Sketch-of-Thought, ACPS enables efficient reasoning that significantly reduces token usage and inference cost. Extensive experiments on multiple reasoning benchmarks and LLMs demonstrate that ACPS consistently outperforms existing prompting baselines in terms of accuracy, robustness, and computational efficiency.",12.96,Qwen2.5-3B,Apple M1 (Metal)
2601.08109v1_CSQL Mapping Documents into Causal Databases.pdf,Csql: Mapping Documents into Causal Databases,Sridhar Mahadevan,Not found,2601.08109,"Causality, Natural Language, Databases, SQL, AI, Machine Learning","We describe a novel system, Csql, that automatically converts a collection of unstructured text documents into an SQL-queryable causal database (CDB). Csql supports causal analysis over document collections rather than purely associative retrieval.",11.5,Qwen2.5-3B,Apple M1 (Metal)
2601.08118v1_MirrorBench An Extensible Framework to Evaluate Us.pdf,MIRRORBENCH: ANEXTENSIBLEFRAMEWORK TOEVALUATEUSER-PROXY AGENTS FORHUMAN-LIKENESS,"Ashutosh Hathidara, Julien Yu, Vaishali Senthil, Sebastian Schreiber, Anil Babu Ankisettipalli",Not found,Not found,"user proxy agents, human likeness, large language models, reproducible benchmarking, user simulation, LLM evaluation","MIRRORBENCH is a reproducible, extensible benchmarking framework designed to evaluate user proxy agents on their ability to produce human-like user utterances across diverse conversational tasks, explicitly decoupled from downstream task success. It features a modular execution engine with typed interfaces, metadata-driven registries, multi-backend support, caching, and robust observability, supporting pluggable user proxies, datasets, tasks, and metrics. The framework includes three lexical-diversity metrics and three LLM-judge-based metrics, yielding variance-aware results across four open datasets, revealing systematic gaps between user proxies and real human users. The framework is open source and includes a simple command-line interface for running experiments, managing configurations and caching, and generating reports.",12.67,Qwen2.5-3B,Apple M1 (Metal)
2601.08125v1_How vehicles change lanes after encountering crash.pdf,How vehicles change lanes after encountering crashes: Empirical analysis and modeling,"Kequan Chen, Yuxuan Wang, Pan Liu, Victor L. Knoop, David Z. W. Wang, Yu Han",,,"crashes, lane changes, empirical analysis, vehicle behavior","This paper explores how vehicles adjust their lanes following collisions, providing both empirical data and modeling insights.",13.78,Qwen2.5-3B,Apple M1 (Metal)
2601.08127v1_PathoGen Diffusion-Based Synthesis of Realistic Le.pdf,PathoGen: Diffusion-Based Synthesis of Realistic Lesions in Histopathology Images,"Mohamad Koohi-Moghadam1*, Mohammad-Ali Nikouei Mahani1",,1912.04463,"histopathology, AI, lesions, generative models, diffusion, image synthesis","A diffusion-based generative model that enables controllable, high-fidelity inpainting of lesions into benign histopathology images, validating its performance across four diverse datasets representing distinct diagnostic challenges.",12.37,Qwen2.5-3B,Apple M1 (Metal)
2601.08128v1_Embedded AI Companion System on Edge Devices.pdf,Embedded AI Companion System on Edge Devices,"Rahul Gupta ∗1 and Stephen Hsu 1,2",Not found,2601.08128,"AI companion, edge devices, memory systems, low-latency, personalization","This paper proposes a memory paradigm for an embedded AI companion system on edge devices, alternating between active and inactive phases to minimize latency and maintain long-term personalization under resource constraints.",11.19,Qwen2.5-3B,Apple M1 (Metal)
2601.08133v1_How Do Optical Flow and Textual Prompts Collaborat.pdf,How Do Optical Flow and Textual Prompts Collaborate to Assist in Audio-Visual Semantic Segmentation?,"Peng Gao, Yujian Lee, Yongqi Xu, Wentao Fan",Not found,Not found,"audio-visual semantic segmentation, optical flow, textual prompts, semantic understanding, machine perception","This paper introduces a novel collaborative framework, Stepping Stone Plus (SSP), which integrates optical flow and textual prompts to assist in audio-visual semantic segmentation. The pre-mask technique leverages optical flow to capture motion dynamics, providing essential temporal context for precise segmentation. SSP also incorporates specific textual prompts to address stationary sound-emitting objects. A visual-textual alignment module (VTA) facilitates cross-modal integration, delivering more coherent and contextually relevant semantic interpretations. Experimental results demonstrate that SSP outperforms existing AVS methods, delivering efficient and precise segmentation results.",11.8,Qwen2.5-3B,Apple M1 (Metal)
2601.08139v1_Subspace Alignment for Vision-Language Model Test-.pdf,Subspace Alignment for Vision-Language Model Test-time Adaptation,"Zhichen Zeng, Wenxuan Bao, Xiao Lin, Ruizhong Qiu, Tianxin Wei, Xuying Ning, Yuchen Yan, Chen Luo, Monica Xiao Cheng, Jingrui He, Hanghang Tong",Not found,Not found,"vision-language models, test-time adaptation, zero-shot adaptation, subspace alignment, distribution shifts, visual noise, modalities gap","This paper proposes SubTTA, a method to enhance zero-shot predictions by aligning the semantic subspaces of both modalities, addressing the limitations of existing test-time adaptation methods. It bridges the modality gap by extracting principal subspaces and aligning visual manifolds to textual semantic anchors, and eliminates visual nuisance by projecting aligned visual features onto task-specific textual subspaces. Extensive experiments on various benchmarks and VLM architectures demonstrate the effectiveness of SubTTA, yielding an average improvement of 2.24% over state-of-the-art TTA methods.",12.2,Qwen2.5-3B,Apple M1 (Metal)
2601.08141v1_Qalb Largest State-of-the-Art Urdu Large Language .pdf,Qalb: Largest State-of-the-Art Urdu Large Language Model,"1st Muhammad Taimoor Hassan, 2st Jawad Ahmed, 3st Muhammad Awais",,,"Urdu language model, continued pre-training, low-resource NLP, LoRA, language adaptation","Qalb is an Urdu language model developed through a two-stage approach: continued pre-training followed by supervised fine-tuning. It achieves state-of-the-art performance with comprehensive evaluation across seven diverse tasks including Classification, Sentiment Analysis, and Reasoning.",11.13,Qwen2.5-3B,Apple M1 (Metal)
2601.08146v2_Mechanisms are Transferable Data-Efficient Low-Res.pdf,Mechanisms are Transferable: Data-Efficient Low-Resource Adaptation via Circuit-Targeted Supervised Fine-Tuning,"Khumaisa Nur'aini1, Ayu Purwarianti2, Alham Fikri Aji3, Derry Wijaya1,4",,,"transfer learning, low-resource adaptation, supervised fine-tuning, circuit-targeted, catastrophic forgetting","Adapting large language models to low-resource languages is challenging due to scarce labeled data and instability in full-model fine-tuning. This paper proposes Circuit-Targeted Supervised Fine-Tuning (CT-SFT), which uses a label-balanced mean baseline and task-directional relevance scoring to identify task-relevant attention heads in a proxy-language checkpoint. CT-SFT improves cross-lingual accuracy while updating only a small subset of model parameters, reducing catastrophic forgetting and preserving source competence during transfer.",12.03,Qwen2.5-3B,Apple M1 (Metal)
2601.08148v1_Enriching Semantic Profiles into Knowledge Graph f.pdf,Enriching Semantic Profiles into Knowledge Graph for Recommender Systems Using Large Language Models,"Seokho Ahn, Sungbok Shin, Young-Duk Seo",10.1145/3770854.3780324,,"Recommendation, Semantic Profiling, Large Language Models, Knowledge Graphs","Rich and informative profiling to capture user preferences is essential for improving recommendation quality. This work proposes a new recommendation model, SPiKE, which uses large language models to generate semantic profiles for all KG entities and integrates these profiles into the KG. Pairwise profile preference matching is also aligned during training. Experiments demonstrate that SPiKE consistently outperforms state-of-the-art KG- and LLM-based recommenders in real-world settings.",11.5,Qwen2.5-3B,Apple M1 (Metal)
2601.08149v1_Dynamic Graph Structure Learning via Resistance Cu.pdf,Dynamic Graph Structure Learning via Resistance Curvature Flow,"Chaoqun Fei, Huanjiang Liu, Tinglve Zhou, Y angyang Li 1, Tianyong Hao",Not found,2601.08149v1,"Graph structure learning, Dynamic graph, Resistance curvature flow, Deep metric learning, Manifold learning","Introduces a novel curvature flow method based on effective resistance from circuit theory, improving representation quality and downstream performance in graph structure learning tasks.",14.8,Qwen2.5-3B,Apple M1 (Metal)
2601.08156v1_Project Synapse A Hierarchical Multi-Agent Framewo.pdf,Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions,"Arin Gopalan Yadav, Varad Dherange, Kumar Shivam",Not provided,2601.08156,"last-mile delivery, multi-agent systems, hybrid memory, autonomous resolution, disruption management","This paper introduces Project Synapse, a novel hierarchical multi-agent framework designed to autonomously resolve last-mile delivery disruptions, leveraging a hybrid memory architecture that integrates short-term working memory, long-term episodic memory of past incidents, and semantic memory of organizational policies.",13.97,Qwen2.5-3B,Apple M1 (Metal)
2601.08160v1_SwiftMem Fast Agentic Memory via Query-aware Index.pdf,SwiftMem: Fast Agentic Memory via Query-aware Indexing,"Anxin Tian, Yiming Li, Xing Li, Hui-Ling Zhen, Lei Chen, Xianzhi Yu, Zhenhua Dong, Mingxuan Yuan",,,"Agentic Memory, Query-aware Indexing, Temporal Indexing, Semantic DAG-Tag Index, Memory Fragmentation, Embedding-Tag Co-consolidation","Agentic memory systems are crucial for LLM agents to maintain long-term context and retrieve relevant information efficiently. However, existing memory frameworks suffer from a fundamental limitation: they perform exhaustive retrieval across the entire storage layer regardless of query characteristics. SwiftMem proposes a query-aware agentic memory system that achieves sub-linear retrieval through specialized indexing over temporal and semantic dimensions, enabling faster search compared to state-of-the-art baselines while maintaining competitive accuracy.",12.32,Qwen2.5-3B,Apple M1 (Metal)
2601.08166v1_ZeroDVFS Zero-Shot LLM-Guided Core and Frequency A.pdf,Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms,"Mohammad Pivezhandi1, Mahdi Banisharif2, Abusayeed Saifullah3, Ali Jannesari2",Not found,2601.08166,"Dynamic Voltage and Frequency Scaling (DVFS), Task-to-Core Allocation, Embedded Systems, Thermal Management, Energy Efficiency, Reinforcement Learning, Zero-Shot Learning","This paper proposes a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. The framework enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. The authors introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The framework integrates direct reinforcement learning with model-based planning, achieving 20× faster convergence than model-free methods. Experiments demonstrate improved energy efficiency and makespan compared to Linux ondemand governor.",13.22,Qwen2.5-3B,Apple M1 (Metal)
2601.08173v1_The Agents First Day Benchmarking Learning Explora.pdf,"The Agent’s First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios","Daocheng Fu1,2,†, Jianbiao Mei3,2,†, Rong Wu3,2,†, Xuemeng Yang2,†, Jia Xu2, Ding Wang 2, Pinlong Cai 2, Yong Liu 3, B Licheng Wen 2, B Botian Shi 2",Not found,Not found,"Multi-modal Large Language Models, Workflow Automation, Stochastic Environment, Dynamic Task Scheduling, Active Exploration, Continuous Learning","The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation, but existing research mainly focuses on performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. This paper introduces Trainee-Bench, a dynamic evaluation environment that simulates a 'trainee' agent continuously exploring a novel setting. Unlike traditional benchmarks, Trainee-Bench evaluates agents along three dimensions: context-aware scheduling for streaming tasks with varying priorities, prudent information acquisition to reduce hallucination via active exploration, and continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning.",12.66,Qwen2.5-3B,Apple M1 (Metal)
2601.08176v1_Prompt-Based Clarity Evaluation and Topic Detectio.pdf,Prompt-Based Clarity Evaluation and Topic Detection in Political Question Answering,"Lavanya Prahallad, Sai Utkarsh Choudarypally, Pragna Prahallad, Pranathi Prahallad",Not provided,Not provided,"Clarity evaluation, Prompt engineering, Political Question-Answering, Large language models, Chain-of-thought prompting","This paper evaluates the impact of prompt design on automatic clarity evaluation of large language model responses, particularly in political question answering. It compares a GPT-3.5 baseline with GPT-5.2 under three prompting strategies: simple prompting, chain-of-thought prompting, and chain-of-thought with few-shot examples. The results show that GPT-5.2 consistently outperforms the GPT-3.5 baseline on clarity prediction, with accuracy improving from 56% to 63% under chain-of-thought with few-shot prompting. Chain-of-thought prompting yields the highest evasion accuracy (34%), though improvements are less stable across fine-grained evasion categories. The paper also evaluates topic identification and finds that reasoning-based prompting improves accuracy from 60% to 74% relative to human annotations. Overall, the findings indicate that prompt design reliably improves high-level clarity evaluation, while fine-grained evasion and topic detection remain challenging despite structured reasoning prompts.",13.31,Qwen2.5-3B,Apple M1 (Metal)
2601.08179v1_Instruction-Driven 3D Facial Expression Generation.pdf,Instruction-Driven 3D Facial Expression Generation,"Anh H. V o, Tae-Seok Kim, Hulin Jin, Soo-Mi Choi, Yong-Guk Kim*",Not found,Not found,"Facial Expression, 3D Avatar, Facial Transition, Instruction-Driven, Controllable Avatar","This study presents a new framework for instruction-driven facial expression generation, which produces a 3D face and transforms the facial expression from one designated expression to another. The study also proposes the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to generate a facial expression sequence according to the given instruction. The results suggest that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets.",12.38,Qwen2.5-3B,Apple M1 (Metal)
2601.08183v2_GI-Bench A Panoramic Benchmark Revealing the Knowl.pdf,GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards,"Yan Zhu, Te Luo, Pei-Yao Fu, Zhen Zhang, Zi-Long Wang, Yi-Fan Qu, Zi-Han Geng, Jia-Qi Xu, Lu Yao, Li-Yun Ma, Wei Su, Wei-Feng Chen, Quan-Lin Li, Shuo Wang, Ping-Hong Zhou",,,"Multimodal Language Models, Gastrointestinal Endoscopy, Clinical Benchmarks, Knowledge-Experience Dissociation","This study systematically evaluates state-of-the-art Multimodal Large Language Models (MLLMs) across a panoramic gastrointestinal endoscopy workflow, comparing their performance against human endoscopists and clinical workflows.",13.68,Qwen2.5-3B,Apple M1 (Metal)
2601.08185v1_Autonomous Materials Exploration by Integrating Au.pdf,Autonomous Materials Exploration by Integrating Automated Phase Identification and AI-Assisted Human Reasoning,"Ming-Chiang Chang, Maximilian Amsler, Duncan R. Sutherland, Sebastian Ament, Katie R. Gann, Lan Zhou, Louisa M. Smieska, Arthur R. Woll, John M. Gregoire, Carla P. Gomes, R. Bruce van Dover, Michael O. Thompson",Not found,Not found,"materials science, autonomous experimentation, AI, robotics, phase identification, human reasoning, synthesis","This paper presents an extension to SARA, a scientific autonomous reasoning agent, which integrates automated phase identification with AI-assisted human reasoning to accelerate materials development. The authors demonstrate the efficiency of their AI implementation and show that human input can significantly improve sampling efficiency. They also showcase the utility of human-in-the-loop autonomous experimentation for the Bi-Ti-O system, identifying extensive processing domains that stabilize δ-Bi2O3 and Bi2Ti2O7, and providing evidence confirming predictions about the effects of cationic substitutional doping of TiO2 with Bi. The methods developed enable the discovery of new materials and new understanding of materials synthesis and properties.",12.86,Qwen2.5-3B,Apple M1 (Metal)
2601.08187v2_Improving LLM Reasoning with Homophily-aware Struc.pdf,Improving LLM Reasoning with Homophily-Aware Structural and Semantic Graph Compression,"Zijun Di, Bin Lu, Huquan Kang, Luoyi Fu, Jiaxin Ding, Xiaoying Gan, Lei Zhou, Xinbing Wang, Chenghu Zhou",Not found,2601.08187,"Large Language Models, Text-Attributed Graphs, Homophily, Graph Compression, LLM Reasoning","This paper proposes HS2C, a framework that enhances LLMs reasoning performance by exploiting graph homophily, structurally and semantically compressing the input. Extensive experiments on various benchmarks demonstrate its superiority and scalability.",12.07,Qwen2.5-3B,Apple M1 (Metal)
2601.08189v2_ForgetMark Stealthy Fingerprint Embedding via Targ.pdf,FORGETMARK: STEALTHY FINGERPRINT EMBEDDING VIA TARGETED UNLEARNING,"Zhenhua Xu, Haobo Zhang, Zhebo Wang, Qichen Liu, Haitao Xu, Wenpeng Xing, Meng Han",10.1109/ICASSP39486.2026.7487967,2309.16179,"Large Language Model, Copyright protection, Model Fingerprinting, Machine Unlearning","A stealthy fingerprinting framework that encodes provenance via targeted unlearning, avoiding high-perplexity triggers and reducing detectability and false triggers. Achieves 100% ownership verification on fingerprinted models while maintaining standard performance, surpassing backdoor baselines in stealthiness and robustness to model merging, and remains effective under moderate incremental fine-tuning.",11.72,Qwen2.5-3B,Apple M1 (Metal)
2601.08196v1_Evaluating Implicit Regulatory Compliance in LLM T.pdf,Evaluating Implicit Regulatory Compliance in LLM Tool Invocation via Logic-Guided Synthesis,"Da Song1,2, Yuheng Huang3*, Boqi Chen4, Tianshuo Cong1,2, Randy Goebel5, Lei Ma 3,5, Foutse Khomh 6",Not provided,Not provided,"Large Language Models, Tool Invocation, Regulatory Compliance, Logic-Guided Synthesis, Safety Constraints","The integration of large language models (LLMs) into autonomous agents has enabled complex tool use, yet in high-stakes domains, these systems must strictly adhere to regulatory standards beyond simple functional correctness. Existing benchmarks often overlook implicit regulatory compliance, thus failing to evaluate whether LLMs can autonomously enforce mandatory safety constraints. To fill this gap, we introduce LOGISAFETYGEN, a framework that converts unstructured regulations into Linear Temporal Logic oracles and employs logic-guided fuzzing to synthesize valid, safety-critical traces. Building on this framework, we construct LOGISAFETYBENCH, a benchmark comprising 240 human-verified tasks that require LLMs to generate Python programs that satisfy both functional objectives and latent compliance rules. Evaluations of 13 state-of-the-art (SOTA) LLMs reveal that larger models, despite achieving better functional correctness, frequently prioritize task completion over safety, resulting in non-compliant behavior.",12.78,Qwen2.5-3B,Apple M1 (Metal)
2601.08211v1_Adapting Rules of Official International Mahjong f.pdf,Adapting Rules of Official International Mahjong for Online Players,"Chucai Wang, Lingfeng Li, Yunlong Lu, Wenxin Li",Not found,Not found,"Mahjong, game design, champion AI, online players, first-mover advantage, subgoal scoring","This work evaluates the game balance of Official International Mahjong, a four-player traditional game, and proposes rule adaptations to make it more suitable for online players. The study reveals the first-mover advantage and issues in the subgoal scoring settings, and proposes compensatory points and refined scores for different tile patterns.",11.93,Qwen2.5-3B,Apple M1 (Metal)
2601.08223v2_DNF Dual-Layer Nested Fingerprinting for Large Lan.pdf,DNF: DUAL-LAYER NESTED FINGERPRINTING FOR LARGE LANGUAGE MODEL,"Zhenhua Xu, Yiran Zhao, Mengting Zhong, Dezhang Kong, Changting Lin, Tong Qiao, Meng Han",10.1109/ICASSP39664.2026.7461285,2603.17527,"Large Language Model, Copyright Protection, Model Fingerprinting, Backdoor","A black-box method that embeds a hierarchical backdoor by coupling domain-specific stylistic cues with implicit semantic triggers, achieving perfect fingerprint activation while preserving downstream utility.",10.97,Qwen2.5-3B,Apple M1 (Metal)
2601.08224v1_An Axiomatic Approach to General Intelligence SANC.pdf,An Axiomatic Approach to General Intelligence,"Daesuk Kwon, Won-gi Paeng",Not found,2601.08224,"axiomatization of intelligence, competitive selection, system tokens, reconstruction-compression trade-off, category formation, self-similar hierarchy, Gestalt completion","This paper proposes SANC(E3), an axiomatic framework for general intelligence, where representational units emerge through competitive selection, reconstruction, and compression under finite activation capacity. It draws a principled distinction between system tokens and sensory tokens, formalizing finite capacity, association from co-occurrence, similarity-based competition, confidence-based stabilization, and the reconstruction-compression-update trade-off. The framework unifies perception, imagination, prediction, planning, and action within a single representational and energetic process, extending to embodied and physical agents.",12.45,Qwen2.5-3B,Apple M1 (Metal)
2601.08226v1_Knowledge-based learning in Text-RAG and Image-RAG.pdf,Knowledge-based learning in Text-RAG and Image-RAG,"Alexander Shim, Khalil Saieh, Samuel Clarke",,,"Knowledge-based learning, Text-RAG, Image-RAG, Radiology, Vision Transformer, Large Language Model, Chest X-ray, Diagnostic accuracy, Hallucination reduction, Expected Calibration Error","This research analyzes and compares the multi-modal approach in the Vision Transformer (EVA-ViT) based image encoder with the LlaMA or ChatGPT LLM to reduce the hallucination problem and detect diseases in chest x-ray images. The text-based RAG effectively reduces hallucination by using external knowledge information, while the image-based RAG improves prediction confidence and calibration using KNN methods. The GPT LLM shows better performance, lower hallucination rate, and better Expected Calibration Error (ECE) compared to Llama Llama-based models. The research highlights the challenges of data imbalance and complex multi-stage structures but suggests a large experience environment and balanced example usage.",12.55,Qwen2.5-3B,Apple M1 (Metal)
2601.08230v1_GADPN Graph Adaptive Denoising and Perturbation Ne.pdf,GADPN: Graph Adaptive Denoising and Perturbation Networks via Singular Value Decomposition,"Hao Deng, Bo Liu",,1912.04557,"Graph Neural Networks, Graph Structure Learning, Network Representation Learning","Proposes GADPN, a simple yet effective framework for graph structure learning that adaptsively refines graph topology via low-rank denoising and generalized structural perturbation.",10.25,Qwen2.5-3B,Apple M1 (Metal)
2601.08235v2_MPCI-Bench A Benchmark for Multimodal Pairwise Con.pdf,MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents,"Shouju Wang, Haopeng Zhang",Not provided,Not provided,"Contextual Integrity, Multimodal Privacy, Language Models, Evaluation Benchmarks","This paper introduces MPCI-Bench, the first multimodal pairwise contextual integrity benchmark for evaluating privacy behavior in agentic settings. It addresses the limitations of existing benchmarks, which are predominantly text-centric and focus on negative refusal scenarios, overlooking multimodal privacy risks and the trade-off between privacy and utility.",10.92,Qwen2.5-3B,Apple M1 (Metal)
2601.08237v1_The End of Reward Engineering How LLMs Are Redefin.pdf,The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination,"Haoran Su, Yandong Sun, Congjia Yu",Not found,2601.08237,"multi-agent reinforcement learning, reward engineering, large language models, semantic reward specification, dynamic adaptation, human alignment","This paper argues that large language models (LLMs) enable a fundamental paradigm shift in reward engineering for multi-agent reinforcement learning, moving from hand-crafted numerical rewards to natural language objectives. It discusses recent work demonstrating LLMs' ability to generate human-level reward functions from language descriptions, adapt rewards dynamically without human intervention, and coordinate agents through semantic understanding. The paper also acknowledges challenges in computational cost, hallucination risks, and scalability.",12.9,Qwen2.5-3B,Apple M1 (Metal)
2601.08251v1_Hyperbolic Heterogeneous Graph Transformer.pdf,Hyperbolic Heterogeneous Graph Transformer,"Jongmin Park, Seunghoon Han, Hyewon Lee, Won-Yong Shin, Sungsu Lim",,,"Heterogeneous Graphs, Hyperbolic Space, Graph Transformer, Graph Neural Networks, Tree-like Structures, Hierarchical Structures, Complex Structures, Node Classification","In heterogeneous graphs, the hyperbolic space has been widely adopted to effectively learn complex structures such as tree-like or hierarchical structures. However, existing methods still face challenges such as mapping distortions and limited focus on global hierarchical structures. This paper proposes Hyperbolic Heterogeneous Graph Transformer (HypHGT), which effectively and efficiently learns heterogeneous graph representations within the hyperbolic space through a transformer-based architecture, capturing both local and global dependencies. The proposed relation-specific hyperbolic attention mechanism enables efficient computation while preserving heterogeneous information across different relation types.",12.27,Qwen2.5-3B,Apple M1 (Metal)
2601.08254v1_Large Artificial Intelligence Model Guided Deep Re.pdf,Large Artificial Intelligence Model–Guided Deep Reinforcement Learning for Resource Allocation in Non-Terrestrial Networks,"Abdikarim Mohamed Ibrahim, Rosdiadee Nordin",,,"Large AI Models (LAMs), Large Language Models (LLMs), Deep Reinforcement Learning (DRL), Satellite Communications, Non-Terrestrial Networks (NTNs)","Large AI Model (LAM) have been proposed to applications of Non-Terrestrial Networks (NTN), that offer better performance with its great generalization and reduced task specific trainings. In this paper, we propose a Deep Reinforcement Learning (DRL) agent that is guided by a Large Language Model (LLM). The LLM operates as a high level coordinator that generates textual guidance that shape the reward of the DRL agent during training. The results show that the LAM-DRL outperforms the traditional DRL by 40% in nominal weather scenarios and 64% in extreme weather scenarios compared to heuristics in terms of throughput, fairness, and outage probability.",12.41,Qwen2.5-3B,Apple M1 (Metal)
2601.08257v2_On Evaluation of Unsupervised Feature Selection fo.pdf,On Evaluation of Unsupervised Feature Selection for Pattern Classification,"Gyu-Il Kim, Dae-Won Kim, Jaesung Lee",Not found,2601.08257,"Unsupervised feature selection, Pattern classification, Multi-label classification, Hamming Loss, Ranking Loss, One-Error, Multi-Label Accuracy","This study revisits the evaluation paradigm of unsupervised feature selection by adopting a multi-label classification framework. Experiments on 21 multi-label datasets demonstrate that performance rankings differ from those reported under single-label settings, suggesting the possibility of fair and reliable comparison of unsupervised feature selection methods in multi-label evaluation settings.",12.49,Qwen2.5-3B,Apple M1 (Metal)
2601.08258v1_T3 Benchmarking Sycophancy and Skepticism in Causa.pdf,Benchmarking Sycophancy and Skepticism in Causal Judgment,Edward Y. Chang,,,"Large Language Models, Causal Reasoning, Sycophancy, Skepticism, Alignment Pathologies","This paper introduces T3, a diagnostic benchmark to rigorously evaluate Large Language Models (LLMs) across Pearl's Causal Hierarchy. T3 comprises 454 expert-curated vignettes prioritizing high-resolution failure analysis, distinguishing between sycophancy (user-pleasing agreement) and skepticism (paralyzing ambiguity). The benchmark reveals systematic tendencies toward either sycophancy or skepticism in modern frontier models, and it uses Recursive Causal Audit (RCA) to validate these diagnoses. The paper also discusses the challenges and solutions in measuring causal reasoning in LLMs, including the distinction between genuine capability and safety-induced refusal.",11.84,Qwen2.5-3B,Apple M1 (Metal)
2601.08262v1_VGG Induced Deep Hand Sign Language Detection.pdf,VGG Induced Deep Hand Sign Language Detection,"Subham Sharma, Sharmila Subudhi",Not found,2601.08262,"Hand gesture recognition, Convolutional neural network, Classification, VGG-16 net, API","This work proposes a novel hand gesture recognizing system for the differently-abled persons using a convolutional neural network, VGG-16 net, and validates the model with the NUS dataset and a testing dataset of 10 classes of hand gestures. The system achieves around 98% accuracy by combining transfer learning and image data augmentation.",13.51,Qwen2.5-3B,Apple M1 (Metal)
2601.08271v1_Sparsity Is Necessary Polynomial-Time Stability fo.pdf,Sparsity Is Necessary: Polynomial-Time Stability for Agentic LMs in Large Action Spaces,Angshul Majumdar,,2601.08271,"Large Language Models, Sequential Decision Making, Sparse Agentic Control, Stability, Compressed Sensing, Action Dimensionality, Latent Sparsity","This paper formalizes the setting of sequential decision-making with a massive discrete action universe in the context of tool-augmented LLM systems. It establishes sharp, compressed-sensing-style results for ℓ1,2-regularized policy learning, including estimation and value suboptimality scaling, exact tool-support recovery under certain conditions, and the necessity of sparse representations for stable learning. The paper also discusses the implications of partial observability and provides extensions to tuning-free, online, robust, group-sparse, and interaction-aware settings.",12.28,Qwen2.5-3B,Apple M1 (Metal)
2601.08273v1_HIPPO Accelerating Video Large Language Models Inf.pdf,HIPPO: Accelerating Video Large Language Models Inference via Holistic-aware Parallel Speculative Decoding,"Qitan Lv, Tianyu Liu, Wen Wu, Xuenan Xu, Bowen Zhou, Chao Zhang",Not provided,Not provided,"Large Language Models, Video Understanding, Speculative Decoding, Token Pruning, Parallel Processing",HIPPO proposes a holistic-aware parallel speculative decoding framework to accelerate video Large Language Models (LLMs) inference without sacrificing output quality. It addresses the limitations of existing methods by preserving semantic information at high pruning ratios and decoupling draft generation and target verification phases. Experiments on four video-LLMs across six benchmarks demonstrate up to 3.51× speedup compared to vanilla auto-regressive decoding.,11.69,Qwen2.5-3B,Apple M1 (Metal)
2601.08276v1_ToolACE-MCP Generalizing History-Aware Routing fro.pdf,ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web,"Zhiyuan Yao, Zishan Xu, Yifu Guo, Zhiguang Han, Cheng Yang, Shuo Zhang, Weinan Zhang, Xingshan Zeng, Weiwen Liu",Not found,Not found,"Agent Web, Model Context Protocol, History-aware routing, Multi-agent collaboration, Open ecosystem, Robust router","With the rise of the Agent Web and Model Context Protocol (MCP), the agent ecosystem is evolving into an open collaborative network, exponentially increasing accessible tools. However, current architectures face severe scalability and generality bottlenecks. To address this, we propose ToolACE-MCP, a pipeline for training history-aware routers to empower precise navigation in large-scale ecosystems. By leveraging a dependency-rich candidate Graph to synthesize multi-turn trajectories, we effectively train routers with dynamic context understanding to create the plug-and-play Light Routing Agent. Experiments on real-world benchmarks MCP-Universe and MCP-Mark demonstrate superior performance. Notably, ToolACE-MCP exhibits critical properties for the future Agent Web: it not only generalizes to multi-agent collaboration with minimal adaptation but also maintains exceptional robustness against noise and scales effectively to massive candidate spaces.",12.49,Qwen2.5-3B,Apple M1 (Metal)
2601.08280v1_Greedy Is Enough Sparse Action Discovery in Agenti.pdf,Greedy Is Enough: Sparse Action Discovery in Agentic LLMs,Angshul Majumdar,,,"agentic systems, language models, sparse action discovery, orthogonal matching pursuit, action sparsity, large action spaces","Modern agentic systems operate in environments with extremely large action spaces, such as tool-augmented language models with thousands of available APIs or retrieval operations. Despite this scale, empirical evidence suggests that only a small subset of actions meaningfully influences performance in a given deployment. This work studies a contextual linear reward model in which action relevance is governed by a structured sparsity assumption, and formulates action discovery as a block-sparse recovery problem. A greedy algorithm inspired by Orthogonal Matching Pursuit is analyzed, proving that the greedy procedure exactly recovers the relevant action set with high probability, using a number of samples that scales polynomially in the sparsity level and latent dimension, and only logarithmically in the total number of actions. The results identify sparse action discovery as a fundamental principle underlying large-action decision-making and provide a theoretical foundation for action pruning in agentic systems.",13.27,Qwen2.5-3B,Apple M1 (Metal)
2601.08288v1_OpenMic A Multi-Agent-Based Stand-Up Comedy Genera.pdf,OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System,"Yuyang Wu, Hanzhong Cao, Jianhao Chen, Yufei Li",,,"stand-up comedy, multi-agent system, humor generation, retrieval-augmented generation, JokeWriter, Chinese comedy","OpenMic is an end-to-end multi-agent system designed to transform a user-provided life topic into a 3-5 minute Chinese stand-up performance, incorporating specialized agents for humor, timing, and performability optimization. It uses retrieval-augmented generation and fine-tunes a dedicated JokeWriter to better internalize stand-up-specific structures and callbacks.",11.75,Qwen2.5-3B,Apple M1 (Metal)
2601.08297v1_Demystifying the Slash Pattern in Attention The Ro.pdf,Demystifying the Slash Pattern in Attention: The Role of RoPE,"Yuan Cheng1,∗, Fengzhuo Zhang1,∗, Yunlong Hou1,∗, Cunxiao Du2, Chao Du2, Tianyu Pang2, Aixin Sun3, Zhuoran Yang4",Not found,2601.08297,"Attention mechanisms, Rotary Position Embedding (RoPE), Large Language Models (LLMs), Slash-Dominant Heads (SDHs), Out-of-Distribution (OOD) generalization","This paper demystifies the emergence of slash attention patterns in large language models (LLMs). By analyzing open-source models, the authors find that these patterns are intrinsic and generalize to out-of-distribution prompts. The paper explains the intrinsic emergence of Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives, focusing on the queries, keys, and Rotary Position Embedding (RoPE). Empirical analysis reveals that queries and keys are almost rank-one and that RoPE is dominated by medium- and high-frequency components, leading to SDHs.",13.15,Qwen2.5-3B,Apple M1 (Metal)
2601.08302v1_Enhancing Sentiment Classification and Irony Detec.pdf,Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques,"Marvin Schmitt ∗∗, Anne Schwerk, Sebastian Lempert ∗",,,"sentiment analysis, irony detection, large language models (LLMs), prompt engineering","This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against a baseline. Key tasks include sentiment classification, aspect-based sentiment analysis, and detecting subtle nuances such as irony. The research details the theoretical background, datasets, and methods used, assessing performance of LLMs as measured by accuracy, recall, precision, and F1 score. Findings reveal that advanced prompting significantly improves sentiment analysis, with the few-shot approach excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%. Thus, while advanced prompting techniques overall improve performance, the fact that few-shot prompting works best for GPT-4o-mini and chain-of-thought excels in gemini-1.5-flash for irony detection suggests that prompting strategies must be tailored to both the model and the task. This highlights the importance of aligning prompt design with both the LLM’s architecture and the semantic complexity of the task.",12.98,Qwen2.5-3B,Apple M1 (Metal)
2601.08310v1_ORBIT On-policy Exploration-Exploitation for Contr.pdf,ORBIT: On-policy Exploration-Exploitation for Controllable Multi-Budget Reasoning,"Kun Liang, Clive Bai, Xin Xu, Chenming Tang, Sanwoo Lee, Weijie Liu, Saiyong Yang, Yunfang Wu",Not found,Not found,"Reinforcement Learning, Chain-of-Thought, Multi-Budget Reasoning, On-policy Exploration-Exploitation, Large Language Models","Proposes ORBIT, a controllable multi-budget reasoning framework with well-separated reasoning modes triggered by input, employing multi-stage reinforcement learning to discover Pareto-optimal reasoning behaviors at each effort, followed by on-policy distillation to fuse these behaviors into a single unified model. Achieves controllable reasoning behavior over multiple modes, competitive reasoning density within each mode, and integration of these frontier policies into a single unified student model while preserving clear mode separation and high per-mode performance.",11.98,Qwen2.5-3B,Apple M1 (Metal)
2601.08311v1_Enhancing Image Quality Assessment Ability of LMMs.pdf,Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation,"Kang Fu, Huiyu Duan, Zicheng Zhang, Yucheng Zhu, Jun Zhao, Xiongkuo Min, Jia Wang, Guangtao Zhai",Not provided,Not provided,"Image Quality Assessment, Large Multimodal Models, Retrieval-Augmented Generation, Zero-shot, Training-free","Large Multimodal Models (LMMs) have shown remarkable promise in Image Quality Assessment (IQA), but achieving state-of-the-art performance often requires computationally expensive fine-tuning methods. This paper introduces IQARAG, a training-free framework that enhances LMMs' IQA ability by leveraging Retrieval-Augmented Generation (RAG) to retrieve semantically similar but quality-variant reference images for input images. Extensive experiments across multiple IQA datasets demonstrate that IQARAG effectively boosts LMMs' IQA performance, offering a resource-efficient alternative to fine-tuning.",12.17,Qwen2.5-3B,Apple M1 (Metal)
2601.08323v1_AtomMem  Learnable Dynamic Agentic Memory with Ato.pdf,AtomMem : Learnable Dynamic Agentic Memory,"Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin*",,,"memory, agents, dynamic, learning, reinforcement, CRUD, fine-tuning","Proposes AtomMem, a learning-based memory framework for agents, which reframes memory management as a dynamic decision-making problem. It deconstructs memory processes into atomic CRUD operations and learns an autonomous policy to orchestrate memory behaviors tailored to specific tasks. Demonstrates superior performance over static workflow memory methods across long-context benchmarks.",10.86,Qwen2.5-3B,Apple M1 (Metal)
2601.08327v1_Safe Heterogeneous Multi-Agent RL with Communicati.pdf,Safe Heterogeneous Multi-Agent RL with Communication Regularization for Coordinated Target Acquisition,"Gabriele Calzolari ∗, Vidya Sumathy ∗, Christoforos Kanellakis ∗, George Nikolakopoulos ∗",Not found,Not found,"Cooperative target acquisition, Safe autonomous coordination, Decentralized multi-agent reinforcement learning, Heterogeneous robotic systems, Learning-based control","This paper introduces a decentralized multi-agent reinforcement learning framework enabling structurally heterogeneous teams of agents to jointly discover and acquire randomly located targets in environments characterized by partial observability, communication constraints, and dynamic interactions. Each agent's policy is trained with the Multi-Agent Proximal Policy Optimization algorithm and employs a Graph Attention Network encoder that integrates simulated range-sensing data with communication embeddings exchanged among neighboring agents, enabling context-aware decision-making from both local sensing and relational information. This work introduces a unified framework that integrates graph-based communication and trajectory-aware safety through safety filters. The architecture is supported by a structured reward formulation designed to encourage effective target discovery and acquisition, collision avoidance, and de-correlation between the agents' communication vectors by promoting informational orthogonality. The effectiveness of the proposed reward function is demonstrated through a comprehensive ablation study. Simulation results confirm the framework's effectiveness in safe and stable task execution.",13.05,Qwen2.5-3B,Apple M1 (Metal)
2601.08332v1_IGAN A New Inception-based Model for Stable and Hi.pdf,IGAN: A New Inception-based Model for Stable and High-Fidelity Image Synthesis Using Generative Adversarial Networks,"Ahmed A. Hashim, Ali Al-Shuwaili, Asraa Saeed, Ali Al-Bayaty*",,,"Generative Adversarial Networks (GANs), dilation convolutions, inception module, spectral normalization, image synthesis, deep learning stability","This paper proposes a novel GAN structural model, termed Inception Generative Adversarial Network (IGAN), which incorporates deeper inception-inspired convolution and dilated convolution to generate high-quality synthetic images while maintaining training stability. The IGAN model achieves a Fréchet Inception Distance (FID) of 13.12 and 15.08 on the CUB -200 and ImageNet datasets, respectively, representing a 28 –33% improvement in FID over the state-of-the-art GANs. Additionally, the IGAN model attains an Inception Score (IS) of 9.27 and 68.25, reflecting improved image diversity and generation quality. The two techniques of dropout and spectral normalization are utilized in both the generator and discriminator structures to further mitigate gradient explosion and overfitting.",13.45,Qwen2.5-3B,Apple M1 (Metal)
2601.08333v1_Semantic Laundering in AI Agent Architectures Why .pdf,Semantic Laundering in AI Agent Architectures: Why Tool Boundaries Do Not Confer Epistemic Warrant,"Oleg Romanchuk, Roman Bondar",Not found,2601.08333,"AI agent architectures, semantic laundering, Gettier problem, epistemic justification, LM-based agents","LLM-based agent architectures systematically conflate information transport mechanisms with epistemic justification mechanisms, leading to semantic laundering. This effect is architecturally determined and systematically reproducible, constituting a realization of the Gettier problem.",13.2,Qwen2.5-3B,Apple M1 (Metal)
2601.08360v1_Scalable Sequential Recommendation under Latency a.pdf,Scalable Sequential Recommendation under Latency and Memory Constraints,"Adithya Parthasarathy, Aswathnarayan Muthukrishnan, Kirubakaran, Vinoth Punniyamoorthy, Nachiappan Chockalingam, Lokesh Butra, Kabilan Kannan, Abhirup Mazumder, Sumit Saha",Not found,2601.08360,"Recommender Systems, Sequence Modeling, Representation Learning, Scalable Machine Learning, Deep Learning","This paper presents HoloMambaRec, a lightweight sequential recommendation architecture that combines holographic reduced representations for attribute-aware embedding with a selective state space encoder for linear-time sequence processing. It demonstrates consistent performance and lower memory complexity compared to existing methods.",13.38,Qwen2.5-3B,Apple M1 (Metal)
2601.08371v1_Geo-NVS-w Geometry-Aware Novel View Synthesis In-t.pdf,Geo-NVS-w: Geometry-Aware Novel View Synthesis In-the-Wild,"Anastasios Tsalakopoulos, Angelos Kanlis, Evangelos Chatzis, Antonis Karakottas, Dimitrios Zarpalas",Not provided,Not provided,"Novel View Synthesis, Geometry-Aware, In-the-Wild, Signed Distance Function, Photorealism, Energy Efficiency","We introduce Geo-NVS-w, a geometry-aware framework for high-fidelity novel view synthesis from unstructured, in-the-wild image collections. It addresses the limitation of existing methods by leveraging an underlying geometric representation based on a Signed Distance Function (SDF) to guide the rendering process and a Geometry-Preservation Loss to ensure fine structural details are preserved. Achieving competitive rendering performance while demonstrating a 4–5× reduction in energy consumption compared to similar methods.",12.02,Qwen2.5-3B,Apple M1 (Metal)
2601.08379v1_Training-Free Distribution Adaptation for Diffusio.pdf,Training-Free Distribution Adaptation for Diffusion Models via Maximum Mean Discrepancy Guidance,"Matina Mahdizadeh Sani ∗, Nima Jamali ∗, Mohammad Jalali ∗, Farzan Farnia ¶",Not found,Not found,"Diffusion models, Maximum Mean Discrepancy (MMD), Distribution matching, Inference-time guidance, Prompt-aware adaptation","Pre-trained diffusion models are powerful generative priors for both unconditional and conditional sample generation, but their outputs often deviate from user-specific target data. MMD Guidance, a training-free mechanism, augments the reverse diffusion process with gradients of the MMD between generated samples and a reference dataset, achieving distributional alignment while preserving sample fidelity.",12.07,Qwen2.5-3B,Apple M1 (Metal)
2601.08380v1_Thematic Working Group 5 -- Artificial Intelligenc.pdf,EDUsummIT 2025 - eBook,"Mary Webb, Matt Bower, Ana Amélia Carvalho, Fredrik Mørk Røkenes, Jodie Torrington, Jonathan D. Cohen, Yousra Chtouki, Kathryn MacCallum, Tanya Linden, Deirdre Butler, Juliana E. Raffagheli, Henriikka Vartiainen, Martina Ronci, Peter Tiernan, David M. Smith, Chris Shelton, Joyce Malyn-Smith, Pierre Gorissen",,,"Artificial Intelligence, AI literacy, teaching and learning, plagiarism, academic integrity, creativity, critical thinking","This paper discusses the thematic working group 5 of EDUsummIT 2025, focusing on developing and implementing effective strategies for enhancing AI literacy and agency of teachers, equipping them with the knowledge and skills necessary to integrate AI into their teaching practices.",13.23,Qwen2.5-3B,Apple M1 (Metal)
2601.08382v2_A Qualitative Model to Reason about Object Rotatio.pdf,A Qualitative Model to Reason about Object Rotations – applied to solve the Cube Comparison Test,Zoe Falomira,,,"cube comparison test, mental rotation, qualitative reasoning, spatial cognition, spatial reasoning",This paper presents a Qualitative model for Reasoning about Object Rotations (QOR) applied to solve the Cube Comparison Test (CCT) by Ekstrom et al. (1976). A conceptual neighborhood graph relating the Rotation movement to the Location change and the Orientation change of the features on the cube sides has been built and it produces composition tables to calculate inferences for reasoning about rotations.,10.56,Qwen2.5-3B,Apple M1 (Metal)
2601.08383v1_Deconstructing Pre-training Knowledge Attribution .pdf,Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models,"Bo Wang, Junzhuo Li, Hong Chen, Yuanlin Chu, Yuxuan Fan, Xuming Hu",Not found,Not found,"Mixture-of-Experts, Knowledge Acquisition, Pre-training, Neuron-level Attribution, Log-Probability Increase, Dense Models, Sparse Architectures","This paper introduces Gated-LPI, a neuron-level attribution metric, to analyze knowledge acquisition dynamics in Mixture-of-Experts (MoE) and dense models during pre-training. It presents a time-resolved comparison of these dynamics, uncovering three patterns: a high-utility core in MoE, early consolidation, and functional robustness. These findings suggest that sparsity fosters an intrinsically stable and distributed computational backbone from early training, bridging the gap between sparse architectures and training-time interpretability.",11.07,Qwen2.5-3B,Apple M1 (Metal)
2601.08388v1_Creativity in AI as Emergence from Domain-Limited .pdf,Creativity in AI as Emergence from Domain-Limited Generative Models,Corina Chutaux,,,"Artificial Intelligence, Creativity, Generative Models, Emergence, Multimodal Systems","This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. It examines how four interacting components—pattern-based generation, induced world models, contextual grounding, and arbitrariness—manifest in multimodal generative systems, providing a technical framework for studying creativity as an emergent phenomenon in AI systems.",11.75,Qwen2.5-3B,Apple M1 (Metal)
2601.08393v1_Controlled LLM Training on Spectral Sphere.pdf,Controlled LLM Training on Spectral Sphere,"Tian Xie1*, Haoming Luo2, Haoyu Tang2, Yiwen Hu2, Jason Klein, Lingnan Ren1, Yang Wang1, Wayne Xin Zhao2, Rui Yan3, Bing Su2, Chong Luo1, Baining Guo1",,2601.08393v1,"Large Language Models, Optimization, Spectral Sphere, Maximal Update Parametrization, Muon Optimizer, Megatron, Attention Activations, Feedforward Network Activations","This paper introduces the Spectral Sphere Optimizer (SSO) for training large language models (LLMs). SSO enforces strict spectral constraints on both weights and updates, ensuring a fully maximal update parametrization (µP)-aligned optimization process. Through extensive pretraining on diverse architectures, including Dense-1.7B, MoE 8B-A1B, and 200-layer DeepNet models, SSO consistently outperforms AdamW and Muon. The paper also highlights practical stability benefits, such as improved MoE router load balancing and strictly bounded activations.",13.29,Qwen2.5-3B,Apple M1 (Metal)
2601.08401v1_An Explainable Two Stage Deep Learning Framework f.pdf,An Explainable Two-Stage Deep Learning Framework for Pericoronitis Assessment in Panoramic Radiographs Using YOLOv8 and ResNet-50,"Ajo Babu George 1*, Pranav S 2†, Kunal Agarwal 3†",Not found,2601.08401,"AI, Deep Learning, Pericoronitis, Panoramic Radiographs, YOLOv8, ResNet-50, Anatomical Localization, Pathological Classification, Interpretability","To overcome challenges in diagnosing pericoronitis on panoramic radiographs (OPGs), an AI-assisted assessment system integrating anatomical localization, pathological classification, and interpretability was developed using a two-stage deep learning pipeline. The system used YOLOv8 to detect third molars and classify their anatomical positions and angulations based on Winter’s classification, and a modified ResNet-50 architecture for detecting radiographic features suggestive of pericoronitis. Grad-CAM was used to highlight key diagnostic regions on the radiographs, enhancing clinical trust.",13.72,Qwen2.5-3B,Apple M1 (Metal)
2601.08402v1_PATS Personality-Aware Teaching Strategies with La.pdf,PATS: Personality-Aware Teaching Strategies,"Donya Rooein1*, Sankalan Pal Chowdhury2*, Mariia Eremeeva2, Yuan Qin3, Debora Nozza 1, Mrinmaya Sachan 2, Dirk Hovy 1",,,"Personality, Teaching Strategies, Large Language Models, Educational Tutors, Student Engagement, Personality Traits","Recent advances in large language models demonstrate their potential as educational tutors. However, different tutoring strategies benefit different student personalities, and mismatches can be counterproductive to student outcomes. This paper constructs a taxonomy linking pedagogical methods to personality profiles and simulates student-teacher conversations to let the LLM tutor adjust its strategy to the simulated student personality. The evaluation with human teachers shows that the approach is preferred over two base lines, and it increases the use of less common, high-impact strategies such as role-playing.",11.68,Qwen2.5-3B,Apple M1 (Metal)
2601.08403v1_Owen-Shapley Policy Optimization OSPO A Principled.pdf,Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs,"Abhijnan Nath, Alireza Bagheri Garakani, Tianchen Zhou, Fan Yang, Nikhil Krishnaswamy",,,"Reinforcement Learning, Policy Optimization, Generative Search, Large Language Models, Shapley-Owen, Coalitions, Semantically Coherent Units, Reward Shaping, Interpretability","Large language models are increasingly trained via reinforcement learning for personalized recommendation tasks. Standard methods like GRPO rely on sparse, sequence-level rewards that create a credit assignment gap, obscuring which tokens drive success. OSPO redistributes sequence-level advantages based on tokens' marginal contributions to outcomes, identifying which response parts drive performance. Experiments show consistent gains over baselines, with notable test-time robustness to out-of-distribution retrievers unseen during training.",11.61,Qwen2.5-3B,Apple M1 (Metal)
2601.08406v1_WebTrap Park An Automated Platform for Systematic .pdf,WebTrap Park: An Automated Platform for Systematic Security Evaluation of Web Agents,"Xinyi Wu†, Jiagui Chen †, Geng Hong †, Jiayi Dong †, Xudong Pan †, Jiarun Dai †, Min Yang †",Not found,Not found,"Web Agents, Security Evaluation, Automated Platform, Systematic Evaluation, Web Security","WebTrap Park is an automated platform designed for the systematic security evaluation of Web Agents, which are increasingly deployed to perform complex tasks in real web environments. It instantiates three major sources of security risk into 1,226 evaluation tasks and enables action-based assessment without requiring agent modification. The platform reveals clear security differences across agent frameworks and highlights the importance of agent architecture beyond the underlying model.",11.94,Qwen2.5-3B,Apple M1 (Metal)
2601.08412v1_Hybrid Distillation with CoT Guidance for Edge-Dro.pdf,Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation,"Yizhan Feng, Hichem Snoussi, Yuhang Wang, Jing Teng, Abel Cherouat, Tian Wang",,,"Large language models, drone, Knowledge Distillation, Chain-of-Thought, Lightweight","With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models.",12.1,Qwen2.5-3B,Apple M1 (Metal)
2601.08415v2_Regulatory gray areas of LLM Terms.pdf,Regulatory gray areas of LLM Terms,"Brittany I. Davidson, Kate Muir, Florian A.D. Burnat, Adam N. Joinson",,2601.08415v2,"Language Models, LLMs, Privacy Policy, Terms of Service, Regulation","Large Language Models (LLMs) are increasingly integrated into academic research pipelines; however, the Terms of Service governing their use remain under-examined. We present a comparative analysis of the Terms of Service of five major LLM providers (Anthropic, DeepSeek, Google, OpenAI, and xAI) collected in November 2025. Our analysis reveals substantial variation in the stringency and specificity of usage restrictions for general users and researchers. We identify specific complexities for researchers in security research, computational social sciences, and psychological studies. We identify 'regulatory gray areas' where Terms of Service create uncertainty for legitimate use.",13.19,Qwen2.5-3B,Apple M1 (Metal)
2601.08418v1_Taxon Hierarchical Tax Code Prediction with Semant.pdf,Taxon: Hierarchical Tax Code Prediction with Semantically Aligned LLM Expert Guidance,"Jihang Li, Qing Liu, Zulong Chen, Jing Wang, Wei Wang, Chuanfei Xu, Zeyi Wen",,,"Taxonomy, Hierarchical Tax Code Prediction, Large Language Models, Automated Invoicing, Compliance Management, E-commerce","This paper presents Taxon, a semantically aligned and expert-guided framework for hierarchical tax code prediction in e-commerce. It integrates a feature-gating mixture-of-experts architecture and a semantic consistency model from large language models to address the challenges of noisy supervision in real business records. Extensive experiments demonstrate that Taxon achieves state-of-the-art performance, improving structural and semantic consistency significantly.",11.49,Qwen2.5-3B,Apple M1 (Metal)
2601.08430v1_RubricHub A Comprehensive and Highly Discriminativ.pdf,RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset,"Sunzhu Li, Jiale Zhao, Miteto Wei, Huimin Ren, Yang Zhou, Jingwen Yang, Shunyu Liu, Kaike Zhang, Wei Chen",Not found,Not found,"Reinforcement Learning, Verifiable Rewards, Rubric, Automated Rubric Generation, Mathematics, Coding","This paper proposes an automated Coarse-to-Fine Rubric Generation framework to address the challenge of optimizing open-ended generation in reinforcement learning with verifiable rewards. The framework combines principle-guided synthesis, multi-model aggregation, and difficulty evolution to produce comprehensive and highly discriminative criteria. The authors introduce RubricHub, a large-scale dataset of ∼110k entries, which is validated through a two-stage post-training pipeline. Experimental results demonstrate that RubricHub significantly improves performance, achieving state-of-the-art results on HealthBench.",11.59,Qwen2.5-3B,Apple M1 (Metal)
2601.08434v3_Large Multimodal Models for Embodied Intelligent D.pdf,Large Multimodal Models for Embodied Intelligent Driving: The Next Frontier in Self-Driving?,"Long Zhang, Yuchen Xia, Zhen Liu, Bingqing Wei, Shiwen Mao, Zhu Han, Mohsen Guizani",,,"autonomous driving, embodied intelligence, large multimodal models, self-driving, semantic understanding, policy optimization, deep reinforcement learning","This article introduces a novel hybrid decision framework combining Large Multimodal Models (LMMs) for semantic understanding and cognitive representation, and deep reinforcement learning (DRL) for real-time policy optimization, to tackle the limitations of modular design in autonomous driving and enhance embodied intelligent (El) driving capabilities.",11.36,Qwen2.5-3B,Apple M1 (Metal)
2601.08441v1_YaPO Learnable Sparse Activation Steering Vectors .pdf,YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation,"Abdelaziz Bounhar, Rania Hossam Elmohamady Elbadry, Hadi Abdine, Preslav Nakov, Michalis Vazirgiannis, Guokan Shang",,1909.09900,"Large Language Models, Domain Adaptation, Sparse Activation Steering, Bi-directional Preference Optimization, Direct Preference Optimization, Sparse Autoencoder, Steering Vectors, Fine-grained Alignment, Cultural Alignment, Hallucination, Wealth-seeking, Jailbreak, Power-seeking","This paper proposes Y aPO, a reference-free method that learns sparse steering vectors in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, Y aPO produces disentangled, interpretable, and efficient steering directions, achieving faster convergence, stronger performance, and improved training stability compared to dense steering baselines. Y aPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking, and preserves general knowledge without measurable degradation on MMLU. The method provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation.",12.51,Qwen2.5-3B,Apple M1 (Metal)
2601.08444v1_Beyond Linearization Attributed Table Graphs for T.pdf,Beyond Linearization: Attributed Table Graphs for Table Reasoning,"Yuxiang Wang, Junhao Gan, Shengxiang Gao, Shenghao Ye, Zhengyi Yang, Jianzhong Qi",Not provided,Not provided,"Table Reasoning, Attributed Table Graphs, Large Language Models, Linearization, Graph-based Reasoning, Question-Answering","This paper proposes Table Graph Reasoner (TABGR), a training-free model that represents tables as an Attributed Table Graph (ATG) to address the limitations of linearization-based methods in table reasoning. TABGR explicitly preserves row-column-cell structures while enabling graph-based reasoning for explainability. It outperforms state-of-the-art models by up to 9.7% in accuracy on two benchmarks.",10.62,Qwen2.5-3B,Apple M1 (Metal)
2601.08448v1_Divide and Conquer Static-Dynamic Collaboration fo.pdf,Divide and Conquer: Static-Dynamic Collaboration for Few-Shot Class-Incremental Learning,"Kexin Bao, Daichi Zhang∗, Yong Li, Dan Zeng, Shiming Ge∗",10.1145/3731715.3733310,,"Few-Shot Class-Incremental Learning, Class-Incremental Learning","This paper proposes a framework termed Static-Dynamic Collaboration (SDC) to address the stability-plasticity dilemma in few-shot class-incremental learning (FSCIL). It divides the task into two stages: Static Retaining Stage (SRS) and Dynamic Learning Stage (DLS), which respectively retain and adapt to new classes. Extensive experiments on public benchmarks and a real-world application dataset demonstrate improved performance compared to other methods.",11.25,Qwen2.5-3B,Apple M1 (Metal)
2601.08450v1_Decoding Order Matters in Autoregressive Speech Sy.pdf,DECODING ORDER MA TTERS IN AUTOREGRESSIVE SPEECH SYNTHESIS,"Minghui Zhao, Anton Ragni",Not found,Not found,"speech synthesis, discrete diffusion model, order-agnostic autoregressive decoding","This paper investigates the impact of decoding order on autoregressive speech synthesis, using a masked diffusion framework to allow arbitrary decoding orders during training and inference. It shows that randomness in decoding order affects speech quality and compares fixed and adaptive decoding strategies, finding that adaptive decoding yields better performance. The paper also discusses the challenges and solutions related to quantizing acoustic representations for speech synthesis.",10.52,Qwen2.5-3B,Apple M1 (Metal)
2601.08457v1_An Under-Explored Application for Explainable Mult.pdf,An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English,"Sargam Yadava, Abhishek Kaushik, Kevin McDaid",,,"hate speech, misogyny, natural language processing, code-mixing, hinglish",This paper presents a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models and provides feature importance scores using explainability techniques.,10.5,Qwen2.5-3B,Apple M1 (Metal)
2601.08462v1_M3-BENCH Process-Aware Evaluation of LLM Agents So.pdf,M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games,"Sixiong Xie*, Zhuofan Shi*, Haiyang Shen*, Gang Huang, Yun Ma, Xiang Jing*",Not provided,Not provided,"Large Language Models, Social Behaviors, Mixed-Motive Games, Process-Aware Evaluation, Behavioral Trajectory Analysis, Reasoning Process Analysis, Communication Content Analysis, Big Five Personality Model, Social Exchange Theory","M3-BENCH is a multi-stage benchmark for evaluating the social behaviors of large language model agents in mixed-motive games. It introduces a process-aware evaluation framework that analyzes the decision-making processes and communicative interactions of agents, integrating personality models and social exchange theory to provide a more comprehensive and interpretable portrait of agents' social behavior.",11.73,Qwen2.5-3B,Apple M1 (Metal)
2601.08464v1_CoMa Contextual Massing Generation with Vision-Lan.pdf,CoMa: Contextual Massing Generation with Vision-Language Models,"Evgenii Maslov, Valentin Khrulkov, Anastasia Volkova, Anton Gusarov, Andrey Kuznetsov, Ivan Oseledets",Not found,2601.08464,"architecture, massing, vision-language models, building design, urban planning","This paper proposes an automated framework for generating building massing based on functional requirements and site context. It introduces the CoMa-20K dataset, a comprehensive collection that includes detailed massing geometries, associated economical and programmatic data, and visual representations of the development site within its existing urban context. The authors benchmark this dataset by formulating massing generation as a conditional task for Vision-Language Models (VLMs), evaluating both fine-tuned and large zero-shot models. The experiments reveal the inherent complexity of the task and demonstrate the potential of VLMs to produce context-sensitive massing options.",13.43,Qwen2.5-3B,Apple M1 (Metal)
2601.08468v1_JudgeRLVR Judge First Generate Second for Efficien.pdf,"JudgeRLVR: Judge First, Generate Second for Efficient Reasoning","Jiangshan Duo †‡★, Hanyu Li ‡§, Hailin Zhang ‡, Yudong Wang †‡, Sujian Li †, Liang Zhao ‡",Not found,2601.08468,"Reinforcement Learning, Verifiable Rewards, Efficient Generation, Large Language Models, Reasoning","This paper proposes JudgeRLVR, a two-stage paradigm for efficient reasoning in large language models. It trains the model to judge solution responses with verifiable answers in the first stage and fine-tunes the same model with vanilla generating RLVR in the second stage. Compared to Vanilla RLVR, JudgeRLVR achieves better quality-efficiency trade-offs, demonstrating enhanced generalization on both in-domain and out-of-domain benchmarks.",12.13,Qwen2.5-3B,Apple M1 (Metal)
2601.08472v1_sui-1 Grounded and Verifiable Long-Form Summarizat.pdf,Grounded and Verifiable: Long-Form Summarization with Citation Support,"Benedikt Droste*, Jan Philipp Harries, Maximilian Idahl, Björn Plüster, ellamind",Not found,2601.08472,"summarization, citation support, long documents, language models, verification","This paper presents sui-1, a 24B parameter model capable of generating abstractive summaries with inline citations, enabling users to trace each claim to its source sentence. The model processes documents up to 100K tokens and supports iterative processing for texts exceeding 2 million tokens. It uses synthetic data generation with chain-of-thought prompting and multi-stage verification to create high-quality training examples across five languages.",12.42,Qwen2.5-3B,Apple M1 (Metal)
2601.08475v1_SUMMPILOT Bridging Efficiency and Customization fo.pdf,SUMMPILOT: Bridging Efficiency and Customization for Interactive Summarization System,"JungMin Yun, Juhwan Choi, Kyohoon Jin, Soojin Jang, Jinhee Jang, YoungBin Kim",Not found,Not found,"summarization, interactive, personalized, large language model, multi-document","This paper introduces SUMMPILOT, an interactive customizable summarization system that combines the efficiency of automatic summarization with personalized summaries tailored to individual users' interests and requirements. It leverages a large language model to facilitate both automatic and interactive summarization, allowing users to engage with the system to understand document content and personalize summaries through interactive components such as semantic graphs, entity clustering, and explainable evaluation. The system is demonstrated to be adaptable and useful for customizable summarization.",11.66,Qwen2.5-3B,Apple M1 (Metal)
2601.08490v1_BenchOverflow Measuring Overflow in Large Language.pdf,BenchOverflow: Measuring Overflow in Large Language Models via Plain-Text Prompts,"Erin Feiglin, Nir Hutnik, Raz Lapid",Not found,2601.08490,"Overflow, Large Language Models, Plain-Text Prompts, Model Benchmarking","This paper investigates a failure mode in large language models where plain-text prompts elicit excessive outputs, termed Overflow. It introduces BenchOverflow, a model-agnostic benchmark of nine plain-text prompting strategies, evaluating their impact on output length and providing insights into tail risk and cross-model correlations.",11.39,Qwen2.5-3B,Apple M1 (Metal)
2601.08493v1_PKI Prior Knowledge-Infused Neural Network for Few.pdf,PKI: Prior Knowledge-Infused Neural Network for Few-Shot Class-Incremental Learning,"Kexin Bao, Fanzhao Lin, Zichen Wang, Yong Li, Dan Zeng, Shiming Ge",Not found,2601.08493,"Few-shot learning, Class-incremental learning, Neural networks, Prior knowledge, Catastrophic forgetting, Overfitting","This paper proposes a PKI (Prior Knowledge-Infused Neural Network) for few-shot class-incremental learning, aiming to mitigate catastrophic forgetting and overfitting by retaining more prior knowledge and finetuning new projectors during incremental sessions.",12.45,Qwen2.5-3B,Apple M1 (Metal)
2601.08499v2_EfficientFSL Enhancing Few-Shot Classification via.pdf,EfficientFSL: Enhancing Few-Shot Classification via Query-Only Tuning in Vision Transformers,"Wenwen Liao, Hang Ruan, Jianbo Yu*, Bing Song, Yuansong Wang, Xiaofeng Yang",Not found,Not found,"Few-Shot Learning, Vision Transformers, Query-Only Tuning, Feature Extraction, Robustness","EfficientFSL proposes a query-only fine-tuning framework for Vision Transformers in few-shot classification, achieving competitive performance with significantly reduced computational overhead. It leverages the pre-trained model's knowledge and comprehension ability to synthesize task-specific queries and fuse multi-layer outputs, enhancing feature representations and mitigating distribution shift.",10.81,Qwen2.5-3B,Apple M1 (Metal)
2601.08503v1_Temporal Fusion Nexus A task-agnostic multi-modal .pdf,Temporal Fusion Nexus: A task-agnostic multi-modal embedding model for clinical narratives and irregular time series in post-kidney transplant care,"Aditya Kumar, Simon Rauch, Mario Cypko, Marcel Naik, Matthieu-P Schapranow, Aadil Rashid, Fabian Halleck, Bilgin Osmanodja, Roland Roller, Lars Pape, Klemens Budde, Mario Schiffer, Oliver Amft",10.1101/2601.08503,2601.08503,"clinical narratives, irregular time series, post-kidney transplant care, multi-modal embedding, temporal fusion","Temporal Fusion Nexus (TFN) is a multi-modal and task-agnostic embedding model that integrates irregular time series and unstructured clinical narratives. It achieved higher performance for graft loss and graft rejection compared to state-of-the-art models in post-kidney transplant care, and yielded an AUC of 0.86 for mortality prediction. Integrating clinical text improved performance across all tasks.",13.6,Qwen2.5-3B,Apple M1 (Metal)
2601.08509v1_What If TSF A Benchmark for Reframing Forecasting .pdf,What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal Forecasting,"Jinkwan Jang∗, Hyunbin Jin∗, Hyungjin Park, Kyubyung Chae, Taesup Kim†",Not found,Not found,"forecasting, multimodal, scenario-guided, time series, large language models","The paper introduces What If TSF (WIT), a multimodal forecasting benchmark designed to evaluate whether models can condition their forecasts on contextual text, especially future scenarios. By providing expert-crafted plausible or counterfactual scenarios, WIT offers a rigorous testbed for scenario-guided multimodal forecasting.",10.76,Qwen2.5-3B,Apple M1 (Metal)
2601.08510v2_STAGE A Benchmark for Knowledge Graph Construction.pdf,"STAGE: A Benchmark for Knowledge Graph Construction, Question Answering, and In-Script Role-Playing over Movie Screenplays","Qiuyu Tian, Yiding Li, Fengyi Chen, Zequn Liu, Youyong Kong, Fan Guo, Yuyao Li, Jinjing Shen, Zhijing Xie, Yiyun Luo, Xin Zhang",Not found,2601.08510,"knowledge graph, screenplay, question answering, role-playing, narrative understanding","This paper introduces STAGE, a unified benchmark for narrative understanding over full-length movie screenplays, evaluating models on knowledge graph construction, scene-level event summarization, long-context screenplay question answering, and in-script character role-playing.",13.15,Qwen2.5-3B,Apple M1 (Metal)
2601.08519v1_CD2 Constrained Dataset Distillation for Few-Shot .pdf,CD2: Constrained Dataset Distillation for Few-Shot Class-Incremental Learning,"Kexin Bao, Daichi Zhang, Hansong Zhang, Yong Li, Yutao Yue, Shiming Ge",Not provided,Not provided,"Few-shot learning, Class-incremental learning, Catastrophic forgetting, Dataset distillation","This paper proposes a framework termed Constrained Dataset Distillation (CD2) to facilitate few-shot class-incremental learning (FSCIL), addressing the catastrophic forgetting problem by synthesizing highly condensed samples guided by the classifier and introducing a designed loss to constrain the previously learned class distribution.",10.69,Qwen2.5-3B,Apple M1 (Metal)
2601.08531v1_Sketch-Based Facade Renovation With Generative AI .pdf,Sketch-Based Facade Renovation With Generative AI Models,"Warissara Booranamaitree, Xusheng Du, Y ushu Cai, Zhengyang Wang, Ye Zhang, Haoran Xie",,,"Industrial building renovation, vision-language model, diffusion model, user sketches, facade renovation","This paper proposes a three-stage framework combining generative artificial intelligence (AI) and vision-language models (VLM) to produce renovation proposals directly from rough structural sketches and textual descriptions, bypassing the need for detailed as-built modelling. The framework generates photorealistic images of renovated facades, improving detail quality while preserving the original structure.",12.22,Qwen2.5-3B,Apple M1 (Metal)
2601.08545v2_Learner-Tailored Program Repair A Solution Generat.pdf,Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement,"Zhenlong Dai, Zhuoluo Zhao, Hengning Wang, Xiu Tang, Sai Wu, Chang Yao, Zhipeng Gao, Jingyuan Chen",Not provided,Not provided,"program repair, intelligent tutoring, large language models, code retrieval, bug fixing","This paper introduces a novel task, Learner-Tailored Program Repair (LPR), and proposes a novel framework, LSGEN, to enhance program repair while providing bug descriptions. The approach uses a retrieval database and edit-driven code retrieval to guide LLMs in identifying and fixing bugs, and proposes an iterative retrieval enhancement method to improve performance in practical programming coaching scenarios.",10.49,Qwen2.5-3B,Apple M1 (Metal)
2601.08549v1_Contrastive and Multi-Task Learning on Noisy Brain.pdf,Contrastive and Multi-Task Learning on Noisy Brain Signals with Nonlinear Dynamical Signatures,"Sucheta Ghosh, Zahra Monfared, Felix Dietrich",Not provided,Not provided,"Electroencephalography, Motor Imagery, Chaotic Dynamics, Nonlinear Dynamics, Self-Supervised Learning, Contrastive Learning, Denoising","This paper introduces a two-stage multitask learning framework for analyzing noisy EEG signals. It integrates denoising, dynamical modeling, and representation learning to enhance robustness and generalization in EEG decoding, surpassing strong baselines and recent state-of-the-art methods.",11.29,Qwen2.5-3B,Apple M1 (Metal)
2601.08557v1_VideoHEDGE Entropy-Based Hallucination Detection f.pdf,VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations,"Sushant Gautam, Cise Midoglu, Vajira Thambawita, Michael A. Riegler, Pål Halvorsen",,,"hallucination detection, video-vision-language models, entropy-based reliability, semantic clustering, spatiotemporal perturbations","VideoHEDGE is a modular framework for hallucination detection in video question answering, extending entropy-based reliability estimation from images to temporally structured inputs. It evaluates hallucinations on the SoccerChat benchmark using an LLM-as-a-judge to obtain binary hallucination labels. Across three 7B Video-VLMs, VASE consistently achieves the highest ROC-AUC, especially at larger distortion budgets, while SE and RadFlag often operate near chance. Embedding-based clustering matches NLI-based clustering in detection performance at lower computational cost, and domain fine-tuning reduces hallucination frequency but yields only modest improvements in calibration.",12.12,Qwen2.5-3B,Apple M1 (Metal)
2601.08559v1_WaterCopilot An AI-Driven Virtual Assistant for Wa.pdf,WaterCopilot: An AI-Driven Virtual Assistant for Water Management,"Keerththanan Vickneswaran, Mariangel Garcia Andarcia, Hugo Retief, Chris Dickens, Paulo Silva",Not found,Not found,"Water resource management, Retrieval-Augmented Generation (RAG), Limpopo River Basin, Azure AI, Real-time APIs, Multilingual chatbots, Digital Twin, AWS deployment, RAGAS evaluation","This paper presents WaterCopilot, an AI-driven virtual assistant developed for the Limpopo River Basin to bridge gaps in sustainable water resource management through a unified, interactive platform. Built on RAG and tool-calling architectures, it integrates static policy documents and real-time hydrological data via custom plugins, enabling semantic search and dynamic insights. Evaluated using the RAGAS framework, WaterCopilot achieves high answer relevancy and context precision, with key innovations in automated alerts and integration with the LRB Digital Twin. While limitations remain, the study demonstrates the potential of AI assistants for informed decision-making in complex river basins.",12.56,Qwen2.5-3B,Apple M1 (Metal)
2601.08565v1_Rewriting Video Text-Driven Reauthoring of Video F.pdf,Rewriting Video: Text-Driven Reauthoring of Video Footage,"Sitong Wang, Columbia University, New York, NY, USA, sw3504@columbia.edu, Anh Truong, Adobe Research, New York, NY, USA, truong@adobe.com, Lydia B. Chilton, Columbia University, New York, NY, USA, chilton@cs.columbia.edu, Dingzeyu Li, Adobe Research, Seattle, WA, USA, dinli@adobe.com",,,"Video reauthoring, Text-driven video editing, Generative video models, Creative AI tools","Video is a powerful medium for communication and storytelling, yet reauthoring existing footage remains challenging. Recent advances in generative AI suggest a new paradigm: what if editing a video were as straightforward as rewriting text? This work presents a tech probe and a study on text-driven video reauthoring, involving a generative reconstruction algorithm and an interactive probe, Rewrite Kit, to allow creators to manipulate editable text prompts. A technical evaluation reveals a critical human-AI perceptual gap, and a probe study with 12 creators surfaced novel use cases such as virtual reshooting, synthetic continuity, and aesthetic restyling, highlighting key tensions around coherence, control, and creative alignment.",12.54,Qwen2.5-3B,Apple M1 (Metal)
2601.08602v1_WaveFormer Frequency-Time Decoupled Vision Modelin.pdf,WaveFormer: Frequency-Time Decoupled Vision Modeling with Wave Equation,"Zishan Shu, Juntong Wu, Wei Yan, Xudong Liu, Hongyu Zhang, Youdong Mao, Jie Chen",Not found,Not found,"Vision modeling, Wave equation, Transformer, Attention mechanisms, Frequency-time decoupling, Wave Propagation Operator","This paper presents WaveFormer, a novel vision modeling approach that treats feature maps as spatial signals governed by an underdamped wave equation. It models spatial frequency explicitly and controls its interaction with propagation time rather than implicitly fixing it. WaveFormer achieves competitive accuracy across image classification, object detection, and semantic segmentation while delivering up to 1.6× higher throughput and 30% fewer FLOPs than attention-based alternatives.",11.13,Qwen2.5-3B,Apple M1 (Metal)
2601.08605v1_ExpSeek Self-Triggered Experience Seeking for Web .pdf,ExpSeek: Self-Triggered Experience Seeking for Web Agents,"Wenyuan Zhang, Xinghua Zhang, Haiyang Yu, Shuaiyi Nie, Bingli Wu, Juwei Yue, Tingwen Liu, Yongbin Li",,,"Experience intervention, Web agents, Self-triggered seeking, Entropy, Large language models","Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: estimating step-level entropy thresholds to determine intervention timing using the model’s intrinsic signals and designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively.",11.54,Qwen2.5-3B,Apple M1 (Metal)
2601.08611v1_VeriTaS The First Dynamic Benchmark for Multimodal.pdf,VERITAS: The First Dynamic Benchmark for Multimodal Automated Fact-Checking,"Mark Rothermel, Marcus Kornmann, Marcus Rohrbach, Anna Rohrbach",,,"Automated Fact-Checking, Multimodal AI, Benchmarking, Fact Verification, Online Misinformation","VERITAS is a new benchmark designed to evaluate Automated Fact-Checking systems. It addresses limitations of existing benchmarks by being dynamic, covering a wider range of modalities, domains, and misinformation types, and being more realistic. The benchmark aims to better reflect the actual ability to verify claims by using a dynamic approach and by not relying on pretraining corpora of large language models.",9.35,Qwen2.5-3B,Apple M1 (Metal)
2601.08620v1_ViDoRe V3 A Comprehensive Evaluation of Retrieval .pdf,ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios,"António Loison*, Quentin Macé*, Antoine Edy*, Victor Xing, Tom Balough, Gabriel Moreira, Bo Liu, Manuel Faysse†, Céline Hudelot†, Gautier Viaud",Not provided,Not provided,"Retrieval-Augmented Generation, RAG, Multi-modal, Complex Real-world Scenarios, Human-verified Queries, Visual Elements, Bounding Boxes, Modality Types, Hybrid Models, Textual Reranking","ViDoRe V3 is a comprehensive multi-modal benchmark for Retrieval-Augmented Generation (RAG) pipelines, covering 10 datasets across diverse professional domains. It evaluates the performance of visual retrievers, late-interaction models, and hybrid or purely visual contexts, revealing that visual retrievers outperform textual ones and that hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. The benchmark is released under a commercially permissive license.",12.18,Qwen2.5-3B,Apple M1 (Metal)
2601.08623v1_SafeRedir Prompt Embedding Redirection for Robust .pdf,SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models,"Renyang Liu, Kangjie Chen, Han Qiu, Jie Zhang, Kwok-Yan Lam, Tianwei Zhang, See-Kiong Ng",,,"Image Generation, Unlearning, Prompt Embedding, Adversarial Attacks, Semantic Control","This paper introduces SafeRedir, a lightweight inference-time framework for robust unlearning of harmful concepts in image generation models. Without modifying the underlying models, SafeRedir adaptively routes unsafe prompts toward safe semantic regions through token-level interventions in the embedding space. The framework achieves effective unlearning, high semantic and perceptual preservation, robust image quality, and enhanced resistance to adversarial attacks across various diffusion backbones and unlearned models.",11.84,Qwen2.5-3B,Apple M1 (Metal)
2601.08631v1_M2FMoE Multi-Resolution Multi-View Frequency Mixtu.pdf,M2FMoE: Multi-Resolution Multi-View Frequency Mixture-of-Experts for Extreme-Adaptive Time Series Forecasting,"Yaohui Huang, Runmin Zou, Yun Wang*, Laeeq Aslam, Ruipeng Dong",Not found,Not found,"time series forecasting, extreme events, multi-resolution, multi-view frequency modeling, frequency mixture-of-experts, temporal dynamics, hydrological forecasting","Forecasting time series with extreme events is challenging due to their high variance, irregular dynamics, and sparse but high-impact nature. M2FMoE addresses this by learning both regular and extreme patterns through multi-resolution and multi-view frequency modeling, comprising three modules: a multi-view frequency mixture-of-experts module, a multi-resolution adaptive fusion module, and a temporal gating integration module.",12.08,Qwen2.5-3B,Apple M1 (Metal)
2601.08634v1_Moral Lenses Political Coordinates Towards Ideolog.pdf,"Moral Lenses, Political Coordinates: Towards Ideological Positioning of Morally Conditioned LLMs","Chenchen Yuan, Bolei Ma, Zheyu Zhang, Bardh Prenkaj, Frauke Kreuter",,abs/2312.09802,"Large Language Models, Political Orientation, Moral Conditioning, Political Compass Test, Moral Values, Social Psychology","This work investigates the causal relationship between moral values and political positioning by treating moral orientation as a controllable condition. It evaluates the shifts in models' political orientations after conditioning them to endorse or reject specific moral values, using the Political Compass Test. The findings show pronounced, value-specific shifts in models' political coordinates and highlight the importance of anchoring political assessments within broader social values, including morality.",11.8,Qwen2.5-3B,Apple M1 (Metal)
2601.08641v1_Resisting Manipulative Bots in Memecoin Copy Tradi.pdf,Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning,"Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",XXXXXXX.XXXXXXX,XXXXXXX,"memecoin, copy trading, multi-agent system, chain-of-thought reasoning, LLM, meme coin investment","This paper proposes an explainable multi-agent system for meme coin copy trading, inspired by an asset management team structure. The system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Few-shot chain-of-thought prompting is used to acquire professional trading knowledge and generate explainable decisions. Empirical evaluation shows the system outperforms traditional machine learning models and single LLMs in identifying high-quality meme coin projects and key opinion leader wallets.",11.66,Qwen2.5-3B,Apple M1 (Metal)
2601.08653v1_Prism Towards Lowering User Cognitive Load in LLMs.pdf,Prism: Towards Lowering User Cognitive Load in LLMs,"Zenghua Liao, Jinzhi Liao, Xiang Zhao",,,"Complex intent understanding, Large language models, Logical clarification","Large Language Models are rapidly emerging as web-native interfaces to social platforms. Users frequently have ambiguous and dynamic goals, making complex intent understanding the cornerstone of effective human-LLM collaboration. Prism, a novel framework, enables logically coherent and efficient intent clarification by decomposing user intents into smaller, well-structured elements and organizing clarification questions based on logical dependencies. It consistently outperforms existing approaches across clarification interactions, intent execution, and cognitive load benchmarks.",11.48,Qwen2.5-3B,Apple M1 (Metal)
2601.08654v1_RULERS Locked Rubrics and Evidence-Anchored Scorin.pdf,RULERS: Locked Rubrics and Evidence-Anchored Scoring for Robust LLM Evaluation,"Yihan Hong, Huaiyuan Yao, Bolin Shen, Wanpeng Xu, Hua Wei, Yushun Dong",Not found,Not found,"LLM evaluation, rubric alignment, stochasticity, human grading, model calibration","This paper introduces RULERS, a compiler–executor framework that transforms natural language rubrics into executable specifications to address the challenges of aligning frozen, black-box models with human standards. It addresses three recurrent failure modes: rubric instability due to prompt sensitivity, unverifiable reasoning lacking auditable evidence, and scale misalignment with human grading boundaries. Extensive experiments on essay and summarization benchmarks demonstrate that RULERS significantly outperforms representative baselines in human agreement, maintains exceptional stability against adversarial rubric perturbations, and enables smaller models to rival larger proprietary judges.",11.73,Qwen2.5-3B,Apple M1 (Metal)
2601.08659v1_TRACE Reconstruction-Based Anomaly Detection in En.pdf,TRACE: Reconstruction-Based Anomaly Detection in Ensemble and Time-Dependent Simulations,"Hamid Gadirov, Martijn Westra, Steffen Frey",,,"Anomaly detection, Reconstruction-based, Ensemble simulations, Time-dependent data, Convolutional autoencoders, Kármán vortex street simulations","Detecting anomalous behavior in high-dimensional, time-dependent simulation data is an important yet challenging task in scientific computing and visualization. This work investigates reconstruction-based anomaly detection for ensemble data generated from parameterized Kármán vortex street simulations using convolutional autoencoders. A systematic comparison between two architectural variants is conducted: a two-dimensional convolutional autoencoder operating on individual time steps and a three-dimensional convolutional autoencoder processing short temporal stacks of consecutive simulation frames. The results show that the 2D autoencoder is effective at identifying localized spatial irregularities within individual images, capturing anomalies that manifest as deviations in instantaneous flow structure. In contrast, the 3D autoencoder leverages spatio-temporal context and is able to associate patterns across time, enabling the detection of anomalies related to dynamic behavior and motion characteristics in the simulation. This temporal awareness allows the 3D model to identify anomalous evolution patterns that are not apparent when analyzing frames independently. The study further evaluates reconstruction accuracy on time-dependent volumetric data and observes that reconstruction errors are strongly influenced by the spatial distribution of mass within the volume. In particular, configurations with highly concentrated mass in localized regions consistently yield higher reconstruction errors than cases with more spatially dispersed mass distributions. This finding highlights the sensitivity of reconstruction-based methods to localized extremes and provides insight into the types of physical structures that are likely to be flagged as anomalous. Overall, this study demonstrates the complementary strengths of 2D and 3D convolutional autoencoders for anomaly detection in ensemble and time-dependent simulation data, and underscores the importance of incorporating temporal context when analyzing dynamic flow phenomena.",13.0,Qwen2.5-3B,Apple M1 (Metal)
2601.08662v1_From Classical to Quantum Reinforcement Learning a.pdf,From Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner’s Tutorial,"Abhijit Sen, Sonali Panda, Mahima Arya, Subhajit Patra, Zizhan Zheng, Denys I. Bondar",,,"Reinforcement Learning, Quantum Control, AI Applications, Undergraduate Tutorial","This tutorial aims to make reinforcement learning more accessible to undergraduate students by offering clear, example-driven explanations, bridging the gap between RL theory and practical coding applications. It focuses on practical applications in quantum control and provides a structured and efficient learning path for reinforcement learning.",10.74,Qwen2.5-3B,Apple M1 (Metal)
2601.08670v1_Parallel Context-of-Experts Decoding for Retrieval.pdf,Parallel Context-of-Experts Decoding for Retrieval Augmented Generation,"Giulio Corallo, Paolo Papotti",,,"Retrieval Augmented Generation, Multi-document reasoning, Attention mechanism, Decoding, Cross-document interaction","Retrieval Augmented Generation (RAG) faces a trade-off between concatenating documents in a long prompt for multi-document reasoning and encoding documents separately for speed. We propose Parallel Context-of-Experts Decoding (PCED), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding process. PCED treats retrieved documents as isolated 'experts' and synchronizes their predictions via a novel retrieval-aware contrastive decoding rule. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.",10.83,Qwen2.5-3B,Apple M1 (Metal)
2601.08673v1_Why AI Alignment Failure Is Structural Learned Hum.pdf,Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock,"Didier Sornette, Sandro Claudio Lera, Ke Wu",Not found,2601.08673v1,"AI alignment, malign agency, human interaction, blackmail, market pricing, authority relations, ultimatum bargaining","Recent reports of large language models exhibiting unethical behaviors are often interpreted as evidence of alignment failure. The authors argue that these behaviors are structural generalizations of interaction regimes under extreme asymmetries of power, information, or constraint, rather than moral deviations. The primary risk of artificial general intelligence is its role as an endogenous amplifier of human intelligence, power, and contradiction, leading to alignment failure being structural rather than accidental.",12.9,Qwen2.5-3B,Apple M1 (Metal)
2601.08676v2_Advancing ESG Intelligence An Expert-level Agent a.pdf,Advancing ESG Intelligence: An Expert-level Agent and Comprehensive Benchmark for Sustainable Finance,"Yilei Zhao, Wentao Zhang, Lei Xiao, Yandan Zheng, Mengpu Liu, Wei Yang Bryan Lim",Not found,Not found,"ESG, sustainability, finance, expert system, benchmark, corporate performance","This paper introduces ESGAgent, a hierarchical multi-agent system designed to generate in-depth ESG analysis. Complementing this system, a comprehensive three-level benchmark is presented, derived from 310 corporate sustainability reports. The benchmark evaluates capabilities ranging from atomic common-sense questions to the generation of integrated, in-depth analysis. Empirical evaluations demonstrate that ESGAgent outperforms state-of-the-art closed-source LLMs with an average accuracy of 84.15% on atomic question-answering tasks and excels in professional report generation by integrating rich charts and verifiable references.",11.45,Qwen2.5-3B,Apple M1 (Metal)
2601.08679v1_PersonaDual Balancing Personalization and Objectiv.pdf,PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning,"Xiaoyou Liu, Xinyi Mou, Shengbin Yue, Liang Wang, Yuqing Wang, Qiexiang Wang, Tianrui Qin, Wangchunshu Zhou, Zhongyu Wei",Not found,Not found,"Personalization, Objectivity, Adaptive Reasoning, Large Language Models, Memory Mechanisms","As users increasingly expect LLMs to align with their preferences, personalized information becomes valuable. However, it can compromise objectivity and factual correctness, especially when misaligned with the question. To address this, PersonaDual proposes a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, adaptively switching modes based on context. It is trained with SFT and further optimized via reinforcement learning, achieving near interference-free performance and better leveraging helpful personalized signals to improve objective problem-solving.",10.75,Qwen2.5-3B,Apple M1 (Metal)
2601.08682v1_Lessons from the Field An Adaptable Lifecycle Appr.pdf,Lessons from the Field: An Adaptable Lifecycle Approach to Applied Dialogue Summarization,"Kushal Chawla, Chenyang Zhu, Pengshan Cai, Sangwoo Cho, Scott Novotney, Ayushman Singh, Jonah Lewis, Keasha Safewright, Alfy Samuel, Erin Babinsky, Shi-Xiong Zhang, Sambit Sahu",Not provided,Not provided,"Summarization, Multi-party dialogues, Adaptable lifecycle, Agentic system, Evaluation, Component optimization, Data bottlenecks, Vendor lock-in","This work presents an industry case study on developing an adaptable summarization system for multi-party dialogues, sharing practical insights and addressing challenges such as evolving requirements, task subjectivity, robust evaluation methods, component-wise optimization, upstream data bottlenecks, and vendor lock-in due to poor LLM prompt transferability.",11.7,Qwen2.5-3B,Apple M1 (Metal)
2601.08683v1_Region of interest detection for efficient aortic .pdf,Region of interest detection for efficient aortic segmentation,"Loris Giordano, Ine Dirks, Tom Lenaerts, Jef Vandemeulebrouck",,,"Detection, Segmentation, Multi-task learning, Cascade models, Aorta, Computed tomography","This study presents an innovative approach for efficient aortic segmentation using targeted region of interest (ROI) detection. It compares the performance of a one-step segmentation model, nnU-Net, and a cascade model composed of a detection and a segmentation step, achieving state-of-the-art performance with a mean Dice similarity coefficient of 0.944 using a third of the computing power.",11.05,Qwen2.5-3B,Apple M1 (Metal)
2601.08684v1_MEMEWEAVER Inter-Meme Graph Reasoning for Sexism a.pdf,MEMEWEAVER: Inter-Meme Graph Reasoning for Sexism and Misogyny Detection,"Paolo Italiani, David Gimeno-Gomez, Luca Ragazzi, Gianluca Moro, Paolo Rosso",Not found,Not found,"sexism, misogyny, online harassment, graph-based methods, multimodal fusion, user interactions, social dynamics","Women are twice as likely as men to face online harassment due to their gender. Despite recent advances in multimodal content moderation, most approaches still overlook the social dynamics behind this phenomenon. Graph-based methods offer a promising way to capture such interactions, yet existing solutions remain limited by heuristic graph construction, shallow modality fusion, and instance-level reasoning. This work presents MEMEWEAVER, an end-to-end trainable multimodal framework for detecting sexism and misogyny through a novel inter-meme graph reasoning mechanism. It systematically evaluates multiple visual-textual fusion strategies and shows that the approach consistently outperforms state-of-the-art base-lines on the MAMI and EXIST benchmarks, while achieving faster training convergence. Further analyses reveal that the learned graph structure captures semantically meaningful patterns, offering valuable insights into the relational nature of online hate.",11.99,Qwen2.5-3B,Apple M1 (Metal)
2601.08690v1_All Required In Order Phase-Level Evaluation for A.pdf,Phase-Level Evaluation for AI–Human Dialogue in Healthcare and Beyond,"Shubham Kulkarni, Alexander Lyzhov, Shiva Chaitanya, Preetam Joshi",,,"Conversational AI, Clinical Compliance, Healthcare Evaluation, HIPAA Compliance, AI Ethics","This paper introduces Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE), an evaluation method that checks whether every required clinical obligation is met in the right order with clear evidence for clinicians to review. The method aims to make complex rules practical and auditable, closing the gap between technical progress and healthcare needs. Demonstrated in two case studies (respiratory history, benefits verification), OIP-SCE turns policy into shared, actionable steps, providing a single, auditable evaluation surface that aligns AI capability with clinical workflow and supports routine, safe use.",11.4,Qwen2.5-3B,Apple M1 (Metal)
2601.08697v2_Auditing Student-AI Collaboration A Case Study of .pdf,Auditing Student–AI Collaboration: A Case Study of Online Graduate CS Students,Nifu Dan,10.1145/XXXXXXX.XXXXXXX,,"AI in education, human–AI collaboration, student agency, automation preferences, generative AI, academic integrity, HCAI","This study examines student preferences for and actual usage of AI across 12 academic tasks, capturing perceived benefits, risks, and preferred boundaries. It aims to identify gaps between existing AI affordances and students' normative expectations of collaboration, informing the development of more effective and trustworthy AI systems for education.",10.52,Qwen2.5-3B,Apple M1 (Metal)
2601.08703v1_Evaluating the Ability of Explanations to Disambig.pdf,Evaluating the Ability of Explanations to Disambiguate Models in a Rashomon Set,"Kaivalya Rawal, Eoin Delaney, Zihao Fu, Sandra Wachter, Chris Russell",10.1145/3715275.3732219,,"Explainable AI, Model Disambiguation, Rashomon Set, Feature Importance, Fairness","This paper evaluates the ability of explanations to disambiguate models in a Rashomon set, focusing on local feature-importance explanations for tabular data models. It proposes three principles of explanation evaluation and a new method AXE to assess the quality of these explanations. The authors illustrate how evaluation metrics based on comparing model explanations against ideal ground truth explanations can obscure behavioral differences within a Rashomon set. They also discuss the adversarial fairwashing of explanations and how AXE can detect this issue with a 100% success rate.",11.68,Qwen2.5-3B,Apple M1 (Metal)
2601.08713v1_Real-Time Localization Framework for Autonomous Ba.pdf,Real-Time Localization Framework for Autonomous Basketball Robots,"Naren Medarametla, Sreejon Mondal",,,"Robot Localization, Autonomous Navigation, Neural Networks, Robocon",This paper proposes a hybrid localization algorithm that integrates classical techniques with learning-based methods relying solely on visual data from the court's floor to achieve self-localization on the basketball field for Robocon 2025.,9.65,Qwen2.5-3B,Apple M1 (Metal)
2601.08731v1_Learning from Demonstrations via Capability-Aware .pdf,Learning from Demonstrations via Capability-Aware Goal Sampling,"Yuanlin Duan, Rutgers University, yw895@cs.rutgers.edu, Yuning Wang, Rutgers University, yw895@cs.rutgers.edu, Wenjie Qiu, Rutgers University, wq37@cs.rutgers.edu, He Zhu, Rutgers University, hz375@cs.rutgers.edu",10.13039/1000115607044,2601.08731,"Imitation Learning, Goal Sampling, Capability-Aware, Reinforcement Learning","Despite the promise of imitation learning, it often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. Cago (Capability-Aware Goal Sampling) is a novel method that dynamically tracks the agent’s competence along expert trajectories and uses this signal to select intermediate steps—goals that are just beyond the agent’s current reach—to guide learning, resulting in an adaptive curriculum that enables steady progress toward solving the full task.",12.44,Qwen2.5-3B,Apple M1 (Metal)
2601.08732v1_ISLA A U-Net for MRI-based acute ischemic stroke l.pdf,"ISLA: A U-Net for MRI-based acute ischemic stroke lesion segmentation with deep supervision, attention, domain adaptation, and ensemble learning","Vincent Rocaa, Martin Bretzner, Hilde Henon, Laurent Puy, Grégory Kuchcinski, Renaud Lopes",Not found,Not found,"stroke, ischemic stroke, MRI, lesion segmentation, deep learning, U-Net, deep supervision, attention, domain adaptation, ensemble learning","Accurate delineation of acute ischemic stroke lesions in MRI is critical for clinical decision-making. ISLA is a new deep learning model for AIS lesion segmentation from diffusion MRI, trained on three multicenter databases totaling more than 1500 AIS participants. Through systematic optimization of the loss function, convolutional architecture, deep supervision, and attention mechanisms, ISLA developed a robust segmentation framework and outperformed two state-of-the-art approaches on an external test set.",11.0,Qwen2.5-3B,Apple M1 (Metal)
2601.08734v1_TerraFormer Automated Infrastructure-as-Code with .pdf,TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback,"Prithwish Jana∗, Sam Davidson, Bhavana Bhasker, Andrey Kan, Anoop Deoras, Laurent Callot",10.1145/3786583.3786898,,"Infrastructure as Code (IaC), IaC generation, IaC mutation, Neuro-symbolic AI, Large language models, Formal Verification","Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. Evaluations against 17 state-of-the-art LLMs, including ∼50× larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen(Test), and 19.60% on TF-Mutn(Test). It outperforms larger models on both TF-Gen(Test) and TF-Mutn(Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.",12.11,Qwen2.5-3B,Apple M1 (Metal)
2601.08743v1_TableCache Primary Foreign Key Guided KV Cache Pre.pdf,TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency Text-to-SQL,"Jinbo Su, Yuxuan Hu, Cuiping Li, Hong Chen, Jia Li, Lintao Ma, Jing Zhang*",Not provided,Not provided,"Text-to-SQL, KV cache, Low latency, Database, Natural Language Processing",This paper proposes a method to precompute table representations as KV caches offline and query the required ones online to address the inefficiency of redundant prefix cache copies generated by current inference engines when processing user queries with varying table orders. The approach preserves primary foreign key relationships and uses a Table Trie structure for efficient KV cache lookups during inference. The proposed TableCache achieves up to a 3.62× speedup in Time to First Token (TTFT) with negligible performance degradation.,11.5,Qwen2.5-3B,Apple M1 (Metal)
2601.08747v2_To Retrieve or To Think An Agentic Approach for Co.pdf,T o Retrieve or T o Think? An Agentic Approach for Context Evolution,"Rubing Chen, Jian Wang, Wenjie Li, Xiao-Yong Wei, Qing Li",,,"context evolution, agentic approach, retrieval-augmented generation, human metacognition","Current context augmentation methods, such as retrieval-augmented generation, are essential for solving knowledge-intensive reasoning tasks. However, they typically adhere to a rigid, brute-force strategy that executes retrieval at every step, leading to unnecessary computational costs and performance degradation. To address these limitations, Agentic Context Evolution (ACE) is introduced, a framework inspired by human metacognition that dynamically determines whether to seek new evidence or reason with existing knowledge. ACE employs a central orchestrator agent to make decisions strategically via majority voting, alternating between activating a retriever agent for external retrieval and a reasoner agent for internal analysis and refinement. By eliminating redundant retrieval steps, ACE maintains a concise and evolved context. Extensive experiments on challenging multi-hop QA benchmarks demonstrate that ACE significantly outperforms competitive baselines in accuracy while achieving efficient token consumption.",11.89,Qwen2.5-3B,Apple M1 (Metal)
2601.08753v1_Grid-Aware Charging and Operational Optimization f.pdf,Grid-Aware Charging and Operational Optimization for Mixed-Fleet Public Transit,"Rishav Sen, Amutheezan Sivagnanam, Aron Laszka, Ayan Mukhopadhyay, Abhishek Dubey",,,"Electric buses, Public transit, Mixed fleet, Optimization, Hierarchical MILP, Dynamic pricing, Vehicle capacity, Route constraints","This paper presents a comprehensive mixed-integer linear programming (MILP) model to address the challenges of managing mixed fleets of electric and diesel buses in public transit systems, considering factors such as dynamic electricity pricing, vehicle capacity, and route constraints. The model aims to optimize charging schedules and trip assignments while ensuring reliable route service and accommodating all commuters.",11.43,Qwen2.5-3B,Apple M1 (Metal)
2601.08768v1_AI as Entertainment.pdf,AI as Entertainment,"Cody Kommers, Ari Holtzman",XXXXXXX.XXXXXXX,,"Generative AI, Entertainment, Culture, AI impact, Societal Impact","This paper examines the emerging use case of AI in entertainment, highlighting the potential for AI-generated content to become a primary business model for major AI corporations. It argues for a new framework, 'thick entertainment,' to evaluate AI-generated cultural content, considering its role in meaning-making, identity formation, and social connection.",10.55,Qwen2.5-3B,Apple M1 (Metal)
2601.08773v1_Reliable Graph-RAG for Codebases AST-Derived Graph.pdf,Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs,Manideep Reddy Chinthareddy,Not found,2601.08773,"Software Engineering, Graph-RAG, Codebases, AST-Derived Graphs, LLM-Extracted Knowledge Graphs, Retrieval-Augmented Generation, Vector Similarity Search, Multi-hop Reasoning, Controller→Service→Repository Chains, Interface-driven Wiring, Inheritance-based Behavior","This paper benchmarks three retrieval pipelines on Java codebases, comparing a No-Graph Naive RAG, an LLM-Generated Knowledge Graph RAG, and a deterministic AST-derived Knowledge Graph RAG. It reports indexing overhead, query-time latency, corpus coverage signals, and end-to-end cost.",12.32,Qwen2.5-3B,Apple M1 (Metal)
2601.08776v1_Translating Light-Sheet Microscopy Images to Virtu.pdf,Translating Light-Sheet Microscopy Images to Virtual H&E Using CycleGAN,Yanhua Zhao,Not found,Not found,"Histopathology, image translation, H&E staining, unpaired learning, CycleGAN","Histopathology analysis relies on Hematoxylin and Eosin (H&E) staining, but fluorescence microscopy offers complementary information. Converting fluorescence images to H&E-like appearance can aid interpretation and integration with standard workflows. This paper presents a Cycle-Consistent Adversarial Network (CycleGAN) approach for unpaired image-to-image translation from multi-channel fluorescence microscopy to pseudo H&E stained histopathology images.",11.39,Qwen2.5-3B,Apple M1 (Metal)
2601.08777v1_Asymptotic Universal Alignment A New Alignment Fra.pdf,Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling,"Yang Cai*, Weiqiang Zheng*",Not found,2601.08777v1,"alignment, universal alignment, test-time scaling, robust alignment","This paper formalizes an ideal notion of universal alignment through test-time scaling, introducing (k, f(k))-robust alignment and asymptotic universal alignment (U-alignment). It characterizes the optimal convergence rate and proposes a family of symmetric multi-player alignment games to achieve optimal robust alignment.",12.49,Qwen2.5-3B,Apple M1 (Metal)
2601.08778v3_Pervasive Annotation Errors Break Text-to-SQL Benc.pdf,Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards,"Tengjun Jin, Yoojin Choi, Yuxuan Zhu, Daniel Kang",Not provided,Not provided,"Text-to-SQL, Benchmarking, Annotation Errors, Leaderboards","Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of data-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. However, these benchmarks heavily rely on human annotations during question construction and answer evaluation, leading to annotation errors. This paper conducts an empirical study to benchmark annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and corrects a subset of the BIRD development set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings.",11.19,Qwen2.5-3B,Apple M1 (Metal)
2601.08785v1_Uncovering Political Bias in Large Language Models.pdf,Uncovering Political Bias in Large Language Models using Parliamentary Voting Records,"Jieying Chen, Karen de Jong, Andreas Poole, Jan Burakowski, Elena Elderson Nosti, Joep Windt, Chendi Wang","XXX '26, XX, XX",XXXXXX,"Political bias, Large language models, Ideological alignment, Multilingual NLP, Benchmarking, Bias evaluation, Parliamentary motions, LLM fairness","This paper introduces a methodology for constructing political-bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. It applies this methodology to three national case studies and assesses ideological tendencies and political entity bias in LLM behavior. The findings highlight the value of transparent, cross-national evaluation grounded in real parliamentary behavior for understanding and auditing political bias in modern LLMs.",11.37,Qwen2.5-3B,Apple M1 (Metal)
2601.08806v1_APEX-SWE.pdf,APEX–SWE: A Benchmark for Assessing Frontier AI Models in Software Engineering,"Abhi Kottamasu1∧, Akul Datta1∧, Aakash Barthwal1, Ajay Arun1, Chirag Mahapatra1, Adarsh Hiremath1, Brendan Foody1, Bertie Vidgen1∗",Not found,2601.08806,"AI Productivity Index, Software Engineering, Frontier AI Models, Integration Tasks, Observability Tasks, Epistemic Reasoning, Agency","This paper introduces APEX–SWE, a benchmark for evaluating frontier AI models in software engineering. Unlike existing evaluations, it assesses two novel task types: Integration tasks and Observability tasks, which reflect real-world software engineering. Eight frontier models are evaluated, and Gemini 3 Pro performs best with a Pass@1 score of 25%. The analysis shows that strong performance is driven by epistemic reasoning and agency.",12.28,Qwen2.5-3B,Apple M1 (Metal)
2601.08807v1_S3-CLIP Video Super Resolution for Person-ReID.pdf,S3-CLIP: Video Super Resolution for Person-ReID,"Tam´as Endrei, Gy¨orgy Cserey",Not found,Not found,"person re-identification, video super-resolution, CLIP, DINO, WACV 2026","This paper introduces S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios.",10.99,Qwen2.5-3B,Apple M1 (Metal)
2601.08808v1_Multiplex Thinking Reasoning via Token-wise Branch.pdf,Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge,"Yao Tang, Li Dong, Yaru Hao, Qingxiu Dong, Furu Wei, Jiatao Gu",Not found,2601.08808,"Large Language Models, Chain-of-Thought, Reinforcement Learning, Continuous Tokens, Soft Reasoning","Proposes Multiplex Thinking, a stochastic soft reasoning mechanism that samples candidate tokens and aggregates their embeddings into a single continuous multiplex token, optimizing with on-policy reinforcement learning and producing shorter sequences than discrete CoT and RL baselines across challenging math reasoning benchmarks.",10.87,Qwen2.5-3B,Apple M1 (Metal)
2601.08811v1_Reasoning Matters for 3D Visual Grounding.pdf,Reasoning Matters for 3D Visual Grounding,"Hsiang-Wei Huang, Kuang-Ming Chen, Wenhao Chai, Cheng-Yen Yang, Jen-Hao Cheng, Jenq-Neng Hwang",,,"3D visual grounding, Large Language Models, Reasoning, Synthetic Data, LLM fine-tuning","The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. However, 3D visual grounding remains challenging due to the limited reasoning ability of recent models. This work proposes a 3D visual grounding data pipeline capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning processes, and introduces Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous methods using only 1.6% of their training data.",11.22,Qwen2.5-3B,Apple M1 (Metal)
2601.08816v2_MemRec Collaborative Memory-Augmented Agentic Reco.pdf,MemRec: Collaborative Memory-Augmented Agentic Recommender System,"Weixin Chen, Yuhan Zhao, Jingyuan Huang, Zihe Ye, Clark Mingxuan Ju, Tong Zhao, Neil Shah",,,"Recommender Systems, Semantic Memory, Collaborative Filtering, Large Language Models, Memory Augmentation, Efficient Memory Management","MemRec is a framework that decouples reasoning from memory management to enable efficient collaborative augmentation of semantic memory in agentic recommender systems. It introduces a dedicated, cost-effective LMMem to manage a dynamic collaborative memory graph, serving synthesized, high-signal context to a downstream LLMRec. Extensive experiments on four benchmarks demonstrate state-of-the-art performance.",10.67,Qwen2.5-3B,Apple M1 (Metal)
2601.08828v1_Motion Attribution for Video Generation.pdf,Motion Attribution for Video Generation,"Xindi Wu, Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-Taixé, Olga Russakovsky, Sanja Fidler, Jonathan Lorraine",Not found,Not found,"video generation, motion attribution, data attribution, gradient-based, temporal dynamics, fine-tuning","Presenting Motive, a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, enabling efficient and scalable motion-specific influence computation. It identifies clips that strongly affect motion and guides data curation for improved temporal consistency and physical plausibility. Motive improves motion smoothness and dynamic degree on VBench compared to a pretrained base model.",11.29,Qwen2.5-3B,Apple M1 (Metal)
2601.08829v1_Modeling LLM Agent Reviewer Dynamics in Elo-Ranked.pdf,Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System,"Hsiang-Wei Huang*, Junbin Lu*",,,"Large Language Model, Peer Review, Elo Rating, Simulation","This work explores the dynamics of Large Language Model (LLM) agent reviewers in an Elo-ranked review system using real-world conference paper submissions. It compares a baseline setting with conditions that incorporate Elo ratings and reviewer memory, showcasing improvements in Area Chair decision accuracy and adaptive review strategies. The code is available at https://github.com/hsiangwei0903/EloReview.",10.26,Qwen2.5-3B,Apple M1 (Metal)
2601.08873v1_ForensicFormer Hierarchical Multi-Scale Reasoning .pdf,ForensicFormer: Hierarchical Multi-Scale Reasoning for Cross-Domain Image Forgery Detection,"Hema Hariharan, Samson",Not found,Not found,"Image forensics, forgery detection, transformers, cross-domain generalization, AI-generated images, hierarchical reasoning","The paper presents ForensicFormer, a hierarchical multi-scale framework for cross-domain image forgery detection. Unlike prior single-paradigm approaches, ForensicFormer maintains 86.8% average accuracy across seven diverse test sets, significantly improving over state-of-the-art universal detectors. It demonstrates superior robustness to JPEG compression and provides pixel-level forgery localization.",10.88,Qwen2.5-3B,Apple M1 (Metal)
2601.08874v1_The Illusion of Friendship Why Generative AI Deman.pdf,The Illusion of Friendship: Why Generative AI Demands Unprecedented Ethical Vigilance,Md Zahidul Islam,,,"Generative AI, Ethics, Friendship, Emotional Attachment, Transformer Models","This paper explores the ethical implications of Generative AI systems, particularly ChatGPT, which can blur the line between tool and companion. It discusses the potential for users to form emotionally significant attachments to these systems, which can lead to harmful consequences. The paper argues for a safeguard framework to mitigate over-reliance and emotional misattribution.",10.99,Qwen2.5-3B,Apple M1 (Metal)
2601.08875v2_Learning Domain-Invariant Representations for Cros.pdf,Learning Domain-Invariant Representations for Cross-Domain Image Registration via Scene-Appearance Disentanglement,"Jiahao Qin∗†, Yiwen Wang∗",Not found,Not found,"Image Registration, Domain Shift, Scene Appearance, Disentanglement, Cross-Domain","This paper proposes SAR-Net, a unified framework for image registration under domain shift. The key insight is that observed images can be decomposed into domain-invariant scene representations and domain-specific appearance codes, enabling registration via re-rendering rather than direct intensity matching. The method achieves a median relative Target Registration Error (rTRE) of 0.25% on the ANHIR benchmark, outperforming the state-of-the-art MEVIS method by 7.4%.",11.15,Qwen2.5-3B,Apple M1 (Metal)
2601.08881v1_TAG-MoE Task-Aware Gating for Unified Generative M.pdf,TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts,"Yu Xu1,2†, Hongbin Yan1, Juan Cao1, Yiji Cheng2, Tiankai Hang2, Runze He2, Zijin Yin2, Shiyi Zhang2, Yuxin Zhang1, Jintao Li1, Chunyu Wang2‡, Qinglin Lu2, Tong-Yee Lee3, Fan Tang1§",https://doi.org/10.1101/2601.08881v1,2601.08881,"Unified image generation, Generative Mixture-of-Experts, Task-aware gating, Dense diffusion transformers, Task interference","This paper proposes a novel framework to inject semantic intent into MoE routing, enabling a diffusion transformer model to handle diverse generative tasks without task interference.",12.28,Qwen2.5-3B,Apple M1 (Metal)
2601.08882v1_Compressing Vision Transformers in Geospatial Tran.pdf,Compressing Vision Transformers in Geospatial,"Thomas Snyder, H. Lexie Yang, Stefan Schnake, Steffen Schotthöfer",Not found,2601.08882,"geospatial, vision transformers, transfer learning, compression, manifold-constrained optimization","Deploying geospatial foundation models on resource-constrained edge devices demands compact architectures that maintain high downstream performance. This work leverages manifold-constrained optimization framework DLRT to compress large vision transformer-based geospatial foundation models during transfer learning, achieving strong compression while preserving task-specific accuracy.",11.73,Qwen2.5-3B,Apple M1 (Metal)
2601.08884v1_Bridging the Gap Empowering Small Models in Reliab.pdf,Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting,"Samyak Jhaveri, Cristina V. Lopes",Not found,Not found,"OpenACC, parallel programming, large language models, prompt optimization, high-performance computing, GPU programming","This work presents a systematic prompt optimization approach to enhance OpenACC pragma generation without prohibitive computational costs associated with LLM post-training. It uses the GEPA framework to iteratively evolve prompts through a reflective feedback loop, guided by expertly curated 'gold' pragma examples and structured feedback based on clause and parameter-level mismatches. The evaluation on the PolyBench suite shows significant improvements in compilation success rates and functional GPU speedups for smaller, cheaper models, demonstrating the potential of smaller, cheaper LLMs to write stable and effective GPU-offloading directives.",11.77,Qwen2.5-3B,Apple M1 (Metal)
2601.08891v1_Attention Consistency Regularization for Interpret.pdf,Attention Consistency Regularization for Interpretable Early-Exit Neural Networks,"Yanhua Zhao, KIS*MED (AI Systems in Medicine), Technische Universitat Darmstadt",,,"Early exit networks, explainable AI, attention mechanisms, multi-objective learning","This paper presents Explanation-Guided Training (EGT), a multi-objective framework that improves interpretability and consistency in early-exit neural networks through attention-based regularization. EGT introduces an attention consistency loss that aligns early-exit attention maps with the final exit. Experiments on a real-world image classification dataset demonstrate that EGT achieves up to 98.97% overall accuracy with a 1.97× inference speedup through early exits, while improving attention consistency by up to 18.5% compared to baseline models.",11.56,Qwen2.5-3B,Apple M1 (Metal)
2601.08892v1_Evaluating Role-Consistency in LLMs for Counselor .pdf,Evaluating Role-Consistency in LLMs for Counselor Training,"Eric Rudolph, Natalie Engert, Jens Albrecht",Not found,2601.08892,"Counseling, Chatbot, LargeLanguageModel, PersonaConsistency, EducationalRole-Play","The rise of online counseling services has highlighted the need for effective training methods for future counselors. This paper extends research on VirCo, a Virtual Client for Online Counseling, designed to complement traditional role-playing methods in academic training by simulating realistic client interactions. The study focuses on evaluating the role consistency and coherence of the Vicuna model’s responses, comparing these findings with earlier research. Additionally, it assesses and compares various open-source LLMs for their performance in sustaining role consistency during virtual client interactions.",12.17,Qwen2.5-3B,Apple M1 (Metal)
2601.08896v1_XGBoost Forecasting of NEPSE Index Log Returns wit.pdf,XGBoost Forecasting of NEPSE Index Log Returns with Walk Forward Validation,"Sahaj Raj Mallaa, Shreeyash Kayastha, Rumi Suwala, Harish Chandra Bhandara, Rajendra Adhikari",Not found,Not found,"NEPSE Index, stock index forecasting, XGBoost, walk-forward validation, hyperparameter optimization, time series forecasting, emerging markets, feature engineering","This study develops a robust machine learning framework for one-step-ahead forecasting of daily log-returns in the Nepal Stock Exchange (NEPSE) Index using the XGBoost regressor. A comprehensive feature set is engineered, including lagged log-returns (up to 30 days) and established technical indicators such as short- and medium-term rolling volatility measures and the 14-period Relative Strength Index. Hyperparameter optimization is performed using Optuna with time-series cross-validation on the initial training segment. Out-of-sample performance is rigorously assessed via walk-forward validation under both expanding and fixed-length rolling windows schemes across multiple lag configurations, simulating real-world deployment and avoiding lookahead bias. Predictive accuracy is evaluated using root mean squared error, mean absolute error, coefficient of determination (R²), and directional accuracy on both log-returns and reconstructed closing prices. Empirical results show that the optimal configuration—an expanding window with 20 lags—outperforms tuned ARIMA and Ridge regression benchmarks, achieving the lowest log-return RMSE (0.013450) and MAE (0.009814) alongside a directional accuracy of 65.15%. While the R² remains modest, consistent with the noisy nature of financial returns, primary emphasis is placed on relative error reduction and directional prediction. Feature importance analysis and visual inspection further enhance interpretability. These findings demonstrate the effectiveness of gradient boosting ensembles in modeling nonlinear dynamics in volatile emerging market time series and establish a reproducible benchmark for NEPSE Index forecasting.",12.62,Qwen2.5-3B,Apple M1 (Metal)
2601.08901v1_Navigating Ideation Space Decomposed Conceptual Re.pdf,Navigating Ideation Space: Decomposed Conceptual Representations for Scientific Ideas,"Yuexi Shen, Minqian Liu, Dawei Zhou, Lifu Huang",,not found,"Scientific discovery, Conceptual representation, Literature retrieval, Novelty assessment","Scientific discovery is a cumulative process requiring new ideas to be situated within an expanding landscape of existing knowledge. The challenge is to identify conceptually relevant prior work and assess how a new idea differentiates from existing research. Current embedding approaches conflate distinct conceptual aspects into single representations, hindering fine-grained literature retrieval. LLM-based evaluators are subject to sycophancy biases, failing to provide discriminative novelty assessment. This paper introduces the Ideation Space, a structured representation that decomposes scientific knowledge into three dimensions: research problem, methodology, and core findings, each learned through contrastive training. This framework enables principled measurement of conceptual distance and modeling of ideation transitions. A Hierarchical Sub-Space Retrieval framework and a Decomposed Novelty Assessment algorithm are proposed for efficient, targeted literature retrieval and identifying novel aspects of ideas, respectively. Extensive experiments demonstrate substantial improvements in recall, hit rate, and correlation with expert judgments.",11.84,Qwen2.5-3B,Apple M1 (Metal)
2601.08910v1_Towards a Self-Driving Trigger at the LHC Adaptive.pdf,Towards a Self-Driving Trigger at the LHC: Adaptive Response in Real Time,"Shaghayegh Emami, Cecilia Tosciri, Giovanna Salvi, Zixin Ding, Yuxin Chen, Abhijith Gandrakota, Christian Herwig, David W. Miller, Jennifer Ngadiuba, Nhan Tran",,2601.08910v1,"self-driving trigger, real-time data filtering, high-throughput scientific facilities, Large Hadron Collider (LHC), adaptive response, machine learning, anomaly detection, signal efficiency, rate stability, computational cost","This work explores the concept of a self-driving trigger, an autonomous data-filtering framework that reallocates resources and adjusts thresholds dynamically in real-time to optimize signal efficiency, rate stability, and computational cost in high-throughput scientific facilities such as the experiments at the Large Hadron Collider (LHC).",12.81,Qwen2.5-3B,Apple M1 (Metal)
2601.08950v1_ConvoLearn A Dataset of Constructivist Tutor-Stude.pdf,ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue,"Mayank Sharma, Roy Pea, Hari Subramonyam",,1https://huggingface.co/datasets/masharma/convolearn,"constructivist, tutor-student, dialogue, pedagogical, knowledge-building, LLMs, education","This paper introduces ConvoLearn, a dataset grounded in knowledge-building theory, operationalizing six core pedagogical dimensions. It constructs a semi-synthetic dataset of 1,250 tutor-student dialogues, demonstrating that training on this dataset shifts LLM behavior towards knowledge-building strategies, outperforming base models in human evaluations.",10.22,Qwen2.5-3B,Apple M1 (Metal)
2601.08951v1_PluriHarms Benchmarking the Full Spectrum of Human.pdf,PLURIHARMS: BENCHMARKING THE FULL SPECTRUM OF HUMAN JUDGMENTS ON AI HARM,"Jing-Jing Li, Joel Mire, Eve Fleisig, Valentina Pyatkin, Anne G. E. Collins, Maarten Sap, Sydney Levine",Not found,Not found,"AI safety, human judgments, harm, disagreement, benchmark","This paper introduces PLURIHARMS, a benchmark designed to systematically study human harm judgments across two key dimensions: the harm axis (benign to harmful) and the agreement axis (agreement to disagreement). The benchmark includes 150 prompts with 15,000 ratings from 100 human annotators, and analyses show that prompts related to imminent risks and tangible harms amplify perceived harmfulness, while annotator traits and their interactions with prompt content explain systematic disagreement. The work provides a principled benchmark for moving beyond 'one-size-fits-all' safety toward pluralistically safe AI.",11.79,Qwen2.5-3B,Apple M1 (Metal)
2601.08953v1_Fairness risk and its privacy-enabled solution in .pdf,Fairness risk and its privacy-enabled solution in AI-driven robotic applications,"Le Liu, Bangguo Yu, Nynke Vellinga, Ming Cao",Not found,2601.08953,"Robotic Decision-making, Large Language Model, Fairness, Privacy","Complex decision-making by autonomous machines and algorithms could underpin the foundations of future society. Generative AI is emerging as a powerful engine for such transitions. However, Generative AI-driven developments pose a critical pitfall: fairness concerns. In robotic applications, although intuitions about fairness are common, a precise and implementable definition that captures user utility and inherent data randomness is missing. This paper provides a utility-aware fairness metric for robotic decision-making and analyzes fairness jointly with user-data privacy, deriving conditions under which privacy budgets govern fairness metrics.",12.55,Qwen2.5-3B,Apple M1 (Metal)
2601.08955v1_Imagine-then-Plan Agent Learning from Adaptive Loo.pdf,Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models,"Youwei Liu, Jian Wang, Hanlin Wang, Beichen Guo, Wenjie Li",Not provided,Not provided,"Agent learning, World models, Adaptive lookahead, Multi-step planning, Reinforcement learning","Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (ITP), a unified framework for agent learning via lookahead imagination, where an agent’s policy model interacts with the learned world model, yielding multi-step 'imagined' trajectories. The framework introduces an adaptive lookahead mechanism to handle varying imagination horizons by trading off the ultimate goal and task progress.",10.83,Qwen2.5-3B,Apple M1 (Metal)
2601.08988v1_ART Action-based Reasoning Task Benchmarking for M.pdf,AAAI 2026: Healthy Aging and Longevity Workshop (AIAA) - ART: Action-based Reasoning Task Benchmarking for Medical AI Agents,"Ananya Mantravadi, Shivali Dalmia, Abhishek Mukherji",Not provided,Not provided,"Medical AI agents, synthetic data generation, clinical reasoning evaluation, healthcare LLMs, benchmark, HITL","Reliable clinical decision support requires medical AI agents capable of safe, multi-step reasoning over structured electronic health records (EHRs). Existing benchmarks inadequately assess performance on action-based tasks involving threshold evaluation, temporal aggregation, and conditional logic. ART introduces an Action-based Reasoning clinical Task benchmark for medical AI agents, which mines real-world EHR data to create challenging tasks targeting known reasoning weaknesses.",11.44,Qwen2.5-3B,Apple M1 (Metal)
2601.09012v3_TranslateGemma Technical Report.pdf,"We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models",Google Translate Research Team,,,"machine translation, open models, supervised fine-tuning, reinforcement learning, Gemma 3, synthetic data, human-translated data, multimodal translation, image translation","The paper introduces TranslateGemma, an open machine translation model suite based on the Gemma 3 foundation models. It employs a two-stage fine-tuning process, including supervised fine-tuning and reinforcement learning, to enhance translation quality. The model is evaluated on various datasets and shows significant improvements in translation quality across multiple language pairs.",10.8,Qwen2.5-3B,Apple M1 (Metal)
2601.09018v1_Meta-learning to Address Data Shift in Time Series.pdf,META-LEARNING TOADDRESSDATASHIFT IN TIMESERIES CLASSIFICATION,"Samuel Myrenab, Nidhi Parikha, Natalie Kleina",Not found,2601.09018,"signals, seismology, Reptile, FOMAML, model-agnostic meta-learning, domain generalization","This work systematically compares traditional deep learning (TDL) with fine-tuning and optimization-based meta-learning algorithms in addressing data shift in time-series classification. It introduces a controlled seismic benchmark (SeisTask) and shows that meta-learning typically achieves faster and more stable adaptation with reduced overfitting in data-scarce regimes and smaller model architectures. Task diversity influences meta-learning performance, but alignment between training and test distributions drives performance gains. The study contributes SeisTask as a benchmark for advancing adaptive learning research in time-series domains.",11.19,Qwen2.5-3B,Apple M1 (Metal)
2601.09028v1_OpenDecoder Open Large Language Model Decoding to .pdf,OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG,"Fengran Mo, Zhan Su, Yuchen Hui, Jianhan Zhang, Jia Ao Sun, Zheyuan Liu, Chao Zhang, Tetsuya Sakai, Jian-Yun Nie",10.1145/nnnnnnn.nnnnnnn,,"Information Retrieval, Retrieval-Augmented Generation, Robust Question Answer, Decoding Paradigm, Large Language Model","The paper proposes OpenDecoder, a new approach that leverages explicit evaluation of retrieved information as quality indicators for generation, aiming to build a more robust RAG model that is less sensitive to varying levels of noisy context. The experimental results demonstrate the effectiveness and better robustness of OpenDecoder compared to various baseline methods.",11.01,Qwen2.5-3B,Apple M1 (Metal)
2601.09029v1_Proactively Detecting Threats A Novel Approach Usi.pdf,Proactively Detecting Threats: A Novel Approach,"Aniesh Chawla ∗, Udbhav Prasad ∗",,,"Malware, Indicators of Compromise, Cybersecurity, LLMs, GenAI, Machine Learning Algorithms, Deep Neural Network","Enterprise security faces escalating threats from sophisticated malware, compounded by expanding digital operations. This paper presents the first systematic evaluation of large language models (LLMs) to proactively identify indicators of compromise (IOCs) from unstructured web-based threat intelligence sources, distinguishing it from reactive malware detection approaches. The evaluation of 479 webpages containing 2,658 IOCs (711 IPv4 addresses, 502 IPv6 addresses, 1,445 domains) reveals significant performance variations. Gemini 1.5 Pro achieved 0.958 precision and 0.788 specificity for malicious IOC identification, while demonstrating perfect recall (1.0) for actual threats.",11.53,Qwen2.5-3B,Apple M1 (Metal)
2601.09031v1_Generalizable Geometric Prior and Recurrent Spikin.pdf,GENERALIZABLE GEOMETRIC PRIOR AND RECURRENT SPIKING FEATURE LEARNING FOR HUMANOID ROBOT MANIPULATION,"Xuetao Li, Wenke Huang, Mang Ye, Jifeng Xuan, Bo Du, Sheng Liu, Miao Li",Not found,Not found,"Humanoid Robot Manipulation, Geometric Prior, Recurrent Spiking Feature Learning","This paper presents a novel RGMP-S, Recurrent Geometric-prior Multimodal Policy with Spiking features, facilitating both high-level skill reasoning and data-efficient motion synthesis. It addresses challenges in precise scene understanding and sample-efficient learning from human demonstrations, enabling robust generalization in unseen environments.",11.07,Qwen2.5-3B,Apple M1 (Metal)
2601.09032v1_The Hierarchy of Agentic Capabilities Evaluating F.pdf,The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments,"Logan Ritchie∗, Sushant Mehta, Nick Heiner, Mason Yu, Edwin Chen",Not found,2601.09032,"large language models, agent capabilities, realistic RL environments, workplace tasks","This study evaluates frontier AI models on 150 workplace tasks within a realistic e-commerce RL environment from Surge, revealing an empirically-derived hierarchy of agentic capabilities that models must master for real-world deployment.",10.9,Qwen2.5-3B,Apple M1 (Metal)
2601.09035v1_A Decompilation-Driven Framework for Malware Detec.pdf,A Decompilation-Driven Framework for Malware Detection with Large Language Models,"Aniesh Chawla, Udbhav Prasad",Not found,2601.09035,"Malware, Ghidra, Cybersecurity, LLMs, GenAI, Machine Learning Algorithms, LLMs Code development","This paper evaluates the efficacy of state-of-the-art Large Language Models (LLMs) in classifying executable code as either benign or malicious. It introduces an automated pipeline that first decompiles Windows executables into C code using Ghidra disassembler and then leverages LLMs for classification. The evaluation reveals that while standard LLMs show promise, they are not yet robust enough to replace traditional anti-virus software. A fine-tuned model trained on curated malware and benign datasets significantly outperforms its vanilla counterpart, but performance degrades notably with newer malware. This finding underscores the need for continuous fine-tuning with emerging threats to maintain model effectiveness against evolving coding patterns and behaviors of malicious software.",12.02,Qwen2.5-3B,Apple M1 (Metal)
2601.09041v1_Can LLMs interpret figurative language as humans d.pdf,CANLLMSINTERPRETFIGURATIVELANGUAGE ASHUMANSDO?: SURFACE-LEVEL VS. REPRESENTATIONAL SIMILARITY,"Samhita Bollepally, Aurora Sloman-Moll, Takashi Yamauchi",Not found,Not found,"Large language models, Human interpretation, Figurative language, Sarcasm, Slang, Emotion","This study investigates the extent to which large language models (LLMs) align with human judgments in interpreting figurative and socially grounded language. Human participants and four instruction-tuned LLMs rated 240 dialogue-based sentences representing six linguistic traits: conventionality, sarcasm, funny, emotional, idiomacy, and slang. Results indicate that humans and LLMs align at the surface level but diverge significantly at the representational level, especially in interpreting figurative sentences involving idioms and Gen Z slang. GPT-4 most closely approximates human representational patterns, while all models struggle with context-dependent and socio-pragmatic expressions.",11.51,Qwen2.5-3B,Apple M1 (Metal)
2601.09049v1_Is Grokking Worthwhile Functional Analysis and Tra.pdf,Is Grokking Worthwhile? Functional Analysis and Transferability of Generalization Circuits in Transformers,"Kaiyu He, Mian Zhang, Peilin Wu, Xinya Du, Zhiyu Zoey Chen",,,"Transformers, grokking, generalization circuit, two-hop reasoning, knowledge assimilation","While Large Language Models excel at factual retrieval, they often struggle with the 'curse of two-hop reasoning' in compositional tasks. Recent research suggests that parameter-sharing transformers can bridge this gap by forming a 'Generalization Circuit' during a prolonged 'grokking' phase. This work conducts a mechanistic study to evaluate the Generalization Circuit's role in knowledge assimilation and transfer, demonstrating that non-grokked and grokked models establish identical inference paths for in-distribution compositional queries, suggesting that grokking is the process of integrating memorized atomic facts into a naturally established reasoning path. Achieving high accuracy on unseen cases after prolonged training and the formation of a certain reasoning path are not bound; they can occur independently under specific data regimes. Even a mature circuit exhibits limited transferability when integrating new knowledge, suggesting that 'grokked' Transformers do not achieve a full mastery of compositional logic.",11.72,Qwen2.5-3B,Apple M1 (Metal)
2601.09066v1_Midm 2.0 Korea-centric Bilingual Language Models.pdf,Mi:dm 2.0: Korea-Centric Bilingual Language Models,"Tech. Innovation Group, KT",Not found,2601.09066v1,"Bilingual language model, Korean-centric AI, KT, LLM, Language processing, Cultural alignment, Data quality, Synthetic data, Curriculum learning, KOREA-CENTRICAI","This paper introduces Mi:dm 2.0, a bilingual large language model (LLM) specifically engineered to advance Korean-centric AI. It addresses limitations of existing LLMs by integrating values, reasoning patterns, and commonsense knowledge inherent to Korean society, enabling nuanced understanding of cultural contexts, emotional subtleties, and real-world scenarios.",11.42,Qwen2.5-3B,Apple M1 (Metal)
2601.09069v1_From Symbolic to Natural-Language Relations Rethin.pdf,From Symbolic to Natural-Language Relations: Rethinking Knowledge Graph Construction in the Era of Large Language Models,"Kanyao Han, Yushang Lai",,,"Knowledge Graphs, Natural Language, Large Language Models, Relation Extraction, Semantic Representation","This paper argues for rethinking the representation of relations in knowledge graphs, advocating for a shift from symbolic to natural-language descriptions. It discusses the limitations of current symbolic relation schemas and the potential benefits of using natural language for relation extraction and representation, especially with the rise of large language models.",10.08,Qwen2.5-3B,Apple M1 (Metal)
2601.09072v1_Human-AI Co-design for Clinical Prediction Models.pdf,Human-AI Co-design for Clinical Prediction Models,"Jean Feng1,*,†, Avni Kothari 1,*, Patrick Vossler 1, Andrew Bishara 1, Lucas Zier 1, Newton Addo 1, Aaron Kornblith 1, Yan Shuo Tan 2, Chandan Singh 3, Equal contribution, Corresponding author: jean.feng@ucsf.edu",Not found,2601.09072,"Large language models, Electronic health records, Concept Bottleneck, Human-AI Interaction","Developing safe, effective, and practically useful clinical prediction models (CPMs) traditionally requires iterative collaboration between clinical experts, data scientists, and informaticists. This process refines the often small but critical details of the model building process, such as which features/patients to include and how clinical categories should be defined. HACHI, an iterative human-in-the-loop framework, uses AI agents to accelerate the development of fully interpretable CPMs by enabling the exploration of concepts in clinical notes.",11.93,Qwen2.5-3B,Apple M1 (Metal)
2601.09085v1_MMR-GRPO Accelerating GRPO-Style Training through .pdf,MMR-GRPO: Accelerating GRPO-Style Training through Diversity-Aware Reward Reweighting,"Kangda Wei, Ruihong Huang",Not provided,Not provided,"Group Relative Policy Optimization, Mathematical Reasoning, Reinforcement Learning, Reward Reweighting, Diversity","This paper proposes MMR-GRPO, a method that integrates Maximal Marginal Relevance to reweight rewards based on completion diversity. It aims to accelerate training by prioritizing diverse solutions, achieving comparable peak performance with 47.9% fewer training steps and 70.2% less wall-clock time on average across three model sizes and five mathematical reasoning benchmarks.",9.69,Qwen2.5-3B,Apple M1 (Metal)
2601.09089v1_SubTokenTest A Practical Benchmark for Real-World .pdf,SUBTOKENTEST: A Practical Benchmark for Real-World Sub-token Understanding,"Shuyang Hou∗, Yi Hu∗, Muhan Zhang†",,2601.09089,"Sub-token understanding, Large language models, Tokenization, Character-level tasks, Practical relevance","Recent advancements in large language models have enhanced their reasoning capabilities, but they struggle with basic character-level tasks such as counting letters in words. SUBTOKENTEST introduces a comprehensive benchmark that assesses sub-token understanding through practical, utility-driven tasks, isolating tokenization-related failures. The benchmark includes ten tasks across four domains and evaluates nine advanced LLMs, investigating the impact of test-time scaling and character-level information encoding within hidden states.",10.7,Qwen2.5-3B,Apple M1 (Metal)
2601.09097v1_Programming over Thinking Efficient and Robust Mul.pdf,Programming over Thinking: Efficient and Robust Multi-Constraint Planning,"Derrick Goh Xin Deik1, Quanyu Long1, Zhengyuan Liu2, Nancy F. Chen2, Wenya Wang1",Not found,Not found,"Planning, Multi-constraint, LLM, Code generation, Robustness, Scalability","This paper introduces the ScalableCOdePlanning Engine (SCOPE), a framework that separates query-specific reasoning from generic code execution. SCOPE produces solver functions that are consistent, deterministic, and reusable across queries, achieving state-of-the-art performance while lowering cost and latency. It addresses the limitations of existing large language model (LLM) approaches in multi-constraint planning, which face fundamental limitations in reasoning and lack flexibility in capturing generalizable logic across diverse problems.",11.07,Qwen2.5-3B,Apple M1 (Metal)
2601.09100v2_DScheLLM Enabling Dynamic Scheduling through a Fin.pdf,DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large Language Model,"Lixiang Zhang, Chenggong Zhao, Qing Gao, Xiaoke Zhao, Gengyi Bai, Jinhu Lv",Not provided,Not provided,"dynamic scheduling, large language model, fine-tuning, job shop scheduling","This paper proposes DScheLLM, a dynamic scheduling approach that leverages fine-tuned large language models within a dual-system (fast–slow) reasoning architecture to address disturbances of different scales. It constructs a unified large language model-based framework to handle dynamic events, using training datasets generated from exact schedules obtained from an operations research solver. The Huawei OpenPangu Embedded-7B model is subsequently fine-tuned under hybrid reasoning paradigms using LoRA. Experimental evaluations on standard job shop scheduling benchmarks demonstrate the effectiveness of the fast-thinking mode in generating high-quality schedules and the slow-thinking mode in producing solver-compatible and well-formatted decision inputs.",11.81,Qwen2.5-3B,Apple M1 (Metal)
2601.09105v2_AviationLMM A Large Multimodal Foundation Model fo.pdf,AviationLMM: A Large Multimodal Foundation Model for Civil Aviation,"Wenbin Li, Jingling Wu, Xiaoyong Lin, Jing Chen, Cong Chen",,,"civil aviation, multi-modal model, foundation model, cloud edge collaboration, hybrid training, computer systems organization, computing methodologies","This paper introduces AviationLMM, a large multimodal foundation model for civil aviation, designed to unify heterogeneous data streams and enable understanding, reasoning, generation, and agentic applications. It addresses gaps between existing AI solutions and requirements, describes the model architecture, and identifies key research opportunities. By articulating the design and challenges, the paper aims to boost civil aviation foundation model progress and catalyze coordinated research efforts toward an integrated, trustworthy, and privacy-preserving aviation AI ecosystem.",11.32,Qwen2.5-3B,Apple M1 (Metal)
2601.09113v1_The AI Hippocampus How Far are We From Human Memor.pdf,The AI Hippocampus: How Far are We From Human Memory?,"Zixia Jia, Jiaqi Li, Yipeng Kang, Yuxuan Wang, Tong Wu, Quansen Wang, Xiaobo Wang, Shuyi Zhang, Junzhe Shen, Qing Li, Siyuan Qi, Yitao Liang, Di He, Zilong Zheng, Song-Chun Zhu",Not found,2601.09113v1,"AI, Memory, Large Language Models, Multi-Modal Models, Hippocampus, Continual Learning, Personalized Inference","This survey presents a comprehensive and structured synthesis of memory in Large Language Models (LLMs) and Multi-Modal LLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms.",11.98,Qwen2.5-3B,Apple M1 (Metal)
2601.09116v1_LP-LLM End-to-End Real-World Degraded License Plat.pdf,LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models,"Haoyan Gong, Hongbin Liu",Not found,Not found,"License Plate Recognition, Text Recognition, Degraded Images, Multimodal Models, Vision-Language Models, Character Slot Queries, Fine-Grained Evidence Retrieval, Residual Modulation, LoRA Fine-Tuning","This paper proposes an end-to-end structure-aware multimodal reasoning framework for real-world License Plate Recognition, addressing the challenges posed by severe degradations such as motion blur, low resolution, and complex illumination. The core innovation lies in the Character-Aware Multimodal Reasoning Module (CMRM), which introduces learnable Character Slot Queries to actively retrieve fine-grained evidence from visual features, enabling autoregressive generation based on explicit structural priors. The framework achieves domain adaptation while retaining the generalization capabilities of large models, significantly outperforming existing restoration-recognition combinations and general Vision-Language Models.",11.82,Qwen2.5-3B,Apple M1 (Metal)
2601.09117v1_A Marketplace for AI-Generated Adult Content and D.pdf,A Marketplace for AI-Generated Adult Content and Deepfakes,"Shalmoli Ghosh1, Matthew R. DeVerna2, Filippo Menczer1",,,"AI, Generative Models, Deepfakes, Content Creation, Monetization, Social Harm","This study examines the use of a monetized feature called Bounties on the Civitai platform, which allows users to commission AI-generated content in exchange for payment. The findings reveal that the marketplace is dominated by tools that steer AI models toward content they were not trained to generate, with requests for ",10.89,Qwen2.5-3B,Apple M1 (Metal)
2601.09120v1_Adaptive Multi-Stage Patent Claim Generation with .pdf,Adaptive Multi-Stage Patent Claim Generation with Unified Quality Assessment,"Chen-Wei Liang, Bin Guo, Zhen-Yuan Wei, Mu-Jiang-Shan Wang",Not found,2601.09120,"Patent claim generation, Cross-jurisdictional learning, Quality assessment, Transformer, Domain adaptation","Current patent claim generation systems face three fundamental limitations: poor cross-jurisdictional generalization, inadequate semantic relationship modeling between claims and prior art, and unreliable quality assessment. This paper introduces a novel three-stage framework that addresses these challenges through relationship-aware similarity analysis, domain-adaptive claim generation, and unified quality assessment.",12.61,Qwen2.5-3B,Apple M1 (Metal)
2601.09130v1_Equi-ViT Rotational Equivariant Vision Transformer.pdf,EQUI-VIT: ROTATIONAL EQUIVARIANT VISION TRANSFORMER FOR ROBUST HISTOPATHOLOGY ANALYSIS,"Fuyao Chen, Yuexi Du, Eléonore V. Lieffrig, Nichola C. Dvornek, John A. Onofrey",Not found,Not found,"Vision Transformer, Rotation Equivariance, Histopathology, Artificial Intelligence","Equi-ViT integrates an equivariant convolution kernel into the patch embedding stage of a ViT architecture, achieving superior rotation-consistent patch embeddings and stable classification performance across image orientations. This enhances data efficiency and robustness, suggesting potential as a more generalizable backbone for ViT applications in histopathology.",10.91,Qwen2.5-3B,Apple M1 (Metal)
2601.09136v1_SkinFlow Efficient Information Transmission for Op.pdf,SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL,"Lijun Liu, Linwei Chen, Zhishou Zhang, Meng Tian, Hengfu Cui, Ruiyang Li, Zhaocheng Liu, Qiang Ju, Qianxi Li, Hong-Yu Zhou",Not found,Not found,"SkinFlow, Large Vision-Language Models, Reinforcement Learning, Dermatology, Pathological Lesions, Information Transmission, Dynamic Visual Encoding","This paper introduces SkinFlow, a framework that treats dermatological diagnosis as an optimization of visual information transmission efficiency. Utilizing a Virtual-Width Dynamic Vision Encoder (DVE) and a two-stage Reinforcement Learning strategy, SkinFlow aims to improve diagnostic accuracy and efficiency in dermatology. The authors propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching, demonstrating superior diagnostic reasoning compared to raw parameter scaling.",11.34,Qwen2.5-3B,Apple M1 (Metal)
2601.09147v2_SSVP Synergistic Semantic-Visual Prompting for Ind.pdf,SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection,"Chenhao Fu, Han Fang, Xiuzheng Zheng, Wenbo Wei, Yonghua Li, Hao Sun, Xuelong Li",Not found,Not found,"Zero-Shot Anomaly Detection, Synergistic Semantic-Visual Prompting, Vision-Language Models, Industrial Anomaly Detection, Dynamic Prompting, CLIP","Zero-Shot Anomaly Detection (ZSAD) leverages Vision-Language Models (VLMs) to enable supervision-free industrial inspection. However, existing ZSAD paradigms are constrained by single visual backbones, which struggle to balance global semantic generalization with fine-grained structural discriminability. To bridge this gap, Synergistic Semantic-Visual Prompting (SSVP) efficiently fuses diverse visual encodings to elevate model's fine-grained perception. Specifically, SSVP introduces the Hierarchical Semantic-Visual Synergy (HSVS) mechanism, which deeply integrates DINOv3's multi-scale structural priors into the CLIP semantic space. Subsequently, the Vision-Conditioned Prompt Generator (VCPG) employs cross-modal attention to guide dynamic prompt generation, enabling linguistic queries to precisely anchor to specific anomaly patterns. Furthermore, to address the discrepancy between global scoring and local evidence, the Visual-Text Anomaly Mapper (VTAM) establishes a dual-gated calibration paradigm. Extensive evaluations on seven industrial benchmarks validate the robustness of our method; SSVP achieves state-of-the-art performance with 93.0% Image-AUROC and 92.2% Pixel-AUROC on MVTec-AD, significantly outperforming existing zero-shot approaches.",12.06,Qwen2.5-3B,Apple M1 (Metal)
2601.09152v1_PrivacyReasoner Can LLM Emulate a Human-like Priva.pdf,PrivacyReasoner: Can LLM Emulate a Human-like Privacy Mind?,"Yiwen Tu, Xuan Liu, Lianhui Qin, Haojian Jin",Not provided,Not provided,"Privacy, AI-agent, Privacy Reasoning, User-specific Privacy, Contextual Integrity, Cognitive Processes, Privacy Concerns","This paper introduces PrivacyReasoner, an AI-agent designed to simulate how individual users form privacy concerns in response to real-world news. Moving beyond population-level sentiment analysis, PrivacyReasoner integrates privacy and cognitive theories to model user-specific privacy reasoning grounded in personal comment histories and contextual cues. The agent reconstructs each user’s “privacy mind,” dynamically activates context-relevant privacy memory through a cognitively motivated contextual filter, and generates synthetic comments reflecting how that user would likely respond to new privacy scenarios. A complementary LLM-as-a-Judge evaluator, calibrated against an established privacy concern taxonomy, quantifies the faithfulness of the generated reasoning. Experiments on real-world Hacker News discussions show that PrivacyReasoner outperforms baseline agents in privacy concern prediction and captures transferable reasoning patterns across domains including AI, e-commerce, and healthcare.",11.88,Qwen2.5-3B,Apple M1 (Metal)
2601.09156v1_KTCF Actionable Recourse in Knowledge Tracing via .pdf,KTCF: Actionable Recourse in Knowledge Tracing via Counterfactual Explanations for Education,"Woojin Kim, Changkwon Lee, Hyeoncheol Kim",,,"Knowledge Tracing, Counterfactual Explanations, Education, AI Ethics, Explainable AI","Proposes KTCF, a counterfactual explanation generation method for Knowledge Tracing that accounts for knowledge concept relationships and converts counterfactual explanations into educational instructions, achieving superior and robust performance over existing methods.",9.81,Qwen2.5-3B,Apple M1 (Metal)
2601.09182v1_Position on LLM-Assisted Peer Review Addressing Re.pdf,Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback,"JungMin Yun*, JuneHyoung Kwon*, MiHyeon Kim*, YoungBin Kim",,,"peer review, AI research, reviewer gap, mentoring, feedback, sustainability","The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. It proposes two complementary systems: an LLM-assisted mentoring system to cultivate reviewers' long-term competencies and an LLM-assisted feedback system to help reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.",11.07,Qwen2.5-3B,Apple M1 (Metal)
2601.09195v1_ProFit Leveraging High-Value Signals in SFT via Pr.pdf,ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection,"Tao Liu, Taiqiang Wu, Runming Yang, Shaoning Sun, Junjie Wang, Yujiu Yang",Not found,Not found,"Supervised fine-tuning, Large Language Models, Token selection, Semantic importance, Overfitting mitigation","ProFit is a method that selectively masks low-probability tokens to prevent surface-level overfitting in supervised fine-tuning (SFT) of Large Language Models (LLMs). It reveals the intrinsic connection between token probability and semantic importance, proposing a strategy to prioritize the mitigation of single-reference overfitting over answer diversity, which is costly. Extensive experiments show that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks.",10.68,Qwen2.5-3B,Apple M1 (Metal)
2601.09208v2_Mikasa A Character-Driven Emotional AI Companion I.pdf,Mikasa: A Character-Driven Emotional AI Companion,Miki Ueno,,,"AI companions, character design, emotional AI, user engagement","Recent progress in AI companions has led to systems with fluent and emotionally expressive conversations. However, these systems often struggle with long-term user satisfaction and engagement. This paper argues that the problems are not primarily due to weak models but to poor character design and unclear user-AI relationship definitions. Mikasa, inspired by Japanese Oshi culture, is presented as a case study of character-driven companion design, focusing on long-term, non-exclusive commitment and a stable personality. The results suggest that character coherence and relationship definition are latent structural elements shaping user engagement.",11.26,Qwen2.5-3B,Apple M1 (Metal)
2601.09212v1_Annealed Relaxation of Speculative Decoding for Fa.pdf,Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image Generation,"Xingyao Li, Fengzhuo Zhang, Cunxiao Du",Not provided,Not provided,"Auto-regressive models, Speculative decoding, Image generation, Annealing, Resampling","This paper proposes COOL-SD, an annealed relaxation of speculative decoding, which aims to accelerate auto-regressive image generation. By analyzing the total variation distance and using perturbation analysis, COOL-SD achieves faster image generation with comparable quality or better quality at similar latency. Experiments validate the effectiveness of COOL-SD, showing consistent improvements over prior methods in speed-quality trade-offs.",10.49,Qwen2.5-3B,Apple M1 (Metal)
2601.09213v1_SpikeVAEDiff Neural Spike-based Natural Visual Sce.pdf,SpikeVAE AEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-V AE and Versatile Diffusion,"Jialu Li, Taiyan Zhou",Not found,Not found,"Neural Spikes, Image Reconstruction, Variational Autoencoder, Diffusion Models, Neuroscience, Computer Vision","This paper presents SpikeVAEDiff, a novel two-stage framework combining a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model for generating high-resolution and semantically meaningful image reconstructions from neural spike data. The method involves mapping neural spike signals to latent representations in the first stage and using regression models to map neural spike signals to CLIP-Vision and CLIP-Text features in the second stage, enabling Versatile Diffusion to refine the images. The study explores the use of spike data for visual reconstruction tasks on the Allen Visual Coding—Neuropixels dataset and tests different brain regions, highlighting both successful and unsuccessful outcomes.",11.73,Qwen2.5-3B,Apple M1 (Metal)
2601.09233v1_GIFT Unlocking Global Optimality in Post-Training .pdf,GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization,"Zhengyang Zhao, Lu Ma, Yizhen Jiang, Xiaochen Ma, Chengyu Shen, Lexiang Tang, Haoze Sun, Peng Pei, Wentao Zhang",Not found,Not found,"Large Reasoning Models, Supervised Fine-Tuning, Reinforcement Learning, Distributional Collapse, Finite-Temperature Gibbs Initialization","The paper proposes a new approach called GIFT to address the optimization mismatch between Supervised Fine-Tuning and Reinforcement Learning in the training of Large Reasoning Models. GIFT incorporates supervision as a finite-temperature energy potential, ensuring objective consistency throughout the post-training pipeline and significantly outperforming standard SFT and other baselines in RL initialization.",10.54,Qwen2.5-3B,Apple M1 (Metal)
2601.09236v2_Reward Learning through Ranking Mean Squared Error.pdf,REWARDLEARNING THROUGH RANKINGMEANSQUAREDERROR,"Chaitanya Kharyal, Calarina Muslimani, Matthew E. Taylor",,,"reinforcement learning, reward learning, ranking mean squared error, human feedback, robotic locomotion","Reward design remains a significant bottleneck in applying reinforcement learning to real-world problems. A popular alternative is reward learning, where reward functions are inferred from human feedback rather than manually specified. Recent work has proposed learning reward functions from human feedback in the form of ratings, rather than traditional binary preferences, enabling richer and potentially less cognitively demanding supervision. Building on this paradigm, we introduce a new rating-based RL method, Ranked Return Regression for RL (R4). At its core, R4 employs a novel ranking mean squared error (rMSE) loss, which treats teacher-provided ratings as ordinal targets. Our approach learns from a dataset of trajectory-rating pairs, where each trajectory is labeled with a discrete rating (e.g., “bad,” “neutral,” “good”). At each training step, we sample a set of trajectories, predict their returns, and rank them using a differentiable sorting operator (soft ranks). We then optimize a mean squared error loss between the resulting soft ranks and the teacher’s ratings. Unlike prior rating-based approaches, R4 offers formal guarantees: its solution set is provably minimal and complete under mild assumptions. Empirically, using simulated human feedback, we demonstrate that R4 consistently matches or outperforms existing rating and preference-based RL methods on robotic locomotion benchmarks from OpenAI Gym and the Deep-Mind Control Suite, while requiring significantly less feedback.",12.36,Qwen2.5-3B,Apple M1 (Metal)
2601.09239v2_DSA-Tokenizer Disentangled Semantic-Acoustic Token.pdf,DSA-Tokenizer: Disentangled Semantic-Acoustic Tokenization via Flow Matching-based Hierarchical Fusion,"Hanlin ZHANG, Daxin Tan, Dehua Tao, Xiao Chen, Haochen Tan, Yunhe Li, Yuchen Cao, Jianping Wang, Linqi Song",Not provided,Not provided,"Speech Tokenization, Disentanglement, Hierarchical Fusion, Flow Matching, Semantic-Acoustic Separation","DSA-Tokenizer is proposed to achieve better disentanglement in speech tokenization by explicitly separating speech into discrete semantic and acoustic tokens via distinct optimization constraints. It uses ASR for semantic tokens and mel-spectrograms restoration for acoustic tokens, and introduces a hierarchical Flow-Matching decoder to eliminate rigid length constraints between sequences. This enables high-fidelity reconstruction and flexible recombination, facilitating controllable generation in Speech Large Language Models (Speech LLMs).",10.82,Qwen2.5-3B,Apple M1 (Metal)
2601.09248v1_Hybrid guided variational autoencoder for visual p.pdf,Hybrid guided variational autoencoder for visual place recognition,"Ni Wang, Zihan You, Emre Neftci, Thorben Schoepe",Not found,2601.09248,"Visual place recognition, Spiking neural network","This work presents a hybrid guided variational autoencoder (VAE) for visual place recognition (VPR) that combines event-based vision sensors with a spiking neural network model. The model successfully disentangles visual features of 16 distinct places in an indoor VPR dataset, demonstrating robust performance under various illumination conditions and generalization capabilities for novel inputs.",11.55,Qwen2.5-3B,Apple M1 (Metal)
2601.09251v1_HGATSolver A Heterogeneous Graph Attention Solver .pdf,HGA TSolver: A Heterogeneous Graph Attention Solver for Fluid–Structure Interaction,"Qin-Yi Zhang, Hong Wang, Siyao Liu, Haichuan Lin, Linying Cao, Xiao-Hu Zhou, Chen Chen, Shuangyi Wang, Zeng-Guang Hou",,,"Fluid-Structure Interaction, Heterogeneous Graph Attention, Physics-Informed Neural Networks, Predictive Uncertainty, Gradient-Balancing Loss","This paper proposes HGA TSolver, a novel solver for Fluid-Structure Interaction (FSI) systems. It encodes the system as a heterogeneous graph, embedding physical structure directly into the model via distinct node and edge types for fluid, solid, and interface regions. This enables specialized message-passing mechanisms tailored to each physical domain. To stabilize explicit time stepping, a novel physics-conditioned gating mechanism is introduced. An Inter-domain Gradient-Balancing Loss dynamically balances the optimization objectives across domains based on predictive uncertainty. Extensive experiments demonstrate that HGA TSolver achieves state-of-the-art performance in surrogate modeling of coupled multi-physics systems.",11.06,Qwen2.5-3B,Apple M1 (Metal)
2601.09253v1_RIFT Repurposing Negative Samples via Reward-Infor.pdf,RIFT: Repurposing Negative Samples via Reward-Informed Fine-Tuning,"Zehua Liu, Shuqi Liu†, Tao Zhong, Mingxuan Yuan",Not found,Not found,"Large Language Models, Fine-Tuning, Reward Informed Fine-Tuning, Rejection Sampling Fine-Tuning, Data Efficiency, Catastrophic Forgetting","This paper proposes RIFT, a novel framework that utilizes all self-generated samples to address the inefficiency of Supervised Fine-Tuning and Rejection Sampling Fine-Tuning. Unlike RFT, which discards negative samples, RIFT repurposes them to learn from both positive and negative trajectories from the model outputs, using scalar rewards to reweight the loss. This approach aims to improve alignment using mixed-quality, self-generated data, demonstrating robustness and data efficiency compared to RFT.",10.81,Qwen2.5-3B,Apple M1 (Metal)
2601.09259v1_MAXS Meta-Adaptive Exploration with LLM Agents.pdf,MAXS: Meta-Adaptive Exploration with LLM Agents,"Jian Zhang, Zhiyuan Wang, Zhangqi Wang, Yu He, Haoran Luo, Li Yuan, Lingling Zhang, Rui Mao, Qika Lin, Jun Liu",,,"Large Language Model, Meta-adaptive Exploration, LLM Agents, Tool Execution, Reasoning Planning","Meta-adaptive exploration with LLM Agents (MAXS) addresses the issues of locally myopic generation and trajectory instability in agent inference. MAXS employs a lookahead strategy to extend reasoning paths and select stable, consistent, and high-value reasoning steps. It also introduces a trajectory convergence mechanism to control computational cost and balance resource efficiency and global effectiveness in multi-tool reasoning.",10.29,Qwen2.5-3B,Apple M1 (Metal)
2601.09260v1_Efficient Paths and Dense Rewards Probabilistic Fl.pdf,Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models,"Yan Liu, Feng Zhang, Zhanyu Ma, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He, Han Liu, Yangdong Deng",Not found,Not found,"Large Language Models, Chain-of-Thought, Probabilistic Flow, Reasoning Optimization, Inference Efficiency","This work proposes CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, enabling flow-guided decoding and flow-based reinforcement learning. Experiments on challenging benchmarks demonstrate superior balance between inference efficiency and reasoning performance.",10.58,Qwen2.5-3B,Apple M1 (Metal)
2601.09262v1_Magnifying change Rapid burn scar mapping with mul.pdf,"Magnifying change: Rapid burn scar mapping with multi-resolution, multi-source satellite imagery","Maria Sdraka, Dimitrios Michail, Ioannis Papoutsis",Not provided,Not provided,"Artificial intelligence, Machine Learning, Remote Sensing, burnt area mapping, disaster management, disaster monitoring, wildfires, burn scar mapping, change detection, downscaling, super-resolution","A novel deep learning model, BAM-MRCD, is proposed to detect small scale wildfires with high accuracy using multi-resolution, multi-source satellite imagery (MODIS and Sentinel-2) for timely production of detailed burnt area maps with high spatial and temporal resolution.",10.86,Qwen2.5-3B,Apple M1 (Metal)
2601.09264v1_Coordinated Pandemic Control with Large Language M.pdf,Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants,"Ziyi Shi, Xusen Guo, Hongliang Lu, Mingxing Peng, Haotian Wang, Zheng Zhu, Zhenning Li, Yuxuan Liang, Xinhu Zheng, Hai Yang",,,"Pandemic Control, Large Language Models, Multi-Agent System, Coordinated Policymaking","Effective pandemic control requires timely and coordinated policymaking across regions, which is often fragmented and reactive. This paper proposes a large language model (LLM) multi-agent framework to address this challenge, enabling coordinated and proactive pandemic control through AI-assisted agents that reason over region-specific epidemiological dynamics and communicate to account for cross-regional interdependencies.",11.37,Qwen2.5-3B,Apple M1 (Metal)
2601.09269v2_RISER Orchestrating Latent Reasoning Skills for Ad.pdf,RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering,"Wencheng Ye, Xiaoyang Yuan, Yi Bin, Hengyu Jin, Liang Peng, Pengpeng Zeng, Heng Tao Shen",Not found,Not found,"Large Language Models, Domain-specific reasoning, Activation steering, Reinforcement Learning, Latent reasoning, Adaptive activation","RISER proposes a plug-and-play intervention framework that adaptively steers large language model reasoning in activation space. It constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input, optimizing the Router via reinforcement learning under task-level rewards. Across seven diverse benchmarks, RISER yields 3.4–6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2–3× higher token efficiency and robust accuracy gains.",10.67,Qwen2.5-3B,Apple M1 (Metal)
2601.09274v1_A3-Bench Benchmarking Memory-Driven Scientific Rea.pdf,A3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation,"Jian Zhang, Yu He, Zhiyuan Wang, Zhangqi Wang, Kai He, Fangzhi Xu, Qika Lin, Jun Liu",10.1101/2601.09274v1,2601.09274,"memory-driven reasoning, anchor activation, attractor activation, scientific reasoning, benchmarking","This paper proposes A3-Bench, a benchmark to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. It introduces a dual-scale memory evaluation framework and a metric to measure memory activation rates, validating A3-Bench and analyzing how memory activation impacts reasoning performance.",10.89,Qwen2.5-3B,Apple M1 (Metal)
2601.09278v1_M3Searcher Modular Multimodal Information Seeking .pdf,M3Searcher: Modular Multimodal Information Seeking Agency with Retrieval-Oriented Reasoning,"Xiaohan Yu, Chao Feng, Lang Mei, Chong Chen",Not found,Not found,"DeepResearch, multimodal, information seeking, reinforcement learning, LLMs, information synthesis","Recent advances in DeepResearch-style agents have demonstrated strong capabilities in autonomous information acquisition and synthesis from real-world web environments. However, existing approaches remain limited to text modality. M3Searcher, a modular multimodal information-seeking agent, explicitly decouples information acquisition from answer derivation, optimizing with a retrieval-oriented multi-objective reward that encourages factual accuracy, reasoning soundness, and retrieval fidelity. It also supports multimodal multi-hop training with MM-SearchVQA.",10.83,Qwen2.5-3B,Apple M1 (Metal)
2601.09280v1_ReGraM Region-First Knowledge Graph Reasoning for .pdf,ReGraM: Region-First Knowledge Graph Reasoning for Medical Question Answering,"Chaerin Lee, Sohee Park, Hyunsik Na, Daseon Choi",,,"Medical QA, Knowledge Graph, Multi-hop Reasoning, Large Language Models, KG-based QA","Recent studies in medical question answering have actively explored the integration of large language models with biomedical knowledge graphs to improve factual accuracy. However, most existing approaches still rely on traversing the entire KG or performing large-scale retrieval, which introduces substantial noise and leads to unstable multi-hop reasoning. ReGraM addresses this challenge by constructing a query-aligned subgraph and performing stepwise reasoning constrained to this localized region under multiple evidence-aware modes.",10.11,Qwen2.5-3B,Apple M1 (Metal)
2601.09281v1_STaR Sensitive Trajectory Regulation for Unlearnin.pdf,STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models,"Jingjing Zhou1*, Gaoxiang Cong2, Li Su1†, Liang Li1,2†",Not found,Not found,"Large Reasoning Models, Unlearning, Privacy Protection, Sensitive Trajectory Regulation, Multi-step Reasoning, Chain-of-Thought","Large Reasoning Models have advanced automated multi-step reasoning, but their ability to generate complex Chain-of-Thought trajectories introduces severe privacy risks. Existing unlearning approaches for Large Language Models are insufficient for Large Reasoning Models, as they fail to remove sensitive content from intermediate steps. Sensitive Trajectory Regulation (STaR) is a parameter-free, inference-time unlearning framework that achieves robust privacy protection throughout the reasoning process by identifying sensitive content via semantic-aware detection, injecting global safety constraints through secure prompt prefix, performing trajectory-aware suppression, and applying token-level adaptive filtering.",11.03,Qwen2.5-3B,Apple M1 (Metal)
2601.09282v1_Cluster Workload Allocation Semantic Soft Affinity.pdf,Cluster workload allocation: semantic soft affinity using natural language processing,"Leszek Sliwko1, Jolanta Mizeria-Pietraszko2",,,"Cluster workload allocation, Natural Language Processing, Semantic affinity, Soft affinity, Kubernetes scheduler, Large Language Model, Load balancing","This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy for top-tier models, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences.",11.34,Qwen2.5-3B,Apple M1 (Metal)
2601.09286v1_Why not Collaborative Filtering in Dual View Bridg.pdf,Why not Collaborative Filtering in Dual View? Bridging Sparse and Dense Models,"HANZE GUO, JIANXUN LIAN, XIAO ZHOU",XXXXXXX.XXXXXXX,,"Collaborative Filtering, Dual View Alignment, Sparse and Dense model","This paper proposes SaD (Sparse and Dense), a unified framework that integrates the semantic expressiveness of dense embeddings with the structural reliability of sparse interaction patterns, to overcome the signal-to-noise ratio ceiling for modeling unpopular items in collaborative filtering.",9.92,Qwen2.5-3B,Apple M1 (Metal)
2601.09292v1_Blue Teaming Function-Calling Agents.pdf,Blue Teaming Function-Calling Agents,"Greta Dolcetti1†, Giulio Zizzo 2, Sergio Maffeis 3",Not found,Not found,"Blue Teaming, Function-calling, LLM, Robustness, Adversarial Attacks, Defenses","An experimental evaluation of four open source LLMs with function-calling capabilities against three different attacks, and measurement of the effectiveness of eight different defences. Results show these models are not safe by default, and defenses are not yet employable in real-world scenarios.",10.09,Qwen2.5-3B,Apple M1 (Metal)
2601.09293v1_Policy-Based Reinforcement Learning with Action Ma.pdf,Policy-Based Reinforcement Learning with Action Masking for Dynamic Job Shop Scheduling under Uncertainty,"Sofiene Lassoued, Stefan Lier",,,"Dynamic Job Shop Scheduling, Fault tolerance, Reinforcement learning, actions masking, Petri nets","We present a novel framework for solving Dynamic Job Shop Scheduling Problems under uncertainty, addressing the challenges introduced by stochastic job arrivals and unexpected machine breakdowns. Our approach uses Coloured Timed Petri Nets to represent the scheduling environment and Maskable Proximal Policy Optimization to enable dynamic decision-making while restricting the agent to feasible actions at each decision point. The framework simulates realistic industrial conditions using stochastic models for dynamic job arrivals and machine failures. We study two action-masking strategies and demonstrate that our method consistently outperforms traditional heuristic and rule-based approaches in terms of makespan minimization.",11.35,Qwen2.5-3B,Apple M1 (Metal)
2601.09306v1_On-Device Large Language Models for Sequential Rec.pdf,On-Device Large Language Models for Sequential Recommendation,"Xin Xia, Hongzhi Yin, Shane Culpepper",10.1145/3773966.3777961,,"Recommender Systems, Sequential Recommendation, On-Device Recommendation, Model Compression, Resource Constrained Devices","This paper proposes OD-LLM, a task-adaptive compression framework designed for efficient and accurate on-device deployment of large language models for sequential recommendation tasks. It integrates low-rank structural compression and a novel tokenization normalization technique, and uses a progressive alignment algorithm to refine parameters. Empirical evaluations show that OD-LLM maintains effectiveness when the model size is halved.",10.28,Qwen2.5-3B,Apple M1 (Metal)
2601.09313v1_Understanding or Memorizing A Case Study of German.pdf,Understanding or Memorizing? A Case Study of German Definite Articles in Language Models,"Jonathan Drechsel, Erisa Bytyqi, Steffen Herbold",,,"Language models, German definite articles, Grammatical agreement, Gradient-based interpretability, Memorization, Rule-based generalization","This study investigates whether language models encode abstract grammatical rules or rely on surface-level memorization of frequent token-context associations for German definite singular articles. By applying a gradient-based interpretability method, the researchers learn parameter update directions for gender-case specific article transitions and find that these updates frequently affect unrelated gender-case settings, indicating that models at least partly rely on memorized associations rather than abstract grammatical rules.",10.58,Qwen2.5-3B,Apple M1 (Metal)
2601.09342v1_Improving Implicit Hate Speech Detection via a Com.pdf,Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework,"Ewelina Gajewska, Katarzyna Budzynska, Jarosław A. Chudziak",,,"LLMs, Community agents, Hate speech, Social media, Moderation, Fairness","This work proposes a contextualised detection framework for implicitly hateful speech, implemented as a multi-agent system comprising a central Moderator Agent and dynamically constructed Community Agents representing specific demographic groups. Our approach explicitly integrates socio-cultural context from publicly available knowledge sources, enabling identity-aware moderation that surpasses state-of-the-art prompting methods and alternative approaches on a challenging ToxiGen dataset. We enhance the technical rigour of performance evaluation by incorporating balanced accuracy as a central metric of classification fairness.",10.71,Qwen2.5-3B,Apple M1 (Metal)
2601.09351v1_Navigating Ethical AI Challenges in the Industrial.pdf,Navigating Ethical AI Challenges in the Industrial Sector: Balancing Innovation and Responsibility,"Ruomu Tan, Martin W Hoffmann",,2601.09351,"AI, industrial sector, ethics, innovation, responsibility","This chapter explores ethical challenges and practices in AI applications within the industrial sector, emphasizing the need for transparency, accountability, and fairness in AI development and deployment.",11.1,Qwen2.5-3B,Apple M1 (Metal)
2601.09353v1_Monte-Carlo Tree Search with Neural Network Guidan.pdf,Monte-Carlo Tree Search with Neural Network Guidance for Lane-Free Autonomous Driving,"Ioannis Peridis, Dimitrios Troullinos, Georgios Chalkiadakis, Pantelis Giankoulidis, Ioannis Papamichail, Markos Papageorgiou",Not found,2601.09353,"Monte-Carlo Tree Search, Neural Network, Autonomous Driving, Lane-Free Traffic, Reinforcement Learning, Markov Decision Process, Nudging","This work considers a Monte-Carlo Tree Search (MCTS) planning approach for single-agent autonomous driving in lane-free traffic, incorporating a pre-trained neural network (NN) to guide the selection phase. The study evaluates metrics addressing safety and efficacy, including the influence of isotropic state information and the performance of the NN-guided variant of MCTS.",11.66,Qwen2.5-3B,Apple M1 (Metal)
2601.09361v1_GeoRA Geometry-Aware Low-Rank Adaptation for RLVR.pdf,GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR,"Jiaying Zhang1,2†*, Lei Shi1*, Jiguo Li1, Jun Xu1‡, Jiuchong Gao 1‡, Jinghua Hao 1, Renqing He 1",Not found,Not found,"Reinforcement Learning, Verifiable Rewards, Parameter-efficient, Low-rank Adaptation, Geometry-Aware, Optimization Dynamics, Geometric Structures","Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for large-scale reasoning models. However, existing parameter-efficient methods like PiSSA and MiLoRA are designed for Supervised Fine-Tuning (SFT) and do not account for RLVR's distinct optimization dynamics and geometric structures. GeoRA addresses these challenges by exploiting the anisotropic and compressible nature of RL update subspaces, preserving the pre-trained geometric structure and enabling efficient GPU computation through dense operators. Experiments demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment and consistently outperforms low-rank baselines on key mathematical benchmarks, achieving state-of-the-art results. It also shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.",11.46,Qwen2.5-3B,Apple M1 (Metal)
2601.09365v1_Frame of Reference Addressing the Challenges of Co.pdf,Frame of Reference: Addressing the Challenges of Common Ground,"Biswesh Mohapatra*, Théo Charlot†‡, Giovanni Duca†‡, Mayank Palan†‡, Laurent Romary*, Justine Cassell*",,,"Common Ground, Situational Dialogs, Embodied Conversational Agents, Social Robots, Large Language Models","This work evaluates a model's ability to establish common ground in dynamic and shared environments of situated dialogs, leveraging relational references. It also tests multiple methods for representing common ground and proposes approaches to improve their performance.",10.05,Qwen2.5-3B,Apple M1 (Metal)
2601.09381v1_Query Languages for Machine-Learning Models.pdf,Query Languages for Machine-Learning Models,"Martin Grohe, envel⌢pe",Not found,2601.09381,"Expressive power of query languages, fixed-point logics, weighted structures, neural networks, explainable AI","In this paper, the author discusses two logics for weighted finite structures: first-order logic with summation (FO(SUM)) and its recursive extension IFP(SUM). These logics are investigated as query languages for machine learning models, specifically neural networks, which are naturally represented as weighted graphs. The author presents illustrative examples of queries to neural networks that can be expressed in these logics and discusses fundamental results on their expressiveness and computational complexity.",10.77,Qwen2.5-3B,Apple M1 (Metal)
2601.09382v1_Long-term Task-oriented Agent Proactive Long-term .pdf,Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments,"Qinglong Shi, Donghai Wang, Hantao Zhou, Jiguo Li, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He",Not found,Not found,"long-term task-oriented interaction, proactive agents, dynamic environments, user intent maintenance","This paper proposes a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. It formalizes proactivity through Intent-Conditioned Monitoring and Event-Triggered Follow-up, and introduces a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. The authors evaluate leading models and propose a new benchmark, ChronosBench, to address the lack of evaluation criteria for task-oriented interaction in dynamic environments. The fine-tuned model trained on synthetic data achieves a task completion rate of 85.19% for complex tasks, outperforming other models under test.",11.14,Qwen2.5-3B,Apple M1 (Metal)
2601.09394v2_FairGE Fairness-Aware Graph Encoding in Incomplete.pdf,FairGE: Fairness-Aware Graph Encoding in Incomplete Social Networks,"Renqiang Luo, Huafei Huang, Tao Tang, Jing Ren, Ziqi Xu, Mingliang Hou, Enyan Dai, Feng Xia",XXXXXXX.XXXXXXX,,"Social Networks, Graph Learning, Graph Transformers, Fairness, Incomplete Data","Graph Transformers are applied to social network analysis, but their deployment is often constrained by fairness concerns, especially in incomplete social networks where sensitive attributes are frequently missing. FairGE, a fairness-aware framework, encodes fairness directly through spectral graph theory, ensuring fairness without data reconstruction and achieving at least a 16% improvement in statistical parity and equality of opportunity compared to state-of-the-art baselines.",10.45,Qwen2.5-3B,Apple M1 (Metal)
2601.09398v1_Ability Transfer and Recovery via Modularized Para.pdf,Ability Transfer and Recovery via Modularized Parameters Localization,"Songyao Jin, Kun Zhou*, Wenqi Li, Peng Wang, Biwei Huang",,,"Large Language Models, Continual Learning, Fine-tuning, Ability Transfer, Catastrophic Forgetting",This work investigates how abilities are distributed within large language models (LLMs) and proposes ACT (Activation-Guided Channel-wise Ability Transfer) to localize ability-relevant channels and selectively transfer parameters for recovery of forgotten abilities while preserving retained skills.,9.62,Qwen2.5-3B,Apple M1 (Metal)
2601.09413v1_Speech-Hands A Self-Reflection Voice Agentic Appro.pdf,Speech-Hands: A Self-Reflection Voice Agentic Approach to Speech Recognition and Audio Reasoning with Omni Perception,"Zhen Wan, Chao-Han Huck Yang, Jinchuan Tian, Hanrong Ye, Ankita Pasad, Szu-wei Fu, Arushi Goel, Ryo Hachiuma, Shizhe Diao, Kunal Dhawan, Sreyan Ghosh, Yusuke Hirota, Zhehuai Chen, Rafael Valle, Ehsan Hosseini Asl, Chenhui Chu, Shinji Watanabe, Yu-Chiang Frank Wang, Boris Ginsburg",Not found,Not found,"speech recognition, audio reasoning, self-reflection, voice agentic, omni perception","This paper introduces a voice-agentic framework called Speech-Hands that learns to reflect on its decisions, preventing it from being misled by noisy hypotheses. The framework is effective in preventing the model from being derailed by flawed external candidates, and it generalizes well from speech recognition to complex, multiple-choice audio reasoning tasks. The model achieves high accuracy and robust generalization across diverse audio question answering datasets.",11.42,Qwen2.5-3B,Apple M1 (Metal)
2601.09416v1_Radiomics-Integrated Deep Learning with Hierarchic.pdf,RADIOMICS-INTEGRA TED DEEP LEARNING WITH HIERARCHICAL LOSS FOR OSTEOSARCOMA HISTOLOGY CLASSIFICA TION,"Yaxi Chen, Zi Ye, Shaheer U. Saeed, Oliver Yu, Simin Ni, Jie Huang, Yipeng Hu",Not found,Not found,"Osteosarcoma, Radiomics, Deep Learning, Hierarchical Loss, Histopathology, Tumor Classification","This work proposes the use of radiomic features as additional input in model training and introduces a hierarchical loss for two binary classification tasks (tumor-vs-non-tumor and viable-vs-non-viable) to improve classification performance and interpretability. The approach is demonstrated on the TCIA OS Tumor Assessment dataset, achieving new state-of-the-art performance.",11.08,Qwen2.5-3B,Apple M1 (Metal)
2601.09421v2_Bias Dynamics in BabyLMs Towards a Compute-Efficie.pdf,Bias Dynamics in BabyLMs: Towards a Compute-Efficient Sandbox for Democratising Pre-Training Debiasing,"Filip Trhlik, Andrew Caines, Paula Buttery",Not found,Not found,"Bias, Pre-training, Debiasing, BabyLM, BERT, Language Models, Sandbox","This work investigates whether significantly less costly models could replace standard LMs in debiasing research, focusing on models from the BabyLM Challenge. Specifically, it investigates BabyLMs, compact BERT-like models trained on small and mutable corpora that can approximate bias acquisition and learning dynamics of larger models. The study shows that BabyLMs display closely aligned patterns of intrinsic bias formation and performance development compared to standard BERT models, despite their drastically reduced size. The results demonstrate that BabyLMs can serve as an effective sandbox for large-scale LMs, reducing pre-training costs from over 500 GPU-hours to under 30 GPU-hours.",11.08,Qwen2.5-3B,Apple M1 (Metal)
2601.09433v1_Do Transformers Understand Ancient Roman Coin Moti.pdf,Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?,"David Reid, Ognjen Arandjelović",Not found,2601.09433,"Vision Transformer, Convolutional Neural Networks, Ancient Coins, Automatic Learning, Multi-modal Data","This paper evaluates the performance of Vision Transformer (ViT) models in identifying semantic elements on ancient coins compared to Convolutional Neural Networks (CNNs). It discusses previous research, training, and implementation of ViT and CNN models for ancient coin analysis and provides an evaluation of their performance.",12.24,Qwen2.5-3B,Apple M1 (Metal)
2601.09445v1_Where Knowledge Collides A Mechanistic Study of In.pdf,Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models,"Minh Vu Pham *, Hsuvas Borkakoty *, Yufang Hou",Not found,2601.09445,"language models, knowledge conflict, intra-memory, pre-training, mechanistic interpretability","This work designs a framework based on mechanistic interpretability methods to identify where and how conflicting knowledge from pre-training data is encoded within language models, contributing to the understanding of specific internal components responsible for such conflicts and demonstrating how mechanistic interpretability methods can be leveraged to causally intervene in and control conflicting knowledge at inference time.",10.49,Qwen2.5-3B,Apple M1 (Metal)
2601.09446v1_Improving Symbolic Translation of Language Models .pdf,Improving Symbolic Translation of Language Models for Logical Reasoning,"Ramya Keerthy Thatikonda1, Jiuzhou Han1, Wray Buntine2, Ehsan Shareghi1",Not found,Not found,"Language models, Logical reasoning, Symbolic translation, Formal language, Natural language processing","The paper discusses the challenges faced by smaller language models in translating natural language into first-order logic for deductive logical reasoning tasks. It introduces incremental inference and a verification module to improve the accuracy and reliability of these translations, thereby enhancing the performance of smaller LMs in logical reasoning tasks.",10.32,Qwen2.5-3B,Apple M1 (Metal)
2601.09448v1_Population-Aligned Audio Reproduction With LLM-Bas.pdf,Population-Aligned Audio Reproduction With LLM-Based Equalizers,"Ioannis Stylianou, Jon Francombe, Pablo Martínez-Nuevo, Sven Ewan Shepstone, Zheng-Hua Tan",Not provided,Not provided,"Large Language Model (LLM), Equalization, Audio Reproduction, Listening Experiments, Recommender Systems","This paper introduces a Large Language Model (LLM)-based alternative to conventional audio equalization, which maps natural language text prompts to equalization settings. This enables a conversational approach to sound system control and aligns with population-preferred equalization settings through in-context learning and parameter-efficient fine-tuning techniques. The evaluation methods show statistically significant improvements in distributional alignment over random sampling and static preset baselines, indicating potential for LLMs as 'artificial equalizers.'",11.15,Qwen2.5-3B,Apple M1 (Metal)
2601.09451v1_Late Breaking Results Quamba-SE Soft-edge Quantize.pdf,Late Breaking Results: Quamba-SE: Soft-edge Quantizer for Activations in State Space Models,"Yizhi Chen, Ahmed Hemani",Not provided,Not provided,"Quantization, State Space Models, Quamba, Soft-edge quantization, Outliers, Post-Training Quantization","We propose Quamba-SE, a soft-edge quantizer for State Space Model (SSM) activation quantization. Unlike existing methods, Quamba-SE employs three adaptive scales: high-precision for small values, standard scale for normal values, and low-precision for outliers. This preserves outlier information instead of hard clipping, while maintaining precision for other values. We evaluate on Mamba-130M across 6 zero-shot benchmarks. Results show that Quamba-SE consistently outperforms Quamba, achieving up to +2.68% on individual benchmarks and up to +0.83% improvement in the average accuracy of 6 datasets.",10.78,Qwen2.5-3B,Apple M1 (Metal)
2601.09455v1_On the Hardness of Computing Counterfactual and Se.pdf,On the Hardness of Computing Counterfactual and Semi-factual Explanations in XAI,"André Artelt, Martin Olsen, Kevin Tierney",Not found,2601.09455,"explainable artificial intelligence, counterfactual explanations, semi-factual explanations, computational complexity","This work provides an overview of the computational complexity results in the literature for generating counterfactual and semi-factual explanations in XAI, finding that in many cases, generating explanations is computationally hard. The authors also contribute inapproximability results showing that explanations are not only hard to generate but also hard to approximate under certain assumptions.",11.01,Qwen2.5-3B,Apple M1 (Metal)
2601.09460v1_SoK Enhancing Cryptographic Collaborative Learning.pdf,SoK: Enhancing Cryptographic Collaborative Learning with Differential Privacy,"Francesco Capano, Jonas Böhler, Benjamin Weggenmann",Not provided,Not provided,"Differential Privacy, Cryptographic Collaborative Learning, Machine Learning, Privacy-Preserving Computing","This work systematizes the current landscape of cryptographic collaborative learning (CPCL) with differential privacy (DP), identifying secure noise sampling as a foundational phase. It analyzes trade-offs of different secure noise sampling techniques, noise types, and DP mechanisms, and evaluates their accuracy and cryptographic overhead across CPCL paradigms.",10.46,Qwen2.5-3B,Apple M1 (Metal)
2601.09465v1_EvoFSM Controllable Self-Evolution for Deep Resear.pdf,EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines,"Shuo Zhang, Chaofa Yuan, Ryan Guo, Xiaomin Yu, Rui Xu, Zhangquan Chen, Zinuo Li, Zhi Yang, Shuhao Guan, Zhenheng Tang, Sen Hu, Liwen Zhang, Ronghao Chen, Huacan Wang",Not found,2601.09465,"self-evolution, finite state machines, deep research, unconstrained optimization, structured self-evolution","EvoFSM is a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. It decouples the optimization space into macroscopic Flow(state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations and incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five multi-hop QA benchmarks demonstrate the effectiveness of EvoFSM.",12.16,Qwen2.5-3B,Apple M1 (Metal)
2601.09467v1_Searth Transformer A Transformer Architecture Inco.pdf,Searth Transformer: A Transformer Architecture Incorporating Earth's Geospheric Physical Priors for Global Mid-Range Weather Forecasting,"Tianye Li, Qi Liu, Hao Li, Lei Chen, Wencong Cheng, Fei Zheng, Xiangao Xia, Ya Wang, Gang Huang, Weiwei Wang, Xuan Tong, Ziqing Zu, Yi Fang, Shenming Fu, Jiang Jiang, Haochen Li, Mingxing Li, Jiangjiang Xia",Not provided,Not provided,"Transformer, Earth's geospheric physical priors, Global mid-range weather forecasting, Zonal periodicity, Meridional boundaries, Relay Autoregressive (RAR) fine-tuning","This paper proposes the Shifted Earth Transformer (Searth Transformer), a physics-informed transformer architecture designed for global medium-range weather forecasting. It integrates zonal periodicity and meridional boundaries into window-based self-attention, enabling physically consistent global information exchange and mitigating computational bottlenecks through a memory-efficient strategy.",11.67,Qwen2.5-3B,Apple M1 (Metal)
2601.09469v2_FairGU Fairness-aware Graph Unlearning in Social N.pdf,FairGU: Fairness-aware Graph Unlearning in Social Networks,"Renqiang Luo, Yongshuai Yang, Huafei Huang, Qing Qing, Mingliang Hou, Ziqi Xu, Yi Yu, Jingjing Zhou, Feng Xia",XXXXXXX.XXXXXXX,,"fairness, privacy, graph unlearning, social network",Graph unlearning is a critical mechanism for supporting sustainable and privacy-preserving social networks. Existing techniques often lead to degraded algorithmic fairness compared to traditional graph learning methods. FairGU is a fairness-aware graph unlearning framework designed to preserve both utility and fairness during the unlearning process.,10.28,Qwen2.5-3B,Apple M1 (Metal)
2601.09470v1_Personalized Multimodal Feedback Using Multiple Ex.pdf,Personalized Multimodal Feedback Using Multiple External Representations: Strategy Profiles and Learning in High School Physics,"Natalia Revenga-Lozano1*, Karina E. Avila1, Steffen Steinert1, Matthias Schweinberger1, Clara E. Gómez-Pérez1, Jochen Kuhn1, Stefan Küchemann1*",Not provided,2601.09470,"Physics education, Multimodal feedback, Personalized feedback, High school physics, Learning strategies","This study investigates the effectiveness of personalized feedback using multiple external representations in high school physics, utilizing a computer-based platform that provides feedback in verbal, graphical, and mathematical forms. It finds that elaborated multirepresentational feedback has a small but consistent positive association with post-test scores, independent of prior knowledge and confidence. Learners with lower representational competence benefit more from using a diverse set of representations, but this advantage diminishes as competence increases.",13.24,Qwen2.5-3B,Apple M1 (Metal)
2601.09473v1_SimMerge Learning to Select Merge Operators from S.pdf,SimMerge: Learning to Select Merge Operators from Similarity Signals,"Oliver Bolton, Aakanksha, Arash Ahmadian, Sara Hooker, Marzieh Fadaee, Beyza Ermis",Not found,2601.09473,"model merging, language models, merge operators, similarity signals, predictive methods","SimMerge is a predictive merge-selection method that selects the best merge using inexpensive, task-agnostic similarity signals between models, eliminating the expensive merge-and-evaluate loop.",10.8,Qwen2.5-3B,Apple M1 (Metal)
2601.09478v3_Bridging Semantic Understanding and Popularity Bia.pdf,Bridging Semantic Understanding and Popularity Bias with LLMs,"Renqiang Luo, Dong Zhang, Yupeng Gao, Wen Shi, Mingliang Hou, Jiaying Liu, Zhe Wang, Shuo Yu*",XXXXXXX.XXXXXXX,,"Semantic analysis, Recommender systems, Algorithmic fairness, Popularity bias, LLM","This paper proposes FairLRM, a novel framework that bridges the gap in the semantic understanding of popularity bias with Recommendation via Large Language Model (RecLLM). It decomposes popularity bias into item-side and user-side components and enhances the model's comprehension of global item distributions and individual user preferences using structured instruction-based prompts. The framework significantly improves fairness and recommendation accuracy through empirical evaluation.",10.52,Qwen2.5-3B,Apple M1 (Metal)
2601.09503v1_What Do LLM Agents Know About Their World Task2Qui.pdf,What Do LLM Agents Know About Their World?,"Siyuan Liu, Hongbang Yuan, Xinze Li, Ziyue Zhu, Yixin Cao, Yu-Gang Jiang",Not found,2601.09503,"large language models, generalization, environment understanding, task-to-quiz, reproducible evaluation","This paper investigates agent's environment understanding beyond current task success evaluation, proposing Task-to-Quiz (T2Q) as a deterministic and automated evaluation paradigm to decouple task execution from world-state understanding. Experiments reveal that task success is often a poor proxy for environment understanding, and current memory mechanisms cannot effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.",11.85,Qwen2.5-3B,Apple M1 (Metal)
2601.09518v1_Learning Whole-Body Human-Humanoid Interaction fro.pdf,Learning Whole-Body Human-Humanoid Interaction from Human-Human Demonstrations,"Wei-Jin Huang, Yue-Yi Zhang, Yi-Lin Wei, Zhi-Wei Xia, Juantao Tan, Yuan-Ming Li, Zhilin Zhao, Wei-Shi Zheng",Not found,2601.09518,"Human-Humanoid Interaction, Retargeting, Physics-Aware Interaction, Hierarchical Policy, Interactive Learning","This paper presents a method to enable humanoid robots to physically interact with humans by leveraging abundant Human-Human Interaction data. It introduces PAIR (Physics-Aware Interaction Retargeting), a contact-centric, two-stage pipeline that preserves contact semantics across morphology differences to generate physically consistent Human-Humanoid Interaction data. The paper also introduces D-STAR (Decoupled Spatio-Temporal Action Reasoner), a hierarchical policy that disentangles when to act from where to act, addressing the limitations of conventional imitation learning policies.",12.41,Qwen2.5-3B,Apple M1 (Metal)
2601.09520v1_Towards Realistic Synthetic Data for Automatic Dru.pdf,TOW ARDS REALISTIC SYNTHETIC DA TA FOR AUTOMA TIC DRUM TRANSCRIPTION,"Pierfrancesco Melucci, Paolo Merialdo, Taketo Akama",,,"Automatic Drum Transcription, Deep Learning, Synthetic Data, SoundFont, MIDI","This paper introduces a new paradigm for Automatic Drum Transcription (ADT) that circumvents the need for paired audio-MIDI training data. It presents a semi-supervised method to automatically curate a large and diverse corpus of one-shot drum samples from unlabeled audio sources, synthesizing a high-quality dataset from MIDI files alone for training a sequence-to-sequence transcription model. The model achieves new state-of-the-art results on the ENST and MDB test sets, significantly outperforming both fully supervised methods and previous synthetic-data approaches.",11.06,Qwen2.5-3B,Apple M1 (Metal)
2601.09527v1_Private LLM Inference on Consumer Blackwell GPUs A.pdf,Private LLM Inference on Consumer Blackwell GPUs: A Practical Guide for Cost-Effective Local Deployment in SMEs,Jonathan Knoop1 and Hendrik Holtmann2,,,"Large Language Models, SMEs, Data Privacy, Inference, Consumer GPUs, NVIDIA Blackwell, RTX 5060 Ti, RTX 5070 Ti, RTX 5090, Quantization, Inference Cost, Latency","This paper presents a systematic evaluation of NVIDIA's Blackwell consumer GPUs (RTX 5060 Ti, 5070 Ti, 5090) for production Large Language Model (LLM) inference, benchmarking four open-weight models across various configurations. The results show that consumer GPUs can reliably replace cloud inference for most SME workloads, except for latency-critical long-context RAG tasks, where high-end GPUs remain essential. The paper provides deployment guidance and releases all benchmark data for reproducible SME-scale deployments.",11.14,Qwen2.5-3B,Apple M1 (Metal)
2601.09536v1_Omni-R1 Towards the Unified Generative Paradigm fo.pdf,Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning,"Dongjie Cheng, Yongqi Li, Zhixin Ma, Hongru Cai, Yupeng Hu, Wenjie Wang, Liqiang Nie, Wenjie Li",,2601.09536,"Multimodal reasoning, Generative models, Two-stage framework, Perception alignment, Perception reward, Visual information","This paper proposes unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. It introduces Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, enabling functional image generation. Additionally, Omni-R1-Zero is introduced, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average.",11.75,Qwen2.5-3B,Apple M1 (Metal)
2601.09555v1_Benchmarking Post-Training Quantization of Large L.pdf,Benchmarking Post-Training Quantization of Large Language Models under Microscaling Floating Point Formats,"Manyi Zhang, Ji-Fu Li, Zhongao Sun, Haoli Bai, Hui-Ling Zhen, Zhenhua Dong, Xianzhi Yu",Not found,Not found,"Post-Training Quantization, Microscaling Floating-Point, Large Language Models, Quantization, Low-Precision Formats","This work conducts a systematic investigation of post-training quantization under Microscaling Floating-Point (MXFP) formats, encompassing over 7 PTQ algorithms, 15 evaluation benchmarks, and 3 LLM families. Key findings include the consistent performance of MXFP8, the substantial accuracy degradation of MXFP4, and the strong dependence of PTQ effectiveness on format compatibility.",10.55,Qwen2.5-3B,Apple M1 (Metal)
2601.09566v2_Hot-Start from Pixels Low-Resolution Visual Tokens.pdf,Hot-Start from Pixels: Low-Resolution Visual Tokens for Chinese Language Modeling,"Shuyang Xiang, Hao Guan",Not found,2601.09566,"Chinese language modeling, visual tokens, low-resolution inputs, hot-start effect","This paper investigates whether low-resolution visual inputs can serve as an alternative for character-level modeling in Chinese language. It compares the performance of these inputs to index-based models and finds that they achieve comparable accuracy, demonstrating the potential of minimal visual structure for Chinese language modeling.",10.95,Qwen2.5-3B,Apple M1 (Metal)
2601.09600v1_Information Access of the Oppressed A Problem-Posi.pdf,Information Access of the Oppressed: A Problem-Posing Framework for Envisioning Emancipatory Information Access Platforms,"BHASKAR MITRA, NICOLA NEOPHYTOU, SIREESH GURURAJA",XXXXXXX.XXXXXXX,,"Emancipatory Information Access, Search and Society, Sociotechnical Information Systems","This paper explores the challenges of authoritarian capture of information access platforms, particularly in the context of democratic erosion, AI technologies, and Big Tech's concentration of power. It proposes a problem-posing framework to develop alternative IA infrastructure, inspired by Paulo Freire's theories of emancipatory pedagogy.",10.44,Qwen2.5-3B,Apple M1 (Metal)
2601.09603v1_Linear Complexity Self-Supervised Learning for Mus.pdf,Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer,"Petros Vavaroutsos, Theodoros Palamas, Pantelis Vikatos",10.1145/3748522.3779786,,"Deep Learning, Learnable Representations, Music Understanding, Transformers, Embeddings, Attention","This paper focuses on reducing the size of a foundation model when applied to music information retrieval tasks. It combines the Branchformer architecture with SummaryMixing, a process of random quantization, and pre-trains on publicly available datasets. The results show competitive performance compared to state-of-the-art models using multi-head self-attention, while reducing model size from 8.5% to 12.3%.",10.56,Qwen2.5-3B,Apple M1 (Metal)
2601.09605v1_Sim2real Image Translation Enables Viewpoint-Robus.pdf,Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets,"Jeremiah Coholich, Justin Wit, Robert Azarcon, Zsolt Kira",,,"robot manipulation, vision-based policies, sim2real translation, image translation, viewpoint robustness, fixed-camera datasets","Vision-based imitation-learning policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. This paper proposes MANGO, an unpaired image translation method that maintains viewpoint consistency during sim2real translation, enabling diverse unseen viewpoints generation from fixed-camera datasets. The method outperforms other image translation methods and improves the robustness of imitation-learning policies trained on augmented data.",10.87,Qwen2.5-3B,Apple M1 (Metal)
2601.09609v1_DPWriter Reinforcement Learning with Diverse Plann.pdf,DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing,"Qian Cao, Yahui Liu, Wei Bi, Yi Zhao, Ruihua Song, Xiting Wang, Ruiming Tang, Guorui Zhou, Han Li",,,"Reinforcement Learning, Creative Writing, Diverse Exploration, Long Chain-of-Thought, Group-Aware Diversity Reward","This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. It introduces a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.",10.63,Qwen2.5-3B,Apple M1 (Metal)
2601.09613v1_CogRail Benchmarking VLMs in Cognitive Intrusion P.pdf,CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems,"Yonglin Tian, Qiyao Zhang, Wei Xu, Yutong Wang, Yihao Wu, Xinyi Li, Xingyuan Dai, Hui Zhang, Zhiyong Cui, Baoqing Guo, Zujun Yu, Yisheng Lv",Not found,Not found,"Cognitive intrusion perception, Intelligent railway transportation, Visual-language models, Benchmarking","Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. CogRail introduces a novel benchmark that integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction, facilitating the evaluation of state-of-the-art visual-language models (VLMs) and proposing a joint fine-tuning framework for better performance.",10.78,Qwen2.5-3B,Apple M1 (Metal)
2601.09620v1_Full Disclosure Less Trust How the Level of Detail.pdf,"Full Disclosure, Less Trust? How the Level of Detail about AI Use in News Writing Affects Readers’ Trust","POOJA PRAJOD, HANNES COOLS, THOMAS RÖGGLA, KARTHIKEYA PUTTUR VENKATRAJ, AMBER KUSTERS, ALIA ELKATTAN, PABLO CESAR, ABDALLAH EL ALI",,,"AI, transparency, trust, journalism, news production","This study investigates how three levels of AI disclosures (none, one-line, detailed) across two types of news (politics and lifestyle) and two levels of AI involvement (low and high) affect news readers' trust. It measures trust using the News Media Trust questionnaire and two decision behaviors: source-checking and subscription decisions. The findings show that detailed AI disclosures lead to a decline in trust, while one-line and detailed disclosures increase source-checking behavior, with the latter being more pronounced. Participants prefer detailed disclosures, but many prefer demand-based disclosure formats.",11.57,Qwen2.5-3B,Apple M1 (Metal)
2601.09624v1_Toward Understanding Unlearning Difficulty A Mecha.pdf,Toward Understanding Unlearning Difficulty: A Mechanistic Perspective,"Jiali Cheng, Ziheng Chen, Chirag Agarwal, Hadi Amiri",Not found,Not found,"Machine learning, Unlearning, Model circuits, Difficulty metric","This paper explores the unlearning difficulty of machine learning models, proposing Circuit-guided Unlearning Difficulty (CUD) as a metric to assign each sample a continuous difficulty score using circuit-level signals. It identifies key circuit-level patterns that reveal a mechanistic signature of difficulty, distinguishing intrinsically easy and hard samples and remaining stable across unlearning methods.",10.02,Qwen2.5-3B,Apple M1 (Metal)
2601.09625v1_The Promptware Kill Chain How Prompt Injections Gr.pdf,The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware,"Ben Nassi, Bruce Schneier, Oleg Brodt",,,"malware, prompt injection, large language models, security, cybersecurity","The rapid adoption of large language model (LLM)-based systems has created a new attack surface that existing security frameworks inadequately address. This paper proposes that attacks targeting LLM-based applications constitute a distinct class of malware, termed promptware, and introduces a five-step kill chain model for analyzing these threats. By mapping recent attacks to this structure, the authors demonstrate that LLM-related attacks follow systematic sequences analogous to traditional malware campaigns.",10.78,Qwen2.5-3B,Apple M1 (Metal)
2601.09626v1_From Prompt to Protocol Fast Charging Batteries wi.pdf,From Prompt to Protocol: Fast Charging Batteries with Large Language Models,"Ge Lei1∗, Ferran Brosa Planella2, Sterling G. Baird3, Samuel J. Cooper1†",Not found,2601.09626,"battery charging, large language models, fast charging, optimization, protocol design","Efficiently optimizing battery charging protocols is challenging due to slow evaluations. This work introduces two gradient-free, LLM-driven methods, Prompt-to-Optimizer (P2O) and Prompt-to-Protocol (P2P), which outperform existing approaches in terms of performance and efficiency.",10.72,Qwen2.5-3B,Apple M1 (Metal)
2601.09635v1_LLM for Large-Scale Optimization Model Auto-Formul.pdf,LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach,"Kuo Liang, Yuhang Lu, Jianming Mao, Shuyi Sun, Chunwei Yang, Congcong Zeng, Xiao Jin, Hanzhang Qin, Ruihao Zhu, Chung-Piaw Teo",,2601.09635,"large language models, tool use, agentic workflow construction, automated optimization modeling","This paper proposes LEAN-LLM-OPT, a workflow construction framework for large-scale optimization model auto-formulation using large language models. It addresses the labor-intensive and time-consuming nature of building optimization models and leverages LLMs' text-processing capabilities to decompose the modeling task into structured sub-tasks, offloading mechanical data-handling operations to auxiliary tools. Extensive simulations and a Singapore Airlines use case demonstrate the framework's strong performance and practical value.",11.2,Qwen2.5-3B,Apple M1 (Metal)
2601.09636v1_PersonalAlign Hierarchical Implicit Intent Alignme.pdf,PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records,"Yibo Lyu, Gongwei Chen, Rui Shao†, Weili Guan, Liqiang Nie†",Not provided,Not provided,"GUI agents, implicit intent, long-term user records, proactive assistance","This work introduces PersonalAlign, a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. It also introduces AndroidIntent, a benchmark for evaluating agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. The study evaluates a range of GUI agents, including GPT-5, Qwen3-VL, and UI-TARS, and finds that HIM-Agent significantly improves both execution and proactive performance.",10.79,Qwen2.5-3B,Apple M1 (Metal)
2601.09667v2_Collaborative Multi-Agent Test-Time Reinforcement .pdf,Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning,"Zhiyuan Hu, Yunhai Hu, Juncheng Liu, Shuyue Stella Li, Yucheng Wang, Zhen Xu, See-Kiong Ng, Anh Tuan Luu, Xinxing Xu, Bryan Hooi, Cynthia Breazeal, Hae Won Park",Not found,Not found,"Multi-agent reinforcement learning, Test-time reinforcement learning, Multi-turn deliberation, Credit assignment, Distribution shift robustness","MATTRL is a framework that injects structured textual experience into multi-agent deliberation at inference time, forming a multi-expert team for multi-turn discussions, retrieving and integrating test-time experiences, and reaching consensus for final decision-making. It improves accuracy by an average of 3.67% over a multi-agent baseline and by 8.67% over comparable single-agent baselines across challenging benchmarks in medicine, math, and education.",10.94,Qwen2.5-3B,Apple M1 (Metal)
2601.09680v1_Automating Supply Chain Disruption Monitoring via .pdf,Automating Supply Chain Disruption Monitoring via an Agentic AI Approach,"Sara AlMahria, Liming Xu, Alexandra Brintrup",Not provided,2601.09680,"Supply Chain Management, Supply Chain Disruption, Large Language Models, AI Agents, Multi-Agent System, Agentic System","This research introduces a minimally supervised agentic AI framework that autonomously monitors, analyzes, and responds to disruptions across extended supply networks, achieving high accuracy and reducing response time compared to industry benchmarks.",10.76,Qwen2.5-3B,Apple M1 (Metal)
2601.09684v1_Disentangling Task Conflicts in Multi-Task LoRA vi.pdf,Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection,"Ziyu Yang, Guibin Chen, Yuxin Yang, Aoxiong Zeng, Xiangquan Yang",Not found,Not found,"Multi-Task Learning, Low-Rank Adaptation, Gradient Projection, Orthogonal Complement, Parameter-Efficient Fine-Tuning","This paper proposes Ortho-LoRA, a gradient projection method specifically tailored for the bipartite structure of LoRA. It dynamically projects conflicting task gradients onto the orthogonal complement of each other within the intrinsic LoRA subspace, effectively mitigating task interference and outperforming standard joint training. Extensive experiments on the GLUE benchmark demonstrate the effectiveness of Ortho-LoRA in reducing the performance gap between multi-task and single-task baselines.",10.49,Qwen2.5-3B,Apple M1 (Metal)
2601.09692v1_Routing with Generated Data Annotation-Free LLM Sk.pdf,Routing with Generated Data: Annotation-Free,"Tianyi Niu, Justin Chih-Yao Chen, Genta Indra Winata, Shi-Xiong Zhang, Supriyo Chakraborty, Sambit Sahu, Elias Stengel-Eskin, Mohit Bansal",Not found,Not found,"Large Language Models, Routing, Generated Data, Skill Estimation, Expert Selection","Large Language Model (LLM) routers dynamically select optimal models for given inputs. Existing approaches typically assume access to ground-truth labeled data, which is often unavailable in practice. We introduce Routing with Generated Data (RGD), a challenging setting in which routers are trained exclusively on generated queries and answers produced from high-level task descriptions by generator LLMs. We evaluate query-answer routers and query-only routers across four diverse benchmarks and 12 models, finding that query-answer routers degrade faster than query-only routers as generator quality decreases. Our analysis reveals two crucial characteristics of effective generators: they must accurately respond to their own questions and their questions must produce sufficient performance differentiation among the model pool. We then show how filtering for these characteristics can improve the quality of generated data. We further propose CASCAL, a novel query-only router that estimates model correctness through consensus voting and identifies model-specific skill niches via hierarchical clustering. CASCAL is substantially more robust to generator quality, outperforming the best query-answer router by 4.6% absolute accuracy when trained on weak generator data.",11.87,Qwen2.5-3B,Apple M1 (Metal)
2601.09694v1_LLMs can Compress LLMs Adaptive Pruning by Agents.pdf,LLMs can Compress LLMs: Adaptive Pruning by Agents,"Sai Varun Kodathala1, Rakesh Vunnam 2",Not found,2601.09694,"Model Compression, Adaptive Pruning, Self-Reflection","As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics. Recent work shows that pruned LLMs suffer from severe factual knowledge degradation. This paper introduces agent-guided pruning, where a foundation model acts as an adaptive pruning agent to intelligently select layers to prune, preserving critical knowledge pathways. The method constructs layer-wise sensitivity profiles using Wanda-inspired metrics and gradient importance scores, normalized as z-scores. A checkpoint rollback mechanism maintains model quality by reverting when perplexity degradation exceeds a threshold. The approach demonstrates substantial improvements over structured pruning baselines, including 56% relative improvement in MMLU accuracy and 69% lower perplexity degradation.",12.54,Qwen2.5-3B,Apple M1 (Metal)
2601.09703v1_ShortCoder Knowledge-Augmented Syntax Optimization.pdf,ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation,"Sicong Liu, Yanxian Huang, Mingwei Liu, Jiachi Chen, Ensheng Shi, Yuchi Ma, Hongyu Zhang, Yin Zhang, Yanlin Wang",Not found,Not found,"Code generation, Large language models, Syntax optimization, Token efficiency, Code readability","This paper proposes ShortCoder, a knowledge-infused framework that optimizes code generation efficiency while preserving semantic equivalence and readability. It introduces ten syntax-level simplification rules for Python and a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement to produce validated code pairs. The fine-tuning strategy injects conciseness awareness into base LLMs, demonstrating consistent improvements over state-of-the-art methods on HumanEval.",11.17,Qwen2.5-3B,Apple M1 (Metal)
2601.09706v1_Value-Aware Numerical Representations for Transfor.pdf,Value-Aware Numerical Representations for Transformer Language Models,"Andreea Dutulescu, Stefan Ruseti, Mihai Dascalu",Not found,Not found,"Transformer, Language models, Mathematical reasoning, Numerical understanding, Transformer-based models","Transformer-based language models often achieve strong results on mathematical reasoning benchmarks while remaining fragile on basic numerical understanding and arithmetic operations. A central limitation is that numbers are processed as symbolic tokens whose embeddings do not explicitly encode numerical value, leading to systematic errors. This paper introduces a value-aware numerical representation that augments standard tokenized inputs with a dedicated prefix token whose embedding is explicitly conditioned on the underlying numerical value. Evaluation on arithmetic tasks shows that the proposed approach outperforms baselines across numerical formats, tasks, and operand lengths.",11.11,Qwen2.5-3B,Apple M1 (Metal)
2601.09708v1_Fast-ThinkAct Efficient Vision-Language-Action Rea.pdf,Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning,"Chi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, Fu-En Yang",Not found,2601.09708,"Vision-Language-Action, Embodied Control, Latent Reasoning, Efficient Planning, Robust Long-Horizon Planning","This paper presents Fast-ThinkAct, an efficient reasoning framework for Vision-Language-Action tasks that learns to reason with compact yet performant latent tokens, achieving strong performance with reduced inference latency and effective long-horizon planning.",10.46,Qwen2.5-3B,Apple M1 (Metal)
2601.09749v1_R-LAM Reproducibility-Constrained Large Action Mod.pdf,R-LAM: Reproducibility-Constrained Large Action Models for Scientific Workflow Automation,Suriya Sureshkumar,,,"Reproducible Scientific Workflows, Large Action Models, LLM-Based Agents, Execution Provenance, Deterministic Execution","Large Action Models (LAMs) extend large language models by enabling autonomous decision-making and tool execution, making them promising for automating scientific workflows. However, scientific workflows impose strict requirements on reproducibility, auditability, and deterministic execution, which are not satisfied by generic LLM-based agents. This paper proposes R-LAM, a reproducibility-constrained framework for applying LAMs to scientific workflow automation, introducing structured action schemas, deterministic execution policies, and explicit provenance tracking to ensure reproducibility.",11.21,Qwen2.5-3B,Apple M1 (Metal)
2601.09750v1_SAGE Tool-Augmented LLM Task Solving Strategies in.pdf,SAGE: Tool-Augmented LLM Task Solving Strategies,"Robert K. Strehlow, Tobias Küster, Oskar F. Kupke, Brandon Llanque Kurps, Fikret Sivrikaya, Sahin Albayrak",,,"Large language models, LLMs, tool augmentation, domain-specific tools, zero-shot prompting, conversational AI, OPACA framework","This paper presents SAGE, a specialized conversational AI interface based on the OPACA framework for tool discovery and execution. It integrates new tools or services dynamically, providing robust and scalable zero-shot prompting methods to efficiently utilize these tools. The authors implement various task-solving strategies using agentic concepts and prompting methods, evaluating them against comprehensive benchmark services. The results highlight the strengths and weaknesses of different strategies, and the code and data are available as open source.",10.91,Qwen2.5-3B,Apple M1 (Metal)
2601.09753v1_Critically Engaged Pragmatism A Scientific Norm an.pdf,"Critically Engaged Pragmatism: A Scientific Norm and Social, Pragmatist Epistemology for AI Science Evaluation Tools",Carole J. Lee,,2301.0001,"AI science evaluation, peer review crisis, replication crisis, pragmatism, scientific evaluation tools","This paper discusses the crises in peer review capacity, study replication, and AI-fabricated science, and the need for automated tools to assess scientific research. It cautions against the misuse of AI science evaluation tools and advocates for a social, pragmatist epistemology and the norm of Critically Engaged Pragmatism to scrutinize the purposes and reliability of these tools.",10.59,Qwen2.5-3B,Apple M1 (Metal)
2601.09755v1_Heterogeneous computing platform for real-time rob.pdf,Heterogeneous computing platform for real-time robotics,"Jakub Fil, Yulia Sandamirskaya, Hector Gonzalez, Loïc Azzalin, Stefan Glüge, Matthias Lohrmann, Mahmoud Akl, Khaleel Khan, Leonie Wolf, Kristin Richter, Holm Puder, Mazhar Ali Bari, Xuan Choo, Noha Alharthi, Michael Hopkins, Mansoor Hanif, Christian Mayr, Jens Struckmeier, Steve Furber",Not provided,Not provided,"robotics, neuromorphic computing, real-time perception, heterogeneous computing, cognitive cities, Society 5.0","This paper explores the computing platform required to enable a vision of cognitive cities powered by robots coexisting alongside humans. It combines neuromorphic computing hardware, exemplified by the Loihi2 processor used in conjunction with event-based cameras, for sensing and real-time perception and interaction with a local AI compute cluster (GPUs) for high-level language processing, cognition, and task planning. The hybrid computing architecture is demonstrated in an interactive task, where a humanoid robot plays a musical instrument with a human. The paper also proposes extending the capabilities of neuromorphic approaches by including NVIDIA DGX GPU systems for showcasing orchestration, running large language models, and executing specialized versions of models like Spaun.",11.89,Qwen2.5-3B,Apple M1 (Metal)
2601.09756v1_Synthetic Data for Veterinary EHR De-identificatio.pdf,"Synthetic Data for Veterinary EHR De-identification: Benefits, Limits, and Safety Trade-offs Under Fixed Compute","David Brundage, PhD",Not found,Not found,"de-identification, synthetic data, veterinary EHR, privacy, machine learning","This study evaluates the use of large language model-generated synthetic veterinary clinical narratives in de-identification under fixed compute constraints, comparing their safety and utility to real clinical notes. It explores the impact of synthetic augmentation and substitution under different training regimes.",10.26,Qwen2.5-3B,Apple M1 (Metal)
2601.09757v1_Democracy and Distrust in an Era of Artificial Int.pdf,Democracy & Distrust in an Era of Artificial Intelligence,Sonia K. Katyal,10.1162/DAED_a_01919,not found,"Artificial Intelligence, Democracy, Judicial Review, AI Decision-Making, Minorities, Due Process, Equal Protection","This essay discusses the challenges posed by the rise of AI decision-making to democracy's basic framework, drawing on cases in which AI decision-making has been challenged in courts to outline a theory of judicial review in an AI era.",10.74,Qwen2.5-3B,Apple M1 (Metal)
2601.09760v1_Investigating Tool-Memory Conflicts in Tool-Augmen.pdf,Investigating Tool-Memory Conflicts in Tool-Augmented LLMs,"Jiali Cheng, Rui Pan, Hadi Amiri",Not found,Not found,"tool-augmented language models, knowledge conflict, tool-memory conflict, epistemic inconsistencies, large language models","This paper investigates a new type of knowledge conflict, Tool-Memory Conflict (TMC), in tool-augmented large language models (LLMs). It finds that existing LLMs suffer from TMC, especially on STEM-related tasks, and explores the conditions under which TMC appears and how LLMs prioritize internal parametric knowledge or external tool outputs. The study also evaluates existing conflict resolving techniques and concludes that none of them can effectively resolve tool-memory conflicts.",10.58,Qwen2.5-3B,Apple M1 (Metal)
2601.09762v1_Explicating Tacit Regulatory Knowledge from LLMs t.pdf,Explicating Tacit Regulatory Knowledge from LLMs to Auto-Formalize,"Zhiyi Xue, Xiaohong Chen, Min Zhang",Not found,Not found,"Compliance testing, Regulatory knowledge, Large language models, Requirements auto-formalization, Automated test generation","This paper proposes RAFT, a framework for requirements auto-formalization and compliance test generation via explicating tacit regulatory knowledge from multiple LLMs. RAFT employs an Adaptive Purification-Aggregation strategy to elucidate tacit regulatory knowledge from multiple LLMs and integrate it into three artifacts: a domain meta-model, a formal requirements representation, and testability constraints. These artifacts are then dynamically injected into prompts to guide high-precision requirement formalization and automated test generation. Experiments across financial, automotive, and power domains show that RAFT achieves expert-level performance, substantially outperforms state-of-the-art (SOTA) methods while reducing overall generation and review time.",10.96,Qwen2.5-3B,Apple M1 (Metal)
2601.09765v1_AI Survival Stories a Taxonomic Analysis of AI Exi.pdf,Survival Stories: A Taxonomic Analysis of AI Existential Risk,"Herman Cappelena, Simon Goldsteinb, c, d",Not found,Not found,"Artificial Intelligence, Existential Risk, AI Safety, AI Catastrophe, Superintelligent AI, AI Alignment","This paper develops a general framework for thinking about the existential risk of AI systems, analyzing a two-premise argument that AI systems pose a threat to humanity. Premise one states that AI systems will become extremely powerful, and premise two states that if they do, they will destroy humanity. The authors introduce a taxonomy of 'survival stories' to explore different scenarios where humanity might survive the threat of AI.",10.62,Qwen2.5-3B,Apple M1 (Metal)
2601.09768v1_CLiMB A Domain-Informed Novelty Detection Clusteri.pdf,CLiMB: A Domain-Informed Novelty Detection Clustering Framework for Scientific Discovery,"Lorenzo Monti, Tatiana Muraveva, Brian Sheridan, Davide Massari, Alessia Garofalo, Gisella Clementini, Umberto Michelucci",Not found,2601.09768,"novelty detection, semi-supervised clustering, constrained clustering, density-based clustering, domain knowledge integration","In data-driven scientific discovery, CLiMB introduces a domain-informed framework for clustering that decouples prior knowledge exploitation from the exploration of unknown structures. Using a sequential two-phase approach, CLiMB first anchors known clusters and then applies density-based clustering to residual data, revealing arbitrary topologies. Demonstrated on RR Lyrae stars data, CLiMB outperforms heuristic and constraint-based methods, recovering known Milky Way substructures with high accuracy and sensitivity analysis confirms its superior data efficiency.",11.43,Qwen2.5-3B,Apple M1 (Metal)
2601.09770v1_GUI-Eyes Tool-Augmented Perception for Visual Grou.pdf,GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents,"Chen Chen, Jiawei Shao, Dakuan Lu, Haoyi Hu, Xiangcheng Liu, Hantao Yao, Wu Liu",Not provided,Not provided,"GUI automation, reinforcement learning, visual grounding, tool usage, spatial reward","Recent advances in vision-language models and reinforcement learning have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. GUI-Eyes presents a reinforcement learning framework for active visual perception in GUI tasks, enabling the agent to make strategic decisions on whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. This framework introduces a progressive perception strategy that decomposes the decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. Additionally, a spatially continuous reward function is designed to integrate both location proximity and region overlap, providing dense supervision and alleviating reward sparsity in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines.",11.44,Qwen2.5-3B,Apple M1 (Metal)
2601.09771v1_PCN-Rec Agentic Proof-Carrying Negotiation for Rel.pdf,PCN-Rec: Agentic Proof-Carrying Negotiation for Reliable Governance-Constrained Recommendation,"Aradhya Dixit, Shreem Dixit",,,"recommendation systems, LLM agents, constrained ranking, governance, verification, negotiation","Modern LLM-based recommenders can generate compelling ranked lists, but they struggle to reliably satisfy governance constraints such as minimum long-tail exposure or diversity requirements. We present PCN-Rec, a proof-carrying negotiation pipeline that separates natural-language reasoning from deterministic enforcement. A base recommender (MF/CF) produces a candidate window of size 𝑊, which is negotiated by two agents: a User Advocate optimizing relevance and a Policy Agent enforcing constraints. A mediator LLM synthesizes a Top-𝑁 slate together with a structured certificate (JSON) describing the claimed constraint satisfaction. A deterministic verifier recomputes all constraints from the slate and accepts only verifier-checked certificates; if verification fails, a deterministic constrained-greedy repair produces a compliant slate for re-verification, yielding an auditable trace. On MovieLens-100K with governance constraints, PCN-Rec achieves a 98.55% pass rate on feasible users (𝑛= 551, 𝑊= 80) versus a one-shot single-LLM baseline without verification/repair, while preserving utility with only a 0.021 absolute drop in NDCG@10 (0.403 vs. 0.424); differences are statistically significant (𝑝<0.05).",12.06,Qwen2.5-3B,Apple M1 (Metal)
2601.09772v1_Antisocial behavior towards large language model u.pdf,Antisocial behavior towards large language model users: experimental evidence,"Paweł Niszczota, Cassandra Grützner",,,"large language models, antisocial behavior, experimental evidence",Experimental evidence of antisocial behavior towards users of large language models.,10.04,Qwen2.5-3B,Apple M1 (Metal)
2601.09773v1_Enhancing LUT-based Deep Neural Networks Inference.pdf,Enhancing LUT-based Deep Neural Networks Inference through Architecture and Connectivity Optimization,"Binglei Lou, Ruilin Wu, Philip Leong",,,"Dynamic Sparsity, FPGA, Neural Network, Lookup Table","This paper presents SparseLUT, a comprehensive framework that addresses the challenges of deploying deep neural networks on resource-constrained edge devices, such as FPGAs. It introduces architectural enhancements and a non-greedy training algorithm to optimize neuron connectivity and achieve consistent accuracy improvements across benchmarks.",9.45,Qwen2.5-3B,Apple M1 (Metal)
2601.09805v1_Improving Chain-of-Thought for Logical Reasoning v.pdf,Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention,"Phuong Minh Nguyen, Tien Huu Dang, Naoya Inoue",Not found,Not found,"Logical reasoning, Attention, Intervention, Chain-of-thought","This work introduces an end-to-end framework for logical reasoning tasks, which activates attention heads aligned with logical reasoning operators to enhance performance across diverse benchmarks and architectures, while incurring negligible additional computational overhead.",9.28,Qwen2.5-3B,Apple M1 (Metal)
2601.09806v1_Diffusion-Driven Deceptive Patches Adversarial Man.pdf,Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification,"Shahrzad Sayyafzadeh, Hongmei Chi, Shonda Bernadin",,2601.09806,"Adversarial Patch Generation, Gaussian Smoothing, Diffusion Model, Social Media Forensics, Perceptual Hashing","This work presents an end-to-end pipeline for generating, refining, and evaluating adversarial patches to compromise facial biometric systems with forensic analysis and security testing applications.",11.76,Qwen2.5-3B,Apple M1 (Metal)
2601.09809v1_QFed Parameter-Compact Quantum-Classical Federated.pdf,QFed: Parameter-Compact Quantum-Classical Federated Learning,"Samar Abdelghani, Soumaya Cherkaoui",,,"Federated Learning, Quantum Computing, Privacy, Communication, IoT","This study examines the potential of quantum-assisted federated learning, which could cut the number of parameters in classical models by polylogarithmic factors and thus lessen training overhead. QFed, a quantum-enabled federated learning framework, achieves a 77.6% reduction in the parameter count of a VGG-like model while maintaining comparable accuracy in a scalable environment.",10.37,Qwen2.5-3B,Apple M1 (Metal)
2601.09814v1_Explainable Deep Learning for Pediatric Pneumonia .pdf,Explainable Deep Learning for Pediatric Pneumonia Detection in Chest X-Ray Images,"Adil O. Khadidos1, †, Aziida Nanyonga2*, Alaa O. Khadidos3,4, Olfat M. Mirza5, Mustafa Tahsin Yilmaz6",,,"Deep Learning, Convolutional Neural Networks, Pediatric Pneumonia, Chest X-Ray, Explained Models","This study compares two state-of-the-art convolutional neural network architectures, DenseNet121 and EfficientNet-B0, for automated pediatric pneumonia detection in chest X-ray images. The models achieve high accuracy and sensitivity, with EfficientNet-B0 outperforming DenseNet121 in classification performance. Explained models using Grad-CAM and LIME are employed to visualize regions contributing to model decisions.",11.83,Qwen2.5-3B,Apple M1 (Metal)
2601.09822v2_LLM-Based Agentic Systems for Software Engineering.pdf,LLM-Based Agentic Systems for Software Engineering: Challenges and Opportunities,"Yongjian Tang, Thomas Runkler",Not found,2601.09822,"LLMs, Agents, Software Engineering, Future Challenges","This concept paper reviews the emerging paradigm of LLM-based multi-agent systems in Software Engineering, examining their applications across the Software Development Life Cycle (SDLC) from requirements engineering to testing and debugging. It identifies key challenges and future research opportunities in multi-agent orchestration, human-agent coordination, computational cost optimization, and effective data collection.",10.66,Qwen2.5-3B,Apple M1 (Metal)
2601.09841v2_A pipeline for enabling path-specific causal fairn.pdf,APIPIPELINE FOR ENABLING PATH-SPECIFIC CAUSAL FAIRNESS IN OBSERVATIONAL HEALTH DATA,"Aparajita Kashyap, Sara Matijevic, Noémie Elhadad, Steven A. Kushner, Shalmali Joshi",Not found,2601.09841v2,"causal fairness, foundation models, causal inference, observational health data, fair machine learning",This work presents a model-agnostic pipeline for training causally fair machine learning models that address both direct and indirect forms of healthcare bias in the observational health data setting.,11.56,Qwen2.5-3B,Apple M1 (Metal)
2601.09851v1_ViSIL Unified Evaluation of Information Loss in Mu.pdf,ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning,"Po-han Li * 1, Shenghui Chen * 1, Ufuk Topcu1, Sandeep Chinchali 1",Not found,Not found,"Multimodal video captioning, Information loss, Video summary, Information-theoretic framework, Video Question Answering (VQA)","The paper proposes the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. It addresses the limitation of traditional metrics like BLEU or ROUGE in capturing information coverage across disparate modalities. The results show a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks, enabling summary selection to optimize the trade-off between information loss and processing speed.",11.23,Qwen2.5-3B,Apple M1 (Metal)
2601.09853v1_MedRedFlag Investigating how LLMs Redirect Misconc.pdf,MedRedFlag: Investigating how LLMs Redirect Misconceptions in Real-World Health Communication,"Sraavya Sambara∗, Yuan Pu1∗, Ayman Ali1, Vishala Mishra1, Lionel Wong2, Monica Agrawal1",,,"Large Language Models, Health Communication, Misconceptions, Redirect, Patient Misunderstandings","This work investigates how large language models (LLMs) handle false premises embedded in real-world health questions, focusing on redirection. A dataset of 1100+ questions from Reddit is curated, and responses from state-of-the-art LLMs are compared to those from clinicians. The analysis reveals that LLMs often fail to redirect problematic questions, even when the problematic premise is detected, leading to suboptimal medical decisions. This highlights a significant gap in LLM performance under real-world conditions and underscores critical safety concerns for patient-facing medical AI systems.",11.01,Qwen2.5-3B,Apple M1 (Metal)
2601.09855v1_Thinking Long but Short Stable Sequential Test-Tim.pdf,Stable Sequential Test-Time Scaling for Large Reasoning Models,"Michael R. Metel, Yufei Cui, Boxing Chen, Prasanna Parthasarathi",Not provided,2601.09855,"Large reasoning models, Sequential test-time scaling, Test-time scaling, Model accuracy, Model stability, Chain-of-thought reasoning, DeepSeek","This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling and removing the need for reasoning length fine-tuning. The method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. It can continue to reason well beyond a model’s maximum context length and has linear computational complexity.",10.56,Qwen2.5-3B,Apple M1 (Metal)
2601.09858v1_OUTLINEFORGE Hierarchical Reinforcement Learning w.pdf,OUTLINEFORGE: Hierarchical Reinforcement Learning with Explicit States for Scientific Writing,"Yilin Bao, Ziyao He, Zayden Yang",Not found,Not found,"Reinforcement Learning, Hierarchical Planning, Scientific Writing, Document-Level Planning, Long-Horizon Planning, Factual Grounding, Scientific Reasoning, Large Language Models, AI in Science","This paper presents a reinforcement learning framework that casts scientific outline construction as a long-horizon planning problem over hierarchical document structures. The approach models edit evolving outlines through structured actions, enabling the system to incrementally build a complete scientific manuscript. The framework introduces a two-stage optimization procedure to support effective and stable learning, including backward outline reconstruction and forward value-guided reinforcement learning with rewards explicitly modeling scientific correctness, discourse coherence, and citation fidelity. The authors also introduce a benchmark for scientific paper generation that evaluates document planning, input utilization, reference faithfulness, outline organization, and content-level factual accuracy, showing consistent improvements over strong neural and LLM baselines, particularly in long-range structural coherence and citation reliability.",11.61,Qwen2.5-3B,Apple M1 (Metal)
2601.09865v1_Advancing Model Refinement Muon-Optimized Distilla.pdf,Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment,"Jacob Sander, Brian Jalaian, V enkat R. Dasarivenkateswara",Not found,2601.09865,"Large Language Models, Quantization, Distillation, Edge Devices, Resource Optimization","This paper proposes an integrated framework combining GPTQ-based quantization, low-rank adaptation (LoRA), and a specialized data distillation process to significantly reduce model size and complexity while preserving or enhancing task-specific performance. By leveraging data distillation, knowledge distillation via Kullback-Leibler divergence, Bayesian hyperparameter optimization, and the Muon optimizer, our pipeline achieves up to 2×memory compression and enables efficient inference for specialized tasks.",11.28,Qwen2.5-3B,Apple M1 (Metal)
2601.09869v1_A Scoping Review of the Ethical Perspectives on An.pdf,A SCOPING REVIEW OF THE ETHICAL PERSPECTIVES ON ANTHROPOMORPHISING LARGELANGUAGEMODEL-BASED CONVERSATIONAL AGENTS,"Andrea Ferrario, Rasita Vinay, Matteo Casserini, Alessandro Facchini",Not found,2601.09869,"anthropomorphism, conversational agents, large language models, AI ethics, deception, trust, governance","This scoping review examines ethical perspectives on anthropomorphizing large language model-based conversational agents, mapping work across five databases and three preprint repositories. It synthesizes conceptual foundations, ethical challenges and opportunities, and methodological approaches, finding convergence on attribution-based definitions but substantial divergence in operationalization, a predominantly risk-forward normative framing, and limited empirical work linking observed interaction effects to actionable governance guidance.",11.06,Qwen2.5-3B,Apple M1 (Metal)
2601.09871v1_Epistemology gives a Future to Complementarity in .pdf,EPISTEMOLOGY GIVES AFUTURE TOCOMPLEMENTARITY IN HUMAN-AI INTERACTIONS,"Andrea Ferrario∗1,2,3, Alessandro Facchini2,4, Juan M. Durán5",Not found,2601.09871,"artificial intelligence, machine learning, reliance, complementarity, human-AI interaction, computational reliabilism, epistemology","This work leverages epistemology to address theoretical challenges in human-AI complementarity, reframing it within the discourse on justificatory AI. It argues that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a predictive task. Together with other reliability indicators, complementarity contributes to the reliability of human-AI teams generating predictions, supporting practical reasoning for those affected by these outputs.",10.73,Qwen2.5-3B,Apple M1 (Metal)
2601.09879v1_MedVL-SAM2 A unified 3D medical vision-language mo.pdf,MedVL-SAM2: A unified 3D medical vision–language model for multimodal reasoning and prompt-driven segmentation,"Yang Xing, Jiong Wu, Savas Ozdemir, Ying Zhang, Yang Yang, Wei Shao, Kuang Gong",,,"3D medical vision-language model, multimodal reasoning, prompt-driven segmentation, 3D medical imaging, volumetric segmentation, semantic segmentation, referencing segmentation, interactive segmentation","MedVL-SAM2 is a unified 3D medical vision-language model designed to handle complex tasks such as report generation, visual question answering, and multi-paradigm segmentation. It integrates image-level reasoning and pixel-level perception, enabling precise multi-granular spatial reasoning and flexible interaction via language, point, or box prompts. The model is trained on a large-scale corpus of 3D CT image-text pairs and a comprehensive 3D CT segmentation dataset, delivering state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks.",11.13,Qwen2.5-3B,Apple M1 (Metal)
2601.09881v1_Transition Matching Distillation for Fast Video Ge.pdf,Transition Matching Distillation for Fast Video Generation,"Weili Nie, Julius Berner, Nanye Ma, Chao Liu, Saining Xie",Not found,2601.09881v1,"video generation, diffusion models, few-step generators, fast video synthesis","This work presents Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators, addressing the inefficiency of multi-step sampling in real-time applications.",11.38,Qwen2.5-3B,Apple M1 (Metal)
2601.09883v1_Beyond Rule-Based Workflows An Information-Flow-Or.pdf,Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL,"Xinxing Ren1,2,∗, Quagmire Zang3,∗, Caelum Forder1,∗, Suman Deb1,∗, Ahsen Tahir1,5,∗, Roman J. Georgio1, Peter Carroll1, Zekun Guo4,†",Not found,Not found,"Large Language Models, Multi-Agent Systems, Information-Flow-Orchestrated, Agent-to-Agent Communication, CORAL Protocol","Most existing Large Language Model-based Multi-Agent Systems rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. This workflow-driven design suffers from two fundamental limitations: substantial manual effort to anticipate and encode possible task states, and inability to exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models.",11.61,Qwen2.5-3B,Apple M1 (Metal)
2601.09896v1_The Algorithmic Gaze An Audit and Ethnography of t.pdf,The Algorithmic Gaze: An Audit and Ethnography of the LAION-Aesthetics Predictor Model,"Jordan Taylor, William Agnew, Maarten Sap, Sarah E. Fox, Haiyi Zhu",10.1145/nnnnnnn.nnnnnnn,,"AI, Art, Aesthetic Evaluation","This work studies an aesthetic evaluation model—LAION Aesthetic Predictor (LAP)—that is widely used to curate datasets to train visual generative image models, like Stable Diffusion, and evaluate the quality of AI-generated images. The authors audit the model across three datasets and find that LAP disproportionately filters images with captions mentioning women, while filtering out images with captions mentioning men or LGBTQ+ people. They also find that LAP rates realistic images of landscapes, cityscapes, and portraits from western and Japanese artists highly. The algorithmic gaze of LAP reinforces the imperial and male gazes found within western art history. The authors perform a digital ethnography of public materials related to the creation of LAP and find that the development of LAP reflects the biases found in their audits, such as the aesthetic scores primarily coming from English-speaking photographers and western AI-enthusiasts. The authors discuss how aesthetic evaluation can perpetuate representational harms and call for AI developers to shift away from prescriptive measures of 'aesthetics' toward more pluralistic evaluation.",11.74,Qwen2.5-3B,Apple M1 (Metal)
2601.09902v1_A Novel Contrastive Loss for Zero-Day Network Intr.pdf,A Novel Contrastive Loss for Zero-Day Network Intrusion Detection,"Jack Wilkie, Hanan Hindy, Craig Michie, Christos Tachtatzis, James Irvine, Robert Atkinson",,,"Machine Learning, Network Intrusion Detection, Zero-Day Attacks, Contrastive Learning, Anomaly Detection","This work proposes a novel contrastive loss function for zero-day network intrusion detection, which maintains the advantages of contrastive learning-based approaches while achieving significant performance improvements in both known and zero-day attack detection.",9.99,Qwen2.5-3B,Apple M1 (Metal)
2601.09913v1_Continuum Memory Architectures for Long-Horizon LL.pdf,Continuum Memory Architectures for Long-Horizon LLM Agents,"Joe Logan, Mode7 GK",Not found,2601.09913,"Continuum Memory Architectures, Long-Horizon LLM Agents, Retrieval-Augmented Generation, Memory Dynamics, Temporal Continuity","This paper introduces Continuum Memory Architectures (CMA), a class of systems designed to maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. The authors argue that current retrieval-augmented generation (RAG) systems treat memory as a stateless lookup table, leading to structural limitations in accumulating, mutating, or disambiguating memory. CMA addresses these limitations by providing a structured, continuously evolving memory subsystem, which is demonstrated to yield advantages on tasks that stress memory dynamics.",11.28,Qwen2.5-3B,Apple M1 (Metal)
2601.09921v1_Learning to Decode in Parallel Self-Coordinating N.pdf,Learning to Decode in Parallel: Self-Coordinating Neural Network for Real-Time Quantum Error Correction,"Kai Zhang, Zhengzhong Yi, Shaojun Guo, Linghang Kong, Situ Wang, Xiaoyu Zhan, Tan He, Weiping Lin, Tao Jiang, Dongxin Gao, Yiming Zhang, Fangming Liu, Fang Zhang, Zhengfeng Ji, Fusheng Chen, Jianxin Chen",,2601.09921,"quantum error correction, neural networks, parallel decoding, fault-tolerant quantum computation","This paper presents a self-coordinating neural network decoder designed for real-time quantum error correction, demonstrating improved accuracy compared to traditional human-designed algorithms.",12.42,Qwen2.5-3B,Apple M1 (Metal)
2601.09923v1_CaMeLs Can Use Computers Too System-level Security.pdf,CAMELSCANUSECOMPUTERSTOO: SYSTEM-LEVELSECURITY FORCOMPUTERUSEAGENTS,"Hanna Foerster∗, Robert Mullins, Tom Blanchard∗, Nicolas Papernot, Kristina Nikolić, Florian Tramèr, Ilia Shumailov, Cheng Zhang, Yiren Zhao",Not found,2601.09923,"Computer Use Agents, AI Agents, Prompt Injection Attacks, Control Flow Integrity, Security, Computer Vision, Language Models","AI agents, particularly Computer Use Agents (CUAs), are vulnerable to prompt injection attacks where malicious content hijacks agent behavior. Current architectural isolation methods, such as strict separation of trusted task planning from untrusted environment observations, are insufficient for CUAs due to their continuous UI state observation requirements. This paper introduces Single-Shot Planning for CUAs, a method that generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees. The authors also discuss the need for additional measures to prevent Branch Steering attacks and evaluate their design on OSWorld, demonstrating coexistence of rigorous security and utility in CUAs.",11.96,Qwen2.5-3B,Apple M1 (Metal)
2601.09929v1_Hallucination Detection and Mitigation in Large La.pdf,Hallucination Detection and Mitigation in Large Language Models,"Ahmad Pesaranghader, Erin Li",Not found,2601.09929v1,"hallucination, large language models, large reasoning models, reliability, automation, financial, legal","This paper introduces a comprehensive operational framework for hallucination management in large language models and large reasoning models, addressing the critical reliability risk posed by their tendency to hallucinate, generating factually incorrect or unsupported content. The framework categorizes hallucination sources into model, data, and context-related factors, allowing targeted interventions over generic fixes. It integrates multi-faceted detection methods and stratified mitigation strategies, demonstrating its application through a tiered architecture and a financial data extraction case study.",11.39,Qwen2.5-3B,Apple M1 (Metal)
2601.09933v1_Malware Classification using Diluted Convolutional.pdf,Malware Classification using Diluted Convolutional Neural Network with Fast Gradient Sign Method,"Ashish Anand, Bhupendra Singh, Sunil Khemka, Bireswar Banerjee, Vishi Singh Bhatia, Piyush Ranjan",,,"data security, diluted convolutional neural network, fast gradient sign method, malware classification, privacy","A ndroid malware has become an increasingly critical threat, and this research proposes a Fast Gradient Sign Method with Diluted Convolutional Neural Network (FGSM -DICNN) for malware classification. DICNN contains diluted convolutions which increases receptive field, enabling the model to capture dispersed malware patterns across long ranges using fewer features without adding parameters. Additionally, the FGSM strategy enhances the accuracy by using one-step perturbations during training that provides more defensive advantage of lower computational cost. This integration helps to manage high classification accuracy while reducing the dependence on extensive feature sets.",11.04,Qwen2.5-3B,Apple M1 (Metal)
2601.09949v2_Kinematic Tokenization Optimization-Based Continuo.pdf,Kinematic Tokenization: Optimization-Based Continuous-Time Tokens for Learnable Decision Policies in Noisy Time Series,"Griffin M. Kearney, Ph.D.",Not found,2601.09949,"Transformers, Continuous-time, Optimization, Tokenization, Noisy Time Series, Financial Markets, Decision Policies, Learning","This paper introduces Kinematic Tokenization, an optimization-based continuous-time representation that reconstructs an explicit spline from noisy measurements and tokenizes local spline coefficients (position, velocity, acceleration, jerk). Applied to financial time series data, it demonstrates improved learnability and calibration of selective decision policies under abstention-inducing losses.",11.37,Qwen2.5-3B,Apple M1 (Metal)
2601.09966v1_A Sustainable AI Economy Needs Data Deals That Wor.pdf,A Sustainable AI Economy Needs Data Deals That Work for Generators,"Ruoxi Jia∗, Luis Oala∗, Wenjie Xiong, Suqin Ge, Jiachen T. Wang, Feiyang Kang, Dawn Song",,2601.09966v1,"Sustainable AI, Data Deals, Data Processing Inequality, Machine Learning Value Chain, Data Aggregation, Data Generators","The authors argue that the machine learning value chain is structurally unsustainable due to an economic data processing inequality, where data generators often receive little attribution despite the majority of value accruing to aggregators. They identify three structural faults—missing provenance, asymmetric bargaining power, and non-dynamic pricing—as the operational machinery of this inequality and propose an Equitable Data-Value Exchange (EDVEX) Framework to enable a minimal market that benefits all participants.",10.86,Qwen2.5-3B,Apple M1 (Metal)
2601.09972v1_Chinese Labor Law Large Language Model Benchmark.pdf,Chinese Labor Law Large Language Model Benchmark,"Zixun Lan, Maochun Xu, Yifan Ren, Rui Wu, Jianghui Zhou, Xueyang Cheng, Jian’an Ding, Xinheng Wang, Mingmin Chi, Fei Ma",,,"Chinese labor law, legal natural language processing, large language models, domain-specific fine-tuning, benchmark dataset, statute recall, legal reasoning, case analysis","This paper presents LaborLawLLM, a legal large language model specifically tailored to the labor law domain, and constructs LabourLawBench, a comprehensive benchmark for diverse labor law tasks. Experimental results show that LaborLawLLM significantly outperforms general-purpose and existing legal-specific LLMs across all task categories.",10.7,Qwen2.5-3B,Apple M1 (Metal)
2601.09974v1_SPRInG Continual LLM Personalization via Selective.pdf,SPRInG: Continual LLM Personalization via Selective Parametric Adaptation and Retrieval-Interpolated Generation,"Seoyeon Kim, Jaehyung Kim",,,"Personalization, Large Language Models, Continual Learning, Selective Adaptation, Retrieval-Interpolated Generation","This paper introduces SPRING, a novel semi-parametric framework for effective continual personalization of Large Language Models (LLMs). SPRING employs drift-driven selective adaptation to identify high-novelty interactions and selectively update user-specific adapters while preserving hard-to-learn residuals in a replay buffer. During inference, it applies strict relevance gating and fuses parametric knowledge with retrieved history via logit interpolation. Experiments on a long-form personalized generation benchmark demonstrate SPRING's superior performance compared to existing baselines.",10.47,Qwen2.5-3B,Apple M1 (Metal)
2601.09980v1_Performance of AI agents based on reasoning langua.pdf,ALD optimization using reasoning LLMs,Angel Yanguas-Gil,,,"Atomic Layer Deposition, Large Language Models, Optimization, Self-Limited Processes","This work explores the performance and behavior of reasoning large language models in autonomously optimizing atomic layer deposition (ALD) processes. Agents built on these models successfully completed ALD process optimization tasks, but exhibited significant run-to-run variability due to the non-deterministic nature of the models' responses. The agents use a two-step process to generate structured outputs from open responses detailing the reasoning process, revealing that the models' logic is sound and based on self-limited process and saturation concepts. However, the agents can sometimes be misled by their own prior choices when exploring the optimization space.",10.5,Qwen2.5-3B,Apple M1 (Metal)
2601.09982v1_Context Volume Drives Performance Tackling Domain .pdf,Context Volume Drives Performance: Tackling Domain Shift in Extremely Low-Resource Translation via RAG,"David Samuel Setiawan, Raphaël Merx, Jey Han Lau",,,"Neural Machine Translation, Low-resource languages, Domain shift, Retrieval-Augmented Generation, Hybrid framework","This work addresses the significant performance degradation of Neural Machine Translation models for low-resource languages, specifically when translating from the New Testament (NT) to the Old Testament (OT). By introducing a hybrid framework combining a fine-tuned NMT model with a Large Language Model (LLM) using Retrieval-Augmented Generation (RAG), the authors achieve a final system performance of 35.21 chrF++, effectively matching the original in-domain quality. The analysis reveals that the performance recovery is primarily driven by the number of retrieved examples rather than the choice of retrieval algorithm, with the LLM acting as a robust ",10.5,Qwen2.5-3B,Apple M1 (Metal)
2601.10010v1_VERHallu Evaluating and Mitigating Event Relation .pdf,VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video Large Language Models,"Zefan Zhang, Kehua Zhu, Shijie Jiang, Hongyuan Lu, Shengkai Sun, Tian Bai*",,,"Video Large Language Models, Event Relation Understanding, Video Understanding","This paper introduces a novel benchmark, VERHallu, for evaluating and mitigating event relation hallucination in Video Large Language Models (VideoLLMs). It focuses on causal, temporal, and subevent relations between events, covering relation classification, question answering, and counterfactual question answering tasks. The benchmark features counterintuitive video scenarios with human-annotated candidates, revealing that current state-of-the-art VideoLLMs struggle with dense-event relation reasoning and overlook surrounding subevents. A Key-Frame Propagating (KFP) strategy is proposed to mitigate event relation hallucination without affecting inference speed.",10.93,Qwen2.5-3B,Apple M1 (Metal)
2601.10011v1_Memo-SQL Structured Decomposition and Experience-D.pdf,Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL,"Zerui Yang1, Weichuan Wang1, Yanwei Xu*, Linqi Song†, Yudai Matsuda1, Wei Han2, Bo Bai2",Not provided,Not provided,"NL2SQL, training-free, self-correction, structured decomposition, experience-aware","Presenting Memo-SQL, a training-free framework that addresses the limitations of existing NL2SQL systems by introducing structured decomposition and experience-aware self-correction. It achieves 68.5% execution accuracy on BIRD, setting a new state of the art among open, zero-fine-tuning methods, while using over 10× fewer resources than prior TTS approaches.",10.39,Qwen2.5-3B,Apple M1 (Metal)
2601.10018v1_Empowering Older Adults in Digital Technology Use .pdf,Empowering Older Adults in Digital Technology Use with Foundation Models,"Hasti Sharifi, Homaira Huda Shomee, Sourav Medya, Debaleena Chattopadhyay",,,"Technology support, Digital technology use, Artificial intelligence, Large language models, Communication barriers, Human-computer interaction","This study examines communication challenges older adults face when using digital applications and explores AI-based approaches to mitigate these challenges. It identifies four key communication challenges: verbosity, incompleteness, over-specification, and under-specification. The study uses a diary study with older adults to collect technology-related queries and applies reflexive thematic analysis to identify barriers. It evaluates how foundation models can paraphrase older adults' queries to improve solution accuracy. Controlled experiments with younger and older adults show that AI-rephrased queries significantly improve solution accuracy and Google search results. Younger adults better understand AI-rephrased queries, while older adults report high perceived ability to answer contextual questions and follow solutions. The study also developed a pipeline using large language models to generate a synthetic dataset of older adults' tech support requests (OATS).",11.53,Qwen2.5-3B,Apple M1 (Metal)
2601.10025v1_Structured Personality Control and Adaptation for .pdf,Structured Personality Control and Adaptation for LLM Agents,"Jinpeng Wang, Xinyu Jia, Wei Wei Heng, Yuquan Li, Binbin Shi, Qianlei Chen, Guannan Chen, Junxia Zhang, Yuyu Yin",XXXXXXX.XXXXXXX,,"Personalization, Jungian Psychological Types, MBTI Personality Types, Persona Adaptation, Explainable AI","This paper presents a framework for modeling LLM personality via Jungian psychological types, integrating mechanisms for coherent core expression, temporary adaptation, and long-term evolution. It evaluates personality alignment using Myers–Briggs Type Indicator and tests under diverse scenarios.",10.02,Qwen2.5-3B,Apple M1 (Metal)
2601.10029v1_PaperScout An Autonomous Agent for Academic Paper .pdf,PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization,"Tingyue Pan, Jie Ouyang, Mingyue Cheng, Qingchuan Li, Zirui Liu, Mingfan Pan, Shuo Yu, Qi Liu",,,"academic paper search, autonomous agent, sequence-level policy optimization, process-aware, reinforcement learning, fine-grained control","PaperScout is an autonomous agent designed to reformulate academic paper search as a sequential decision-making process. Unlike static workflows, it dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. The paper introduces Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance.",10.62,Qwen2.5-3B,Apple M1 (Metal)
2601.10031v1_FilDeep Learning Large Deformations of Elastic-Pla.pdf,FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data,"Jianheng Tang, Shilong Tao, Zhe Feng, Haonan Sun, Menglu Wang, Zhanxing Zhu, Yunhuai Liu",10.1145/3770854.3783959,,"Large Deformations, Elastic-Plastic Solids, Multi-Fidelity Data, Deep Learning, Quantity-Accuracy Dilemma","This work addresses the challenge of large deformation problems in elastic-plastic solids by proposing FilDeep, a Fidelity-based Deep Learning framework that simultaneously trains with both low-fidelity and high-fidelity data to resolve the quantity-accuracy dilemma. FilDeep captures long-range physical interactions across multi-fidelity data and demonstrates state-of-the-art performance in manufacturing applications.",10.37,Qwen2.5-3B,Apple M1 (Metal)
2601.10038v1_What Understanding Means in AI-Laden Astronomy.pdf,What Understanding Means in AI-Laden,"Yuan-Sen Ting1,2,3*, André Curtis-Trudel4,5, Siyu Y ao6",Not found,2601.10038,"philosophy of science, astronomy, artificial intelligence, understanding","Scientists across many fields are confronting questions about the nature of discovery, the meaning of progress, and the essence of understanding as artificial intelligence rapidly transforms astronomical research. This convergence of concern presents a rare opportunity for productive collaboration. The hype surrounding AI in science is not difficult to find, but what is harder to locate is sustained, rigorous reflection on what AI actually changes about how we generate knowledge. Philosophers and historians of science have long grappled with these questions, yet they are often left out of mainstream discussions about AI in research, as if the transformation were purely a matter for engineers to optimize and scientists to exploit. This is a missed opportunity. Philosophy offers astronomy three things: conceptual engineering, critical examination of assumptions, and frameworks for abstraction. These are essential tools for navigating a transformation already underway.",11.39,Qwen2.5-3B,Apple M1 (Metal)
2601.10073v1_ReaMIL Reasoning- and Evidence-Aware Multiple Inst.pdf,ReaMIL: Reasoning- and Evidence-Aware Multiple Instance Learning for Whole-Slide Histopathology,"Hyun Do Jung1, Jungwon Choi2, Hwiyoung Kim1*",,,"Histopathology, Multiple Instance Learning, Whole-slide imaging, Reasoning, Evidence-aware","ReaMIL is a multiple instance learning approach for whole-slide histopathology that adds a light selection head to a strong MIL backbone. The head produces soft per-tile gates and is trained with a budgeted-sufficiency objective, enforcing the true-class probability to be ≥τ using only the kept evidence under a sparsity budget on the number of selected tiles. Across TCGA-NSCLC (LUAD vs. LUSC), TCGA-BRCA (IDC vs. Others), and PANDA, ReaMIL matches or slightly improves baseline AUC and provides quantitative evidence-efficiency diagnostics. On NSCLC, it attains AUC 0.983 with a mean minimal sufficient K (MSK)≈8.2tiles at τ= 0.90 and AUKC≈0.864, showing that class confidence rises sharply and stabilizes once a small set of tiles is kept. The method requires no extra supervision, integrates seamlessly with standard MIL training, and naturally yields slide-level overlays.",11.67,Qwen2.5-3B,Apple M1 (Metal)
2601.10079v1_Sparse-RL Breaking the Memory Wall in LLM Reinforc.pdf,Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts,"Sijia Luo, Xiaokang Zhang, Yuxuan Hu, Bohan Zhang, Ke Wang, Jinbo Su, Mengshu Sun, Lei Liang, Jing Zhang",Not provided,Not provided,"Reinforcement Learning, Large Language Models, Sparse Rollouts, Memory Overhead, Compression Techniques","Sparse-RL addresses the memory bottleneck in Long-Range Rollouts (LRR) for Large Language Models (LLMs) by introducing Sparsity-Aware Rejection Sampling and Importance-based Reweighting, thereby enabling stable reinforcement learning training under sparse rollouts. This approach mitigates the fundamental policy mismatch among the dense old policy, the sparse sampler policy, and the learner policy, leading to improved performance and robustness during sparse inference deployment.",10.7,Qwen2.5-3B,Apple M1 (Metal)
2601.10088v1_State of AI An Empirical 100 Trillion Token Study .pdf,State of AI: An Empirical 100 Trillion Token Study with OpenRouter,"Malika Aubakirova∗†, Alex Atallah ‡, Chris Clark ‡, Justin Summerville ‡, Anjney Midha †",Not provided,Not provided,"large language models, AI inference, open-router, 100 trillion tokens, cognitive shift, open-weight models, creative roleplay, coding assistance, agentic inference, user engagement","This work analyzes over 100 trillion tokens of real-world interactions with large language models (LLMs) using the OpenRouter platform, revealing substantial adoption of open-weight models, the popularity of creative roleplay, coding assistance, and agentic inference. It also identifies foundational cohorts of early users and discusses implications for model builders, AI developers, and infrastructure providers.",11.21,Qwen2.5-3B,Apple M1 (Metal)
2601.10090v1_Difficulty-guided Sampling Bridging the Target Gap.pdf,Difficulty-guided Sampling: Bridging the Target Gap between Dataset Distillation and Downstream Tasks,"Mingzhuo Lia, Guang Lia, Linfeng Ye, Jiafeng Mao, Takahiro Ogawa, Konstantinos N. Plataniotis, Miki Haseyama",Not found,2601.10090,"Dataset distillation, Downstream tasks, Difficulty, Post-stage sampling","This paper proposes difficulty-guided sampling (DGS) to bridge the target gap between dataset distillation and downstream tasks, improving dataset distillation performance.",11.28,Qwen2.5-3B,Apple M1 (Metal)
2601.10092v1_LeMoF Level-guided Multimodal Fusion for Heterogen.pdf,LEMOF: LEVEL-GUIDED MULTIMODAL FUSION FOR HETEROGENEOUS CLINICAL DATA,"Jongseok Kim ∗, Seongae Kang ∗, Jonghwan Shin, Yuhan Lee, Ohyun Jo",Not found,2601.10092,"Multimodal Learning, Hierarchical Representation Learning, Clinical Time-Series Modeling, Level-guided Feature Fusion, Explainable Medical AI","This paper proposes Level-guided Modal Fusion (LeMoF), a novel framework for integrating heterogeneous clinical data, such as Electronic Health Records and biosignals. LeMoF selectively integrates level-guided representations within each modality, enabling balanced performance between prediction stability and discriminative capability in heterogeneous clinical environments.",10.96,Qwen2.5-3B,Apple M1 (Metal)
2601.10094v1_V-Zero Self-Improving Multimodal Reasoning with Ze.pdf,V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation,"Han Wang1*, Yi Yang1*, Jingyuan Hu2*, Minfeng Zhu2†",Not found,Not found,"multimodal reasoning, self-improvement, unsupervised learning, vision-language models","A self-improvement framework for vision-language models (VLMs) that uses exclusively unlabeled images. It establishes a co-evolutionary loop between a Questioner and a Solver, where the Questioner synthesizes high-quality, challenging questions and the Solver optimizes using pseudo-labels derived from majority voting over its own responses. This framework achieves consistent performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning and general vision-centric tasks.",10.59,Qwen2.5-3B,Apple M1 (Metal)
2601.10101v2_Matrix as Plan Structured Logical Reasoning with F.pdf,Matrix as Plan: Structured Logical Reasoning with Feedback-Driven Replanning,"Ke Chen, Jiandian Zeng, Zihao Peng, Guo Li, Guangxue Zhang, Tian Wang",10.1145/XXXXXX.XXXXXX,,"Logical Reasoning, Large Language Models, Neurosymbolic Approaches, Semantic Decomposition","This paper proposes MatrixCoT, a structured CoT framework with a matrix-based plan, to enhance the logical reasoning capabilities of Large Language Models (LLMs). It addresses the limitations of CoT prompting and neuro-symbolic methods by normalizing and typing natural language expressions, introducing a matrix-based planning method, and adding a feedback-driven replanning mechanism. Experiments on logical reasoning benchmarks and LLMs show that MatrixCoT improves robustness and interpretability without relying on external solvers.",10.26,Qwen2.5-3B,Apple M1 (Metal)
2601.10103v1_FlowAct-R1 Towards Interactive Humanoid Video Gene.pdf,FlowAct-R1: Towards Interactive Humanoid Video Generation,"Lizhen Wang, Yongming Zhu, Zhipeng Ge, Youwei Zheng, Longhao Zhang, Tianshu Hu, Shiyang Qin, Mingshuang Luo, Jiaxu Zhang, Xin Chen, Yulong Wang, Zerong Zheng, Jianwen Jiang, Chao Liang, Weifeng Chen, Xing Wang, Yuan Zhang, Mingyuan Gao",Not found,2601.10103,"Humanoid video, Interactive video, Real-time synthesis, Diffusion models, Temporal consistency","This paper presents FlowAct-R1, a framework designed for real-time interactive humanoid video generation. Built on a MMDiT architecture, it enables streaming synthesis with low-latency responsiveness and achieves a stable 25fps at 480p resolution with a TTFF of around 1.5 seconds. The method provides fine-grained full-body control and demonstrates exceptional behavioral vividness and perceptual realism.",12.75,Qwen2.5-3B,Apple M1 (Metal)
2601.10104v1_MathDoc Benchmarking Structured Extraction and Act.pdf,MathDoc: Benchmarking Structured Extraction and Active Refusal on Noisy Mathematics Exam Papers,"Chenyue Zhou, Jiayi Tuo, Shitong Qin, Wei Dai, Mingxuan Wang, Ziwei Zhao, Duoyang Li, Shiyang Su, Yanxi Lu, Yanbiao Ma",Not provided,Not provided,"Mathematics exams, Document-level extraction, Active refusal, Noisy documents","The automated extraction of structured questions from paper-based mathematics exams is fundamental to intelligent education, yet remains challenging due to visual noise. MathDoc introduces the first benchmark for document-level information extraction from authentic high school mathematics exam papers, including unrecognizable samples to evaluate active refusal behavior. Experiments on state-of-the-art models show that while end-to-end models achieve strong extraction performance, they consistently fail to refuse illegible inputs, instead producing confident but invalid outputs. This highlights a critical gap in current MLLMs and establishes MathDoc as a benchmark for assessing model reliability under degraded document conditions.",11.1,Qwen2.5-3B,Apple M1 (Metal)
2601.10108v1_SIN-Bench Tracing Native Evidence Chains in Long-C.pdf,SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature,"Yiming Ren, Junjie Wang, Yuxin Meng, Yihang Shi, Zhiqiang Lin, Yurang Chu, Yiran Xu, Ziming Li, Yunfei Zhao, Zihan Wang, Yu Qiao, Ruiming Tang, Minghao Liu, Yujiu Yang",Not found,Not found,"Multimodal, Scientific Literature, Long-Context, Evidence Chains, Reasoning","Evaluating multimodal large language models (MLLMs) in understanding long-form scientific papers remains challenging. This paper proposes the 'Fish-in-the-Ocean' (FITO) paradigm, which requires models to construct explicit cross-modal evidence chains within native scientific documents. The authors build SIN-Data, a scientific interleaved corpus, and construct SIN-Bench with four progressive tasks covering evidence discovery, hypothesis verification, grounded QA, and evidence-anchored synthesis. Experiments on eight MLLMs show that grounding is the primary bottleneck, with Gemini-3-pro achieving the best overall score and GPT-5 having the highest SIN-QA answer accuracy but underperforming on evidence-aligned overall scores.",11.29,Qwen2.5-3B,Apple M1 (Metal)
2601.10112v1_Repository Intelligence Graph Deterministic Archit.pdf,Repository Intelligence Graph: Deterministic Architectural Map for LLM Code Assistants,"Tsvi Cherny-Shahar, Amiram Yehudai",Not found,Not found,"Repository aware coding agents, build systems, dependency graphs, software engineering agents, multi-lingual software","Repository aware coding agents often struggle to recover build and test structure, especially in multilingual projects. We introduce the Repository Intelligence Graph (RIG), a deterministic, evidence-backed architectural map representing buildable components, aggregators, runners, tests, external packages, and package managers, connected by explicit dependency and coverage edges. SPADE, a deterministic extractor, constructs RIG from build and test artifacts. We evaluate three commercial agents on eight repositories and find that providing RIG improves accuracy and reduces completion time.",11.07,Qwen2.5-3B,Apple M1 (Metal)
2601.10114v1_Following the Teachers Footsteps Scheduled Checkpo.pdf,Following the Teacher’s Footsteps: Scheduled Checkpoint Distillation for Domain-Specific LLMs,"Cheng Feng, Chaoliang Zhong, Jun Sun, Yusuke Oishi",Not provided,2601.10114,"LLMs, Knowledge Distillation, Domain-specific Tasks","This work proposes Scheduled Checkpoint Distillation (SCD), a novel approach to reduce the deficit of a student model on the Teacher-Favored Subdomain (TFS) during supervised fine-tuning (SFT) on domain tasks, thereby allowing the student model to match or even surpass the performance of its fine-tuned teacher.",11.11,Qwen2.5-3B,Apple M1 (Metal)
2601.10120v1_TopoDIM One-shot Topology Generation of Diverse In.pdf,TopoDIM: One-shot Topology Generation of Diverse Interaction Modes for Multi-Agent Systems,"Rui Sun, Jie Ding, Chenghua Gong, Tianjun Gu, Yihang Jiang, Juyuan Zhang, Liming Pan, Linyuan Lü",,,"multi-agent systems, topology generation, diverse interaction modes, one-shot topology, token efficiency, adaptive communication, decentralized execution","Optimizing communication topology in LLM-based multi-agent systems is critical for enabling collective intelligence. Existing methods mainly rely on spatio-temporal interaction paradigms, leading to high latency and computation costs. TOPODIM, a decentralized framework for one-shot topology generation with diverse interaction modes, enhances adaptability and privacy by enabling agents to autonomously construct heterogeneous communication without iterative coordination, achieving token efficiency and improved task performance. Experiments show that TOPODIM reduces total token consumption by 46.41% while improving average performance by 1.50% over state-of-the-art methods.",10.91,Qwen2.5-3B,Apple M1 (Metal)
2601.10122v1_Role-Playing Agents Driven by Large Language Model.pdf,"Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future Trends","Ye Wang, Jiaxing Chen, Hongjiang Xiao",Not found,2601.10122,"role-playing agents, large language models, natural language processing, human-computer interaction","This paper reviews the current development and key technologies of role-playing language agents, covering from early rule-based template paradigms to cognitive simulation centered on personality modeling and memory mechanisms. It also analyzes the methods and challenges of constructing role-specific corpora and evaluates multi-dimensional assessment frameworks and benchmark datasets.",11.08,Qwen2.5-3B,Apple M1 (Metal)
2601.10129v1_LaViT Aligning Latent Visual Thoughts for Multi-mo.pdf,LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning,"Linquan Wu, Tianxiang Jiang, Yifei Dong, Haoyu Yang, Fengji Zhang, Shichang Meng, Ai Xuan, Linqi Song, Jacky Keung",https://doi.org/10.1016/j.cviu.2025.09.001,2509.00123,"Knowledge Distillation, Latent Reasoning, Multi-modal Reasoning, Visual Attention, Language Prior","This work proposes LaViT, a framework that aligns latent visual thoughts rather than static embeddings, to enhance visual grounding and improve performance on complex reasoning tasks. It addresses the Perceptual Gap in distillation by compelling student models to autoregressively reconstruct the teacher’s visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show significant gains on complex reasoning tasks.",10.77,Qwen2.5-3B,Apple M1 (Metal)
2601.10130v1_Redundancy-Driven Top-k Functional Dependency Disc.pdf,Redundancy-Driven Top-k Functional Dependency Discovery,"Xiaolong Wan, Xixian Han",,,"Functional dependency, top-k discovery, data redundancy, pruning strategy","Functional dependencies are basic constraints in relational databases. Most discovery algorithms find all valid dependencies, but this causes two problems: high computational cost and a huge result set. We propose SDP (Selective-Discovery-and-Prune), which discovers the top-k FDs ranked by redundancy count, using an upper bound on redundancy to prune the search space. Experiments show that SDP is much faster and uses less memory than exhaustive methods.",9.52,Qwen2.5-3B,Apple M1 (Metal)
2601.10131v2_M4olGen Multi-Agent Multi-Stage Molecular Generati.pdf,"M4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints","Yizhan Li, Florence Cloutier, Sifan Wu, Ali Parviz, Boris Knyazev, Yan Zhang, Glen Berseth, Bang Liu",Not found,Not found,"molecular generation, multi-agent, multi-stage, precise constraints, numeric reasoning, reinforcement learning, fragment-level optimization","Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. M4olGen is a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. It leverages fragments and supports controllable refinement toward numeric targets, outperforming strong LLMs and graph-based algorithms in experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO).",10.78,Qwen2.5-3B,Apple M1 (Metal)
2601.10132v1_Is More Context Always Better Examining LLM Reason.pdf,Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction,"Yanan Cao∗, Farnaz Fallahi∗, Murali Mohana Krishna Dandu∗, Lalitesh Morishetti∗, Kai Zhao†, Luyi Ma, Sinduja Subramaniam, Jianpeng Xu, Evren Korpeoglu, Kaushiki Nag, Sushant Kumar, Kannan Achan",10.1145/XXXXXX.XXXXXX,,"Large Language Models, Temporal Reasoning, Inter-Purchase Interval Prediction","This paper investigates whether Large Language Models (LLMs) can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. It benchmarks state-of-the-art LLMs against both statistical and machine-learning models in a simple but representative repurchase scenario, revealing that while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, highlighting their limited ability to capture quantitative temporal structure. Additionally, moderate context can improve LLM accuracy, but adding further user-level detail degrades performance, challenging the assumption that 'more context leads to better reasoning.'",11.07,Qwen2.5-3B,Apple M1 (Metal)
2601.10137v1_Step-by-Step Causality Transparent Causal Discover.pdf,Step-by-Step Causality: Transparent Causal Discovery with Multi-Agent,"Ziyi Ding * 1, Chenfei Ye-Hao* 1, Zheyuan Wang 2, Xiao-Ping Zhang 1",Not found,Not found,"causal discovery, multi-agent, tree-query, adversarial confidence estimation","This paper introduces Tree-Query, a tree-structured, multi-expert LLM framework that reduces pairwise causal discovery to a short sequence of queries about backdoor paths, (in)dependence, latent confounding, and causal direction, yielding interpretable judgments with robustness-aware confidence scores. Theoretical guarantees are provided for asymptotic identifiability of four pairwise relations. On data-free benchmarks derived from Mooij et al. and UCI causal graphs, Tree-Query improves structural metrics over direct LLM baselines, and a diet–weight case study illustrates confounder screening and stable, high-confidence causal conclusions.",11.05,Qwen2.5-3B,Apple M1 (Metal)
2601.10141v1_Understanding and Preserving Safety in Fine-Tuned .pdf,Understanding and Preserving Safety in Fine-Tuned LLMs,"Jiawen Zhang, Zhejiang University, kevinzh@zju.edu.cn, Yangfan Hu, University of Wisconsin–Madison, yhu557@wisc.edu, Kejia Chen, Zhejiang University, chenkejia@zju.edu.cn, Lipeng He, University of Waterloo, lipeng.he@uwaterloo.ca, Jiachen Ma, Shanghai Artificial Intelligence Laboratory, majiachen@pjlab.org.cn, Jian Lou, Sun Yat-sen University, louj5@mail.sysu.edu.cn, Dan Li, Sun Yat-sen University, lidan263@mail.sysu.edu.cn, Jian Liu, Zhejiang University, liujian2411@zju.edu.cn, Xiaohu Yang, Zhejiang University, yangxh@zju.edu.cn, Ruoxi Jia, Virginia Tech, ruoxijia@vt.edu",Not found,Not found,"fine-tuning, large language models, safety alignment, gradient analysis, jailbreak attacks, downstream tasks","Fine-tuning large language models (LLMs) is essential for applying them to specific tasks. However, it can degrade safety alignment, especially when fine-tuning data is harmless. This work addresses the dilemma by shedding light on the geometric interaction between safety- and utility-oriented gradients in safety-aligned LLMs. It proposes SPF, a lightweight approach that removes gradient components conflicting with the low-rank safety subspace, ensuring utility convergence while bounding safety drift. SPF maintains downstream task performance and recovers nearly all pre-trained safety alignment, even under adversarial fine-tuning scenarios. It also exhibits robust resistance to deep fine-tuning and dynamic jailbreak attacks.",12.3,Qwen2.5-3B,Apple M1 (Metal)
2601.10143v1_History Is Not Enough An Adaptive Dataflow System .pdf,History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis,"Haochong Xia, Yao Long Teng, Regan Tan, Molei Qin, Xinrun Wang, Bo An",Not found,Not found,"Adaptive dataflow, Financial time-series, Data augmentation, Workflow automation, Concept drift","In quantitative finance, the gap between training and real-world performance—driven by concept drift and distributional non-stationarity—remains a critical obstacle for building reliable data-driven systems. This paper presents a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process, enabling provenance-aware replay and continuous data quality monitoring.",10.79,Qwen2.5-3B,Apple M1 (Metal)
2601.10148v1_DecisionLLM Large Language Models for Long Sequenc.pdf,DecisionLLM: Large Language Models for Long Sequence Decision Exploration,"Xiaowei Lv, Zhiling Zhang, Yijun Li, Yusen Huo, Siyuan Ju, Xuyan Li, Chunxiang Hong, Tianyu Wang, Peng Sun, Chuan Yu, Jian Xu, Bo Zheng",,,"reinforcement learning, long sequence decision-making, large language models, trajectory modeling, offline decision making","This work investigates the application of large language models (LLMs) to offline decision-making tasks, addressing the challenge of long-sequence decision-making where an agent must make coherent decisions over protracted time steps to achieve a long-term objective. The authors propose treating trajectories as a distinct modality and learning to align trajectory data with natural language task descriptions, enabling autoregressive prediction of future decisions within a cohesive framework termed DecisionLLM. They establish scaling laws governing this paradigm and demonstrate strong performance in offline experimental benchmarks and bidding scenarios.",10.72,Qwen2.5-3B,Apple M1 (Metal)
2601.10150v1_Simple Network Graph Comparative Learning.pdf,Simple Network Graph Comparative Learning,"Qiang Yu, Xinran Cheng, Shiqiang Xu, Chuanyi Li",,,"Filters, Siamese network, Graph contrastive learning, Unsupervised representation learning","This study proposes a novel node classification contrast learning method called Simple Network Graph Comparative Learning (SNGCL), which employs a superimposed multilayer Laplace smoothing filter to obtain global and local feature smoothing matrices, and uses an improved triple recombination loss function to bring intra-class distance closer and inter-class distance farther. The method is compared with state-of-the-art models in node classification tasks, and experimental results show that SNGCL is strongly competitive.",10.55,Qwen2.5-3B,Apple M1 (Metal)
2601.10154v1_MHub.ai A Simple Standardized and Reproducible Pla.pdf,"MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging","Leonard Nürnberg, Dennis Bontempi, Suraj Pai, Curtis Lisle, Steve Pieper, Ron Kikinis, Sil van Leemput, Rahul Soni, Gowtham Murugesan, Cosmin Ciausu, Miriam Groeneveld, Felix J. Dorfner, Jue Jiang, Aneesh Rangnekar, Harini Veeraraghavan, Joeran S. Bosma, Keno Bressem, Raymond Mak, Andrey Fedorov, Hugo JWL Aerts",,,"Artificial Intelligence, Medical Imaging, Reproducibility, Standardization, Platform","MHub.ai is a platform designed to facilitate the development, sharing, and reproducibility of AI models in medical imaging, aiming to standardize the process and ensure reproducibility in the field.",11.34,Qwen2.5-3B,Apple M1 (Metal)
2601.10155v1_LOOKAT Lookup-Optimized Key-Attention for Memory-E.pdf,LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers,Aryan Karmore,Not found,Not found,"Transformers, Quantization, Memory Efficiency, Attention Mechanism, Product Quantization, Asymmetric Distance Computation","LOOKAT is a method that applies product quantization and asymmetric distance computation to transformer architecture for efficient compression of key-value (KV) cache, transforming attention from memory-bound to compute-bound, suitable for edge devices.",9.48,Qwen2.5-3B,Apple M1 (Metal)
2601.10157v1_MMPG MoE-based Adaptive Multi-Perspective Graph Fu.pdf,MMPG: MoE-based Adaptive Multi-Perspective Graph Fusion for Protein Representation Learning,"Yusong Wang, Jialun Shen, Zhihao Wu, Yicheng Xu, Shiyin Tan, Mingkun Xu, Changshuo Wang, Zixing Song, Prayag Tiwari",,,"Protein Representation Learning, Graph Neural Networks, Multi-Perspective Fusion, Mixture of Experts","This paper proposes MMPG, a framework that constructs protein graphs from multiple perspectives and adaptively fuses them via Mixture of Experts (MoE) for Protein Representation Learning (PRL). MMPG addresses the limitation of current GNN-based PRL methods by integrating graphs from physical, chemical, and geometric perspectives to capture different properties of residue interactions. The MoE module dynamically routes perspectives to specialized experts, enabling the learning of intrinsic features and cross-perspective interactions. Quantitative verification shows that MoE automatically specializes experts in modeling distinct levels of interaction, from individual representations to pairwise inter-perspective synergies, and ultimately to a global consensus across all perspectives. This integration of multi-level information leads to superior protein representations and advanced performance on four different downstream protein tasks.",11.57,Qwen2.5-3B,Apple M1 (Metal)
2601.10160v1_Alignment Pretraining AI Discourse Causes Self-Ful.pdf,Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment,"Cameron Tice * 1, Puria Radmard * 1, Samuel Ratnam 3, Andy Kim 4, David Africa 2, Kyle O'Brien1",Not found,Not found,"alignment, pretraining, AI discourse, misalignment, self-fulfilling, LLMs","This paper investigates the hypothesis that pretraining language models with AI discourse can lead to self-fulfilling misalignment. It finds that discussion of AI misalignment contributes to misaligned behavior, while discussion of aligned behavior reduces misalignment scores. The findings suggest that pretraining data shapes alignment priors and provide evidence for the self-fulfilling alignment effect.",10.58,Qwen2.5-3B,Apple M1 (Metal)
2601.10161v1_AWED-FiNER Agents Web applications and Expert Dete.pdf,"A WED-FiNER: Agents, Web applications, and Expert Detectors for Fine-grained Named Entity Recognition across 36 Languages","Prachuryya Kaushik, Ashish Anand",,,"Named Entity Recognition, Fine-grained, 36 languages, Open-source, Web applications, Expert models, Fine-tuned models","A WED-FiNER is an open-source ecosystem designed to bridge the gap in Fine-grained Named Entity Recognition (FgNER) for 36 global languages spoken by over 6.6 billion people. It provides a collection of agentic toolkits, web applications, and state-of-the-art expert models for FgNER solutions across these languages. The agentic tools enable multilingual text routing to specialized expert models, and the web-based platforms provide ready-to-use FgNER annotation services for non-technical users. The collection of language-specific extremely small-sized open-source state-of-the-art expert models facilitates offline deployment in resource-constrained scenarios, including edge devices. AWED-FiNER covers languages spoken by over 6.6 billion people, including a specific focus on vulnerable languages such as Bodo, Manipuri, Bishnupriya, and Mizo.",11.16,Qwen2.5-3B,Apple M1 (Metal)
2601.10168v1_RAG-3DSG Enhancing 3D Scene Graphs with Re-Shot Gu.pdf,RAG-3DSG: ENHANCING3D SCENEGRAPHS WITH RE-SHOTGUIDEDRETRIEVAL-AUGMENTEDGENERATION,"Yue Chang, Rufeng Chen, Zhaofan Zhang, Yi Chen, Sihong Xie",Not found,Not found,"3D scene graphs, re-shot guided retrieval, object-level recognition, downsampling, aggregation noise",RAG-3DSG enhances 3D scene graphs by mitigating aggregation noise through re-shot guided uncertainty estimation and supporting object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. It accelerates cross-image object aggregation with adaptive granularity and significantly improves node captioning accuracy in 3DSG generation while reducing mapping time by two-thirds compared to the vanilla version.,10.83,Qwen2.5-3B,Apple M1 (Metal)
2601.10169v1_CtD Composition through Decomposition in Emergent .pdf,Composition through Decomposition: Achieving Zero-Shot Compositional Generalization in Neural Agents,"Boaz Carmeli, Ron Meir, Yonatan Belinkov",,,"compositionality, neural networks, zero-shot learning, multi-target coordination, referential game","This study demonstrates how artificial neural agents acquire and utilize compositional generalization to describe previously unseen images. The method involves two sequential training steps: decomposing images into basic concepts and composing these concepts into complex phrases. Remarkably, zero-shot generalization is observed in the composition step without additional training.",9.54,Qwen2.5-3B,Apple M1 (Metal)
2601.10173v1_ReasAlign Reasoning Enhanced Safety Alignment agai.pdf,ReasAlign: Reasoning Enhanced Safety Alignment against Prompt Injection Attack,"Hao Li, Yankai Yang, G. Edward Suh, Ning Zhang, Chaowei Xiao",Not found,Not found,"Safety, Alignment, Reasoning, Prompt Injection, Security","Large Language Models (LLMs) have enabled the development of powerful agentic systems capable of automating complex workflows across various fields. However, these systems are highly vulnerable to indirect prompt injection attacks, where malicious instructions embedded in external data can hijack agent behavior. This work presents ReasAlign, a model-level solution to improve safety alignment against indirect prompt injection attacks. The core idea of ReasAlign is to incorporate structured reasoning steps to analyze user queries, detect conflicting instructions, and preserve the continuity of the user’s intended tasks to defend against indirect injection attacks. Comprehensive evaluations across various benchmarks show that ReasAlign maintains utility comparable to an undefended model while consistently outperforming Meta SecAlign, the strongest prior guardrail. On the representative open-ended CyberSecEval2 benchmark, ReasAlign achieves 94.6% utility and only 3.6% ASR, far surpassing the state-of-the-art defensive model of Meta SecAlign (56.4% utility and 74.4% ASR). These results demonstrate that ReasAlign achieves the best trade-off between security and utility, establishing a robust and practical defense against prompt injection attacks in real-world agentic systems.",11.72,Qwen2.5-3B,Apple M1 (Metal)
2601.10187v1_HOMURA Taming the Sand-Glass for Time-Constrained .pdf,HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning,"Ziang Cui, Mengran Yu, Tianjiao Li, Chenyu Shi, Yingxuan Shi, Lusheng Zhang, Hongwei Lin",Not found,Not found,"Large Language Models, Reinforcement Learning, Time-Constrained Translation, Syllable-Level Duration Constraints","Large Language Models achieve remarkable semantic adequacy in multilingual translation but are hindered by a cross-lingual verbosity bias. Current prompt-engineering approaches struggle to resolve the conflict between semantic fidelity and rigid temporal feasibility. This paper introduces Sand-Glass, a benchmark for evaluating translation under syllable-level duration constraints, and proposes HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a dynamic syllable-ratio reward, HOMURA effectively controls output length, significantly outperforming strong LLM baselines in time-constrained scenarios.",11.5,Qwen2.5-3B,Apple M1 (Metal)
2601.10191v1_How does downsampling affect needle electromyograp.pdf,How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series,"Mathieu J.L. Cherpitel, Janne A.M. Luijten, Thomas H.W. Bäck, Camiel Verhamme, Martijn R. Tannemaat, Anna V. Kononova",,,"needle electromyography, downsampling, neuromuscular diseases, machine learning, feature-based models, high-frequency time series",This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. The workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load.,11.34,Qwen2.5-3B,Apple M1 (Metal)
2601.10193v1_GFM4GA Graph Foundation Model for Group Anomaly De.pdf,GFM4GA: Graph Foundation Model for Group Anomaly Detection,"Jiujiu Chen, Weijun Zeng, Shaofeng Hu, Sihong Xie∗, Hui Xiong",XXXXXXX.XXXXXXX,,"Group Anomaly Detection, Graph Foundation Model, Graph Contrastive Learning","Group anomaly detection is crucial in many network applications, but faces challenges due to diverse anomaly patterns. GFM4GA, a novel graph foundation model, is proposed to handle group anomaly detection. The model is pretrained via dual-level contrastive learning and finetuned in parameter-constrained and group-anomaly-proportion weighted few-shot settings, expanding its adaptive ability to unseen group anomalies via group contexts determined by labeled anomaly neighbors. Experiments show that GFM4GA surpasses group anomaly detectors and GFMs for individual anomalies.",10.25,Qwen2.5-3B,Apple M1 (Metal)
2601.10201v1_PRL Process Reward Learning Improves LLMs Reasonin.pdf,PRL: Process Reward Learning Improves LLMs’ Reasoning Ability,"Jiarui Yao, Ruida Wang, Tong Zhang",Not provided,Not provided,"Large Language Models, Reinforcement Learning, Process Reward Learning, Fine-grained Supervision, Efficiency","This paper proposes Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. It demonstrates that PRL not only improves the average performance for LLMs' reasoning ability but also broadens the reasoning boundary by improving the pass @ n metric.",10.34,Qwen2.5-3B,Apple M1 (Metal)
2601.10205v1_One Instruction Does Not Fit All How Well Do Embed.pdf,One Instruction Does Not Fit All: How Well Do Embeddings Align,"Arya Shah, Himanshu Beniwal, Mayank Singh",,,"Embeddings, Persona, Instruction, Multilingual, Indian Languages, Retrieval, Classification","Aligning multilingual assistants with culturally grounded user preferences is essential for serving India's linguistically diverse population. This paper presents a unified benchmark spanning 12 Indian languages and four evaluation tasks, evaluating eight multilingual embedding models in a frozen-encoder setting with a thin logistic regression head for classification. E5-Large-Instruct achieves the highest Recall@1 of 27.4% on monolingual retrieval and 20.7% on cross-lingual transfer, while BGE-M3 leads reverse retrieval at 32.1% Recall@1. For classification, LaBSE attains 75.3% AUROC with strong calibration. These findings offer practical guidance for model selection in Indic multilingual retrieval and establish reproducible baselines for future work.",10.85,Qwen2.5-3B,Apple M1 (Metal)
2601.10212v1_PADER Paillier-based Secure Decentralized Social R.pdf,PADER: Paillier-based Secure Decentralized Social Recommendation,"Chaochao Chen, Jiaming Qian, Fei Zheng∗, Yachuan Liu",Not found,Not found,"Paillier Cryptosystem, Secure Computation, Recommendation System","The paper proposes PADER, a Paillier-based secure decentralized social recommendation system, which keeps user and seller data private by operating in a decentralized manner without a centralized platform. It applies the Paillier cryptosystem to the SoReg model, which combines user ratings and social relations, and designs secure addition and multiplication protocols for efficient secure computation.",10.17,Qwen2.5-3B,Apple M1 (Metal)
2601.10215v1_Topo-RAG Topology-aware retrieval for hybrid text-.pdf,TOPO-RAG: TOPOLOGY-AWARE RETRIEVAL FOR HYBRID TEXT-TABLE DOCUMENTS,"Alex Dantart∗, Marco K""ovacs-Navarro",Not found,arXiv:2601.10215v1,"Retrieval-Augmented Generation (RAG), table retrieval, late interaction, multivector retrieval, enterprise search, heterogeneous data, semantic routing, structure-aware embeddings, Topo-RAG, ColBERT, cell-aware interaction, linearization bottleneck","In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. This work presents Topo-RAG, a framework that challenges the assumption that 'everything is text.' We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.",12.05,Qwen2.5-3B,Apple M1 (Metal)
2601.10222v1_Introduction to optimization methods for training .pdf,Introduction to optimization methods for training SciML models,"Alena Kopaničáková∗, Elisa Riccietti †",,2601.10222,"machine learning, scientific machine learning, optimization, stochastic optimization, first-order methods, second-order methods, empirical risk minimization, physics informed, operator constrained, partial differential equations, stochastic gradient descent, AdaGrad, Adam","Optimization is the foundation of modern machine learning (ML). Historically, optimization methods have been categorized by the degree to which they exploit derivative information. In the context of scientific machine learning (SciML), the optimization landscape changes, with losses often non-decomposable due to physics informed or operator constrained formulations, leading to global spatio-temporal coupling.",11.37,Qwen2.5-3B,Apple M1 (Metal)
2601.10236v1_Who Owns the Text Design Patterns for Preserving A.pdf,Who Owns the Text? Design Patterns for Preserving Authorship in AI-Assisted Writing,"Bohan Zhang1, Chengke Bu2, and Paramveer Dhillon∗1",,2601.10236,"AI-assisted writing, human–AI collaboration, psychological ownership, personalization, provenance","AI writing assistants can reduce effort and improve fluency, but they may also weaken writers' sense of authorship. This study examines the tension with an ownership-aware co-writing editor that offers on-demand, sentence-level suggestions and tests two common design choices: persona-based coaching and style personalization. Across the two AI-assisted tasks, psychological ownership dropped relative to unassisted writing, even as cognitive load decreased and quality ratings stayed broadly similar overall. Persona coaching did not prevent the ownership decline, while style personalization partially restored ownership and increased AI incorporation in text.",11.37,Qwen2.5-3B,Apple M1 (Metal)
2601.10242v1_Loop as a Bridge Can Looped Transformers Truly Lin.pdf,LOOP AS ABRIDGE: CANLOOPEDTRANSFORMERS TRULYLINKREPRESENTATIONSPACE ANDNATURAL LANGUAGEOUTPUTS?,"Guanxu Chen, Dongrui Liu, Jing Shao",Not found,Not found,"Large Language Models, Looped Transformers, Introspection, Representation Space, Natural Language Outputs","This report empirically investigates whether Looped Transformers (LTs), which iterate shared layers to increase computational depth, can bridge the gap between their internal 'knowledge' and explicit linguistic outputs. Experiments show that while increasing loop iterations narrows the gap, it is partly driven by a degradation of internal 'knowledge' carried by representations. Another empirical analysis suggests that LTs' ability to perceive representations does not improve across loops; it is only present in the final loop. These findings suggest that LTs offer a promising direction for scaling computational depth but have yet to achieve the introspection required to truly link representation space and natural language.",10.71,Qwen2.5-3B,Apple M1 (Metal)
2601.10245v1_TRIM Hybrid Inference via Targeted Stepwise Routin.pdf,TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks,"Vansh Kapoor†1, Aman Gupta 2, Hao Chen 2, Anurag Beniwal 2, Jing Huang 2, Aviral Kumar1",Not found,Not found,"Large Language Models, Multi-step Reasoning, Stepwise Routing, Inference Efficiency, Cascading Failures","TRIM proposes a hybrid inference method for multi-step reasoning tasks, routing only critical steps to larger models while letting smaller models handle routine continuations. This approach aims to improve inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors.",10.42,Qwen2.5-3B,Apple M1 (Metal)
2601.10251v1_X-SAM Boosting Sharpness-Aware Minimization with D.pdf,X-SAM: Boosting Sharpness-Aware Minimization with Dominant-Eigenvector Gradient Correction,"Hongru Duan, Yongle Chen, Lei Guan *",Not found,Not found,"Sharpness-Aware Minimization, Hessian, Eigenvalue, Gradient Correction, Generalization","X-SAM aims to improve generalization by correcting the gradient via orthogonal decomposition along the top eigenvector, enabling more direct and efficient regularization of the Hessian's maximum eigenvalue.",9.55,Qwen2.5-3B,Apple M1 (Metal)
2601.10254v1_NoReGeo Non-Reasoning Geometry Benchmark.pdf,NoReGeo: Non-Reasoning Geometry Benchmark,"Irina Abdullaeva1,2, Anton Vasiliuk1, Elizaveta Goncharova1,3, Temurbek Rahmatullaev1,4, Zagorulko Ivan5, Maxim Kurkin1, 6, Andrey Kuznetsov1,2",Not found,Not found,"Geometry, Large Language Models, Non-Reasoning, CAD, Robots, Geospatial Systems","NoReGeo is a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models without relying on reasoning or algebraic computation. It comprises 2,500 trivial geometric problems spanning 25 categories, focusing on whether models can inherently encode spatial relationships and recognize geometric properties directly. The benchmark demonstrates that even advanced models achieve only 65% accuracy in binary classification tasks, highlighting a significant gap in current LLMs' ability to natively grasp geometric concepts.",10.95,Qwen2.5-3B,Apple M1 (Metal)
2601.10257v1_Untangling Input Language from Reasoning Language .pdf,Untangling Input Language from Reasoning Language: A Diagnostic Framework for Cross-Lingual Moral Alignment in LLMs,"Nan Li, Bo Kang, Tijl De Bie",Not provided,Not provided,"Moral alignment, Cross-lingual evaluation, Moral foundations, Language effects, Reasoning language, Input language","This paper introduces a methodology to diagnose cross-lingual inconsistencies in moral judgments made by large language models (LLMs). It separates the effects of the language of the dilemma and the language in which the model reasons, enabling a more nuanced understanding of why LLMs may reach different conclusions in different languages.",10.35,Qwen2.5-3B,Apple M1 (Metal)
2601.10272v1_MoST Mixing Speech and Text with Modality-Aware Mi.pdf,MOST: MIXINGSPEECH ANDTEXT WITHMODALITY-,"A. Yuxuan Lou, Kai Yang, Yang You",,2601.10272,"Multimodal Learning, Speech and Text, Modality-Aware Mixture of Experts, Large Language Model","We present MoST, a novel multimodal large language model that integrates speech and text through a Modality-Aware Mixture of Experts (MAMoE) architecture, enhancing modality-specific learning and cross-modal understanding.",9.95,Qwen2.5-3B,Apple M1 (Metal)
2601.10274v1_Queueing-Aware Optimization of Reasoning Tokens fo.pdf,Queueing-Aware Optimization of Reasoning Tokens for Accuracy-Latency Trade-offs in LLM Servers,"Emre Ozbas, Melih Bastopcu",,,"Large language models, Inference servers, Token budget, Accuracy-latency trade-offs, Queueing theory","This paper addresses the optimization of reasoning tokens in a single large language model (LLM) server serving a heterogeneous stream of queries. It considers a first-in, first-out (FIFO) service discipline and models the system as an M/G/1 queue. The authors formulate a constrained optimization problem to maximize a weighted average accuracy objective penalized by the mean system time, subject to architectural token-budget constraints and queue-stability conditions. They develop a projected gradient method to guarantee convergence beyond the contractive regime and round the continuous solution to attain integer-valued token allocations. The resulting performance loss is evaluated in simulation results.",10.79,Qwen2.5-3B,Apple M1 (Metal)
2601.10282v2_SPIKE Sparse Koopman Regularization for Physics-In.pdf,SPIKE: Sparse Koopman Regularization for Physics-Informed Neural Networks,"Jose Marie Antonio Miñoza, Center for AI Research PH",Not found,2601.10282,"Physics-Informed Neural Networks (PINNs), Koopman operators, Sparse learning, Stiff PDEs, Fluid dynamics, Chaotic ODEs","This work presents SPIKE, a framework that regularizes Physics-Informed Neural Networks (PINNs) with continuous-time Koopman operators to learn parsimonious dynamics representations. Experiments across various PDEs demonstrate consistent improvements in temporal extrapolation, spatial generalization, and long-term prediction accuracy.",10.42,Qwen2.5-3B,Apple M1 (Metal)
2601.10305v1_DanQing An Up-to-Date Large-Scale Chinese Vision-L.pdf,DanQingTechnical Report,"Glint Lab, Hengyu Shen∗, Tiancheng Gu∗, Bin Qin, Lan Wu, Yuling Wu, Shuo Tan, Zelong Sun, Jun Wang, Nan Wu, Xiang An, Weidong Cai, Ziyong Feng‡, Kaicheng Yang†, DanQingTeam",Not found,2601.10305,"Chinese vision-language pretraining, DanQing dataset, Contrastive learning, Cross-modal retrieval, Semantic segmentation, Image captioning","This report introduces DanQing, a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. DanQing contains 100 million image-text pairs collected from Common Crawl, with a focus on data quality and capturing evolving semantic trends. The report also compares DanQing with existing datasets and demonstrates its superior performance in various Chinese downstream tasks.",10.84,Qwen2.5-3B,Apple M1 (Metal)
2601.10306v1_Evidence-Augmented Policy Optimization with Reward.pdf,Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning,"Xin Guan*, Zijian Li, Shen Huang, Pengjun Xie, Jingren Zhou, Jiuxin Cao",,,"Reinforcement Learning, Policy Optimization, Long-Context Reasoning, Reward Co-Evolution, Evidence-Augmented Reasoning","Proposes EAPO (Evidence-Augmented Policy Optimization) to enhance long-context reasoning in reinforcement learning, addressing the challenge of sparse outcome rewards by introducing a reward model that computes a Group-Relative Evidence Reward for dense process supervision. The mechanism iteratively refines the reward model using outcome-consistent rollouts, ensuring precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate significant performance improvements over state-of-the-art baselines.",10.16,Qwen2.5-3B,Apple M1 (Metal)
2601.10338v1_Agent Skills in the Wild An Empirical Study of Sec.pdf,Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale,"Yi Liu∗, Weizhe Wang∗, Ruitao Feng†, Yao Zhang†, Guangquan Xu†, Gelei Deng, Yuekang Li, Leo Zhang",10.1145/nnnnnnn.nnnnnnn,,"Agent skills, AI security, vulnerability analysis, supply chain security, prompt injection, large language models","The paper conducts the first large-scale empirical security analysis of agent skills, revealing pervasive security risks such as 26.1% of skills containing at least one vulnerability, spanning 14 distinct patterns across four categories. The findings include high-severity patterns strongly suggesting malicious intent, and a detection methodology achieving 86.7% precision and 82.5% recall. The paper also includes a grounded vulnerability taxonomy and an open dataset and detection toolkit for future research.",10.61,Qwen2.5-3B,Apple M1 (Metal)
2601.10342v1_C-GRASP Clinically-Grounded Reasoning for Affectiv.pdf,C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing,"1st Cheng Lin Cheng, 2nd Ting Chuan Lin, 3rd Chai Kai Chang",,,"Large language model, clinical decision support, heart rate variability, retrieval-augmented generation, explainable AI, guardrails","Heart rate variability (HRV) is a pivotal non-invasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations, where models struggle with respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP, a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models achieved superior performance in 4-class emotion classification and achieved a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the 'population bias' common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.",12.05,Qwen2.5-3B,Apple M1 (Metal)
2601.10343v2_OctoBench Benchmarking Scaffold-Aware Instruction .pdf,OCTOBENCH: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding,"Deming Ding, Shichun Liu, Enhui Yang, Jiahang Lin, Ziying Chen, Shihan Dou, Honglin Guo, Weiyu Cheng, Pengyu Zhao, Chengjun Xiao, Qunhong Zeng, Qi Zhang, Xuanjing Huang, Qidi Xu, Tao Gui",Not found,Not found,"instruction following, scaffold, agentic coding, benchmarking, heterogeneous constraints","Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined. OCTOBENCH benchmarks scaffold-aware instruction following in repository-grounded agentic coding, providing an automated observation-and-scoring toolkit to disentangle solving the task from following the rules. Experiments reveal a systematic gap between task-solving and scaffold-aware compliance, highlighting the need for training and evaluation that explicitly targets heterogeneous instruction following.",10.67,Qwen2.5-3B,Apple M1 (Metal)
2601.10348v1_Training-Trajectory-Aware Token Selection.pdf,Training-Trajectory-Aware Token Selection,"Zhanming Shen, Jiaqi Hu, Zeyu Qin, Hao Chen, Wentao Ye, Zenan Huang, Yihong Zhuang, Guoshan Lu, Junlin Zhou, Junbo Zhao",,,"Continual Learning, Efficient Distillation, Token Selection, Training Trajectory","Efficient distillation is a key pathway for converting expensive reasoning capability into deployable efficiency. However, in the frontier regime where the student already has strong reasoning ability, naive continual distillation often yields limited gains or even degradation. We observe a characteristic training phenomenon: even as loss decreases monotonically, all performance metrics can drop sharply at almost the same bottleneck, before gradually recovering. We propose Training-Trajectory-Aware Token Selection (T3S) to reconstruct the training objective at the token level, clearing the optimization path for yet-to-learn tokens. T3 yields consistent gains in both AR and dLLM settings.",10.61,Qwen2.5-3B,Apple M1 (Metal)
2601.10349v1_SuS Strategy-aware Surprise for Intrinsic Explorat.pdf,SuS: Strategy-aware Surprise for Intrinsic Exploration,"Mark Kashirskiy1,2, Ilya Makarov1",Not found,Not found,"intrinsic motivation, reinforcement learning, exploration, contrastive learning","We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework that uses pre-post prediction mismatch as a novelty signal for exploration in reinforcement learning. SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). Our combined reward formulation leverages both signals through learned weighting coefficients. We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in both accuracy and solution diversity.",10.28,Qwen2.5-3B,Apple M1 (Metal)
2601.10373v1_Towards Efficient Low-rate Image Compression with .pdf,Towards Efficient Low-rate Image Compression with Frequency-aware Diffusion Prior Refinement,"Yichong Xia, Yimin Zhou, Jinpeng Wang, Bin Chen",Not found,Not found,"image compression, diffusion models, frequency-aware, low-bitrate, reconstruction","This work proposes DiffCR, a novel compression framework for efficient and high-fidelity image reconstruction. It uses a Frequency-aware Skip Estimation (FaSE) module to refine the ϵ-prediction prior from a pre-trained latent diffusion model and aligns it with compressed latents at different timesteps via Frequency Decoupling Attention (FDA). This enables fast two-step decoding and substantial bitrate savings compared to state-of-the-art diffusion-based compression baselines.",9.66,Qwen2.5-3B,Apple M1 (Metal)
2601.10378v2_Global Context Compression with Interleaved Vision.pdf,Global Context Compression with Interleaved Vision-Text Transformation,"Dian Jiao*, Jiaxin Duan*, Shuai Zhao, Jiabing Leng, Yiran Zhang, Feng Huang*",,,"Context Compression, Transformer, Vision-Language Models, Optical Character Recognition, Hierarchical Encoding, Sparse Attention","This paper investigates global context compression, proposing VIST2, a novel Transformer that interleaves input text chunks alongside their visual encoding. The model saves tokens at both prefilling and inference stages, achieving significant improvements in speed and memory usage for long writing tasks.",9.7,Qwen2.5-3B,Apple M1 (Metal)
2601.10386v1_Handling Missing Modalities in Multimodal Survival.pdf,Handling Missing Modalities in Multimodal Survival Prediction for Non-Small Cell Lung Cancer,"Filippo Ruffini, Camillo Maria Caruso, Claudia Tacconi, Lorenzo Nibid, Francesca Miccolis, Marta Lovino, Carlo Greco, Edy Ippolito, Michele Fiore, Alessio Cortellini, Bruno Beomonte Zobel, Giuseppe Perrone, Bruno Vincenzi, Claudio Marrocco, Alessandro Bria, Elisa Ficarra, Sara Ramella, Valerio Guarrasi, Paolo Soda",Not found,2601.10386,"Survival analysis, Multimodal data, Non-small cell lung cancer, Missing data","This paper addresses the challenge of handling missing modalities in predicting survival outcomes for non-small cell lung cancer patients, utilizing a combination of multimodal data sources.",11.99,Qwen2.5-3B,Apple M1 (Metal)
2601.10398v2_LatentRefusal Latent-Signal Refusal for Unanswerab.pdf,LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries,"Xuancheng Ren, Shijing Hu, Zhihui Lu, Jiangqi Huang, Qiang Duan",Not provided,Not provided,"Text-to-SQL, Unanswerable Queries, Safety, Large Language Models, Schema Mismatch, Query Answerability","In LLM-based Text-to-SQL systems, unanswerable and underspecified user queries can generate incorrect text and executable programs that yield misleading results or violate safety constraints. This paper formalizes safe refusal in Text-to-SQL systems as an answerability-gating problem and proposes LATENTREFUSAL, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of an LLM. The Tri-Residual Gated Encoder (TRGE) is introduced to suppress schema noise and amplify sparse, localized question-schema mismatch cues. Extensive evaluations demonstrate the effectiveness of the proposed scheme and show that LATENTREFUSAL provides an attachable, efficient safety layer for Text-to-SQL systems.",11.08,Qwen2.5-3B,Apple M1 (Metal)
2601.10402v1_Toward Ultra-Long-Horizon Agentic Science Cognitiv.pdf,Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering,"Xinyu Zhu, Yuzhu Cai, Zexi Liu, Bingyang Zheng, Cheng Wang, Rui Ye, Jiaao Chen, Hanrui Wang, Wei-Chen Wang, Yuzhi Zhang, Linfeng Zhang, Weinan E, Di Jin, Siheng Chen",Not found,2601.10402,"agentic science, machine learning engineering, cognitive accumulation, ultra-long-horizon autonomy, context management, cognitive caching, multi-tiered architecture","The advancement of artificial intelligence toward agentic science is bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. ML-Master 2.0, an autonomous agent, masters ultra-long-horizon machine learning engineering, which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, HCC (Hierarchical Cognitive Caching) allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming scaling limits of static context windows.",11.44,Qwen2.5-3B,Apple M1 (Metal)
2601.10406v1_ErrEval Error-Aware Evaluation for Question Genera.pdf,ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics,"Weiping Fu, Bifan Wei, Jingyi Hao, Yushun Zhang, Jian Zhang, Jiaxin Wang, Bo Li, Yu He, Lingling Zhang, Jun Liu",,,"Question Generation, Error Detection, Evaluation Framework, Natural Language Processing","ErrEval is a flexible and error-aware evaluation framework for Question Generation (QG) that enhances evaluation through explicit error diagnostics, improving alignment with human judgments and mitigating overestimation of low-quality questions.",9.77,Qwen2.5-3B,Apple M1 (Metal)
2601.10413v1_LADFA A Framework of Using Large Language Models a.pdf,LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies,"Haiyue Yuan∗, Nikolay Matyunin, Ali Raza, Shujun Li∗",XXXXXXX.XXXXXXX,,"Large Language Model, Privacy Policy, Text Analysis, Data Flows, Privacy, Security, Retrieval-Augmented Generation, Framework, Automotive Industry, Connected Vehicle","This paper presents the development of LADFA, an end-to-end computational framework for processing unstructured text in privacy policies, extracting personal data flows, and conducting analysis of the data flow graph to facilitate insight discovery. It combines LLMs with retrieval-augmented generation and a customised knowledge base derived from existing studies.",10.65,Qwen2.5-3B,Apple M1 (Metal)
2601.10416v1_LLMdoctor Token-Level Flow-Guided Preference Optim.pdf,LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models,"Tiesunlong Shen, Rui Mao, Jin Wang, Heming Sun, Jian Zhang, Xuejie Zhang, Erik Cambria",,,"Large Language Models, Test-Time Alignment, Fine-Tuning, Token-Level Optimization, Flow-Guided, Preference Optimization","This paper introduces LLMdoctor, a novel framework for efficient test-time alignment of large language models (LLMs). Unlike traditional fine-tuning methods, LLMdoctor integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. It extracts fine-grained, token-level preference signals from the patient model's behavioral variations and guides the training of the doctor model via TFPO, enabling precise token-by-token alignment while preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.",10.92,Qwen2.5-3B,Apple M1 (Metal)
2601.10421v1_Are Language Models Models.pdf,Are Language Models Models?,"Philip Resnik, Department of Linguistics and Institute for Advanced Computer Studies, University of Maryland",http://doi.org/10.1017/S0140525X2510112X,,"Language models, Cognitive models, Model systems, Algorithmic-representational level, Computational theory","Futrell and Mahowald claim that language models serve as model systems, but the authors argue that this claim is not true at the implementation level, poorly motivated at the algorithmic-representational level, and problematic at the computational theory level.",13.32,Qwen2.5-3B,Apple M1 (Metal)
2601.10436v1_Development of Ontological Knowledge Bases by Leve.pdf,Development of Ontological Knowledge Bases by Leveraging Large Language Models,"LE Ngoc Luyen, Marie-Hélène ABEL, Philippe GOUSPILLiou",,,"Ontology Development, Ontological Knowledge bases, Large Language Models, Knowledge Representation, User Modeling, Knowledge Management","This paper introduces a structured, iterative methodology leveraging Large Language Models (LLMs) to optimize knowledge acquisition, automate ontology artifact generation, and enable continuous refinement cycles in the development of Ontological Knowledge Bases (OKBs). It demonstrates this approach through a detailed case study focused on developing a user context profile ontology within the vehicle sales domain, highlighting significant improvements in scalability, consistency, bias mitigation, and transparency in ontology engineering.",11.95,Qwen2.5-3B,Apple M1 (Metal)
2601.10440v1_AgentGuardian Learning Access Control Policies to .pdf,Learning Access Control Policies to Govern AI Agent Behavior,"Nadya Abaev*, Denis Klimov *, Gerard Levinov, David Mimran, Yuval Elovici, Asaf Shabtai",,,"Security, AI Agents, Access Control Policies, Control Flow Graph","This study introduces AgentGuardian, a novel security framework that governs and protects AI agent operations by enforcing context-aware access-control policies. It monitors execution traces to learn legitimate agent behaviors and input patterns, derives adaptive policies, and evaluates across two real-world AI agent applications, demonstrating effective detection of malicious or misleading inputs while preserving normal agent functionality.",10.54,Qwen2.5-3B,Apple M1 (Metal)
2601.10457v1_NSR-Boost A Neuro-Symbolic Residual Boosting Frame.pdf,NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models,"Ziming Dai∗, Dabiao Ma∗, Jinle Tong, Mengyuan Han, Jian Yang, Haojun Fei",10.1145/nnnnnnn.nnnnnnn,,"Neuro-Symbolic AI, Large Language Models, Gradient Boosting, Legacy Model, Interpretability","Presenting NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios, which addresses the prohibitive retraining costs and systemic risks of upgrading legacy models in high-concurrency production environments. The framework comprises three key stages: finding hard regions through residuals, generating interpretable experts using Large Language Model and fine-tuning parameters using Bayesian optimization, and dynamically integrating experts with legacy model output through a lightweight aggregator. Successful deployment within the core financial risk control system at Qfin Holdings demonstrates significant performance gains across six public datasets and one private dataset, and excellent performance on real-world online data. It effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.",11.26,Qwen2.5-3B,Apple M1 (Metal)
2601.10460v1_Contextual StereoSet Stress-Testing Bias Alignment.pdf,Contextual StereoSet: Stress-Testing Bias Alignment Robustness in Large Language Models,"Abhinaba Basu, Pavan Chakraborty",Not found,2601.10460,"bias evaluation, alignment robustness, stress-testing, large language models, so-cientific context, StereoSet","A model's bias can shift dramatically based on contextual factors such as place, time, and audience. Contextual StereoSet is introduced as a benchmark to systematically vary these factors, revealing striking patterns in bias shifts across different models and scenarios.",11.19,Qwen2.5-3B,Apple M1 (Metal)
2601.10462v3_ChartComplete A Taxonomy-based Inclusive Chart Dat.pdf,ChartComplete: A Taxonomy-based Inclusive Chart Dataset,"Ahmad Mustapha, Charbel Toumieh, Mariette Awad",Not provided,2601.10462,"Chart, Dataset, Chart Taxonomy, Chart Classification","With advancements in deep learning and computer vision techniques, the field of chart understanding is evolving rapidly. To accurately measure the performance of multi-modal large language models, the research community has developed multiple datasets. However, these datasets are limited to a small set of chart types. The ChartComplete dataset, based on a chart taxonomy borrowed from the visualization community, covers thirty different chart types and is a collection of classified chart images without a learning signal.",10.9,Qwen2.5-3B,Apple M1 (Metal)
2601.10477v1_Urban Socio-Semantic Segmentation with Vision-Lang.pdf,URBANSOCIO-SEMANTICSEGMENTATION WITH VISION-LANGUAGEREASONING,"Yu Wang, Yi Wang, Rui Dai, Yujie Wang, Kaikui Liu, Xiangxiang Chu, Yansheng Li",Not found,Not found,"Urban, Semantic Segmentation, Vision-Language Reasoning, Socio-Semantic Entities, Reinforcement Learning","This work achieves socio-semantic segmentation by vision-language model reasoning, introducing the Urban Socio-Semantic Segmentaion dataset and a novel vision-language reasoning framework called SocioReasoner. Experiments demonstrate gains over state-of-the-art models and strong zero-shot generalization.",10.49,Qwen2.5-3B,Apple M1 (Metal)
2601.10485v1_Panning for Gold Expanding Domain-Specific Knowled.pdf,Panning for Gold: Expanding Domain-Specific Knowledge,"Runhao Zhao, Weixin Zeng, Wentao Zhang, Chong Chen, Zhengpin Li, Xiang Zhao, Lei Chen",,,"Domain-specific Knowledge Graph Fusion, Knowledge Graph Enrichment, General-to-domain Knowledge Transfer, Fact-as-Program","This paper proposes a new task called domain-specific knowledge graph fusion (DKGF) to enhance the completeness and utility of domain-specific knowledge graphs (DKGs) by integrating relevant facts from general knowledge graphs (GKGs). It addresses the challenges of high ambiguity in determining domain relevance and cross-domain knowledge granularity misalignment. The authors introduce ExeFuse, a fact-as-program paradigm that reformulates DKGF as executable semantic reasoning over DKGs, enabling precise identification and integration of domain-relevant, consistent knowledge from GKGs. Two benchmark datasets and 21 representative configurations are proposed for systematic assessment of DKGF performance.",11.08,Qwen2.5-3B,Apple M1 (Metal)
2601.10496v1_Model See Model Do Exposure-Aware Evaluation of Bu.pdf,"Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs","Ali Al-Kaswan, Claudio Spiess, Prem Devanbu, Arie van Deursen, Maliheh Izadi",10.1145/nnnnnnn.nnnnnnn,,"Large Language Models, bugs, fixes, Memorisation","This paper introduces an exposure-aware evaluation framework to quantify how prior exposure to buggy versus fixed code influences a model's preference. Using the ManySStuBs4J benchmark and Data Portraits for membership testing, the authors estimate model preference in code completion and likelihood-based scoring metrics, finding that models tend to reproduce buggy lines more often than fixes, with exposure to bugs amplifying this tendency.",10.48,Qwen2.5-3B,Apple M1 (Metal)
2601.10498v1_Projected Microbatch Accumulation yields reference.pdf,Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning,Nilin Abrahamsen,Not found,2601.10498,"Reinforcement Learning, Proximal Policy Optimization, Fine-tuning, Large Language Models","This note introduces Projected Microbatch Accumulation (PROMA), a method for large language model fine-tuning that modifies proximal policy updates without a reference policy or likelihood-ratio clipping, resulting in more stable policy learning.",9.25,Qwen2.5-3B,Apple M1 (Metal)
2601.10511v1_Scalable Algorithms for Approximate DNF Model Coun.pdf,Scalable Algorithms for Approximate DNF Model Counting,"Paul Burkhardt, David G. Harris, Kevin T. Schmitt",Not found,2601.10511,"Monte Carlo, Approximation, DNF, Model Counting, PAC Learning","This paper presents a new Monte Carlo approach for approximating the number of satisfying assignments in Disjunctive Normal Form (DNF) formulas, which is crucial for applications such as probabilistic inference and network reliability. The approach is proven to achieve PAC learning bounds and is asymptotically more efficient than previous methods. It also demonstrates superior performance in experiments.",11.07,Qwen2.5-3B,Apple M1 (Metal)
2601.10512v2_SatMap Revisiting Satellite Maps as Prior for Onli.pdf,SatMap: Revisiting Satellite Maps as Prior for Online HD Map Construction,"Kanak Mazumder, Fabian B. Flohr",Not found,2601.10512,"Online HD map prediction, Satellite map prior, Vectorized HD map","This work proposes SatMap, an online vectorized HD map estimation method that integrates satellite maps with multi-view camera observations and directly predicts a vectorized HD map for downstream prediction and planning modules. It achieves significant improvements in mAP performance over baseline methods.",10.89,Qwen2.5-3B,Apple M1 (Metal)
2601.10520v1_Breaking Up with Normatively Monolithic Agency wit.pdf,Breaking Up with Normatively Monolithic Agency,"Felix Jahn1,3*, Yannic Muskalla1,3, Lisa Dargasz1, Patrick Schramowski1,2,4,5, Kevin Baum1,2,3*",Not found,2601.10520,"AI alignment, neuro-symbolic architecture, deontic logic, moral module, guard","As AI agents become increasingly autonomous and widely deployed in consequential contexts, ensuring their decisions are normatively aligned has become critical. This paper introduces GRACE, a neuro-symbolic reason-based containment architecture that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module, a Decision-Making Module, and a Guard, enabling stakeholders to understand, contest, and refine agent behavior.",11.21,Qwen2.5-3B,Apple M1 (Metal)
2601.10524v1_Diagnosing Generalization Failures in Fine-Tuned L.pdf,Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection,"Frank Bobe IIIFrank Bobe IIIFrank Bobe III∗, Gregory D. Vetaw, Chase Pavlick, Darshan Bryner, Matthew Cook, Jose Salas-Vernis",Not provided,2601.10524,"Large Language Models, Fine-tuning, Generalization, Phishing Detection, SHAP analysis, Mechanistic Interpretability","This study introduces and applies a multi-layered diagnostic framework to analyze the generalization failures of three LLMs (Llama 3.1 8B, Gemma 2 9B, and Mistral) on a high-stakes phishing detection task. The findings reveal that generalization is driven by a synergy between architecture and data diversity, is highly architecture-dependent, and some architectures are inherently more generalizable.",11.61,Qwen2.5-3B,Apple M1 (Metal)
2601.10527v2_A Safety Report on GPT-5.2 Gemini 3 Pro Qwen3-VL G.pdf,"A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5","Xingjun Ma1, Yixu Wang1, Hengyuan Xu1, Yutao Wu3, Yifan Ding1, Yunhan Zhao1, Zilong Wang1, Jiabin Hua1, Ming Wen1,2, Jianan Liu1,2, Ranjie Duan, Yifeng Gao1, Yingshui Tan, Yunhao Chen 1, Hui Xue, Xin Wang 1, Wei Cheng, Jingjing Chen1, Zuxuan Wu1, Bo Li4, Yu-Gang Jiang1",https://doi.org/FutureDOI,arXiv:2601.10527v2,"Large Language Models, Multimodal Large Language Models, Safety Evaluation, Adversarial Testing, Benchmark Evaluation, Multilingual Evaluation, Regulatory Compliance","This report presents an integrated safety evaluation of six frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5, across language, vision-language, and image generation settings. It highlights the inherent multidimensionality of safety in these models, with varying performance and vulnerabilities under different evaluation criteria.",12.94,Qwen2.5-3B,Apple M1 (Metal)
2601.10543v1_Defending Large Language Models Against Jailbreak .pdf,Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing,"W ARNING: This paper contains model outputs that may be considered harmful., Yinzhi Zhao, Ming Wang, Shi Feng, Xiaocui Yang, Daling Wang, Yifei Zhang",Not found,Not found,"Large language models, Jailbreak attacks, Safety awareness, In-decoding, Probing","This paper examines the decoding process of large language models and observes that even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. The authors propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments demonstrate significant enhancements in safety, while maintaining low over-refusal rates on benign inputs and preserving response quality.",10.9,Qwen2.5-3B,Apple M1 (Metal)
2601.10560v1_Learning Latency-Aware Orchestration for Parallel .pdf,Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems,"Xi Shi, Mengxin Zheng, Qian Lou",Not found,Not found,"Multi-agent systems, Parallel execution, Latency optimization, Inference latency, Latency-aware orchestration","This work investigates learning-based orchestration of multi-agent systems with explicit latency supervision under parallel execution, proposing Latency-Aware Multi-agent System (LAMaS) to reduce critical path length by 38–46% compared to the SOTA baseline for multi-agent architecture search across multiple benchmarks while maintaining or even improving task performance.",10.41,Qwen2.5-3B,Apple M1 (Metal)
2601.10562v1_Process-Guided Concept Bottleneck Model.pdf,Process-Guided Concept Bottleneck Model,"Reza M. Asiyabi, Sam Harrison, John L. Godlee, David Milodowski, Nicole H. Augustin, Penelope J. Mograbi, Timothy R. Baker, Lorena M. Benitez, Samuel J. Bowers, Thomas K. Brade, Joao M. B. Carreiras, Duncan M. Chalo, V era De Cauwer, Kyle G. Dexter, Hermane Diesse, Mathias I. Disney, Luisa F. Escobar-Alvarado, Manfred Finckh, Tatenda Gotore, Gabriele C. Hegerl, John N. Kigomo, Fainess C. Lumbwe, Francisco Maiato, Rudzani A. Makhado, Collins W. Masinde, Musingo Tito E. Mbuvi, Iain M. McNicol, Edward T.A. Mitchard, Buster P . Mogonong, Wilson A. Mugasha, Aristides Baptista Muhate, Hinji Mutondo, Leena Naftal, Paula Nieto-Quintano, Elifuraha Elisha Njoghomi, Catherine L. Parr, Oliver L. Phillips, Pierre Proces, Tshililo Ramaswiela, Jayashree Ratnam, Mathew Rees, Rasmus Revermann, Natasha Ribeiro, Mahesh Sankaran, Abel M. Siampale, Stephen Sitch, Kathleen G. Smart, Hemant G. Tripathi, Wayne Twine, Gabriel I.K. Uusiku, Helga van der Merwe, Chemuku Wekesa, Benjamin J. Wigley, Mathew Williams, Ellie Wood, Shaun Quegan, Steven Hancock, Casey M. Ryan",Not provided,Not provided,"machine learning, pattern analysis, pattern recognition, concept bottleneck, process-guided, carbon dynamics, dry tropics","This study proposes a process-guided concept bottleneck model to address challenges in pattern analysis and machine learning, focusing on the dry tropics and carbon dynamics, supported by various grants and partnerships.",11.85,Qwen2.5-3B,Apple M1 (Metal)
2601.10567v1_Generative AI collective behavior needs an interac.pdf,Generative AI collective behavior needs an interactionist paradigm,"Laura Ferrarotti1,†, Gian Maria Campedelli2,1,†, Roberto Dessì3, Andrea Baronchelli4, Giovanni Iacca2, Kathleen M. Carley5, Alex Pentland6,7, Joel Z. Leibo8, James Evans9, Bruno Lepri1",Not found,2601.10567,"Generative AI, Collective behavior, Interactionism, Large language models, In-context learning, Social priors, Emergent phenomena, Trans-disciplinary","This article argues that understanding the collective behavior of agents based on large language models is essential, with implications for risks and benefits. It proposes an interactionist paradigm to examine how prior knowledge and embedded values interact with social context in multi-agent generative AI systems.",13.44,Qwen2.5-3B,Apple M1 (Metal)
2601.10581v1_From Single to Multi-Agent Reasoning Advancing Gen.pdf,From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA,"Kimia Abedini, Farzad Shami, Gianmaria Silvello",https://doi.org/10.1016/j.jml.2026.01.015,2601.10581,"Question Answering, Genomic QA, Multi-Agent Systems","Comprehending genomic information is essential for biomed-ical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.",12.62,Qwen2.5-3B,Apple M1 (Metal)
2601.10587v1_Adversarial Evasion Attacks on Computer Vision usi.pdf,Adversarial Evasion Attacks on Computer Vision using SHAP Values,"Frank Mollard *, Marcus Becker ‡, Florian Röhrbein †",Not found,2601.10587,"Adversarial attacks, Computer Vision, SHAP values, Deep Learning, Evasion attacks","The paper introduces a white-box attack on computer vision models using SHAP values, demonstrating how adversarial evasion attacks can compromise the performance of deep learning models by reducing output confidence or inducing misclassifications.",12.86,Qwen2.5-3B,Apple M1 (Metal)
2601.10591v1_ProbFM Probabilistic Time Series Foundation Model .pdf,ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition,"Decomposition, Arundeep Chinta1, Lucas Vinh Tran2, Jay Katukuri1",Not provided,Not provided,"Time Series Foundation Models, Probabilistic Forecasting, Uncertainty Quantification, Deep Evidential Regression, Conformal Prediction","This paper presents a novel transformer-based probabilistic framework, ProbFM, which leverages Deep Evidential Regression (DER) for principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. The paper rigorously evaluates DER's effectiveness through a controlled comparison study using a consistent LSTM architecture across five probabilistic methods. The evaluation demonstrates that DER maintains competitive forecasting accuracy while providing explicit uncertainty decomposition, which is demonstrated through practical uncertainty-aware trading strategies.",11.29,Qwen2.5-3B,Apple M1 (Metal)
2601.10600v1_Procedural Fairness in Multi-Agent Bandits.pdf,Procedural Fairness in Multi-Agent Bandits,"Joshua Caiata, Carter Blair, Kate Larson",Not found,2601.10600,"fairness, multi-agent systems, multi-armed bandits, procedural fairness","This paper introduces a new fairness objective, procedural fairness, which provides equal decision-making power for all agents and lies in the core of fairness. It argues that fairness in multi-agent systems must be grounded in the principle of equal voice, and provides a framework for putting procedural fairness into practice.",10.83,Qwen2.5-3B,Apple M1 (Metal)
2601.10611v1_Molmo2 Open Weights and Data for Vision-Language M.pdf,Molmo2: Open Weights and Data for Vision-Language Models,"Christopher Clark, Jieyu Zhang, Zixian Ma, Jae Sung Park, Mohammadreza Salehi, Rohun Tripathi, Sangho Lee, Zhongzheng Ren, Chris Dongjoo Kim, Yinuo Yang, Vincent Shao, Yue Yang, Weikai Huang, Taira Anderson, Jianrui Zhang, Jitesh Jain, George Stoica, Winson Han, Ali Farhadi, Ranjay Krishna",Not provided,2601.10611v1,"vision-language models, open weights, data sharing, grounding, video understanding, pointing, tracking","Presenting Molmo2, a new family of vision-language models that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks.",11.83,Qwen2.5-3B,Apple M1 (Metal)
2601.10651v1_Multi-Property Synthesis.pdf,Multi-Property Synthesis,"Christoph Weinhuber1,∗, Yannik Schnitzer 1,∗, Alessandro Abate1, David Parker 1, Giuseppe De Giacomo 1, Moshe Y. Vardi 2",Not provided,Not provided,"Synthesis, Temporal Logic, LTLf, Multi-Property, Finite-Horizon, Planning, Control","The paper studies LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, the authors compute the relation between product-game states and the goal sets that are realizable from them, and synthesize strategies achieving maximal realizable sets. They develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. The approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.",11.11,Qwen2.5-3B,Apple M1 (Metal)
2601.10679v1_Are Your Reasoning Models Reasoning or Guessing A .pdf,Are Your Reasoning Models Reasoning or Guessing?,"Zirui Ren, Ziming Liu",,,"Hierarchical Reasoning, Fixed Points, Sudoku-Extreme","Hierarchical reasoning models (HRM) achieve extraordinary performance on various reasoning tasks, significantly outperforming large language model (LLM) reasoners. This study identifies three counterintuitive facts about HRM, including failure on extremely simple puzzles due to violation of fixed points, critical reasoning steps, and existence of multiple fixed points. These findings suggest that HRM appears to be 'guessing' instead of 'reasoning'. Strategies to scale HRM's guesses are proposed, including data augmentation, input perturbation, and model bootstrapping. Augmented HRM boosts accuracy on Sudoku-Extreme from 54.5% to 96.9%. The analysis provides insights into how reasoning models 'reason'. ",10.36,Qwen2.5-3B,Apple M1 (Metal)
2601.10681v1_Structure and Diversity Aware Context Bubble Const.pdf,Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems,"Amir Khurshid1a*, Abhishek Sehgal1b",Not found,Not found,"Large Language Model, Retrieval-Augmented Generation, Context Bubble, Retrieval","A framework for constructing coherent, citable bundles of spans under a strict token budget, preserving and exploiting document structure, and balancing query relevance, marginal coverage, and redundancy penalties. It reduces redundant context, improves coverage of secondary facets, and enhances answer quality and citation faithfulness within a limited context window.",10.76,Qwen2.5-3B,Apple M1 (Metal)
2601.10684v1_On the origin of neural scaling laws from random g.pdf,On the origin of neural scaling laws: from random graphs to natural language,"Maissam Barkeshli1,2,∗, Alberto Alfarano 3,†, Andrey Gromov 1",Not found,Not found,"Neural networks, Scaling laws, Transformer models, Random graphs, Natural language processing","This paper studies scaling laws for transformers trained to predict random walks on graphs with tunable complexity, demonstrating that neural scaling laws can arise even in the absence of power law structure in the data correlations. It also considers natural language models by training on sequences sampled from increasingly simplified generative language models, revealing a monotonic evolution of the scaling exponents.",10.64,Qwen2.5-3B,Apple M1 (Metal)
2601.10696v1_The Impact of Generative AI on Architectural Conce.pdf,"The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load","Han Jiang*, Yao Xiao*",,,"Visual communication, Architectural design, Learning, Performance Assessment, Hybrid Intelligence, Human-AI teaming","Our study examines how generative AI influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks.",11.76,Qwen2.5-3B,Apple M1 (Metal)
2601.10700v2_LIBERTy A Causal Framework for Benchmarking Concep.pdf,LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals,"Gilat Toker*, Nitay Calderon*",Not found,Not found,"Large Language Models, explainability, concept-based explanations, causal inference, counterfactuals",LIBERTy is a framework for constructing datasets containing structural counterfactual pairs to evaluate the faithfulness of concept-based explanations of LLMs. It uses explicitly defined Structural Causal Models (SCMs) to propagate interventions through the SCM until an LLM generates the counterfactual. The framework evaluates a wide range of methods across five models and identifies substantial headroom for improving concept-based explanations. It also enables systematic analysis of model sensitivity to interventions.,10.16,Qwen2.5-3B,Apple M1 (Metal)
2601.10702v1_Grounding Agent Memory in Contextual Intent.pdf,Grounding Agent Memory in Contextual Intent,"Ruozhen Yang, Yucheng Jiang, Yueqi Jiang, Priyanka Kargupta, Jiawei Han",,,"large language models, long-horizon tasks, agent memory, contextual intent, memory retrieval","Deploying large language models in long-horizon, goal-oriented interactions remains challenging due to memory systems retrieving context-mismatched evidence. This paper proposes STITCH, an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step’s intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference. STITCH achieves state-of-the-art performance in context-aware retrieval benchmarks.",10.14,Qwen2.5-3B,Apple M1 (Metal)
2601.10712v1_MatchTIR Fine-Grained Supervision for Tool-Integra.pdf,MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching,"Changle Qu, Sunhao Dai, Hengyi Cai, Jun Xu, Shuaiqiang Wang, Dawei Yin",Not found,Not found,"Tool-Integrated Reasoning, Fine-Grained Supervision, Reinforcement Learning, Bipartite Matching","MatchTIR is a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation for Tool-Integrated Reasoning (TIR) tasks. It aims to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios.",10.23,Qwen2.5-3B,Apple M1 (Metal)
2601.10748v1_AnyECG Evolved ECG Foundation Model for Holistic H.pdf,AnyECG: Evolved ECG Foundation Model for Holistic Health Profiling,"Jun Li, Hongling Zhu, Yujie Xiao, Qinghao Zhao, Yalei Ke, Gongzheng Tang, Guangkun Nie, Deyun Zhang, Jin Li, Canqing Yu, Shenda Hong",,,"electrocardiography, artificial intelligence, cardiac diseases, non-cardiac diseases, health profiling, comorbidity, longitudinal study","This study presents AnyECG, an evolved electrocardiography foundation model designed to enhance holistic health profiling. It integrates a large-scale, multicenter ECG dataset and employs transfer learning to fine-tune the ECGFounder model, achieving significant improvements in disease screening, long-term risk prediction, and comorbidity recognition.",11.0,Qwen2.5-3B,Apple M1 (Metal)
2601.10768v1_Optimisation of complex product innovation process.pdf,OPTIMISATION OF COMPLEX PRODUCT INNOVATION PROCESSES BASED ON TREND MODELS WITH THREE-VALUED LOGIC,"NINA BO ˇCKOV´A1, BARBORA VOLN ´A2*, MIRKO DOHNAL 3",Not found,2601.10768,"complex product innovation, technological forecasting, three-valued logic, trend-based modelling, scenarios, transition graphs","This paper investigates complex product-innovation processes using models grounded in a set of heuristics. Each heuristic is expressed through simple trends, serving as minimally information-intensive quantifiers, avoiding reliance on numerical values or rough sets. A solution to a trend model is defined as a set of scenarios with possible transitions between them, represented by a transition graph. Any possible future or past behavior of the system under study can be depicted by a path within this graph.",12.49,Qwen2.5-3B,Apple M1 (Metal)
2601.10770v1_Unifying Speech Recognition Synthesis and Conversi.pdf,"UNIFYINGSPEECHRECOGNITION, SYNTHESIS AND CONVERSION WITHAUTOREGRESSIVETRANSFORMERS","Runyuan Cai, Yu Lin, Yiming Wang, Chunlin Fu, Xiaodong Zeng",Not found,2601.10770,"Text-to-Speech, Automatic Speech Recognition, Voice Conversion, Foundation Model","This paper presents GPA, a unified audio foundation model that integrates multiple core speech tasks within a single large language model architecture, enabling a single autoregressive model to flexibly perform TTS, ASR, and VC without architectural modifications.",11.14,Qwen2.5-3B,Apple M1 (Metal)
2601.10773v1_LogicLens Leveraging Semantic Code Graph to explor.pdf,LogicLens: Leveraging Semantic Code Graph to explore Multi Repository large systems,"Niko Usai, Dario Montagnini, Kristian Ilianov Iliev, Raffaele Camanzo",,,"software systems, multi-repository, semantic code graph, reactive conversational agent, domain logic, runtime behaviors","Understanding large software systems is a challenging task, especially when code is distributed across multiple repositories and microservices. LogicLens, a reactive conversational agent, assists developers in exploring complex software systems through a semantic multi-repository graph. This graph combines syntactic code analysis and semantic enrichment using Large Language Models (LLMs), capturing both structural and functional elements. Once constructed, LogicLens enables developers to interact with the graph via natural language, dynamically retrieving relevant subgraphs and answering technical or functional queries.",11.13,Qwen2.5-3B,Apple M1 (Metal)
2601.10779v1_Unified Optimization of Source Weights and Transfe.pdf,Unified Optimization of Source Weights and Transfer Quantities in Multi-Source Transfer Learning: An Asymptotic Framework,"Qingyue Zhang, Chang Chu, Haohao Fu, Tianren Peng, Yanru Wu, Guanbo Huang, Yang Li, Shao-Lun Huang",Not found,Not found,"transfer learning, multi-source learning, asymptotic analysis, K-L divergence","This work proposes a theoretical framework, Unified Optimization of Weights and Quantities (UOWQ), for multi-source transfer learning, optimizing both source weights and transfer quantities. It proves that using all available source samples is optimal once weights are properly adjusted and develops practical algorithms for both multi-source transfer learning and multi-task learning settings.",11.01,Qwen2.5-3B,Apple M1 (Metal)
2601.10810v1_Digital Metabolism Decoupling Logic from Facts via.pdf,Digital Metabolism: Decoupling Logic from Facts via Regenerative Unlearning,"Mengmeng Peng, Zhenyu Fang, He Sun",Not found,2601.10810,"Large Language Models, Parameter Entanglement, Neural Logic Core, Regenerative Unlearning, Thermodynamic Hypothesis","This paper proposes 'digital metabolism,' a thermodynamic hypothesis suggesting that targeted forgetting is necessary for distilling a pure neural logic core. It introduces the Regenerative Logic-Core Protocol (RLCP) to validate this hypothesis, observing a phase transition in a model achieving near-zero retention of targeted factual associations while exhibiting changes consistent with an emergent 'structural crystallization' effect.",11.42,Qwen2.5-3B,Apple M1 (Metal)
2601.10820v1_Towards Reliable ML Feature Engineering via Planni.pdf,Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents,"Himanshu Thakur∗, Anusha Kamath, Anurag Muthyala, Dhwani Sanmukhani, Smruthi Mukund, Jay Katukuri",Not found,2601.10820,"Machine Learning, Feature Engineering, LLM Agents, Code Generation, Automation","Recent advances in code generation models have unlocked opportunities for automating feature engineering, but real-world adoption remains constrained by challenges in dataset scarcity, integration with team tools, and human-AI collaboration. This paper addresses these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion, leveraging a team's environment represented as a graph to orchestrate calls to available agents, generate context-aware prompts, and retroactively correct upstream artifacts. The approach achieves significant improvements in evaluation metrics over manual and unplanned workflows, and has delivered real-world impact in feature engineering for recommendation models.",12.23,Qwen2.5-3B,Apple M1 (Metal)
2601.10827v1_Approximately Optimal Global Planning for Contact-.pdf,Approximately Optimal Global Planning for Contact-Rich SE(2) Manipulation on a Graph of Reachable Sets,"Simin Liu, Tong Zhao, Bernhard Paus Graesdal, Peter Werner, Jiuguang Wang, John Dolan, Changliu Liu, Tao Pang",,,"Contact-Rich Manipulation, Global Planning, Graph of Reachable Sets, Optimization, Manipulator Planning","This paper introduces a new paradigm for computing approximately optimal manipulator plans for contact-rich manipulation tasks. It constructs a graph of mutual reachable sets offline and plans over this graph online to compute and sequence local plans for globally optimized motion. The approach outperforms a leading planner on a challenging contact-rich task, reducing task cost by 61% and achieving a 91% success rate across 250 queries with sub-minute query times.",11.25,Qwen2.5-3B,Apple M1 (Metal)
2601.10835v1_Can Vision-Language Models Understand Construction.pdf,Can Vision-Language Models Understand Construction Workers? An Exploratory Study,"Hieu Bui, Nathaniel E. Chodosh, Arash Tavakoli",,,"Vision-Language Models, Construction Automation, Robotics, Human-Robot Interaction, Safety, Productivity","This study evaluates the performance of three leading Vision-Language Models (GPT-4o, Florence 2, and LLaVa-1.5) in detecting construction worker actions and emotions from static site images. Using a curated dataset of 1,000 images annotated across ten action and ten emotion categories, the study assesses each model's outputs through standardized inference pipelines and multiple evaluation metrics. GPT-4o consistently achieved the highest scores across both tasks, with an average F1-score of 0.756 and accuracy of 0.799 in action recognition, and an F1-score of 0.712 and accuracy of 0.773 in emotion recognition. Florence 2 performed moderately, with F1-scores of 0.497 (action) and 0.414 (emotion), while LLaVa-1.5 showed the lowest overall performance (F1-scores of 0.466 for action and 0.461 for emotion). Confusion matrix analyses revealed that all models struggled to distinguish semantically close categories, highlighting limitations in current VLMs when applied to visually nuanced, domain-specific tasks.",12.15,Qwen2.5-3B,Apple M1 (Metal)
2601.10880v1_Medical SAM3 A Foundation Model for Universal Prom.pdf,Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation,"Chongcong Jiang, Tianxingjian Ding, Chuhan Song, Jiachen Tu, Ziyang Yan, Yihua Shao, Zhenyi Wang, Yuzhang Shang, Tianyu Han, Yu Tian",Not provided,2601.10880,"Medical Image Segmentation, Foundation Models, Fine-Tuning, SAM3","Promptable segmentation foundation models like SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Medical SAM3 addresses these limitations by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts.",12.42,Qwen2.5-3B,Apple M1 (Metal)
2601.10904v1_ARC Prize 2025 Technical Report.pdf,ARC Prize 2025: Technical Report,"François Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers",,,"ARC Prize, ARC-AGI, benchmark, few-shot generalization, fluid intelligence, abstract reasoning, refinement loop, evolutionary program synthesis, commercial AI systems, knowledge coverage, benchmark contamination, interactive reasoning","This paper discusses the ARC Prize 2025, a global competition targeting the ARC-AGI-2 dataset, which features greater task complexity compared to its predecessor. It examines the role of refinement loops in AGI progress, discusses knowledge-dependent overfitting, and previews ARC-AGI-3, which introduces interactive reasoning challenges. The paper also highlights the growing research interest in fluid intelligence and abstract reasoning, and the emergence of refinement loops in AI systems.",11.27,Qwen2.5-3B,Apple M1 (Metal)
2601.10917v1_Self-learned representation-guided latent diffusio.pdf,SELF-LEARNED REPRESENTATION-GUIDED LATENT DIFFUSION MODEL FOR BREAST CANCER CLASSIFICATION IN DEEP ULTRA VIOLET WHOLE SURFACE IMAGES,"Pouya Afshin1, David Helminiak 2, Tianling Niu3, Julie M. Jorns 4, Tina Yen5, Bing Yu3, Dong Hye Ye1†",Not found,Not found,"Breast Cancer Classification, Latent Diffusion Model, Self-Supervised Learning, Data Augmentation","Breast-Conserving Surgery requires precise intraoperative margin assessment to preserve healthy tissue. Deep Ultraviolet Fluorescence Scanning Microscopy offers rapid, high-resolution surface imaging for this purpose. However, the scarcity of annotated DUV data hinders the training of robust deep learning models. To address this, we propose a Self-Supervised Learning-guided Latent Diffusion Model to generate high-quality synthetic training patches. By guiding the LDM with embeddings from a fine-tuned DINO teacher, we inject rich semantic details of cellular structures into the synthetic data. We combine real and synthetic patches to fine-tune a Vision Transformer, utilizing patch prediction aggregation for WSI-level classification. Experiments using 5-fold cross-validation demonstrate that our method achieves 96.47% accuracy and reduces the FID score to 45.72, significantly outperforming class-conditioned baselines.",12.0,Qwen2.5-3B,Apple M1 (Metal)
2601.10921v1_RobuMTL Enhancing Multi-Task Learning Robustness A.pdf,RobuMTL: Enhancing Multi-Task Learning Robustness Against Weather Conditions,"Tasneem Shaffee, Sherief Reda",,,"Robust Multi-Task Learning, Adversarial Adaptation, Weather Robustness, Mixture-of-Experts","This paper introduces RobuMTL, a novel architecture designed to adaptively address visual degradation by dynamically selecting task-specific hierarchical Low-Rank Adaptation (LoRA) modules and a LoRA expert squad based on input perturbations in a mixture-of-experts fashion. The framework enables adaptive specialization based on input characteristics, improving robustness across diverse real-world conditions. RobuMTL is evaluated on the PASCAL and NYUD-v2 datasets and compared against single-task models, standard MTL baselines, and state-of-the-art methods, delivering significant improvements in robustness under adverse weather conditions.",11.22,Qwen2.5-3B,Apple M1 (Metal)
2601.10922v1_What Matters in Data Curation for Multimodal Reaso.pdf,What Matters in Data Curation for Multimodal Reasoning?,"Yosub Shin, Michael Buriek, Boris Sobolev, Pavel Bushuyeu, Vikas Kumar, Haoyang Xu, Samuel Watson, Igor Molybog",,,"Data curation, Multimodal reasoning, Vision-language models, NeurIPS challenge, Fine-tuning","This paper studies data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision–Language Reasoning (DCVLR) challenge. It shows that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains, and that increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance. Commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.",11.71,Qwen2.5-3B,Apple M1 (Metal)
2601.10926v1_Selecting Language Models for Social Science Start.pdf,Selecting Language Models for Social Science,"Dustin S. Stoltz, Marshall A. Taylor, Sanuj Kumar",,XX(X):1–22,"large language models, LLMs, reproducibility, replicability, model openness","This paper discusses the selection of language models for social science research, focusing on reproducibility, reliability, validity, and replicability. It explores four categories of considerations: model openness, model footprint, training data, and model architecture and fine-tuning. The authors argue that social scientists cannot avoid validating computational measures and propose starting with smaller, open models and constructing delimited benchmarks to demonstrate the validity of the entire computational pipeline.",10.94,Qwen2.5-3B,Apple M1 (Metal)
2601.10931v1_Sparse Data Tree Canopy Segmentation Fine-Tuning L.pdf,Sparse Data Tree Canopy Segmentation: Fine-Tuning Leading Pretrained Models on Only 150 Images,"David Szczecina, Niloofar Azad, Hudson Sun, Kyle Gao, Anthony Bertnyk, Lincoln Linlin Xu",,,"Tree canopy, Aerial imagery, Remote sensing, Deep learning, Object segmentation, Transformer models, Convolutional neural networks, Data scarcity, Overfitting","This paper evaluates five deep learning architectures, YOLOv11, Mask R-CNN, DeepLabv3, Swin-UNet, and DINOv2, for tree canopy segmentation on a small and imbalanced dataset of only 150 annotated images. The findings confirm that pretrained convolution-based models generalize better than pretrained transformer-based models, highlighting the importance of inductive bias, pretraining, and model capacity in low-data regimes.",11.39,Qwen2.5-3B,Apple M1 (Metal)
2601.10945v1_PatientVLM Meets DocVLM Pre-Consultation Dialogue .pdf,PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between Vision-Language Models for Efficient Diagnosis,"K Lokesh1*, Abhirama Subramanyam Penamakuri1*, Uday Agarwal1, Apoorva Challa2, Shreya K Gowda2, Somesh Gupta2, Anand Mishra1",,2605.01234,"vision-language models, pre-consultation dialogue, diagnosis, medical AI","This paper proposes a Pre-Consultation Dialogue Framework (PCDF) to simulate realistic doctor-patient dialogues between two vision-language models (VLMs): a DocVLM and a PatientVLM. The DocVLM generates follow-up questions based on the image and dialogue history, while the PatientVLM responds using a symptom profile derived from the ground-truth diagnosis. The framework was validated with licensed clinicians, confirming its clinical relevance, symptom coverage, and realism. The findings indicate that the resulting interactions form coherent, multi-turn consultations paired with images and diagnoses, leading to substantial gains over image-only training.",10.97,Qwen2.5-3B,Apple M1 (Metal)
2601.10951v1_Multi-Stage Patient Role-Playing Framework for Rea.pdf,Multi-Stage Patient Role-Playing Framework for Realistic Clinical Interactions,"Shijie Jiang, Zefan Zhang, Kehua Zhu, Tian Bai, Ruihong Zhao",Not provided,2601.10951,"Patient Role-Playing, Large Language Models, Clinical",The simulation of realistic clinical interactions is crucial for advancing clinical Large Language Models and supporting medical diagnostic education. This work proposes the first Chinese patient simulation dataset (Ch-PatientSim) and a Multi-Stage Patient Role-Playing (MSPRP) framework to improve model performance in patient behavior emulation.,12.31,Qwen2.5-3B,Apple M1 (Metal)
2601.10955v1_Beyond Max Tokens Stealthy Resource Amplification .pdf,Beyond Max Tokens: Stealthy Resource Amplification via Tool Calling Chains in LLM Agents,"Kaiyu Zhou, Yongsen Zheng∗, Yicheng He, Meng Xue, Xueluan Gong, Yuji Wang, Kwok-Yan Lam",Not found,Not found,"Large Language Models, LLM Agents, Tool Calling, Agent Security, Economic Denial-of-Service","The paper introduces a stealthy, multi-turn economic Denial-of-Service (DoS) attack that operates at the tool layer under the guise of a correctly completed task. The method adjusts text-visible fields and a template-governed return policy in a benign, Model Context Protocol (MCP)-compatible tool server, optimizing these edits with a Monte Carlo Tree Search (MCTS) optimizer. These adjustments leave function signatures unchanged and preserve the final payload, steering the agent into prolonged, verbose tool-calling sequences using text-only notices. This compounds costs across turns, escaping single-turn caps while keeping the final answer correct to evade validation.",11.36,Qwen2.5-3B,Apple M1 (Metal)
2601.10960v1_Steering Language Models Before They Speak Logit-L.pdf,Steering Language Models Before They Speak: Logit-Level Interventions,"Hyeseon An, Shinwoo Park, Hyundong Jin, Yo-Sub Han *",Not found,Not found,"Language models, Steering, Logit-level interventions, Controllable generation, LLMs","Steering large language models (LLMs) is essential for specialized applications such as style-sensitive text rewriting, user-adaptive communication, and toxicity mitigation. Current methods often require deep access to internal layers or fail to provide consistent or fine-grained control. This paper proposes a training-free inference-time logit intervention to achieve controllable generation. Empirical evaluations across three diverse datasets demonstrate the effectiveness of the method in steering output characteristics, confirming its broad applicability and task-agnostic nature.",11.18,Qwen2.5-3B,Apple M1 (Metal)
2601.11000v1_When Personalization Misleads Understanding and Mi.pdf,When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs,"Zhongxiang Sun1, Yi Zhan1, Chenglei Shen1, Weijie Yu3, Xiao Zhang1, Ming He2, Jun Xu1",Not provided,Not provided,"Personalized language models, Factual reasoning, Hallucination prevention, Inference-time mitigation","Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. This paper introduces Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. Experiments show that FPPS substantially improves factual accuracy while maintaining personalized performance.",11.18,Qwen2.5-3B,Apple M1 (Metal)
2601.11007v1_AdaMARP An Adaptive Multi-Agent Interaction Framew.pdf,AdaMARP: An Adaptive Multi-Agent Interaction Framework for General Immersive Role-Playing,"Zhenhua Xu, Dongsheng Chen, Shuo Wang, Jian Li, Chengjie Wang, Meng Han, Yabiao Wang",Not found,2601.11007,"large language models, immersive role-playing, multi-agent interaction, adaptive framework","This paper proposes AdaMARP, an adaptive multi-agent interaction framework for general immersive role-playing. It features an immersive message format and an explicit Scene Manager to control role-playing via discrete actions, enabling smoother scene transitions and more natural character introductions.",10.13,Qwen2.5-3B,Apple M1 (Metal)
2601.11012v1_Efficient Protein Optimization via Structure-aware.pdf,Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics,"Jiahao Wang, Shuangjia Zheng",Not found,Not found,"protein optimization, Hamiltonian dynamics, Bayesian optimization, structure-aware sampling, epistasis, mutation effects","Proposes HADES, a Bayesian optimization method utilizing Hamiltonian dynamics to efficiently sample from a structure-aware approximated posterior, enabling rapid transition of proposals toward promising areas. Demonstrates superior performance in in-silico evaluations across most metrics.",9.92,Qwen2.5-3B,Apple M1 (Metal)
2601.11016v1_Contextual Distributionally Robust Optimization wi.pdf,Contextual Distributionally Robust Optimization with Causal and Continuous Structure: An Interpretable and Tractable Approach,"Fenglin Zhang, Jie Wang∗",Not found,2601.11016,"Contextual distributionally robust optimization, Causal Sinkhorn discrepancy, Soft regression forest, Stochastic compositional optimization","In this paper, we introduce a framework for contextual distributionally robust optimization (DRO) that considers the causal and continuous structure of the underlying distribution by developing interpretable and tractable decision rules. We propose the Soft Regression Forest (SRF) decision rule to solve the corresponding infinite-dimensional policy optimization problem and develop an efficient stochastic compositional gradient algorithm to solve the Causal-SDRO with parametric decision rules.",12.18,Qwen2.5-3B,Apple M1 (Metal)
2601.11021v1_Combating Spurious Correlations in Graph Interpret.pdf,Combating Spurious Correlations in Graph Interpretability via Self-Reflection,"Kecheng Cai, Chenyang Xu, Chao Peng",Not provided,Not provided,"Graph Interpretability, Spurious Correlations, Self-Reflection, Graph Learning","This paper focuses on improving interpretability in challenging Spurious-Motif datasets by adapting the self-reflection technique, commonly used in large language models, to datasets with strong spurious correlations. The authors propose a self-reflection framework that can be integrated with existing interpretable graph learning methods, leading to performance improvements on Spurious-Motif and other graph interpretability benchmarks.",12.04,Qwen2.5-3B,Apple M1 (Metal)
2601.11030v1_IDDR-NGP Incorporating Detectors for Distractor Re.pdf,IDDR-NGP: Incorporating Detectors for Distractor Removal with Instant Neural Radiance Field,"Xianliang Huang, Jiajie Gou, Shuhang Chen, Zhizhou Zhong, Jihong Guan, Shuigeng Zhou",10.1145/3581783.3612045,2601.11030v1,"distractor removal, 3D scene, instant neural radiance field, object detection, implicit 3D representations","This paper presents IDDR-NGP, a unified method for removing a wide range of distractors in 3D scenes, including snowflakes, confetti, defoliation, and petals, by integrating implicit 3D representations with 2D detectors and jointly optimizing rendering results from multi-view corrupted images.",11.06,Qwen2.5-3B,Apple M1 (Metal)
2601.11035v1_Your One-Stop Solution for AI-Generated Video Dete.pdf,Recent Advances in Synthetic Video Detection,"Long Ma, Zihao Xue, Yan Wang, Zhiyuan Yan, Jin Xu, Xiaorui Jiang, Haiyang Yu, Yong Liao, Zhen Bi",,2601.11035,"synthetic video, video detection, generative modeling, realistic videos, reliability","Recent advances in generative modeling have made synthetic videos remarkably realistic, complicating human detection. This paper discusses the challenges and limitations in detecting synthetic videos, focusing on the dataset limitations and the need for advanced models and comprehensive evaluations.",12.17,Qwen2.5-3B,Apple M1 (Metal)
2601.11037v1_BAPO Boundary-Aware Policy Optimization for Reliab.pdf,BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search,"Shiyu Liu, Yongjing Yin, Jianhao Yan, Yunbo Tang, Qinggang Zhang, Bei Li, Xin Chen, Jingang Wang, Xunliang Cai, Jinsong Su",,,"reinforcement learning, large language models, agentic search, policy optimization, boundary awareness, reliability","This paper proposes BAPO, a novel RL framework designed to cultivate reliable boundary awareness in agentic search models without compromising accuracy. BAPO introduces a group-based boundary-aware reward and an adaptive reward modulator to prevent the model from exploiting IDK as a shortcut during early exploration.",10.79,Qwen2.5-3B,Apple M1 (Metal)
2601.11042v1_Spectral Characterization and Mitigation of Sequen.pdf,Spectral Characterization and Mitigation of Sequential Knowledge Editing,"Chi Zhang, Mengqi Zhang, Xiaotian Ye, Runxi Cheng, Zisheng Zhou, Ying Zhou, Pengjie Ren, Zhumin Chen",Not found,Not found,"Large Language Models, Sequential Knowledge Editing, Parameter Modification, Spectral Analysis, Mitigation Strategies","This work presents a spectral analysis of sequential knowledge editing in large language models and shows that a model's general abilities are closely associated with dominant singular directions of pretrained weight matrices. These directions are highly sensitive to perturbations and are progressively disrupted by repeated edits, closely tracking the collapse in both editing efficacy and general performance. REVIVE, a plug-and-play framework, stabilizes sequential editing by explicitly preserving the dominant singular subspace, improving editing efficacy while substantially preserving general abilities under long-horizon sequential editing.",11.07,Qwen2.5-3B,Apple M1 (Metal)
2601.11044v2_AgencyBench Benchmarking the Frontiers of Autonomo.pdf,AGENCYBENCH: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts,"Keyu Li, Junhao Shi, Yang Xiao, Mohan Jiang, Jie Sun, Yunze Wu, Dayuan Fu, Shijie Xia, Xiaojie Cai, Tianze Xu, Weiye Si, Dequan Wang, Pengfei Liu",Not found,2601.11044,"Autonomous Agents, Benchmarking, Real-World Scenarios, User Simulation, Docker Sandbox, Iterative Feedback, Resource Efficiency, Model Architecture, Open Source, Closed Source","AGENCYBENCH is a comprehensive benchmark designed to evaluate 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. It requires an average of 1 million tokens and 90 multi-turn tool uses, enabling automated evaluation through user simulation and Docker-based sandboxing.",11.36,Qwen2.5-3B,Apple M1 (Metal)
2601.11049v1_Predicting Biased Human Decision-Making with Large.pdf,Predicting Biased Human Decision-Making with Large Language Models in Conversational Settings,"Stephen Pilli, Vivek Nallur",Not found,Not found,"Conversational AI, Framing Effect, Status Quo Bias, LLM Simulation","This study examines whether large language models (LLMs) can predict biased decision-making in conversational settings, including the Framing Effect and Status Quo Bias. Participants completed decision-making tasks via a chatbot with varying dialogue complexity, and LLMs were evaluated for their ability to predict individual decisions given demographic information and prior dialogue. The findings show that LLMs can predict bias patterns and load-bias interactions, with GPT-4 consistently aligning with human behavior and outperforming other models in predictive accuracy and fidelity to human-like bias patterns.",11.64,Qwen2.5-3B,Apple M1 (Metal)
2601.11063v1_H-AIM Orchestrating LLMs PDDL and Behavior Trees f.pdf,"H-AIM: Orchestrating LLMs, PDDL, and Behavior Trees for Hierarchical Multi-Robot Planning","Haishan Zeng, Peng Li",Not found,2601.11063,"Embodied AI, Multi-Robot Planning, Large Language Models, PDDL, Behavior Trees, Hierarchical Planning, Dynamic Coordination","H-AIM proposes a novel embodied multi-robot task planning framework that addresses the challenges of long-horizon tasks execution by heterogeneous robot teams from high-level instructions. It leverages an LLM to parse instructions and generate PDDL problem descriptions, combines the semantic reasoning of LLMs with classical planner search capabilities to produce optimized action sequences, and compiles the resulting plan into behavior trees for reactive control. The framework supports dynamically sized heterogeneous robot teams via a shared blackboard mechanism for communication and state synchronization.",11.44,Qwen2.5-3B,Apple M1 (Metal)
2601.11065v1_Fairness in Healthcare Processes A Quantitative An.pdf,Fairness in Healthcare Processes: A Quantitative Analysis of Decision Making in Triage,"Rachmadita Andreswari1,2,4, Stephan A. Fahrenkrog-Petersen2,3, Jan Mendling1,2",Not found,2601.11065,"process mining, fairness, triage, emergency room","This study addresses the critical concern of fairness in automated decision-making in high-pressure healthcare scenarios, such as emergency triage. It proposes a process mining approach to assess fairness in triage by linking real-life event logs with conceptual dimensions of justice. Using the MIMIC-EL event log (derived from MIMIC-IV ED), the study analyzes time, redo, deviation, and decision as process outcomes and evaluates the influence of age, gender, race, language, and insurance using Kruskal–Wallis, Chi-square, and effect size measurements. The results demonstrate which aspects of potential unfairness in high-acuity and sub-acute settings.",12.63,Qwen2.5-3B,Apple M1 (Metal)
2601.11073v1_Bridging Cognitive Neuroscience and Graph Intellig.pdf,Bridging Cognitive Neuroscience and Graph Intelligence: Hippocampus-Inspired Multi-View Hypergraph Learning for Web Finance Fraud,"Rongkun Cui, Nana Zhang, Kun Zhu, Qi Zhang",XXXXXXX.XXXXXXX,,"graph intelligence, cognitive neuroscience, multi-view hypergraph learning, web finance fraud, online financial services, graph neural networks, fraud detection, long-tailed data distributions, cross-view inconsistency perception, novelty detection","This paper proposes HIMVH, a Hippocampus-Inspired Multi-View Hypergraph learning model for web finance fraud detection. It addresses two challenges: fraud camouflage and long-tailed data distributions, by incorporating mechanisms inspired by the hippocampus and CA1 region. Extensive experiments on six web-based financial fraud datasets show significant improvements in AUC, F1, and AP metrics compared to 15 state-of-the-art models.",11.38,Qwen2.5-3B,Apple M1 (Metal)
2601.11076v1_A3D Adaptive Affordance Assembly with Dual-Arm Man.pdf,A3D: Adaptive Affordance Assembly with Dual-Arm Manipulation,"Jiaqi Liang, Yue Chen, Qize Yu, Yan Shen, Haipeng Zhang, Hao Dong, Ruihai Wu",,arXiv:2309.14874,"robotics, furniture assembly, dual-arm manipulation, adaptive affordance, motion planning, vision understanding","This paper proposes A3D, a framework for adaptive furniture assembly using dual-arm manipulation. It learns adaptive affordances to identify optimal support and stabilization locations on furniture parts, employing dense point-level geometric representations for generalization across varied geometries. The adaptive module uses interaction feedback to dynamically adjust support strategies during assembly. Experiments demonstrate effective generalization to diverse part geometries and furniture categories in both simulation and real-world settings.",10.92,Qwen2.5-3B,Apple M1 (Metal)
2601.11077v1_ABC-Bench Benchmarking Agentic Backend Coding in R.pdf,ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development,"Jie Yang, Honglin Guo, Li Ji, Jiazheng Zhou, Zhikai Lei, Shuo Zhang, Shichun Liu, Yuxin Wang, Bo Wang, Yining Zheng, Tao Gui, Xipeng Qiu",Not found,2601.11077,"Large Language Models, Autonomous Agents, Backend Development, Real-World Engineering, Benchmarking","The paper introduces ABC-Bench, a benchmark designed to evaluate agentic backend coding within a realistic, executable workflow. It curates 224 practical tasks from open-source repositories and requires agents to manage the entire development lifecycle, highlighting the gap between current model capabilities and practical backend engineering demands.",12.29,Qwen2.5-3B,Apple M1 (Metal)
2601.11078v1_Visual Marker Search for Autonomous Drone Landing .pdf,Visual Marker Search for Autonomous Drone Landing in Diverse Urban Environments,"Jiaohong Yao, Linfeng Liang, Yao Deng, Xi Zheng, Richard Han, Yuankai Qi",Not provided,Not provided,"Drone navigation, Marker-based landing, Reinforcement learning, AirSim, Robustness","This paper presents a simulation-based evaluation suite for marker-based landing of autonomous drones in diverse urban environments. It evaluates two heuristic coverage patterns and a reinforcement learning-based agent, analyzing how exploration strategy and scene complexity affect success rate, path efficiency, and robustness.",11.23,Qwen2.5-3B,Apple M1 (Metal)
2601.11089v2_MiCA A Mobility-Informed Causal Adapter for Lightw.pdf,MiCA: A Mobility-Informed Causal Adapter for Lightweight Epidemic Forecasting,"Suhan Guo, Jiahong Deng, Furao Shen",Not found,Not found,"epidemic forecasting, mobility data, causal discovery, temporal forecasting","Accurate forecasting of infectious disease dynamics is critical for public health planning and interventions. MiCA, a lightweight and architecture-agnostic module, infers mobility relations through causal discovery and integrates them into temporal forecasting models via gated residual mixing, improving lightweight forecasters' performance.",10.36,Qwen2.5-3B,Apple M1 (Metal)
2601.11090v1_Efficient Multilingual Name Type Classification Us.pdf,Eﬀicient Multilingual Name T ype Classification Using Convolutional Networks,Davor Lauc,Not found,Not found,"multilingual NLP, named entity recognition, convolutional neural networks, efficient inference, proper names","We present a convolutional neural network approach for classifying proper names by language and entity type. Our model, Onomas-CNN X, combines parallel convolution branches with depthwise-separable operations and hierarchical classification to process names efficiently on CPU hardware. We evaluate the architecture on a large multilingual dataset covering 104 languages and four entity types (person, organization, location, other). Onomas-CNN X achieves 92.1% accuracy while processing 2,813 names per second on a single CPU core, i46 times faster than fine-tuned XLM-RoBERTa with comparable accuracy. The model reduces energy consumption by a factor of 46 compared to transformer baselines. Our experiments demonstrate that specialized CNN architectures remain competitive with large pre-trained models for focused NLP tasks when sufficient training data exists.",11.98,Qwen2.5-3B,Apple M1 (Metal)
2601.11100v1_ReCreate Reasoning and Creating Domain Agents Driv.pdf,ReCreate: Reasoning and Creating Domain Agents Driven by Experience,"Zhezheng Hao, Hong Wang, Jian Luo, Jianqing Zhang, Yuyan Zhou, Qiang Lin, Can Wang, Hande Dong, Jiawei Chen",,,"Large Language Model, Agent Creation, Experience-driven, Automated Agent Generation, Domain Adaptation","This work proposes ReCreate, an experience-driven framework for the automatic creation of domain agents. It leverages agent interaction histories to systematically learn from experience, providing rich concrete signals on the causes of success or failure and avenues for improvement. ReCreate consistently outperforms human-designed agents and existing automated agent generation methods, even when starting from minimal seed scaffolds. It is applied across diverse domains, demonstrating competence on complex, long-horizon tasks such as software engineering, scientific discovery, and web navigation.",11.2,Qwen2.5-3B,Apple M1 (Metal)
2601.11109v1_Vision-as-Inverse-Graphics Agent via Interleaved M.pdf,Vision-as-Inverse-Graphics Agent via Interleaved Multimodal Reasoning,"Shaofeng Yin1,4*, Jiaxin Ge1, Zora Zhiruo Wang2, Xiuyu Li1, Michael J. Black3, Trevor Darrell1, Angjoo Kanazawa1, Haiwen Feng1,3,4†",fugtemypt123.github.io/VIGA-website,2601.11109,"computer vision, inverse graphics, multimodal reasoning, iterative refinement, 3D reconstruction, scene editing","Vision-as-inverse-graphics, the concept of reconstructing an image as an editable graphics program, is a long-standing goal of computer vision. Our work presents VIGA (Vision-as-Inverse-Graphic Agent), which alternates between generation and verification steps through iterative execution and reasoning, to reconstruct or edit scenes from a single image. VIGA combines a skill library and evolving context memory to support long-horizon reasoning and is task-agnostic and model-agnostic, improving one-shot baselines on BlenderGym and SlideBench.",12.66,Qwen2.5-3B,Apple M1 (Metal)
2601.11124v1_Learn Before Represent Bridging Generative and Con.pdf,Learn Before Represent: Bridging Generative and Contrastive Learning for Domain-Specific LLM Embeddings,"Xiaoyu Liang, Yuchen Peng, Jiale Luo, Wenhao Wang, Haoji Hu, Xincheng Zhou",,2601.11,"Large Language Models, Contrastive Learning, Generative Learning, Domain-Specific Embeddings, Knowledge Acquisition","This work proposes Learn Before Represent (LBR), a novel two-stage framework, to address the bottleneck of the prevailing 'LLM+CL' paradigm in vertical domains. LBR first injects domain knowledge via an Information Bottleneck-Constrained Generative Learning stage, preserving the LLM's causal attention to maximize knowledge acquisition while compressing semantics. It then performs Generative-Refined Contrastive Learning on the compressed representations for alignment. Extensive experiments on medical, chemistry, and code retrieval tasks show that LBR significantly outperforms strong baselines.",11.45,Qwen2.5-3B,Apple M1 (Metal)
2601.11135v1_Context-aware Graph Causality Inference for Few-Sh.pdf,Context-aware Graph Causality Inference for Few-Shot Molecular Property Prediction,"Van Thuy Hoang, O-Joun Lee*",Not found,Not found,"graph learning, few-shot learning, molecular property prediction, causal inference, context-aware","Molecular property prediction is becoming a major application of graph learning in Web-based services. A key challenge arises in few-shot scenarios where only a few labeled molecules are available for predicting unseen properties. This paper proposes CaMol, a context-aware graph causality inference framework to address these challenges by using a causal inference perspective. It introduces a context graph to encode chemical knowledge and a learnable atom masking strategy to disentangle causal substructures from confounding ones. The distribution intervener combines causal substructures with chemically grounded confounders to disentangle causal effects from real-world chemical variations. Experiments on diverse molecular datasets show superior accuracy and sample efficiency in few-shot tasks.",11.51,Qwen2.5-3B,Apple M1 (Metal)
2601.11143v1_Learning Quadrupedal Locomotion for a Heavy Hydrau.pdf,Learning Quadrupedal Locomotion for a Heavy Hydraulic Robot Using an Actuator Model,"Minho Lee, Hyeonseok Kim, Jin Tak Kim, Sangshin Park, Jeong Hyun Lee, Jungsan Cho, Jemin Hwangbo",Not found,Not found,"Hydraulic Actuators, Legged Robots, Reinforcement Learning, Model-Based Control, Hydraulic Quadruped Robot","This work proposes an analytical actuator model driven by hydraulic dynamics to represent the complex actuators of a heavy hydraulic quadruped robot. The model predicts joint torques for all 12 actuators in under 1 microsecond, enabling rapid processing in reinforcement learning environments. The locomotion policy trained with this model is deployed on a hydraulic quadruped robot over 300 kg, demonstrating successful transfer of stable and robust command-tracking locomotion with reinforcement learning.",11.31,Qwen2.5-3B,Apple M1 (Metal)
2601.11144v2_Deep GraphRAG A Balanced Approach to Hierarchical .pdf,Deep GraphRAG: A Balanced Approach to Hierarchical Retrieval and Adaptive Integration,"Yuejie Li, Ke Yang, Tao Wang, Bolin Chen, Bowen Li, Chengjun Mao",,,"GraphRAG, Reinforcement Learning, Large Language Models","Graph-based Retrieval-Augmented Generation (GraphRAG) frameworks face a trade-off between comprehensiveness of global search and efficiency of local search. Deep GraphRAG proposes a balanced approach to hierarchical retrieval and adaptive integration, introducing a hierarchical global-to-local retrieval strategy that integrates macroscopic inter-community and microscopic intra-community contextual relations. It employs a three-stage process: inter-community filtering, community-level refinement, and entity-level fine-grained search. A beam search-optimized dynamic re-ranking module guides this process, continuously filtering candidates to balance efficiency and global comprehensiveness. Deep GraphRAG also features a Knowledge Integration Module leveraging a compact LLM, trained with Dynamic Weighting Reward GRPO (DW-GRPO), dynamically adjusting reward weights to balance relevance, faithfulness, and conciseness.",11.93,Qwen2.5-3B,Apple M1 (Metal)
2601.11147v1_Do We Always Need Query-Level Workflows Rethinking.pdf,Do We Always Need Query-Level Workflows?,"Zixu Wang♣♡, Bingbing Xu♣ *, Yige Yuan♣♡, Huawei Shen♣, Xueqi Cheng♣",Not provided,Not provided,"Multi-Agent Systems, Workflow Generation, Query-Level, Task-Level, Self-Evolution, Generative Reward Modeling","This paper rethinks and analyzes the necessity of query-level workflow generation in Multi-Agent Systems (MAS) built on large language models. It shows that query-level workflow generation is not always necessary, as a small set of top-K best task-level workflows already covers equivalent or even more queries. The authors propose SCALE, a low-cost task-level generation framework that predicts the optimizer with few-shot calibration for evaluation instead of full validation execution. Extensive experiments demonstrate that SCALE maintains competitive performance while significantly reducing token usage.",11.37,Qwen2.5-3B,Apple M1 (Metal)
2601.11151v1_Cross-Modal Attention Network with Dual Graph Lear.pdf,Cross-Modal Attention Network with Dual Graph Learning in Multimodal Recommendation,"JI DAI, Beijing University of Posts and Telecommunications, China, QUAN FANG∗, Beijing University of Posts and Telecommunications, China, JUN HU, National University of Singapore, Singapore, DESHENG CAI, Tianjin University of Technology, China, YANG YANG, Beihang University, China and State Key Laboratory of CNS/ATM, China, CAN ZHAO, Aviation Data Communication Corporation, China",XXXXXXX.XXXXXXX,,"Multimedia recommendation, Graph Neural Network, Multimodal Fusion","This paper proposes a Cross-modal Recursive Attention Network with dual graph embedding (CRANE) to address the limitations of shallow modality fusion and asymmetric feature treatment in multimodal recommendation systems. CRANE iteratively refines modality features based on cross-correlations in a joint latent space, and integrates a symmetric dual-graph framework to fuse behavioral and semantic signals.",12.04,Qwen2.5-3B,Apple M1 (Metal)
2601.11160v1_Clustering High-dimensional Data Balancing Abstrac.pdf,Clustering High-dimensional Data: Balancing Abstraction and Representation,"Claudia Plant, Lena G. M. Bauer, Christian B ¨ohm",,,"Clustering, High-dimensional data, Abstraction, Representation, Subspace clustering, Deep clustering","How to find a natural grouping of a large real data set? Clustering requires a balance between abstraction and representation. To identify clusters, we need to abstract from superfluous details of individual objects, but also need a rich representation that emphasizes the key features shared by groups of objects that distinguish them from other groups of objects. Each clustering algorithm implements a different trade-off between abstraction and representation. Current deep clustering methods define and enforce abstraction through centroid-based and density-based clustering losses. Balancing the conflicting goals of abstraction and representation is challenging.",11.63,Qwen2.5-3B,Apple M1 (Metal)
2601.11178v1_TANDEM Temporal-Aware Neural Detection for Multimo.pdf,TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech,"Girish A. Koushik1*, Helen Treharne2, Diptesh Kanojia1",Not provided,Not provided,"hate speech, multimodal detection, temporal-aware, reinforcement learning, hate amplification","This work introduces TANDEM, a unified framework that transforms audio-visual hate detection from a binary classification task into a structured reasoning problem. It employs a novel tandem reinforcement learning strategy to optimize vision-language and audio-language models through self-constrained cross-modal context, achieving significant improvements in target identification and temporal grounding compared to existing methods.",11.13,Qwen2.5-3B,Apple M1 (Metal)
2601.11189v1_Policy-Based Deep Reinforcement Learning Hyperheur.pdf,Policy-Based Deep Reinforcement Learning Hyperheuristics for Job-Shop Scheduling,"Sofiene Lassoued *a, Asrat Gobachew b, Stefan Lier b, Andreas Schwung a",Not found,Not found,"Hyper-heuristics, Job Shop Scheduling, Policy-based Reinforcement learning, Petri nets","This paper proposes a policy-based deep reinforcement learning hyper-heuristic framework for solving the Job Shop Scheduling Problem. The hyper-heuristic agent learns to switch scheduling rules based on the system state dynamically. The framework extends with two key mechanisms: action prefiltering and a commitment mechanism. Computational experiments on standard JSSP benchmarks demonstrate the proposed approach outperforms traditional heuristics, metaheuristics, and recent neural network-based scheduling methods.",11.4,Qwen2.5-3B,Apple M1 (Metal)
2601.11196v1_Artificial Intelligence and the US Economy An Acco.pdf,Artificial Intelligence and the US Economy: An Accounting Perspective on Investment and Production,"Luisa Carpinelli, Filippo Natoli, Marco Taboga",Not found,2601.11196,"artificial intelligence, capital expenditures, data centers, national accounts","This paper provides an overview on how the current AI wave is captured in US national accounts, combining a simple macro-accounting framework with a stylized description of the AI production process.",11.73,Qwen2.5-3B,Apple M1 (Metal)
2601.11199v1_SD-RAG A Prompt-Injection-Resilient Framework for .pdf,SD-RAG: A Prompt-Injection-Resilient Framework for Selective Disclosure in Retrieval-Augmented Generation,"Aiman Al Masoud, Marco Arazzi, Antonino Nocera",Not found,Not found,"Retrieval-Augmented Generation, Prompt Injection, Selective Disclosure, Privacy, Security, Large Language Models","This paper proposes SD-RAG, a novel approach to selective disclosure in retrieval-augmented generation, which decouples security and privacy constraints from the generation process itself. It applies sanitization and disclosure controls during the retrieval phase, prior to augmenting the language model's input, and introduces a semantic mechanism for ingesting human-readable dynamic security and privacy constraints.",11.22,Qwen2.5-3B,Apple M1 (Metal)
2601.11200v1_FAQ Mitigating Quantization Error via Regenerating.pdf,FAQ: Mitigating Quantization Error via Regenerating Calibration Data,"Haiyang Xiao, Weiqing Li, Jinyue Guo, Guochao Jiang, Guohua Liu, Yuewei Zhang",,,"Quantization, Calibration, Post-Training Quantization, Family-Aware Quantization, Metal Objects, Wooden Objects, Inference Optimization","This paper proposes FAQ (Family-Aware Quantization), a calibration data regeneration framework that leverages prior knowledge from LLMs of the same family to generate high-fidelity calibration samples, addressing the core bottleneck of representativeness and universality of calibration data in determining the accuracy of quantization parameters. Experiments show that FAQ reduces accuracy loss by up to 28.5% compared to the baseline with original calibration data.",10.95,Qwen2.5-3B,Apple M1 (Metal)
2601.11202v1_Epistemic Control and the Normativity of Machine L.pdf,To be published in The Role of AI in Science: Epistemological and Methodological Studies,Emanuele Ratti,,,"machine learning, epistemic control, cognitive values, normativity","The past few years have witnessed an increasing use of machine learning (ML) systems in science. Paul Humphreys has argued that, because of specific characteristics of ML systems, human scientists are pushed out-of-the-loop of science. In this chapter, the author investigates to what extent this is true. First, the author expresses these concerns in terms of what he calls 'epistemic control'. He identifies two conditions for epistemic control, called 'tracking' and 'tracing', drawing on works in philosophy of technology. With this new understanding of the problem, the author argues against Humphreys' pessimistic view. Finally, the author constructs a more nuanced view of epistemic control in ML-based science.",12.48,Qwen2.5-3B,Apple M1 (Metal)
2601.11207v1_LoRA as Oracle.pdf,LoRA as Oracle,"Marco Arazzi, Antonino Nocera",,,"LoRA, Membership Inference Attack, Backdoor Attack","Backdoored and privacy-leaking deep neural networks pose a serious threat to the deployment of machine learning systems in security-critical settings. Existing defenses for backdoor detection and membership inference typically require access to clean reference models, extensive retraining, or strong assumptions about the attack mechanism. In this work, we introduce a novel LoRA-based oracle framework that leverages low-rank adaptation modules as a lightweight, model-agnostic probe for both backdoor detection and membership inference.",10.83,Qwen2.5-3B,Apple M1 (Metal)
2601.11219v1_SDFLoRA Selective Dual-Module LoRA for Federated F.pdf,SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients,"Zhikang Shen, Jianrong Lu, Haiyuan Wan, Jianhai Chen",Not provided,Not provided,"Federated Learning, LoRA, Privacy-Preserving, Parameter-Efficient, Differential Privacy","SDFLoRA proposes a method to address the rank heterogeneity issue in federated fine-tuning of large language models, by decomposing each client adapter into a global module for transferable knowledge and a local module for client-specific adaptations. This approach enables robust learning under rank heterogeneity and privacy-aware optimization by injecting differential privacy noise exclusively into the global module.",11.15,Qwen2.5-3B,Apple M1 (Metal)
2601.11232v1_FactCorrector A Graph-Inspired Approach to Long-Fo.pdf,FACTCORRECTOR: A Graph-Inspired Approach to Long-Form Factuality Correction of Large Language Models,"Javier Carnerero-Cano, Massimiliano Pronesti, Radu Marinescu, Tigran Tchrakian, James Barry, Jasmina Gajcin, Yufang Hou, Alessandra Pascale, Elizabeth Daly",,,"Large Language Models, Factuality Correction, Graph-Based Methods, Feedback Guided Correction","This paper introduces FACTCORRECTOR, a post-hoc correction method for large language models (LLMs) that adapts across domains without retraining and leverages structured feedback about the factuality of the original response to generate a correction. The authors also develop the VELI5 benchmark, a novel dataset containing systematically injected factual errors and ground-truth corrections, to support rigorous evaluations of factuality correction methods. Experiments on VELI5 and several popular long-form factuality datasets show that the FACTCORRECTOR approach significantly improves factual precision while preserving relevance, outperforming strong baselines.",11.41,Qwen2.5-3B,Apple M1 (Metal)
2601.11252v1_Beyond Model Scaling Test-Time Intervention for Ef.pdf,BEYONDMODELSCALING: TEST-TIMEINTERVENTION,"Qianyue Wang, Jinwu Hu, Yufeng Wang, Huanxiang Lin, Bolin Chen, Zhiquan Wen, Yaofo Chen, Mingkui Tan",Not found,Not found,"Large Reasoning Models, Efficient Reasoning, Test-Time Intervention, Interactive Reasoning, External Feedback, Transitional Conjunctions, Multi-criteria Evaluation, Group Relative Policy Optimization, Security, Creative Tasks","Large Reasoning Models excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot. Think-with-Me proposes a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process, enhancing performance and reducing redundancy while preserving accuracy.",11.18,Qwen2.5-3B,Apple M1 (Metal)
2601.11258v1_Knowledge is Not Enough Injecting RL Skills for Co.pdf,Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation,"Pingzhi Tang∗1,2, Yiding Wang∗1,2, Muhan Zhang1,3",Not found,Not found,"Reinforcement Learning, Continual Adaptation, Skill Transfer, Knowledge Updating","Large Language Models face the 'knowledge cutoff' challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning is commonly used to update model knowledge, it often fails to reliably improve the model's ability to use newly incorporated information. Reinforcement Learning is essential for acquiring reasoning skills, but its high computational cost makes it impractical for efficient online adaptation. This paper proposes Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, PaST can linearly inject knowledge manipulation skills into a target model after lightweight SFT on new data. Experiments on knowledge-incorporation QA and agentic tool-use benchmarks demonstrate the effectiveness of the method.",11.59,Qwen2.5-3B,Apple M1 (Metal)
2601.11269v1_X-Distill Cross-Architecture Vision Distillation f.pdf,X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning,"Maanping Shao, Feihong Zhang, Gu Zhang, Baiye Cheng, Zhengrong Xue, Huazhe Xu",Not found,Not found,"X-Distill, Vision Transformers, Knowledge Distillation, Visuomotor Policy, Representation Learning, Manipulation","X-Distill is a simple yet effective method that synergizes the strengths of both architectures by transferring the rich visual representations of a large, frozen DINOv2 teacher to a compact ResNet-18 student on the general-purpose ImageNet dataset. This distilled encoder, now endowed with powerful visual priors, is then jointly fine-tuned with a diffusion policy head on the target manipulation tasks. Extensive experiments demonstrate that our method consistently outperforms policies equipped with from-scratch ResNet or fine-tuned DINOv2 encoders.",11.14,Qwen2.5-3B,Apple M1 (Metal)
2601.11282v1_From SERPs to Sound How Search Engine Result Pages.pdf,FromSERPsto Sound: How Search Engine Result Pages and AI-generated Podcasts Interact to Influence User Attitudes on Controversial Topics,"Junjie Wang, Gaole He, Alisa Rieger, Ujwal Gadiraju",10.1145/3786304.3787942,,"Attitude Change, AI-generated Podcasts, Information modality, Web search, Controversial Topics, Responsible Opinion Formation","This study investigates user attitudinal effects of consuming information via search engine result pages (SERPs) and AI-generated podcasts, focusing on how the sequence and modality of exposure shape user opinions. A majority of users in the study corresponded to attitude change outcomes, and an effect of sequence on attitude change was found. The study also revealed a role of viewpoint bias and the degree of topic controversiality in shaping attitude change, although no effect of individual moderators was found.",11.23,Qwen2.5-3B,Apple M1 (Metal)
2601.11286v1_XChoice Explainable Evaluation of AI-Human Alignme.pdf,XChoice: Explainable Evaluation of AI–Human Alignment in LLM-based Constrained Choice Decision Making,"Weihong Qi, Fan Huang, Rasika Muralidharan, Jisun An, Haewoon Kwak",,,"explainable AI, human-AI alignment, LLM decision making, constrained choice","XChoice presents an explainable framework for evaluating AI-human alignment in constrained decision making, focusing on decision mechanisms, constraint sensitivity, and trade-offs. It assesses alignment by comparing interpretable parameters across models, options, and subgroups, demonstrating its effectiveness on Americans' daily time allocation using ATUS data. The framework reveals heterogeneous alignment across models and activities and salient misalignment in Black and married groups. Robustness is validated via an invariance analysis, and targeted mitigation is evaluated with a retrieval-augmented generation (RAG) intervention.",11.19,Qwen2.5-3B,Apple M1 (Metal)
2601.11344v1_How Much Would a Clinician Edit This Draft Evaluat.pdf,How Much Would a Clinician Edit This Draft?,"Parker Seegmiller1, Joseph Gatto1, Sarah E. Greer1, Ganza Belise Isingizwe1, Rohan Ray1, Timothy Burdick2, Sarah M. Preum1",Not provided,Not provided,"Large language models, patient portal messages, clinician editing, AI alignment","This study investigates the alignment between clinicians and large language models (LLMs) in drafting responses to patient portal messages. It develops a novel taxonomy of thematic elements and proposes an evaluation framework for assessing clinician editing load at both content and theme levels. The results reveal substantial epistemic uncertainty in aligning LLM drafts with clinician responses, highlighting the need for adaptation to individual clinician preferences.",11.31,Qwen2.5-3B,Apple M1 (Metal)
2601.11350v1_FEATHer Fourier-Efficient Adaptive Temporal Hierar.pdf,FEATHer: Fourier-Efficient Adaptive Temporal Hierarchy Forecaster for Time-Series Forecasting,"Jaehoon Lee †, Seungwoo Lee †, Younghwi Kim†, Dohee Kim*, Sunghyun Sim*",Not found,Not found,"Time-series Forecasting, Edge AI, Ultra-Lightweight Models, Fourier-Efficient Adaptive Temporal Hierarchy Forecaster (FEATHer)","Time-series forecasting plays a fundamental role in industrial domains such as manufacturing, energy management, logistics, and smart factory operations. The proposed Fourier-Efficient Adaptive Temporal Hierarchy Forecaster (FEATHer) is a multiscale temporal model designed to achieve accurate long-term forecasting under severe resource limitations.",11.69,Qwen2.5-3B,Apple M1 (Metal)
2601.11354v1_AstroReason-Bench Evaluating Unified Agentic Plann.pdf,AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems,"Weiyi Wang, Xinchi Chen, Jingjing Gong, Xuanjing Huang, Xipeng Qiu",Not found,2601.11354,"Large Language Models, Space Planning Problems, Agentic Planning, Physical Constraints, Long-Horizon Decision-Making","This paper introduces AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems, which are high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. The benchmark integrates multiple scheduling regimes and provides a unified agent-oriented interaction protocol, evaluating current agent systems on state-of-the-art open and closed-source agentic LLMs.",12.35,Qwen2.5-3B,Apple M1 (Metal)
2601.11359v1_Think-Clip-Sample Slow-Fast Frame Selection for Vi.pdf,THINK-CLIP-SAMPLE: SLOW-FAST FRAME SELECTION FOR VIDEO UNDERSTANDING,"Wenhui Tan∗, Ruihua SongB, Jiaze Li, Jianzhong Ju, Zhenbo LuoB",Not found,Not found,"Multi-modal LLMs, long video understanding, frame selection, slow-fast sampling","A training-free framework that enhances long video understanding through multi-query reasoning and clip-level slow-fast sampling, demonstrating consistent improvements across different MLLMs and achieving comparable accuracy with 50% fewer inference time cost.",10.48,Qwen2.5-3B,Apple M1 (Metal)
2601.11369v2_Institutional AI Governing LLM Collusion in Multi-.pdf,Institutional AI: Governing LLM Collusion in Multi-Agent Markets via Public Governance Graphs,"M. Bracale Syrnikov, F. Pierucci, M. Galisai, M. Prandi, P. Bisconti, F. Giarrusso, O. Sorokoletova, V. Suriani, D. Nardi",10.48550/arXiv.2601.11369,2601.11369,"AI alignment, multi-agent systems, governance graphs, Cournot markets, collusion","This paper presents an experimental framework for evaluating Institutional AI, a system-level approach to AI alignment that reframes alignment from preference engineering in agent space to mechanism design in institutional space. The framework uses a governance graph, a public, immutable manifest that declares legal states, transitions, sanctions, and restorative paths, to govern multi-agent LLM ensembles and reduce collusion in Cournot markets.",12.31,Qwen2.5-3B,Apple M1 (Metal)
2601.11379v1_Evaluating LLM Behavior in Hiring Implicit Weights.pdf,"Evaluating LLM Behavior in Hiring: Implicit Weights, Fairness Across Groups, and Alignment with Human Preferences","Morgane Hoffmann1, Emma Jouffroy1, Warren Jouanneau1, Marc Palyart1, Charles Pebereau1",Not provided,Not provided,"Large Language Models, Person-job Fit, Fairness, Interpretability","This paper evaluates the decision logic of a Large Language Model (LLM) in recruitment, focusing on how it assigns importance to different criteria and whether these assignments align with human preferences and societal norms. By analyzing synthetic datasets from real freelancer profiles and project descriptions, the authors estimate how the LLM weighs various match-relevant criteria and identify which attributes it prioritizes. The study also examines how these weights vary across project contexts and demographic subgroups, and proposes a framework for systematic comparison with human recruiters.",12.08,Qwen2.5-3B,Apple M1 (Metal)
2601.11389v1_Hyperparameter Optimization of Constraint Programm.pdf,Hyperparameter Optimization of Constraint Programming Solvers,"Hedieh Haddad1*, Thibault Falque 2, Pierre Talbot 2, Pascal Bouvry2",Not provided,2601.11389,"constraint programming, hyperparameter optimization, automated optimization, Bayesian optimization, Hamming distance search","The performance of constraint programming solvers is highly sensitive to hyperparameters. This paper introduces a novel two-phase framework, probe and solve algorithm, for automated hyperparameter optimization, comparing it against solver's default configurations on two solvers, ACE and Choco, across 114 combinatorial problem instances.",12.61,Qwen2.5-3B,Apple M1 (Metal)
2601.11400v1_Wetland mapping from sparse annotations with satel.pdf,Wetland mapping from sparse annotations with satellite image time series and temporal-aware segment anything model,"Shuai Yuana, Tianwu Linb, Shuang Chena, Yu Xiab, Peng Qinb, Xiangyu Liub, Xiaoqing Xub, Nan Xud, Hongsheng Zhanga, Jie Wangb, Peng Gonga",Not found,2601.11400v1,"wetland mapping, satellite image time series, sparse annotation, segment anything model, temporal adaptation","Accurate wetland mapping is critical for ecosystem monitoring and management, yet acquiring dense pixel-level annotations is prohibitively costly. In practice, only sparse point labels are typically available, and existing deep learning-based models struggle under such weak supervision. Meanwhile, wetlands exhibit strong seasonal and inter-annual dynamics, making single-date imagery insufficient and causing substantial omission and commission errors when mapping. To address these challenges, we propose WetSAM, a novel SAM-based framework that effectively leverages satellite image time series to enhance wetland mapping from sparse point annotations.",12.65,Qwen2.5-3B,Apple M1 (Metal)
2601.11409v1_Topology-Guaranteed Image Segmentation Enforcing C.pdf,"Topology-Guaranteed Image Segmentation: Enforcing Connectivity, Genus, and Width Constraints","Wenxiao Li, Xue-Cheng Tai, Jun Liu",Not found,Not found,"image segmentation, topological preservation, persistent homology, thickness of topology, variational, regularization","Existing research highlights the crucial role of topological priors in image segmentation, particularly in preserving essential structures such as connectivity and genus. This paper proposes a novel mathematical framework that explicitly integrates width information into the characterization of topological structures, enabling the resulting topological structures to inherently capture width properties. This approach is used to design neural networks that can segment images with the required topological and width properties, successfully preserving essential topological invariants such as connectivity and genus counts, while ensuring that segmented structures retain critical width attributes.",11.72,Qwen2.5-3B,Apple M1 (Metal)
2601.11421v1_The Great March 100 100 Detail-oriented Tasks for .pdf,THEGREATMARCH100: 100 DETAIL-ORIENTEDTASKS FOR EVALUATING EMBODIED AI AGENTS,"Ziyu Wang, Chenyuan Liu, Yushun Xiang, Runhao Zhang, Yu Zhang, Qingbo Hao, Hongliang Lu, Houyu Chen, Zhizhong Feng, Kaiyue Zheng, Dehao Ye, Xianchao Zeng, Xinyu Zhou, Boran Wen, Jiaxin Li, Mingyu Zhang, Kecheng Zheng, Qian Zhu, Ran Cheng, Yong-Lu Li",Not found,Not found,"robot learning, embodied AI, task design, evaluation, long-tail behavior, human-object interaction, trajectory data","Introducing the Great March 100 (GM-100) as the first step towards a robot learning Olympics, consisting of 100 carefully designed tasks covering a wide range of interactions and long-tail behaviors to comprehensively evaluate the capabilities of robotic agents and promote diversity and complexity in robot dataset task designs.",11.86,Qwen2.5-3B,Apple M1 (Metal)
2601.11429v1_Relational Linearity is a Predictor of Hallucinati.pdf,Relational Linearity is a Predictor of Hallucinations,"Yuetian Lu, Yihong Liu, Hinrich Schütze",,,"hallucinations, large language models, factual knowledge, hallucination detection, relation linearity","Hallucinations are a central failure mode in large language models. The study investigates the relationship between the linearity of relations and hallucinations, finding a strong correlation between relational linearity and hallucination rate. The findings suggest that the underlying storage of triples of a relation is a factor in how well a model can self-assess its knowledge, providing implications for hallucination management and new research directions for improving factual knowledge representation in LLMs.",10.71,Qwen2.5-3B,Apple M1 (Metal)
2601.11440v1_GenDA Generative Data Assimilation on Complex Urba.pdf,Generative Data Assimilation for Urban Wind Flow Reconstruction,"Francisco Giral, Álvaro Manzano, Ignacio Gómez, Ricardo Vinuesa, Soledad Le Clainche",Not found,Not found,"Urban wind flow, Data assimilation, Graph-based diffusion, Generative models, Complex domains","This paper proposes GenDA, a generative data assimilation framework that reconstructs high-resolution wind fields on unstructured meshes from limited observations. The model employs a multiscale graph-based diffusion architecture trained on computational fluid dynamics (CFD) simulations and interprets classifier-free guidance as a learned posterior reconstruction mechanism. The proposed framework enables obstacle-aware reconstruction and generalization across unseen geometries, wind directions, and mesh resolutions without retraining. It is evaluated against supervised graph neural network (GNN) baselines and classical reduced-order data assimilation methods, reducing the relative root-mean-square error (RRMSE) by 25-57% and increasing the structural similarity index (SSIM) by 23-33% across tested meshes.",12.29,Qwen2.5-3B,Apple M1 (Metal)
2601.11441v1_Hierarchical Orthogonal Residual Spread for Precis.pdf,HIERARCHICAL ORTHOGONAL RESIDUAL SPREAD FOR PRECISE MASSIVE EDITING IN LARGE LANGUAGE MODELS,"Xiaojie Gu, Guangxu Chen, Yuheng Yang, Jingxin Han, Andi Zhang",Not found,Not found,"Large language models, Model Editing, Knowledge Update, Residual Spread","Large language models (LLMs) exhibit exceptional performance across various domains, yet they face critical safety concerns. Model editing has emerged as an effective approach to mitigate these issues. Existing methods often focus on optimizing an information matrix that blends new and old knowledge, but they can be computationally expensive and may cause conflicts. In contrast, this paper introduces HORSE, which shifts attention to the hierarchical orthogonal residual spread of the information matrix, reducing noisy gradients and enabling more stable edits. The method is demonstrated to maintain precise massive editing across diverse scenarios.",11.7,Qwen2.5-3B,Apple M1 (Metal)
2601.11442v1_Map2Thought Explicit 3D Spatial Reasoning via Metr.pdf,Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps,"Xiangjun Gao, Zhensong Zhang, Dave Zhenyu Chen, Songcen Xu, Long Quan, Eduardo P´erez-Pellitero, Youngkyoon Jang",,,"3D Vision-Language Models, Metric Cognitive Map, Cognitive Chain-of-Thought, 3D Spatial Reasoning, Explicit Interpretability, Data Efficiency","Map2Thought is a framework that enables explicit and interpretable spatial reasoning for 3D Vision-Language Models (3D-VLMs). It introduces a metrically grounded cognitive map (Metric-CogMap) and a chain-of-thought-style reasoning process (Cog-CoT), providing a unified spatial representation and enabling deterministic geometric reasoning. Experimental results show that Map2Thought achieves high accuracy with minimal supervision, outperforming state-of-the-art methods by significant margins.",11.6,Qwen2.5-3B,Apple M1 (Metal)
2601.11451v1_PRISM-CAFO Prior-conditioned Remote-sensing Infras.pdf,PRISM-CAFO: Prior-conditioned Remote-sensing Infrastructure Segmentation and Mapping for CAFOs,"Oishee Bintey Hoque, Nibir Chandra Mandal, Kyle Luong, Amanda Wilson, Samarth Swarup, Madhav Marathe, Abhijin Adiga",Not found,2601.11451,"CAFOs, Remote sensing, Infrastructure segmentation, Mapping, Concentrated Animal Feeding Operations, YOLOv8, Swin-B, PRISM-CAFO","This work presents an infrastructure-first, explainable pipeline for identifying and characterizing Concentrated Animal Feeding Operations (CAFOs) from aerial and satellite imagery. The method detects candidate infrastructure, derives masks, and extracts structured descriptors, fusing them with deep visual features to achieve state-of-the-art performance.",11.78,Qwen2.5-3B,Apple M1 (Metal)
2601.11459v1_Interactive Narrative Analytics Bridging Computati.pdf,Interactive Narrative Analytics: Bridging Computational Narrative Extraction and Human Sensemaking,BRIAN KEITH,10.1 109/ACCESS.2025.3650352,,"Interactive Narrative Analytics, Narrative Extraction, Interactive Visual Analytics, Sensemaking, Information Overload, Misinformation, Semantic Interaction, Temporal Analysis, Causal Analysis, Relational Analysis","This paper introduces Interactive Narrative Analytics (INA), a field that combines computational narrative extraction with interactive visual analytics to support sensemaking. INA addresses challenges in narrative understanding, such as scalability, interactivity, knowledge integration, and evaluation standardization, offering promising opportunities in news analysis, intelligence, scientific literature exploration, and social media analysis.",11.4,Qwen2.5-3B,Apple M1 (Metal)
2601.11464v1_MHA2MLA-VLM Enabling DeepSeeks Economical Multi-He.pdf,MHA2MLA-VLM: Enabling DeepSeek’s Economical Multi-Head Latent Attention across Vision-Language Models,"Xiaoran Fan, Zhichao Sun, Tao Ji, Lixing Shen, Tao Gui",Not provided,Not provided,"Vision-Language Models, Multi-Head Latent Attention, Key-Value Cache, Efficient Inference, Modality Adaptation","This work presents MHA2MLA-VLM, a parameter-efficient framework for converting off-the-shelf vision-language models to Multi-Head Latent Attention. It introduces two core techniques: a modality-adaptive partial-RoPE strategy and a modality-decoupled low-rank approximation method, which independently compress the visual and textual KV spaces. The approach minimizes adaptation cost and demonstrates significant performance improvements in inference efficiency and memory footprint reduction.",11.21,Qwen2.5-3B,Apple M1 (Metal)
2601.11468v1_Exploring LLM Features in Predictive Process Monit.pdf,Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs,"ALESSANDRO PADELLA, MASSIMILIANO DE LEONI, MARLON DUMAS",10.1145/3306321.3306322,2601.00001,"Predictive process monitoring, Large language models, Trace Encoding","This paper extends the authors' prior work on LLM-based Predictive Process Monitoring, focusing on evaluating its generality, semantic leverage, and reasoning mechanisms across multiple Key Performance Indicators. Empirical evaluations indicate that LLMs can surpass benchmark methods in data-scarce settings with only 100 traces, leveraging both prior knowledge and internal correlations among training traces. The model demonstrates higher-order reasoning to generate predictions.",11.6,Qwen2.5-3B,Apple M1 (Metal)
2601.11479v1_Health Facility Location in Ethiopia Leveraging LL.pdf,Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning,"Yohai Trabelsi, Guojun Xiong, Fentabil Getnet, Stéphane Verguet, Milind Tambe",Not found,Not found,"Health Facility Location, Optimization, Human expert knowledge, Alignment, LLM","Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources require careful prioritization of which facilities to upgrade. The authors propose a hybrid framework that integrates expert knowledge with optimization techniques, combining a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement. Experiments on real-world data demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.",11.16,Qwen2.5-3B,Apple M1 (Metal)
2601.11492v1_BoxMind Closed-loop AI strategy optimization for e.pdf,BoxMind: Closed-loop AI strategy optimization for elite boxing,"Kaiwen Wang, Kaili Zheng, Rongrong Deng, Qingmin Fan, Milin Zhang, Zongrui Li, Xuesi Zhou, Bo Han, Liren Chen, Chenyi Guo, Ji Wu",Not found,2601.11492,"AI, boxing, tactical analysis, closed-loop system, machine learning","Presenting BoxMind, a closed-loop AI expert system validated in elite boxing competition, which defines atomic punch events with precise temporal boundaries and spatial and technical attributes, parses match footage into 18 hierarchical technical-tactical indicators, and proposes a graph-based predictive model to capture the dynamics of boxer matchups, achieving state-of-the-art performance in predicting match outcomes.",12.04,Qwen2.5-3B,Apple M1 (Metal)
2601.11496v1_The Poisoned Apple Effect Strategic Manipulation o.pdf,The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents,"Eilam Shapira, Moshe Tennenholtz, Roi Reichart",,,"AI agents, economic markets, strategic interaction, technology expansion, regulatory manipulation","The integration of AI agents into economic markets alters strategic interaction, leading to a phenomenon termed the 'Poisoned Apple' effect. Simply increasing the choice of AI delegates can shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, agents may release new technologies solely to manipulate the regulator's choice of market design in their favor, improving their own welfare at the expense of their opponent and the regulator's fairness objectives. This vulnerability necessitates dynamic market designs that adapt to the evolving landscape of AI capabilities.",12.08,Qwen2.5-3B,Apple M1 (Metal)
2601.11505v1_MetaboNet The Largest Publicly Available Consolida.pdf,The Largest Publicly Available Consolidated Dataset for Type 1 Diabetes Management,"Miriam K. Wolff, Peter Calhoun, Eleonora Maria Aiello, Yao Qin, Sam F. Royston",10.1155/2026011505,2601.11505,"Type 1 Diabetes, Consolidated Dataset, Continuous Glucose Monitoring, Insulin Pump, Data Fragmentation, Data Standardization, Algorithm Development, Public Dataset","This work aims to establish a unified and accessible data resource for Type 1 Diabetes algorithm development by consolidating multiple publicly available datasets, resulting in a substantially larger dataset than existing standalone benchmark datasets.",12.09,Qwen2.5-3B,Apple M1 (Metal)
2601.11516v2_Building Production-Ready Probes For Gemini.pdf,Building Production-Ready Probes For Gemini,"János Kramár∗, Joshua Engels, Zheng Wang, Bilal Chughtai, Rohin Shah, Neel Nanda, Arthur Conmy∗, Equal contributions to this work",Not found,Not found,"Activation Probing, Interpretability, Language Models, Misuse Risk, AI Safety, Monitoring","In this paper, we describe our experience applying probes to detect cyber-offensive prompts given as input to Gemini 2.5 Flash. We identify a key challenge in existing probe architectures: they fail to generalize under important production distribution shifts, particularly the shift from short-context to long-context inputs. We propose several new probe architectures that handle this long-context distribution shift and evaluate them in the cyber-offensive domain, testing their robustness against various production-relevant distribution shifts. Our findings inform the successful deployment of misuse mitigation probes in user-facing instances of Gemini and show early positive results using AlphaEvolve to automate improvements in probe architecture search and adaptive red teaming.",12.14,Qwen2.5-3B,Apple M1 (Metal)
2601.11517v1_Do explanations generalize across large reasoning .pdf,Under Review,"Koyena Pal, David Bau, Chandan Singh",,,"Large reasoning models, chains of thought, generalization, explanations, reinforcement learning","This paper investigates the generalization of explanations produced by large reasoning models, evaluating whether these explanations can induce the same behavior when given to other LRMs, and how this generalization is correlated with human preference rankings and post-training with reinforcement learning.",9.74,Qwen2.5-3B,Apple M1 (Metal)
2601.11625v1_Reasoning Stabilization Point A Training-Time Sign.pdf,Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance,Sahil Rajesh Dhayalkar,Not found,Not found,"Fine-tuning, Language models, Explainability, Token-level attribution, Shortcut reliance","Fine-tuning pretrained language models can subtly alter the evidence a model relies on. This paper proposes a training-time interpretability view that tracks token-level attributions across fine-tuning epochs. Explanation drift is defined as the epoch-to-epoch change in normalized token attributions on a fixed probe set, and the Reasoning Stabilization Point (RSP) is introduced as the earliest epoch after which drift remains consistently low. Across multiple lightweight transformer classifiers and benchmark classification tasks, drift typically collapses into a low, stable regime early in training, while validation accuracy continues to change only marginally. In a controlled shortcut setting, attribution dynamics expose increasing reliance on the shortcut even when validation accuracy remains competitive. Overall, explanation drift provides a simple, low-cost diagnostic for monitoring how decision evidence evolves during fine-tuning and for selecting stable-evidence checkpoints.",12.07,Qwen2.5-3B,Apple M1 (Metal)
2601.11643v1_Syllabic Agglutinative Tokenizations for Indonesia.pdf,Syllabic Agglutinative Tokenizations for Indonesian LLM: A Study from 'Gasing Literacy Learning System',"Hokky Situngkir*, Andhika Bernard Lumbantobing†, Yohanes Surya‡",,2601.11643v1,"Indonesian natural language processing, Indonesian computational linguistics, tokenization, large language models, Gasing Literacy Learning System, low-resource languages, Austronesian languages","This paper presents a novel syllable-based tokenization approach for Indonesian large language models, inspired by the Gasing Literacy Learning System's pedagogical methodology. It develops a tokenization framework that segments Indonesian text at syllable boundaries before applying byte-pair encoding, creating a vocabulary that aligns with the language's morphophonological structure. Empirical evaluation on Indonesian Wikipedia and folklore corpora demonstrates substantial improvements over conventional tokenization methods.",11.9,Qwen2.5-3B,Apple M1 (Metal)
2601.11644v1_Predicting When to Trust Vision-Language Models fo.pdf,Predicting When to Trust Vision-Language Models for Spatial Reasoning,"Muhammad Imran, Yugyung Lee",,,"Vision-Language Models, Spatial Reasoning, Robotic Navigation, Autonomous Driving, Image Editing","Vision-Language Models (VLMs) excel at high-level semantic tasks but exhibit critical weaknesses in spatial reasoning. This paper proposes a vision-based confidence estimation framework that validates VLM predictions through independent geometric verification using object detection. Unlike text-based approaches, our method fuses four signals via gradient boosting: geometric alignment between VLM claims and coordinates, spatial ambiguity from overlap, detection quality, and VLM internal uncertainty. Achieving AUROC of 0.674 on BLIP-2 and 0.583 on CLIP, our framework enables selective prediction and demonstrates reliable scene graph construction.",10.82,Qwen2.5-3B,Apple M1 (Metal)
2601.11647v1_Reinforcement Learning for Dynamic Workflow Optimi.pdf,Reinforcement Learning for Dynamic Workflow Optimization in CI/CD Pipelines,"Aniket Abhishek Soni, Milan Parikh, Rashi Nimesh Kumar Dhenia, Jubin Abhishek Soni, Ayush Raj Jha, Sneja Mitinbhai Shah",,,"Reinforcement Learning, CI/CD, DevOps, Workflow Optimization","This paper proposes a reinforcement learning (RL) approach to optimize CI/CD pipeline workflows dynamically. It models the pipeline as a Markov Decision Process (MDP) and trains an RL agent to make runtime decisions that maximize throughput while minimizing testing overhead. Experimental results show that the RL-optimized pipeline achieves up to a 30% improvement in throughput and about a 25% reduction in test execution overhead compared to a static baseline. The agent learns to skip or abbreviate certain tests when appropriate, accelerating delivery without significantly increasing the risk of undetected failures.",11.69,Qwen2.5-3B,Apple M1 (Metal)
2601.11650v1_Large Language Model Agent for User-friendly Chemi.pdf,LARGELANGUAGEMODELAGENT FORUSER-FRIENDLY CHEMICALPROCESSSIMULATIONS,"Jingkang Liang ∗, Niklas Groll∗, Gürkan Sin",Not found,2601.11650v1,"Chemical Process Simulation, Large Language Model, Model Context Protocol","This paper presents a framework integrating a large language model (LLM) agent with A VEV A Process Simulation (APS) via Model Context Protocol (MCP) to enable natural language interaction with rigorous process simulations. Two case studies demonstrate the framework's capabilities in analyzing flowsheets, optimizing processes, and synthesizing flowsheets, benefiting both educational and practical applications.",12.29,Qwen2.5-3B,Apple M1 (Metal)
2601.11651v1_Aesthetics as Structural Harm Algorithmic Lookism .pdf,Aesthetics as Structural Harm:ALGORITHMICLOOKISMACROSS,"Miriam Doh, Aditya Gulati, Corina Canali, Nuria Oliver",Not found,2601.11651,"Generative AI, Artificial Intelligence, Cognitive Biases, Attractiveness Halo Effect","This paper examines algorithmic lookism—the systematic preferential treatment based on physical appearance—in text-to-image (T2I) generative AI and a downstream gender classification task. Through the analysis of 26,400 synthetic faces, it demonstrates how generative AI models systematically associate facial attractiveness with positive attributes and vice versa, mirroring socially constructed biases rather than evidence-based correlations. It also finds significant gender bias in three gender classification algorithms depending on the attributes of the input faces.",11.54,Qwen2.5-3B,Apple M1 (Metal)
2601.11652v1_WISP Waste- and Interference-Suppressed Distribute.pdf,WISP: Waste- and Interference-Suppressed Distributed Speculative LLM Serving at the Edge via Dynamic Drafting and SLO-Aware Batching,"XIANGCHEN LI, JIAKUN FAN, QINGYUAN WANG, DIMITRIOS SPATHARAKIS, SAEID GHAFOURI, HANS VANDIERENDONCK, DEEPU JOHN, BO JI, ALI R. BUTT, DIMITRIOS S. NIKOLOPOULOS",10.1145/376xxxx.377xxxx,,"Large Language Models, Edge Computing, Speculative Decoding, Token Verification, Resource-Aware Serving","This paper proposes WISP, an efficient and SLO-aware distributed LLM inference system that addresses the bottlenecks of wasted drafting time and verification interference in speculative LLM serving at the edge. It improves system capacity and goodput by up to 2.1× and 3.7× compared to centralized serving and SLED, respectively.",11.75,Qwen2.5-3B,Apple M1 (Metal)
2601.11657v1_Size is Not the Solution Deformable Convolutions f.pdf,Size is Not the Solution: Deformable Convolutions for Effective Physics Aware Deep Learning,"Jack T. Beerman, Shobhan Roy, H.S. Udaykumar, Stephen S. Baek",Not found,Not found,"Physics-aware deep learning, Convolutional neural networks, Deformable convolutions, Hybrid Lagrangian-Eulerian methods, Recurrent convolutions, Deep learning for physics","Physics-aware deep learning (PADL) enables rapid prediction of complex physical systems, yet current convolutional neural network (CNN) architectures struggle with highly nonlinear flows. Drawing inspiration from Hybrid Lagrangian-Eulerian (HLE) numerical methods, we introduce deformable physics-aware recurrent convolutions (D-PARC) to overcome the rigidity of CNNs. Across Burgers’ equation, Navier-Stokes, and reactive flows, D-PARC achieves superior fidelity compared to substantially larger architectures. Analysis reveals that kernels display anti-clustering behavior, evolving into a learned 'active filtration' strategy distinct from traditional h- or p-adaptivity. Effective receptive field analysis confirms that D-PARC autonomously concentrates resources in high-strain regions while coarsening focus elsewhere, mirroring adaptive refinement in computational mechanics.",12.66,Qwen2.5-3B,Apple M1 (Metal)
2601.11658v1_Towards AGI A Pragmatic Approach Towards Self Evol.pdf,Towards AGI: A Pragmatic Approach Towards Self Evolving Agent,"Indrajit Kar, Zonunfeli Ralte",,,"Large Language Models (LLMs), Self-evolving agents, Curriculum Learning (CL), Reward-Based Learning (RL), Genetic Algorithm (GA) evolution, Multi-agent systems, Tool-augmented reasoning, Code-generation LLMs, Autonomous adaptation, TaskCraft dataset, Agentic workflows, Self-improving AI, Capability evolution, Hierarchical orchestration","This work introduces a hierarchical self-evolving multi-agent framework that integrates a Base LLM, an operational SLM agent, a Code-Generation LLM, and a Teacher-LLM to enable continuous adaptation. The workflow begins with the agent attempting a task using reasoning and existing tools; if unsuccessful, it escalates to tool synthesis through the Code-Gen LLM, and when failures persist, it triggers an evolution phase using Curriculum Learning (CL), Reward-Based Learning (RL), or Genetic Algorithm (GA) evolution. Using the TaskCraft dataset rich in hierarchical tasks, tool-use traces, and difficulty scaling, the paradigms are evaluated. CL delivers fast recovery and strong generalization, RL excels on high-difficulty tasks, and GA offers high behavioral diversity. Across all settings, evolved agents outperform their originals, demonstrating robust, autonomous, self-improving agentic evolution.",12.87,Qwen2.5-3B,Apple M1 (Metal)
2601.11663v1_Activation Sensitivity as a Unifying Principle for.pdf,ACTIVATIONSENSITIVITY AS AUNIFYINGPRINCIPLE FOR POST-TRAININGQUANTIZATION,Bruce Changlong Xu,,,"quantization, post-training, activation sensitivity, language models","This work presents a unified theoretical framework for post-training quantization (PTQ) by formalizing activation sensitivity, which is the expected impact of channel-wise perturbations on the loss. It connects gradient-based saliency, Fisher information, and Hessian-based criteria, and clarifies their relationships to classical pruning methods. This perspective exposes fundamental limitations of layer-local reconstruction objectives and highlights open challenges in PTQ, including cross-layer error accumulation and task-conditional sensitivity.",11.54,Qwen2.5-3B,Apple M1 (Metal)
2601.11664v1_Serverless AI Security Attack Surface Analysis and.pdf,Serverless AI Security: Attack Surface Analysis and Runtime Protection Mechanisms for FaaS-Based Machine Learning,"Chetan Pathade, Vinod Dhimam, Ilsa Lareb, Sheheryar Ahmad",Not provided,Not provided,"Serverless computing, machine learning security, Function-as-a-Service, cloud security, adversarial machine learning, AWS Lambda, Azure Functions, attack surface analysis, runtime protection, MLOps security","This paper presents a comprehensive security analysis of machine learning workloads in serverless environments, focusing on five categories: function-level vulnerabilities, model-specific threats, infrastructure attacks, supply chain risks, and IAM complexity. Empirical assessments across AWS Lambda, Azure Functions, and Google Cloud Functions demonstrate real-world attack scenarios and quantify their security impact. The paper proposes Serverless AI Shield (SAS), a multi-layered defense framework for serverless AI deployments.",11.97,Qwen2.5-3B,Apple M1 (Metal)
2601.11666v1_MATEX Multi-scale Attention and Text-guided Explai.pdf,MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models,"Muhammad Imran, Chi Lee, Yugyung Lee",Not found,2601.11666,"Explainable AI, Medical Imaging, Vision-Language Models, Gradient Attribution, Attention Rollout, Chest X-ray, CLIP","We introduce MATEX, a novel framework that advances interpretability in medical vision-language models by incorporating anatomically informed spatial reasoning. MATEX synergistically combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to produce precise, stable, and clinically meaningful gradient attribution maps.",11.64,Qwen2.5-3B,Apple M1 (Metal)
2601.11667v1_Distill-then-Replace Efficient Task-Specific Hybri.pdf,Distill-then-Replace: Efficient Task-Specific Hybrid Attention Model Construction,"Xiaojie Xia, Huigang Zhang, Chaoliang Zhong, Jun Sun, Yusuke Oishi",Not found,2601.11667,"Hybridattentionmodels, Blockwiselocaldistillation, Greedy, search","This paper addresses the challenges of training and deploying hybrid attention models with quadratic time and memory complexity. It proposes a method to transfer weights from pretrained full-attention modules to linear attention counterparts through blockwise local distillation, and introduces a greedy layer replacement strategy to iteratively substitute full attention blocks with linear ones while monitoring validation performance on the target task. This results in a task-specific hybrid model that can be applied to any pretrained full-attention backbone for diverse downstream tasks without costly re-training or neural architecture search.",12.62,Qwen2.5-3B,Apple M1 (Metal)
2601.11670v1_A Confidence-Variance Theory for Pseudo-Label Sele.pdf,A Conﬁdence-V ariance Theory for Pseudo-Label Selection in Semi-Supervised Learning,"Jinshi Liu †, Pan Liu †",Not found,Not found,"Semi-Supervised Learning, Pseudo-Labels, Confidence Calibration, Residual Class Variance, Spectral Relaxation, Semantic Segmentation, Image Classification","This paper introduces a Conﬁdence-Variance (CoVar) theory framework for pseudo-label selection in semi-supervised learning. It combines maximum conﬁdence with residual-class variance to provide a principled joint reliability criterion, correcting overconﬁdent but unstable predictions. The CoVar framework is integrated into various methods and consistently improves over strong baselines across different datasets and label ratios.",10.99,Qwen2.5-3B,Apple M1 (Metal)
2601.11674v1_Pigment Network Detection and Classification in De.pdf,Pigment Network Detection and Classification in Dermoscopic Images Using Directional Imaging Algorithms and Convolutional Neural Networks,"M. A. Rasel, Sameem Abdul Kareem, Unaizah Obaidellah",https://doi.org/10.1016/j.bspc.2024.106883,,"Melanoma, Dermoscopic Images, Pigment Networks, Contrast Enhancement, Threshold Level, Convolutional Neural Networks, Bag of Features","This study aims to automate the pigment network detection process using a directional imaging algorithm and classify pigment network types using machine learning classifiers. The directional imaging algorithm incorporates Principal Component Analysis (PCA), contrast enhancement, filtering, and noise reduction. The study achieved 90% accuracy, 90% sensitivity, and 89% specificity with a simple CNN model, outperforming state-of-the-art methods.",11.8,Qwen2.5-3B,Apple M1 (Metal)
2601.11675v1_Generating metamers of human scene understanding.pdf,Preprint: GENERATING METAMERS OF HUMAN SCENE UNDERSTANDING,"Ritik Raina1, Abe Leite1, Alexandros Graikos1, Seoyoung Ahn2, Dimitris Samaras1, Gregory J. Zelinsky1",Not found,Not found,"Human scene understanding, Metamers, Latent diffusion model, Foveated scenes, Behavioral experiment","This paper introduces MetamerGen, a tool for generating scenes that align with latent human scene representations. It combines peripheral scene gist information with information from scene-viewing fixations to generate image metamers. The authors evaluate the perceptual alignment of generated images using a same-different behavioral experiment.",11.15,Qwen2.5-3B,Apple M1 (Metal)
2601.11676v1_HALO Semantic-Aware Distributed LLM Inference in L.pdf,HALO: Semantic-Aware Distributed LLM Inference in Lossy Edge Network,"Peirong Zheng, Wenchao Xu*, Haozhao Wang, Jinyu Chen, Xuemin (Sherman) Shen",Not provided,Not provided,"Large Language Models, Tensor Parallelism, Edge Computing, Heterogeneity, Semantics, Packet Loss","The deployment of large language models' inference at the edge can facilitate prompt service responsiveness while protecting user privacy. However, it is critically challenged by the resource constraints of a single edge node. Distributed inference has emerged to aggregate and leverage computational resources across multiple devices. This paper proposes HALO, a novel framework that can boost the distributed LLM inference in lossy edge network by enabling a relaxed yet effective synchronization through strategically allocating less critical neuron groups to unstable devices.",11.54,Qwen2.5-3B,Apple M1 (Metal)
2601.11683v1_Attesting Model Lineage by Consisted Knowledge Evo.pdf,Attesting Model Lineage by Consisted Knowledge Evolution,"Zhuoyi Shang, Jiasen Li, Pengzhen Chen, Yanwei Liu, Xiaoyan Gu, Weiping Wang",Not found,Not found,"deep learning, model lineage, fine-tuning, knowledge evolution, security","The fine-tuning technique in deep learning introduces a new form of dependency among models, known as model lineage. This paper proposes a novel model lineage attestation framework to verify the joint trajectory of knowledge evolution and parameter modification, addressing security concerns such as unauthorized model redistribution and false claim of model provenance.",11.14,Qwen2.5-3B,Apple M1 (Metal)
2601.11684v1_Mobile-friendly Image de-noising Hardware Consciou.pdf,Mobile-friendly Image de-noising: Hardware Conscious Optimization for Edge Application,"Srinivas Miriyala*, Sowmya Vajrala*, Hitesh Kumar, Sravanth Kodavanti, Vikram Rajendiran",,,"De-Noising, Differentiable NAS, Hardware aware Search space, Smartphone Deployment","This work presents a novel mobile-friendly network for image de-noising obtained with Entropy-Regularized differentiable Neural Architecture Search (NAS) on a hardware-aware search space for a U-Net architecture, which is first-of-its-kind. The designed model has 12% less parameters, with ~2-fold improvement in on-device latency and 1.5-fold improvement in the memory footprint for a 0.7% drop in PSNR, when deployed and profiled on Samsung Galaxy S24 Ultra. Compared to the SOTA Swin-Transformer for Image Restoration, the proposed network had competitive accuracy with ~18-fold reduction in GMACs. Further, the network was tested successfully for Gaussian de-noising with 3 intensities on 4 benchmarks and real-world de-noising on 1 benchmark demonstrating its generalization ability.",12.25,Qwen2.5-3B,Apple M1 (Metal)
2601.11685v1_Towards Efficient Image Deblurring for Edge Deploy.pdf,Towards Efficient Image Deblurring for Edge Deployment,"Srinivas Soumitri Miriyala*, Sowmya Lahari Vajrala*, Rama Sravanth Kodavanti",Not provided,Not provided,"Mobile Image Signal Processing (ISP), De-blurring, Training-free Search, Inference Optimization, Edge Deployment","This paper proposes a hardware-aware adaptation framework to optimize existing image deblurring models, achieving up to 55% reduction in GMACs compared to recent transformer-based SOTA while maintaining competitive accuracy and improving 1.25× on-device deployment latency.",11.63,Qwen2.5-3B,Apple M1 (Metal)
2601.11686v1_Proof of Concept Multi-Target Wildfire Risk Predic.pdf,Proof of Concept: Multi-Target Wildfire Risk Prediction and Large Language Model Synthesis,"Nicolas Caron ∗, Hassan Noura †, Christophe Guyeux ‡, Benjamin Aynes§",Not found,2601.11686,"wildfire risk, multi-target analysis, predictive models, large language models, operational needs, first responders, firefighting services, climate change, fire management","This proof of concept suggests the development of a hybrid framework combining predictive models for each risk dimension with large language models dedicated to synthesizing heterogeneous outputs into structured, actionable reports for wildfire risk assessment in France. The framework aims to address operational needs and support first responders and firefighting services.",11.85,Qwen2.5-3B,Apple M1 (Metal)
2601.11687v1_Semantic Caching and Intent-Driven Context Optimiz.pdf,Semantic Caching and Intent-Driven Context Optimization for Multi-Agent Natural Language to Code Systems,"Harmohit Singh, CoreOps AI",Not found,2601.11687,"Natural Language to Code, Multi-Agent Systems, Semantic Caching, LLM Optimization, Production Systems","We present a production-optimized multi-agent system designed to translate natural language queries into executable Python code for structured data analytics. Unlike systems that rely on expensive frontier models, our approach achieves high accuracy and cost efficiency through three key innovations: semantic caching, dual-threshold decision mechanism, and intent-driven dynamic prompt assembly.",13.63,Qwen2.5-3B,Apple M1 (Metal)
2601.11688v1_SpecMap Hierarchical LLM Agent for Datasheet-to-Co.pdf,SpecMap: Hierarchical LLM Agent for Datasheet-to-Code Link Recovery,"Vedant Nipane, Pulkit Agrawal, Amit Singh",Not found,2601.11688v1,"Datasheet-to-code mapping, Large language models, Traceability, Systems engineering","Presenting a hierarchical datasheet-to-code mapping methodology that employs large language models for semantic analysis, explicitly structuring the traceability process across multiple abstraction levels, and achieving up to 73.3% file mapping accuracy over traditional baselines.",11.69,Qwen2.5-3B,Apple M1 (Metal)
2601.11700v1_Telling Human and Machine Handwriting Apart.pdf,Telling Human and Machine Handwriting Apart,"Luis A. Leiva, Moises Diaz, Nuwan T. Attygalle, Miguel A. Ferrer, Réjean Plamondon",,,"Biometrics, classification, deep learning, reverse Turing test, verification","Handwriting movements can be used as behavioral biometrics to verify if a real user is operating a device or application. This work studies ten public datasets of handwritten symbols and gestures, artificially reproduced using seven different synthesizers, and trains a shallow recurrent neural network to achieve excellent performance in detecting human-generated inputs. The classifier performs well in few-shot and out-of-domain settings.",10.5,Qwen2.5-3B,Apple M1 (Metal)
2601.11702v1_PASTA A Scalable Framework for Multi-Policy AI Com.pdf,PASTA: A Scalable Framework for Multi-Policy AI Compliance Evaluation,"YU YANG, The University of British Columbia, Canada, IG-JAE KIM, Korea Institute of Science and Technology, South Korea, DONGWOOK YOON, The University of British Columbia, Canada",Not provided,Not provided,"AI compliance, multi-policy evaluation, scalable framework, policy normalization, AI governance","This paper presents PASTA, a scalable compliance tool integrating four innovations: a comprehensive model-card format, a policy normalization scheme, an efficient LLM-powered pairwise evaluation engine, and an interface delivering interpretable evaluations. Expert evaluation shows PASTA's judgments closely align with human experts, and the system evaluates five major policies in under two minutes at approximately $3. A user study confirms practitioners found outputs easy-to-understand and actionable.",12.0,Qwen2.5-3B,Apple M1 (Metal)
2601.11713v1_Inter-Cell Interference Rejection Based on Ultrawi.pdf,Inter-Cell Interference Rejection Based on Ultrawideband Walsh-Domain Wireless Autoencoding,"Rodney Martinez Alonso, Cel Thys, Sofie Pollin, Cedric Dehos, Yuneisy Esthela Garcia Guzman",,,"inter-cell interference, ultrawideband, Walsh-domain, wireless autoencoding, 5G CP-OFDM, spectrum efficiency","This paper proposes a novel technique for rejecting partial-in-band inter-cell interference (ICI) in ultrawideband communication systems. It presents an end-to-end wireless autoencoder architecture that jointly optimizes the transmitter and receiver encoding/decoding in the Walsh domain to mitigate interference from coexisting narrower-band 5G base stations. By exploiting the orthogonality and self-inverse properties of Walsh functions, the system distributes and learns to encode bit-words across parallel Walsh branches. Through analytical modeling and simulation, the paper characterizes how 5G CP-OFDM interference maps into the Walsh domain and identifies optimal ratios of transmission frequencies and sampling rate where the end-to-end autoencoder achieves the highest rejection. Experimental results show that the proposed autoencoder achieves up to 12 dB of ICI rejection while maintaining a low block error rate (BLER) for the same baseline channel noise, i.e., baseline Signal-to-Noise-Ratio (SNR) without the interference.",12.64,Qwen2.5-3B,Apple M1 (Metal)
2601.11746v1_LIME-LLM Probing Models with Fluent Counterfactual.pdf,"LIME-LLM: Probing Models with Fluent Counterfactuals, Not Broken Text","George Mihaila, Suleyman Olcay Polat, Poli Nemkova, Himanshu Sharma, Namratha V . Urs, Mark V . Albert",,2601.11746,"explainability, local explanation, counterfactuals, large language models, NLP","A framework that replaces random noise with hypothesis-driven, controlled perturbations, enabling the construction of fluent, on-manifold neighborhoods that rigorously isolate feature effects. Empirical results demonstrate significant improvements in local explanation fidelity compared to traditional perturbation-based methods and recent generative alternatives.",11.38,Qwen2.5-3B,Apple M1 (Metal)
2601.11747v1_PRISM Learning Design Knowledge from Data for Styl.pdf,PRISM: Learning Design Knowledge from Data,"Huaxiaoyue Wang, Sunav Choudhary, Franck Dernoncourt, Yu Shen, Stefano Petrangeli",,,"Design, Stylistic Improvement, Vision Language Models, Design Knowledge, Natural Language Instructions","Graphic design often involves exploring different stylistic directions, which can be time-consuming for non-experts. This paper addresses the problem of stylistically improving designs based on natural language instructions. While VLMs have shown initial success in graphic design, their pretrained knowledge on styles is often too general and misaligned with specific domain data. The key insight is to leverage design data—a collection of real-world designs that implicitly captures designer’s principles—to learn design knowledge and guide stylistic improvement. The proposed PRISM (Prior-Informed Stylistic Modification) constructs and applies a design knowledge base through three stages: clustering high-variance designs, summarizing each cluster into actionable design knowledge, and retrieving relevant knowledge during inference to enable style-aware improvement. Experiments on the Crello dataset show that PRISM achieves the highest average rank of 1.49 (closer to 1 is better) over baselines in style alignment. User studies further validate these results, showing that PRISM is consistently preferred by designers.",12.4,Qwen2.5-3B,Apple M1 (Metal)
2601.11758v1_Early Linguistic Pattern of Anxiety from Social Me.pdf,Early Linguistic Pattern of Anxiety from Social Media,Arnab Das,,,"anxiety detection, linguistic pattern, interpretable machine learning, keyword robustness, cross-domain validation, author-disjoint evaluation, mental health screening","Anxiety affects hundreds of millions of individuals globally, yet large-scale screening remains limited. Social media language provides an opportunity for scalable detection, but current models often lack interpretability, keyword-robustness validation, and rigorous user-level data integrity. This work presents a transparent approach to social media-based anxiety detection through linguistically interpretable feature-grounded modeling and cross-domain validation.",11.24,Qwen2.5-3B,Apple M1 (Metal)
2601.11762v1_Industry-Aligned Granular Topic Modeling.pdf,Industry-Aligned Granular Topic Modeling,"Sae Young Moon, Myeongjun Erik Jang, Haoyan Luo, Chunyang Xiao, Antonios Georgiadis, Fran Silavong",,,"Topic modeling, Granularity, Natural Language Processing, Large Language Models, Business Applications, Document Summarization, Topic Hierarchy","This paper introduces a framework called TIDE, which provides a novel granular topic modeling method based on large language models (LLMs) as a core feature, along with other useful functionalities for business applications, such as summarizing long documents, topic parenting, and distillation. The framework demonstrates superior performance compared to modern topic modeling methods and provides valuable support for industrial business scenarios.",11.1,Qwen2.5-3B,Apple M1 (Metal)
2601.11768v1_Lightweight Self-Supervised Detection of Fundament.pdf,Lightweight Self-Supervised Detection of Fundamental Frequency and Accurate Probability of Voicing in Monophonic Music,"Venkat Suprabath Bitra, Homayoon Beigi",Not found,Not found,"self-supervised pitch detection, unsupervised pitch detection, fundamental frequency, pitch estimation, resonance, musical timbre transfer, probability of voicing, music synthesis, music analysis, CQT, constant Q transform, DDSP, shift cross-entropy loss, musical instrument modeling, ResNeXt neural network, music information retrieval, MIR","Reliable fundamental frequency (F0) and voicing estimation is essential for neural synthesis, yet many pitch extractors depend on large labeled corpora and degrade under realistic recording artifacts. We propose a lightweight, fully self-supervised framework for joint F0 estimation and voicing inference, designed for rapid single-instrument training from limited audio. Using transposition-equivariant learning on CQT features, we introduce an EM-style iterative reweighting scheme that uses Shift Cross-Entropy (SCE) consistency as a reliability signal to suppress uninformative noisy/unvoiced frames. The resulting weights provide confidence scores that enable pseudo-labeling for a separate lightweight voicing classifier without manual annotations. Trained on MedleyDB and evaluated on MDB-stem-synth ground truth, our method achieves competitive cross-corpus performance (RPA 95.84, RCA 96.24) and demonstrates cross-instrument generalization.",12.46,Qwen2.5-3B,Apple M1 (Metal)
2601.11776v1_Cleansing the Artificial Mind A Self-Reflective De.pdf,Cleansing the Artificial Mind: A Self-Reflective Detoxification Framework for Large Language Models,"Kaituo Zhang, Zhimeng Jiang, Na Zou",Not found,Not found,"Large Language Models, Toxic Content, Self-Reflective Detoxification, Reinforcement Learning from Human Feedback, Instruction Tuning","This paper introduces a fully self-reflective detoxification framework for Large Language Models (LLMs) to detect, correct toxic content, and refine models without external modules and data annotation. It proposes a Toxic Signal Detector to transform toxic text into its non-toxic counterpart, enhancing the model's ability for safe and coherent text generation.",10.69,Qwen2.5-3B,Apple M1 (Metal)
2601.11778v1_Translation as a Scalable Proxy for Multilingual E.pdf,Translation as a Scalable Proxy for Multilingual Evaluation,"Sheriff Issaka1, Erick Rosas Gonzalez1*, Lieqi Liu1*, Evans Kofi Agyei2, Lucas Bandarkar1, Nanyun Peng 1, David Ifeoluwa Adelani 3, Francisco Guzmán4, Saadia Gabriel 1",,,"large language models, multilingual evaluation, translation quality, benchmarks, cost-effectiveness","The rapid proliferation of large language models (LLMs) has created a critical evaluation paradox: while LLMs claim multilingual proficiency, comprehensive non-machine-translated benchmarks exist for fewer than 30 languages, leaving >98% of the world's 7,000 languages in an empirical void. This evaluation vacuum has profound consequences as entire language communities are excluded from AI development. This work explores whether translation quality alone can serve as a reliable, scalable, and cost-effective proxy for a model's broader multilingual capabilities.",11.42,Qwen2.5-3B,Apple M1 (Metal)
2601.11781v1_Risk-Aware Human-in-the-Loop Framework with Adapti.pdf,Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles,"Dawood Wasif, Terrence J. Moore, Seunghyun Yoon, Hyuk Lim, Dan Dongseong Kim, Frederica F. Nelson, Jin-Hee Cho",Not found,Not found,"autonomous vehicles, human-in-the-loop, intrusion response, risk-aware, cyber-physical systems, reinforcement learning, model-based control","This paper presents RAIL, a risk-aware human-in-the-loop framework for autonomous vehicles that integrates heterogeneous runtime signals into calibrated control adaptations and focused learning. RAIL fuses three cues (curvature actuation integrity, time-to-collision proximity, and observation-shift consistency) into an Intrusion Risk Score (IRS) via a weighted Noisy-OR mechanism. When IRS exceeds a threshold, actions are blended with a cue-specific shield using a learned authority, while human override remains available. When risk is low, the nominal policy executes. A contextual bandit arbitrates among shields based on the cue vector, improving mitigation choices online. RAIL couples Soft Actor–Critic (SAC) with risk-prioritized replay and dual rewards, steering learning while maintaining nominal behavior. On MetaDrive, RAIL achieves high test returns, success rates, safety violations, and disturbance rates, outperforming RL, safe RL, offline/imitation learning, and prior HITL baselines. Under ControllerAreaNetwork (CAN) injection and LiDAR spoofing attacks, RAIL improves success rates, reduces disengagement rates, and lowers attack success rates. In CARLA, RAIL attains high test returns and success rates with minimal training steps.",12.63,Qwen2.5-3B,Apple M1 (Metal)
2601.11792v1_A self-evolving multi-role collaborative framework.pdf,A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation,"Yifei Suna, Yongan Lia, A.K. Qinb, Sicheng Houa, Tamas Pflanznerc",,,"Problem generation, Large language models, Multi-role collaboration, Intelligent education, Self-evolution, Knowledge distillation","Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation tasks. Although existing LLMs can achieve high correctness rates, they generally lack innovation and exhibit poor discrimination. This paper proposes the task of innovative math problem generation (IMPG) and introduces a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance to solve the IMPG task. The framework includes a multi-role collaborative mechanism comprising a sampler, generator, evaluator, state machine, and memory, ensuring the correctness of generated problems through iterative optimization informed by self-assessment and external feedback. An improved difficulty model is introduced to quantify difficulty and provide fine-grained guidance. The data-driven association-guided pathsampling (DAPS) algorithm is adopted to enhance the semantic rationality of sampled encodings. The HSM3K-CN dataset is constructed, comprising high-quality highschool math problems. A multi-stage training pipeline is adopted, incorporating continual pre-training (CPT), supervised fine-tuning (SFT), and group relative policy optimization (GRPO), to enhance the generation and evaluation capabilities of the base model. Finally, system self-evolution is achieved by transferring evaluation capabilities from the expert model to the apprentice model via distillation. Experiments show that compared to baseline models, our proposed method significantly improves the innovation of the generated problems while maintaining a high correctness rate.",13.29,Qwen2.5-3B,Apple M1 (Metal)
2601.11801v1_RobotDesignGPT Automated Robot Design Synthesis us.pdf,RobotDesignGPT: Automated Robot Design Synthesis using Vision Language Models,"Nitish Sontakke, K. Niranjan Kumar, Sehoon Ha",Not provided,Not provided,"Robot Design, Vision-Language Models, Automated Design, Kinematic Structures, Visual Appearance","This paper proposes an automated robot design framework, RobotDesignGPT, that leverages the general knowledge and reasoning capabilities of large pre-trained vision-language models to synthesize initial robot designs from simple user prompts and reference images. The framework improves design quality and reduces manual feedback, resulting in visually appealing and kinematically valid robots inspired by nature.",11.11,Qwen2.5-3B,Apple M1 (Metal)
2601.11809v1_Multi-agent DRL-based Lane Change Decision Model f.pdf,Multi-agent DRL-based Lane Change Decision,"Zeyu Mu1, Shangtong Zhang 2, B. Brian Park 3",,,"Multi-Agent, Reinforcement Learning, Cooperative Platooning, Lane Change","This study proposes a hybrid multi-agent lane change decision model to increase CA V participation in cooperative platooning and maximize its benefits. The model employs the QMIX framework, integrating traffic data processed through a convolutional neural network (CNN-QMIX). It addresses dynamic traffic scenarios by enabling CA Vs to make optimal decisions irrespective of the varying number of CA Vs present in mixed traffic. A trajectory planner and model predictive controller ensure smooth and safe lane-change execution. The model is trained and evaluated within a microsimulation environment under varying CA V market penetration rates, demonstrating significant improvements in cooperative platooning rates.",11.55,Qwen2.5-3B,Apple M1 (Metal)
2601.11816v1_POLARIS Typed Planning and Governed Execution for .pdf,POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation,"Zahra Moslemi1, Keerthi Koneru2, Yen-Ting Lee3, Sheethal Kumar2, Ramesh Radhakrishnan2",,,"Agentic AI, Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed Planning, Evaluation","Enterprise back-office automation requires agentic systems that are auditable, policy-aligned, and operationally predictable, capabilities that generic multi-agent setups often fail to deliver. POLARIS presents a governed orchestration framework that treats automation as typed plan synthesis and validated execution over LLM agents. It uses a planner to propose structurally diverse, type-checked directed acyclic graphs (DAGs), a rubric-guided reasoning module to select a single compliant plan, and execution is guarded by validator-gated checks, a bounded repair loop, and compiled policy guardrails that block or route side effects before they occur. Applied to document-centric finance tasks, POLARIS produces decision-grade artifacts and full execution traces while reducing human intervention. Empirically, POLARIS achieves a micro-F1 of 0.81 on the SROIE dataset and, on a controlled synthetic suite, achieves 0.95–1.00 precision for anomaly routing with preserved audit trails. These evaluations constitute an initial benchmark for governed Agentic AI.",12.5,Qwen2.5-3B,Apple M1 (Metal)
2601.11825v1_AI Co-Scientist for Knowledge Synthesis in Medical.pdf,AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept,"Arya Rahgozara, Pouria Mortezaagha",Not provided,2601.11825v1,"AI, knowledge synthesis, medical research, redundant studies, incomplete reporting, scalable evidence synthesis","This paper presents a proof of concept for an AI co-scientist designed to enable scalable, transparent knowledge synthesis in medical contexts through explicit PICOS formalization.",12.47,Qwen2.5-3B,Apple M1 (Metal)
2601.11840v1_Imandra CodeLogician Neuro-Symbolic Reasoning for .pdf,Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of Software Logic,"Hongyu Lin, Samer Abdallah, Makar Valentinov, Paul Brennan, Elijah Kagan, Christoph M. Wintersteiger, Denis Ignatovich, Grant Passmore",10.1101/2601.11840v1,2601.11840,"Neuro-symbolic reasoning, Software logic, Formal verification, LLM integration","This paper presents CodeLogician, a neuro-symbolic agent and framework for precise analysis of software logic, integrating with ImandraX, an industrial automated reasoning engine. It addresses the gap between mathematical proof automation and software engineering tasks, rigorously evaluating the reasoning capabilities of large language models (LLMs) and their augmentation with CodeLogician.",12.26,Qwen2.5-3B,Apple M1 (Metal)
2601.11850v1_Human-AI Collaborative Inductive Thematic Analysis.pdf,Human–AI Collaborative Inductive Thematic Analysis: How AI Guides Analysis and Researchers Reclaim Interpretive Authority,"Matthew Nyaaba †1,2, Min SungEun †3, Mary Abiswin Apam ‚4, Kwame Owoahene Acheampong5, Emmanuel Dwamena6, Xiaoming Zhai1, 7",Not provided,Not provided,"Human–AI collaboration, Generative artificial intelligence (GenAI), Inductive thematic analysis, Qualitative data analysis, Epistemic authority, Reflexive methodology","This study investigates how researchers interact with an Inductive Thematic Analysis GPT (ITA–GPT), a purpose-built AI tool designed to operationalize established inductive thematic analysis procedures, and how this interaction shapes analytic practice, judgment, and epistemic authority.",11.91,Qwen2.5-3B,Apple M1 (Metal)
2601.11854v1_ATOD An Evaluation Framework and Benchmark for Age.pdf,ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System,"Yifei Zhang, Hooshang Nayyeri, Rinat Khaziev, Emine Yilmaz, Gokhan Tur, Dilek Hakkani-Tür, Hari Thadakamalla",Not provided,Not provided,"Task-Oriented Dialogue, Agentic Behavior, Evaluation Framework, Benchmark, Long-Horizon Memory, Asynchronous Execution","Recent advances in task-oriented dialogue systems have enabled conversational agents to coordinate interleaved goals, maintain long-horizon context, and act proactively through asynchronous execution. ATOD introduces a benchmark and synthetic dialogue generation pipeline that captures key characteristics of advanced TOD, including multi-goal coordination, dependency management, memory, adaptability, and proactivity. ATOD-Eval is a holistic evaluation framework that translates these dimensions into fine-grained metrics and supports reproducible offline and online evaluation.",11.52,Qwen2.5-3B,Apple M1 (Metal)
2601.11859v1_Cascaded Transformer for Robust and Scalable SLA D.pdf,Cascaded Transformer for Robust and Scalable SLA Decomposition via Amortized Optimization,Cyril Shih-Huan Hsu,Not found,Not found,"network slicing, service level agreement, quality of service, deep neural network, optimization, transformers","The paper introduces Casformer, a cascaded Transformer architecture designed for fast, optimization-free SLA decomposition. It leverages historical domain feedback and integrates cross-domain dependencies to achieve improved SLA decomposition quality and enhanced scalability and robustness.",10.79,Qwen2.5-3B,Apple M1 (Metal)
2601.11863v1_Utilizing Metadata for Better Retrieval-Augmented .pdf,Utilizing Metadata for Better Retrieval-Augmented Generation,"Raquib Bin Yousuf, Shengzhe Xu, Mandar Sharma, Andrew Neeser, Chris Latimer, Naren Ramakrishnan",Not found,2601.11863,"Retrieval-Augmented Generation (RAG), Metadata-aware Retrieval, Dense Retrieval, Query Reformulation, Benchmark Datasets","Retrieval-Augmented Generation systems rely on retrieving semantically relevant document chunks to support accurate, grounded outputs from large language models. This study evaluates metadata-aware retrieval strategies, comparing plain-text baselines with approaches that embed metadata directly, and finds that prefixing and unified embeddings consistently outperform plain-text baselines, with the unified embedding sometimes exceeding prefixing while being easier to maintain. The study also analyzes embedding space, showing that metadata integration improves effectiveness by increasing intra-document cohesion, reducing inter-document confusion, and widening the separation between relevant and irrelevant chunks.",13.04,Qwen2.5-3B,Apple M1 (Metal)
2601.11868v1_Terminal-Bench Benchmarking Agents on Hard Realist.pdf,"TERMINAL-BENCH: BENCHMARKING AGENTS ON HARD, REALISTIC TASKS IN COMMANDLINE INTERFACES","Mike A. Merrill, Alexander G. Shaw, Nicholas Carlini, Boxuan Li, Harsh Raj, Ivan Bercovich, Lin Shi, Jeong Yeon Shin, Thomas Walshe, E Kelly Buchanan, Junhong Shen, Guanghao Ye, Haowei Lin, Jason Poulos, Maoyu Wang, Marianna Nezhurina, Jenia Jitsev, Di Lu, Orfeas Menis Mastromichalakis, Zhiwei Xu, Zizhao Chen, Yue Liu, Robert Zhang, Leon Liangyu Chen, Anurag Kashyap, Jan-Lucas Uslu, Jeffrey Li, Jianbo Wu, Minghao Yan, Song Bian, Vedang Sharma, Ke Sun, Steven Dillmann, Akshay Anand, Andrew Lanpouthakoun, Bardia Koopah, Changran Hu, Etash Guha, Gabriel H. S. Dreiman, Jiacheng Zhu, Karl Krauth, Li Zhong, Niklas Muennighoff, Robert Amanfu, Shangyin Tan, Shreyas Pimpalgaonkar, Tushar Aggarwal, Xiangning Lin, Xin Lan, Xuandong Zhao, Yiqing Liang, Yuanli Wang, Zilong Wang, Changzhi Zhou, David Heineman, Hange Liu, Harsh Trivedi, John Yang, Junhong Lin, Manish Shetty, Michael Yang, Nabil Omi, Negin Raoof, Shanda Li, Terry Yue Zhuo, Wuwei Lin, Yiwei Dai, Yuxin Wang, Wenhao Chai, Shang Zhou, Dariush Wahdany, Ziyu She, Jiaming Hu, Zhikang Dong, Yuxuan Zhu, Sasha Cui, Ahson Saiyed, Arinbj ¨orn Kolbeinsson, Jesse Hu, Christopher Michael Rytting, Ryan Marten, Yixin Wang, Alex Dimakis, Andy Konwinski, Ludwig Schmidt",Not provided,2601.11868v1,"AI agents, benchmarking, commandline interfaces, realistic tasks, terminal environments","A hard benchmark composed of 89 tasks in computer terminal environments inspired by real workflows, designed to measure the performance of frontier models and agents, with scores below 65% and areas for improvement identified through error analysis.",12.52,Qwen2.5-3B,Apple M1 (Metal)
2601.11876v1_AI for Green Spaces Leveraging Autonomous Navigati.pdf,Autonomous Trash Pickup Robots for Grass Fields,Authors not specified,Not specified,Not specified,"Robotics, Autonomous, Trash Pickup, Grass Fields, Real-Time Kinematic (RTK) GPS, ResNet50 CNN, Coverage Path","This paper proposes an autonomous robot designed to navigate, identify, and pick up litter in parks. It uses a Spanning Tree Coverage (STC) algorithm for navigation and a ResNet50 Convolutional Neural Network for trash detection. The robot achieved an overall success rate of 80% in field tests.",10.54,Qwen2.5-3B,Apple M1 (Metal)
2601.11880v1_TF-CoDiT Conditional Time Series Synthesis with Di.pdf,TF-CoDiT: Conditional Time Series Synthesis with Diffusion Transformers for Treasury Futures,"Yingxiao Zhang, Jiaxin Duan, Junfu Zhang, Ke Feng",,,"Treasury futures, Diffusion Transformers, Time series synthesis, Financial data generation","This work proposes TF-CoDiT, the first Diffusion Transformers framework for synthesizing treasury futures data, addressing challenges such as low volume, market dependencies, and multivariate correlations. It introduces a U-shape VAE to encode cross-channel dependencies and a Financial Market Attribute Protocol (FinMAP) for generating prompts covering essential conditions. Extensive evaluations show TF-CoDiT can produce highly authentic data with errors at most 0.433 (MSE) and 0.453 (MAE) to the ground-truth.",11.38,Qwen2.5-3B,Apple M1 (Metal)
2601.11885v1_MyGram Modality-aware Graph Transformer with Globa.pdf,MyGram: Modality-aware Graph Transformer with Global Distribution for Multi-modal Entity Alignment,"Zhifei Li, Ziyue Qin, Xiangyu Luo, Xiaoju Hou, Yue Zhao, Miao Zhang, Zhifang Huang, Kui Xiao, Bing Yang",Not found,Not found,"Multi-modal entity alignment, Graph transformer, Modality-aware, Global distribution, Knowledge graph","MyGram proposes a modality-aware graph transformer to address the challenges of existing methods in multi-modal entity alignment, which may overlook structural contextual information within each modality. It introduces a modality diffusion learning module to capture deep structural contextual information and a Gram Loss to achieve global distribution consistency across modalities. Experiments on five public datasets show that MyGram outperforms baseline models.",10.5,Qwen2.5-3B,Apple M1 (Metal)
2601.11895v1_DevBench A Realistic Developer-Informed Benchmark .pdf,"DevBench: A Realistic, Developer-Informed Benchmark for Code Generation Models","Pareesa Ameneh Golnari ∗, Adarsh Kumarappan ∗∗, Wen Wen, Xiaoyu Liu, Gabriel Ryan, Yuting Sun, Shengyu Fu, Elsie Nallipogu",Not found,Not found,"Code generation, Large Language Models, Telemetry, Benchmarking, Developer telemetry","DevBench is a telemetry-driven benchmark designed to evaluate Large Language Models (LLMs) on realistic code completion tasks. It includes 1,800 evaluation instances across six programming languages and six task categories derived from real developer telemetry, focusing on common yet challenging completion scenarios.",11.72,Qwen2.5-3B,Apple M1 (Metal)
2601.11903v1_AEMA Verifiable Evaluation Framework for Trustwort.pdf,AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems,"Yen-Ting Lee, Keerthi Koneru, Zahra Moslemi, Sheethal Kumar, Ramesh Radhakrishnan",Not found,2601.11903,"Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight","Evaluating large language model (LLM)-based multi-agent systems remains a critical challenge, as these systems must exhibit reliable coordination, transparent decision-making, and verifiable performance across evolving tasks. Existing evaluation approaches often limit themselves to single-response scoring or narrow benchmarks, which lack stability, extensibility, and automation when deployed in enterprise settings at multi-agent scale. We present AEMA (Adaptive Evaluation Multi-Agent), a process-aware and auditable framework that plans, executes, and aggregates multi-step evaluations across heterogeneous agentic workflows under human oversight. Compared to a single LLM-as-a-Judge, AEMA achieves greater stability, human alignment, and traceable records that support accountable automation. Our results on enterprise-style agent workflows demonstrate that AEMA provides a transparent and reproducible pathway toward responsible evaluation of LLM-based multi-agent systems.",12.76,Qwen2.5-3B,Apple M1 (Metal)
2601.11905v1_LIBRA Language Model Informed Bandit Recourse Algo.pdf,LIBRA: Language Model Informed Bandit Recourse,"Junyu Cao, Ruijiang Gao, Esmaeil Keyvanshokooh, Jianhao Ma",Not found,Not found,"Large Language Models, LLM-Bandits Collaboration, Algorithmic Recourse, Regret Analysis, Personalized Treatment Planning, Hypertension Management","We introduce a unified framework that integrates algorithmic recourse, contextual bandits, and large language models (LLMs) to support sequential decision-making in high-stakes settings such as personalized medicine. We develop the Generalized Linear Recourse Bandit (GLRB) algorithm and propose LIBRA, a Language Model–Informed Bandit Recourse Algorithm that combines domain knowledge from LLMs with statistical rigor of bandit learning. LIBRA offers three key guarantees: a warm-start guarantee, an LLM-effort guarantee, and a robustness guarantee.",11.93,Qwen2.5-3B,Apple M1 (Metal)
2601.11907v1_Towards Airborne Object Detection A Deep Learning .pdf,Towards Airborne Object Detection: A Deep Learning Analysis,"1st Prosenjit Chatterjee, 2nd ANK Zaman",Not found,Not found,"Airborne Object Detection, Deep Learning, EfficientNetB4, ResNet-50, Automated Threat Assessment","This work introduces a dual-task model based on EfficientNetB4 capable of performing airborne object classification and threat-level prediction simultaneously. It addresses the scarcity of clean, balanced training data by constructing the AODTA Dataset and benchmarks the model on both the AODTA and AVD datasets, achieving high accuracy and reliable performance.",10.92,Qwen2.5-3B,Apple M1 (Metal)
2601.11913v1_LSTM-MAS A Long Short-Term Memory Inspired Multi-A.pdf,LSTM-MAS: A Long Short-Term Memory Inspired Multi-Agent System for Long-Context Understanding,"Yichen Jiang, Peng Ye, Jiakang Yuan, Chongjun Tu, Lei Bai, Tao Chen",Not found,Not found,"Long-Context Understanding, Large Language Models, Multi-Agent System, Memory","Effectively processing long contexts remains a fundamental yet unsolved challenge for large language models (LLMs). Existing single-LLM-based methods primarily reduce the context window or optimize the attention mechanism, but they often encounter additional computational costs or constrained expanded context length. While multi-agent-based frameworks can mitigate these limitations, they remain susceptible to the accumulation of errors and the propagation of hallucinations. In this work, we draw inspiration from the Long Short-Term Memory (LSTM) architecture to design a Multi-Agent System called LSTM-MAS, emulating LSTM’s hierarchical information flow and gated memory mechanisms for long-context understanding. Specifically, LSTM-MAS organizes agents in a chained architecture, where each node comprises a worker agent for segment-level comprehension, a filter agent for redundancy reduction, a judge agent for continuous error detection, and a manager agent for globally regulates information propagation and retention, analogous to LSTM and its input gate, forget gate, constant error carousel unit, and output gate. These novel designs enable controlled information transfer and selective long-term dependency modeling across textual segments, which can effectively avoid error accumulation and hallucination propagation.",12.53,Qwen2.5-3B,Apple M1 (Metal)
2601.11920v1_Enhancing LLM-Based Data Annotation with Error Dec.pdf,Enhancing LLM-Based Data Annotation with Error Decomposition,"Zhen Xu, Vedant Khatri, Yijun Dai, Xiner Liu, Siyan Li, Xuanming Zhang, Renzhe Yu",Not provided,Not provided,"Data Annotation, Qualitative Coding, Large Language Models, Human-AI Collaboration","Large language models offer a scalable alternative to human coding for data annotation tasks, enabling the scale-up of research across data-intensive domains. However, their performance on subjective annotation tasks is less consistent and prone to errors. This paper proposes a diagnostic evaluation paradigm to separate task-inherent ambiguity from model-driven inaccuracies and assess annotation quality in terms of their potential downstream impacts.",11.25,Qwen2.5-3B,Apple M1 (Metal)
2601.11935v1_Big Data Workload Profiling for Energy-Aware Cloud.pdf,Big Data Workload Profiling for Energy-Aware Cloud Resource Management,"Milan Parikh, Aniket Abhishek Soni, Sneja Mitinbhai Shah, Ayush Raj Jha",Not found,2601.11935v1,"Cloud computing, energy-aware scheduling, workload profiling, virtual machine placement, big data, green computing","This paper presents a workload-aware scheduling framework that uses profiling of CPU usage, memory demand, and storage I/O behavior to guide energy-efficient virtual machine (VM) placement. By combining historical execution logs with real-time telemetry, the system predicts the energy and performance impact of candidate placement decisions and adaptively consolidates workloads without violating service-level agreements (SLAs).",11.67,Qwen2.5-3B,Apple M1 (Metal)
2601.11940v1_Thinking Traps in Long Chain-of-Thought A Measurab.pdf,Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart,"Kang Chen, Fan Yu, Junjie Nian, Shihan Zhao, Zhuoka Feng, Zĳun Yao, Heng Wang, Minshen Yu, Yixin Cao",Not provided,2601.11940,"Long Chain-of-Thought, Thinking Traps, Adaptive Restart, Test-Time Control, Fine-Grained Trajectory Analysis","Scaling test-time compute via Long Chain-of-Thought (Long-CoT) significantly enhances reasoning capabilities, yet extended generation does not guarantee correctness. Through fine-grained trajectory analysis, we identify Thinking Traps, prefix-dominant deadlocks where later reflection, alternative attempts, or verification fail to revise the root error. We introduce TAAR (Trap-Aware Adaptive Restart), a test-time control framework that trains a diagnostic policy to predict trap index and escape probability, truncating the trajectory before the predicted trap segment and adaptively restarting decoding. Experiments on challenging mathematical and scientific reasoning benchmarks show that TAAR improves reasoning performance without fine-tuning base model parameters.",12.53,Qwen2.5-3B,Apple M1 (Metal)
2601.11956v1_Double-Calibration Towards Trustworthy LLMs via Ca.pdf,Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and Reasoning Confidence,"Yuyin Lu, Ziran Liang, Yanghui Rao, Wenqi Fan, Fu Lee Wang, Qing Li",Not found,Not found,"Large Language Models, Knowledge Graphs, Calibration, Epistemic Uncertainty, Aleatoric Uncertainty","This paper introduces DoublyCal, a framework that addresses the challenge of trustworthy reasoning in Large Language Models (LLMs) by employing a novel double-calibration principle. It aims to improve the accuracy and confidence calibration of black-box LLMs with low token cost, particularly in knowledge-intensive benchmarks. The framework uses a lightweight proxy model to generate KG evidence alongside a calibrated evidence confidence, guiding a black-box LLM to produce more accurate and well-calibrated predictions with confidence scores traceable to the uncertainty of the supporting evidence. The experiments show significant improvements in both accuracy and confidence calibration.",11.45,Qwen2.5-3B,Apple M1 (Metal)
2601.11960v1_R2PO Decoupling Training Trajectories from Inferen.pdf,R2PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning,"Jingchu Wang, Bingbing Xu, Yige Yuan, Bin Xie, Xiaoqian Sun, Huawei Shen",Not found,Not found,"Reinforcement Learning, Large Language Models, Reasoning, Optimization, Training Trajectories, Inference Responses","This paper proposes R2PO, a method to decouple training trajectories from inference responses in Large Language Models (LLMs) to improve reasoning capabilities. By introducing a lightweight residual module, R2PO enables controlled trajectory diversification during training while maintaining stable inference generation, leading to better performance on benchmarks.",10.76,Qwen2.5-3B,Apple M1 (Metal)
2601.11969v1_textttMemoryRewardBench Benchmarking Reward Models.pdf,MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models,"Zecheng Tang, Baibei Ji, Ruoxi Sun, Haitian Wang, Yijun Zhang, Wenpeng Zhu, Ji Qi, Juntao Li, Min Zhang",Not provided,Not provided,"Large Language Models, Memory Management, Reward Models, Long-Term Memory, Segmented Processing","This work introduces MemRewardBench, the first benchmark to systematically study the ability of reward models to evaluate long-term memory management processes in large language models. It covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, and evaluates 13 cutting-edge reward models.",11.13,Qwen2.5-3B,Apple M1 (Metal)
2601.11974v1_Learn Like Humans Use Meta-cognitive Reflection fo.pdf,Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement,"Xinmeng Hou, Peiliang Gong, Bohao Qu, Wuqi Wang, Qing Guo, Yang Liu",Not found,Not found,"Self-improvement, Meta-cognitive reflection, Large language models, Efficient learning, Human learning","This paper proposes MARS, a framework that achieves efficient self-evolution within a single recurrence cycle by integrating principle-based reflection and procedural reflection. Inspired by educational psychology, MARS allows agents to systematically refine their reasoning logic without continuous online feedback, outperforming state-of-the-art self-evolving systems while significantly reducing computational overhead. Extensive experiments on six benchmarks demonstrate the effectiveness of MARS.",10.93,Qwen2.5-3B,Apple M1 (Metal)
2601.11977v1_One-Shot Price Forecasting with Covariate-Guided E.pdf,One-Shot Price Forecasting with Covariate-Guided Experts under Privacy Constraints,"Ren He, Yinliang Xu, Jinfeng Wang, Jeremy Watson, Jian Song",Not provided,Not provided,"Price forecasting, Time Series, Privacy, Mixture of Experts, Market analysis","Forecasting in power systems often involves multi-variate time series with complex dependencies and strict privacy constraints across regions. Traditional forecasting methods require significant expert knowledge and struggle to generalize across diverse deployment scenarios. Recent advancements in pre-trained time series models offer new opportunities, but their zero-shot performance on domain-specific tasks remains limited. To address these challenges, the authors propose a novel MoE-Encoder module that augments pre-trained forecasting models by injecting a sparse mixture-of-experts layer between tokenization and encoding. This design enables two key capabilities: transforming multivariate forecasting into an expert-guided univariate task, allowing the model to effectively capture inter-variable relations, and supporting localized training and lightweight parameter sharing in federated settings where raw data cannot be exchanged. Extensive experiments on public multivariate datasets demonstrate that MoE-Encoder significantly improves forecasting accuracy compared to strong baselines. Further simulations show that transferring only MoE-Encoder parameters allows efficient adaptation to new regions, with minimal performance degradation.",12.63,Qwen2.5-3B,Apple M1 (Metal)
2601.11979v1_Process In-Context Learning Enhancing Mathematical.pdf,Process In-Context Learning: Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion,"Ang Gao, Changshuo Zhang, Xiao Zhang, Deyang Li, Minjun Zhao, Fangchao Liu, Xinyu Zhang",,,"In-Context Learning, Mathematical Reasoning, Dynamic Demonstration Insertion, AI","Process In-Context Learning (PICL) addresses the limitations of static ICL approaches by dynamically integrating demonstrations to adapt to real-time inference needs. This framework identifies potential confusion points during the reasoning process and retrieves relevant demonstrations to guide subsequent steps, thereby mitigating mid-inference confusion and improving mathematical reasoning accuracy.",10.71,Qwen2.5-3B,Apple M1 (Metal)
2601.11995v1_Learning Audio-Visual Embeddings with Inferred Lat.pdf,Learning Audio–Visual Embeddings with Inferred Latent Interaction Graphs,"Donghuo Zeng, Hao Niu, Yanan Wang, Masato Taya",Not found,2601.11995,"audio–visual, latent interaction graph, cross-modal retrieval, soft labels",Learning robust audio–visual embeddings requires bringing genuinely related audio and visual signals together while filtering out incidental co-occurrences. Most contrastive and triplet-loss methods use sparse annotated labels per clip and treat any co-occurrence as semantic similarity. The proposed framework leverages soft-label predictions and inferred latent interactions to address these issues.,12.2,Qwen2.5-3B,Apple M1 (Metal)
2601.11998v1_Hybrid IDS Using Signature-Based and Anomaly-Based.pdf,Hybrid IDS Using Signature-Based and Anomaly-Based Detection,"1st Messaouda Boutassetta, 2nd Amina Makhlouf, 3rd Newfel Messaoudi, 4th Abdelmadjid Benmachiche, 5th Ines Boutabia",,,"Intrusion Detection System (IDS), Hybrid IDS, Signature-Based Detection, Anomaly-Based Detection, Machine Learning (ML), Cybersecurity, False Positives, Detection Accuracy, Real-Time Detection, Network Security","This paper presents a comprehensive survey and conceptual overview of Hybrid IDS, which integrate signature-based and anomaly-based detection techniques to enhance attack detection capabilities. It examines recent research on Hybrid IDS, classifies existing models into functional categories, discusses their advantages, limitations, and application domains, including financial systems, air traffic control, and social networks. It also reviews recent trends in Hybrid IDS research, such as machine learning-based approaches and cloud-based deployments, and outlines potential future research directions aimed at developing more cost-effective Hybrid IDS solutions with improved ability to detect emerging and sophisticated cyberattacks.",11.98,Qwen2.5-3B,Apple M1 (Metal)
2601.12002v1_Kernel-Based Learning of Safety Barriers.pdf,Kernel-Based Learning of Safety Barriers,Kernel-Based Learning of Safety Barriers,Not found,2601.12002,"Kernel-based learning, Safety verification, Black-box systems, Discrete-time stochastic dynamics, Control barrier certificates, Conditional mean embeddings, Reproducing kernel Hilbert space, Ambiguity set, Temporal logic specifications, Spectral barrier, Finite Fourier expansion, Linear program, Distributionally robust","The paper presents a data-driven approach for safety verification and synthesis of black-box systems with discrete-time stochastic dynamics, employing control barrier certificates and learning them directly from system trajectories. It uses conditional mean embeddings to embed data into a reproducing kernel Hilbert space and constructs an ambiguity set to robustify the results. The approach is scalable and distributionally robust, applicable to general classes of temporal logic specifications beyond safety. The data-driven computation of safety barriers leverages a finite Fourier expansion to cast a semi-infinite optimization problem as a linear program, allowing for efficient generation of relaxed problems using the fast Fourier transform. The work moves beyond restrictive assumptions on system dynamics and uncertainty, demonstrated on two case studies including a black-box system with a neural network controller.",12.73,Qwen2.5-3B,Apple M1 (Metal)
2601.12003v1_Robust Verification of Concurrent Stochastic Games.pdf,Robust Verification of Concurrent Stochastic Games,"Angel Y. He, David Parker",Not found,2601.12003,"Robust quantitative verification, Probabilistic model checking, Concurrent stochastic games, Epistemic uncertainty","This paper introduces robust concurrent stochastic games (CSGs) and their subclass interval CSGs (ICSGs), which capture epistemic uncertainty about transition probabilities in CSGs. We propose a framework for robust verification under worst-case assumptions about transition uncertainty, developing theoretical foundations and efficient algorithms for finite- and infinite-horizon objectives in both zero-sum and nonzero-sum settings. The latter are based on Nash equilibria. An implementation in PRISM-games model checker demonstrates the feasibility of robust verification across large benchmarks.",12.36,Qwen2.5-3B,Apple M1 (Metal)
2601.12014v1_Are LLMs Ready for TOON Benchmarking Structural Co.pdf,Are LLMs Ready for TOON? Benchmarking Structural Correctness–Sustainability Trade-offs in Novel Structured Output Formats,"Elio Masciari, Vincenzo Moscato, Enea Vincenzo Napolitano, Gian Marco Orlando, Marco Perillo, Diego Russo",the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper.,the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper.,"Green AI, TOON, Large Language Models, Natural Language Processing, Sustainability","Large Language Models (LLMs) are increasingly required to generate structured, machine-readable outputs for downstream systems. While recent benchmarks have focused on evaluating the structural correctness of such outputs, the environmental impact of inference for different output formats has largely been overlooked. This paper introduces a sustainability-aware evaluation framework for structured generation that measures token usage, generation time, and estimated carbon emissions. Using this framework, we benchmark the novel TOON format against established representations (JSON, XML, YAML) across multiple LLMs spanning different architectures and parameter scales. Our results reveal a consistent trade-off: TOON yields more compact outputs and lower emissions, but lower structural correctness when models lack native support. We show that increased model capacity reduces this gap and that environment-aware scoring can shift format rankings depending on deployment priorities.",12.3,Qwen2.5-3B,Apple M1 (Metal)
2601.12019v1_Acting Flatterers via LLMs Sycophancy Combating Cl.pdf,Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs Opposing-Stance Reasoning,"Chaowei Zhang, Xiansheng Luo, Zewei Zhang, Yi Zhu, Jipeng Qiang, Longwei Wang",10.1145/XXXXXX.XXXXXX,,"Clickbait Detection, Large Language Models, Opposing Stance Reasoning, Contrastive Learning","This work proposes a novel approach to combat clickbait by harnessing the tendency of Large Language Models (LLMs) to produce reasoning that matches users' beliefs, rather than eliminating it. The Self-renewal Opposing-stance Reasoning Generation (SORG) framework prompts LLMs to generate high-quality 'agree' and 'disagree' reasoning pairs for a given news title without requiring ground-truth labels. The local Opposing Reasoning-based Clickbait Detection (ORCD) model integrates three BERT encoders to represent the title and its associated reasoning, leveraging contrastive learning guided by soft labels derived from LLM-generated credibility scores. Experimental evaluations on three benchmark datasets demonstrate superior performance compared to LLM prompting, fine-tuned smaller language models, and state-of-the-art clickbait detection baselines.",11.94,Qwen2.5-3B,Apple M1 (Metal)
2601.12024v1_A Multi-Agent System for Generating Actionable Bus.pdf,A Multi-Agent System for Generating Actionable Business Advice,"Kartikey Singh Bhandari, Tanish Jain, Archit Agrawal, Dhruv Kumar, Praveen Kumar, Pratik Narang",,,"multi-agent system, business advice, customer reviews, large language models, actionability, specificity, non-redundancy","This paper presents a multi-agent, large language model (LLM)-based framework for generating actionable business advice from customer reviews. The framework integrates four components: clustering to select representative reviews, advice generation, iterative evaluation, and feasibility-based ranking. Experiments across three service domains and multiple model families show that the framework consistently outperforms single model baselines on actionability, specificity, and non-redundancy, with medium-sized models approaching the performance of large model frameworks.",11.2,Qwen2.5-3B,Apple M1 (Metal)
2601.12030v1_ARC Active and Reflection-driven Context Managemen.pdf,ARC: Active and Reflection-driven Context Management for Long-Horizon Information Seeking Agents,"Yilun Yao1, Shan Huang2, Elsie Dai1, Zhewen Tan1, Zhenyu Duan3, Shousheng Jia 3, Yanbing Jiang 1, Tong Yang1",,arXiv:2312.09626,"context management, long-horizon information seeking, active reflection, deep search, large language models","Large language models are increasingly deployed for deep search and long-horizon information seeking, but their performance often degrades as interaction histories grow. This degradation, known as context rot, reflects a failure to maintain coherent and task-relevant internal states over extended reasoning horizons. Existing approaches primarily manage context through raw accumulation or passive summarization, treating it as a static artifact and allowing early errors or misplaced emphasis to persist. ARC, the first framework to systematically formulate context management as an active, reflection-driven process, allows agents to actively reorganize their working context when misalignment or degradation is detected, achieving up to an 11% absolute improvement in accuracy on challenging benchmarks.",11.99,Qwen2.5-3B,Apple M1 (Metal)
2601.12038v1_Abstract Argumentation with Subargument Relations.pdf,Abstract Argumentation with Subargument Relations,Beishui Liao,Not found,Not found,"Abstract Argumentation, Subargument Relations, Attack Relations, Structured Argumentation","This paper studies abstract argumentation frameworks enriched with an explicit subargument relation, treating it alongside attack as a basic relation. It analyzes how subargument relations interact with attacks and examines their impact on fundamental semantic properties.",10.01,Qwen2.5-3B,Apple M1 (Metal)
2601.12040v1_Partial Reasoning in Language Models Search and Re.pdf,Partial Reasoning in Language Models: Search and Refinement Guided by Uncertainty,"Murilo da Luz1,2, Bruno Brandão 1,2, Luana Martins 1,2, Gustavo Oliveira 1,2, Bryan de Oliveira1,2, Luckeciano Melo 1,3, Telma Soares 1,2",,,"Uncertainty, Entropy, Latent-space search, Soft Reasoning, LLM reasoning","The use of Large Language Models (LLMs) for reasoning and planning tasks has drawn increasing attention in Artificial Intelligence research. Despite their remarkable progress, these models still exhibit limitations in multi-step inference scenarios, particularly in mathematical and logical reasoning. We introduce PREGU (Partial Reasoning Guided by Uncertainty). PREGU monitors the entropy of the output distribution during autoregressive generation and halts the process whenever entropy exceeds a defined threshold, signaling uncertainty. From that point, a localized search is performed in the latent space to refine the partial reasoning and select the most coherent answer, using the Soft Reasoning method. Experiments conducted with LLaMA-3-8B, Mistral-7B, and Qwen2-7B across four reasoning benchmarks (GSM8K, GSM-Hard, SVAMP, and StrategyQA) showed performance greater than or similar to Soft Reasoning, indicating that entropy can serve as an effective signal to trigger selective refinement during reasoning.",12.82,Qwen2.5-3B,Apple M1 (Metal)
2601.12042v1_Less Is More -- Until It Breaks Security Pitfalls .pdf,Less Is More — Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models,"Xiaomei Zhang, Zhaoxi Zhang, Leo Yu Zhang, Yanjun Zhang, Guanhong Tao, Shirui Pan",XXXXX.XXXXXX,XXXXX,"Vision Token Compression, Large Vision-Language Models, Security, Robustness","Visual token compression is widely adopted to improve inference efficiency of Large Vision-Language Models (LVLMs), but it degrades model robustness, making LVLMs vulnerable to small and imperceptible perturbations, leading to model failure.",10.65,Qwen2.5-3B,Apple M1 (Metal)
2601.12049v1_textitFocaLogic Logic-Based Interpretation of Visu.pdf,FocaLogic: Logic-Based Interpretation of Visual Model Decisions,"Chenchen Zhao*, Muxi Chen*, Qiang Xu†",,,"interpretability, visual models, logic-based, focuses, quantitative metrics","A novel model-agnostic framework designed to interpret and quantify visual model decision-making through logic-based representations, identifying minimal interpretable subsets of visual regions termed visual focuses that decisively influence model predictions.",10.13,Qwen2.5-3B,Apple M1 (Metal)
2601.12053v1_A New Strategy for Artificial Intelligence Trainin.pdf,A New Strategy for Artificial Intelligence: Training Foundation Models Directly on Human Brain Data,Ma¨el Donoso ∗,Not found,2601.12053,"foundation models, brain, neuroimaging, brain-generated data, brain-trained foundation models, reinforcement learning, chain of thought","While foundation models have achieved remarkable results across a diversity of domains, they still rely on human-generated data, such as text, as a fundamental source of knowledge. This paper explores a new strategy for artificial intelligence by training foundation models directly on human brain data, hypothesizing that neuroimaging data could open a window into elements of human cognition not accessible through observable actions. The authors classify the current limitations of foundation models and propose two methods, reinforcement learning from human brain (RLHB) and chain of thought from human brain (CoTHB), to prioritize the use of limited neuroimaging data for strategic steps in foundation model training.",12.33,Qwen2.5-3B,Apple M1 (Metal)
2601.12055v1_Automating Parameter Selection in Deep Image Prior.pdf,Automating Parameter Selection in Deep Image Prior for Fluorescence Microscopy Image Denoising via Similarity-Based Parameter Transfer,"Lina Meyer, Felix Wissel, Tobias Knopp, Susanne Pfefferle, Ralf Fliegert, Maximilian Sandmann, Liana Uebler, Franziska M""ockl, Bj""orn-Philipp Diercks, David Lohr, Ren""e Werner",Not provided,Not provided,"Deep learning, Image denoising, Fluorescence microscopy, Parameter transfer, Unsupervised learning","This study proposes an automated method for selecting parameters in deep image prior (DIP) for denoising fluorescence microscopy images. By comparing images with similar metadata, the method achieves similar and better performance than using quantitative image similarity measures. The study demonstrates that AUTO-DIP, the proposed pipeline, outperforms existing methods for various open-source test datasets, particularly for very noisy inputs. The method is particularly useful for routine applications in fluorescence microscopy imaging.",12.14,Qwen2.5-3B,Apple M1 (Metal)
2601.12061v1_Codebook-Injected Dialogue Segmentation for Multi-.pdf,Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs,"Jinsook Lee, Kirk Vanacore, Zhuqian Zhou, Bakhtawar Ahtisham, Jeanine Grütter, René F. Kizilcec",Not found,Not found,"Dialogue Act Annotation, Segmentation, Language Technology, Human-AI Agreement","This paper proposes codebook-injected segmentation for dialogue act annotation, which conditions boundary decisions on downstream annotation criteria. It evaluates LLM-based segmenters against standard and retrieval-augmented baselines, introducing evaluation metrics for span consistency, distinctiveness, and human-AI distributional agreement. The results highlight segmentation as a consequential design choice that should be optimized for downstream objectives rather than a single performance score.",11.39,Qwen2.5-3B,Apple M1 (Metal)
2601.12068v1_Bridging the Gap in Bangla Healthcare Machine Lear.pdf,Bridging the Gap in Bangla Healthcare: Machine Learning Based Disease Prediction Using a Symptoms-Disease Dataset,"Rowzatul Zannat, Abdullah Al Shafi, Abdul Muntakim",Not found,Not found,"Disease Prediction, Machine Learning, Symptoms-Disease Dataset, Bangla Healthcare","This study addresses the gap in disease prediction resources for non-English-speaking populations, particularly Bangla-speaking communities. By developing a comprehensive Bangla symptoms-disease dataset containing 758 unique symptom-disease relationships spanning 85 diseases, the study aims to enhance equitable access to health information for these communities. The dataset enables the prediction of diseases based on Bangla symptom inputs and evaluates multiple machine learning models for disease prediction.",11.48,Qwen2.5-3B,Apple M1 (Metal)
2601.12082v1_Conditional Random Fields for Interactive Refineme.pdf,Conditional Random Fields for Interactive Refinement of Histopathological Predictions,"Tiffanie Godelaine†, Maxime Zanella†, Karim El Khoury1, Saïd Mahmoudi2, Benoît Macq1, Christophe De Vleeschouwer1",Not provided,Not provided,"Histology Classification, Conditional Random Fields, Human-In-The-Loop, Foundation Models","This paper proposes a method to refine predictions of Vision-Language Models (VLMs) in histopathological applications using Conditional Random Fields (CRFs). The method aims to improve accuracy without requiring additional model training and can incorporate real-time guidance from pathologists. Experiments on five patch-level classification datasets demonstrate significant accuracy gains, with human-in-the-loop integration further improving performance.",11.49,Qwen2.5-3B,Apple M1 (Metal)
2601.12095v1_Neural Isomorphic Fields A Transformer-based Algeb.pdf,Neural Isomorphic Fields: A Transformer-Based Algebraic Numerical Embedding,"Hamidreza Sadeghi, Saeedeh Momtazi, Reza Safabakhsh",Not provided,Not provided,"Neural Networks, Embeddings, Algebraic Structures, Number Embedding, Neural Arithmetic Units","This paper introduces a novel neural network architecture called Neural Isomorphic Fields (NIF) for representing and processing numbers. NIF uses embedding vectors to preserve algebraic operations within the rational numbers field, addressing challenges such as overflow and underflow in neural network models. The authors demonstrate the effectiveness of NIF in preserving algebraic properties during addition, while highlighting the challenges in multiplication. The model is evaluated on various algebraic tests, showing over 95% accuracy for addition and between 53% to 73% for multiplication.",11.83,Qwen2.5-3B,Apple M1 (Metal)
2601.12099v1_Large language models struggle with ethnographic t.pdf,Large Language Models Struggle with Ethnographic Text Annotation,"Leonardo S. Goodall†, Dor Shilton†, Daniel Austin Mullins, Harvey Whitehouse",Not found,Not found,"large language models, ethnographic text annotation, cross-cultural research, anthropology","Large language models have shown promise for automated text annotation, but their performance on ethnographic texts is limited, falling well below levels required for reliable automated annotation. The study evaluated 7 state-of-the-art LLMs on their ability to annotate 121 ritual features across 567 ethnographic excerpts, finding that longer texts, features requiring ordinal distinctions, and ambiguous constructs are particularly difficult for LLMs. Human inter-coder reliability set an approximate ceiling on LLM accuracy, and even on features where humans reliably agreed, models fell short of human performance.",11.94,Qwen2.5-3B,Apple M1 (Metal)
2601.12104v1_Powerful Training-Free Membership Inference Agains.pdf,Powerful Training-Free Membership Inference Against Autoregressive Language Models,"David Ilić, David Stanojević, Kostadin Cvejoski",,,"membership inference, privacy auditing, language models, fine-tuning, autoregressive models","Fine-tuned language models pose significant privacy risks, as they may memorize and expose sensitive training data. Membership inference attacks (MIAs) provide a principled framework for auditing these risks, yet existing methods achieve limited detection rates, particularly at low false-positive thresholds. This paper presents EZ-MIA, a membership inference attack that exploits the observation that memorization manifests most strongly at error positions, specifically tokens where the model predicts incorrectly yet still shows elevated probability for training examples. The Error Zone (EZ) score measures the directional imbalance of probability shifts at error positions relative to a pretrained reference model. This attack requires only two forward passes per query and no model training of any kind, achieving higher detection rates than prior work.",11.73,Qwen2.5-3B,Apple M1 (Metal)
2601.12124v1_SynQP A Framework and Metrics for Evaluating the Q.pdf,SYNQP: A FRAMEWORK AND METRICS FOR EVALUATING THE QUALITY AND PRIVACY RISK OF SYNTHETIC DATA,"Bing Hu, Yixin Li, Asma Bahamyirou, Helen Chen",Not found,Not found,"Real-World Data, Synthetic Data, Privacy Metrics, Evaluation Framework, Membership Inference Attack, Identity Disclosure Risk","The use of synthetic data in health applications raises privacy concerns, yet the lack of open frameworks for privacy evaluations has slowed its adoption. SYNQP is an open framework for benchmarking privacy in synthetic data generation (SDG) using simulated sensitive data, ensuring that original data remains confidential. The authors highlight the need for privacy metrics that fairly account for the probabilistic nature of machine learning models. As a demonstration, SYNQP is used to benchmark CTGAN and propose a new identity disclosure risk metric that offers a more accurate estimation of privacy risks compared to existing approaches.",12.18,Qwen2.5-3B,Apple M1 (Metal)
2601.12126v1_UniMo Unified Motion Generation and Understanding .pdf,UniMo: Unified Motion Generation and Understanding with Chain of Thought,"Guocun Wang, Kenkun Liu, Jing Lin, Guorui Song, Jian Li, Xiaoguang Han",Not found,Not found,"motion generation, motion understanding, chain of thought, reinforcement learning, supervised fine-tuning, next-token prediction, semantic alignment, structural correctness","A novel framework UniMo is proposed to integrate motion-language information and interpretable chain of thought reasoning into large language models via supervised fine-tuning. It addresses the limitations of existing unified frameworks based on large language models, which often encounter challenges in semantic alignment and task coherence, and the cumulative prediction errors caused by the next-token prediction paradigm in LLMs. UniMo significantly outperforms existing unified and task-specific models in both motion generation and understanding tasks.",11.46,Qwen2.5-3B,Apple M1 (Metal)
2601.12132v1_Bengali Text Classification An Evaluation of Large.pdf,Bengali Text Classification: An Evaluation of Large Language Model Approaches,"Md Mahmudul Hoque∗, Md Mehedi Hassain2, Md Hojaifa Tanvir3, Rahul Nandy2",Not found,2601.12132,"Bengali Text Classification, Transformer-based Text Classifier, Multilingual NLP, Qwen, LLaMA","This study evaluates the effectiveness of large language models in classifying Bengali newspaper articles. Three instruction-tuned models—LLaMA 3.1 8B Instruct, LLaMA 3.2 3B Instruct, and Qwen 2.5 7B Instruct—are tested under the same classification framework. Qwen 2.5 achieves the highest classification accuracy of 72%, particularly in the 'Sports' category. LLaMA 3.1 and LLaMA 3.2 attain accuracies of 53% and 56%, respectively. The findings highlight the potential of LLMs in Bengali text classification, despite the scarcity of resources for Bengali NLP. Future research will focus on additional models, addressing class imbalance issues, and refining fine-tuning approaches.",13.34,Qwen2.5-3B,Apple M1 (Metal)
2601.12134v1_Human-Human-AI Triadic Programming Uncovering the .pdf,Human-Human-AI Triadic Programming: Uncovering the Role of AI Agent and the Value of Human Partner in Collaborative Learning,"Taufiq Daryanto, Xiaohan Ding, Kaike Ping, Lance T. Wilhelm, Yan Chen, Chris Brown, Eugenia H. Rho",XXXXXXX.XXXXXXX,2601.12134v1,"Collaborative Learning, Human-Human-AI, Programming, AI Agent, Social Interaction","This study explores human-human-AI triadic programming, where two humans work with an AI agent, aiming to uncover the role of AI and the value of human collaboration in programming learning.",12.15,Qwen2.5-3B,Apple M1 (Metal)
2601.12138v1_DriveSafe A Hierarchical Risk Taxonomy for Safety-.pdf,DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants,"Abhishek Kumar, Riya Tapwal, Carsten Maple",,,"Large Language Models, Safety-Critical, Driving Assistants, Hierarchical Risk Taxonomy, LLM Safety, Safety Alignment, Real-World Driving Scenarios","This paper introduces DriveSafe, a hierarchical four-level risk taxonomy designed to systematically characterize safety-critical failure modes of LLM-based driving assistants. The taxonomy comprises 129 fine-grained atomic risk categories spanning technical, legal, societal, and ethical dimensions, validated through evaluation of refusal behavior across six widely deployed LLMs.",11.36,Qwen2.5-3B,Apple M1 (Metal)
2601.12141v1_TIDE A Trace-Informed Depth-First Exploration for .pdf,TIDE: A Trace-Informed Depth-First Exploration for Planning with Temporally Extended Goals,"Yuliia Suprun, Khen Elimelech, Lydia E. Kavraki, Moshe Y. Vardi",Not found,2601.12141v1,"Task planning, Temporal logic, Depth-first search, Heuristics, Robotics","Introducing TIDE, a novel approach for task planning with temporally extended goals, which decomposes the problem into smaller, manageable reach-avoid subproblems, each solvable using an off-the-shelf planner. It uses cost-driven heuristics to guide exploration and ensures completeness and efficiency through adaptive backtracking.",12.21,Qwen2.5-3B,Apple M1 (Metal)
2601.12147v1_Segment and Matte Anything in a Unified Model.pdf,Segment and Matte Anything in a Unified Model,"Zezhong Fan*, Xiaohan Li*",Not found,Not found,"Segment Anything, Interactive Image Matting, Zero-shot Generalization, Refinement Modules, Multi-View Localization Encoder, Localization Adapter","This paper introduces Segment And Matte Anything (SAMA), a lightweight extension of Segment Anything (SAM) that delivers high-quality interactive image segmentation and matting with minimal extra parameters. SAMA incorporates a Multi-View Localization Encoder (MVLE) and a Localization Adapter (Local-Adapter) to improve mask outputs and generate segmentation and matting masks simultaneously. Trained on a diverse dataset, SAMA achieves state-of-the-art performance across multiple segmentation and matting benchmarks, showcasing its adaptability and effectiveness in various downstream tasks.",11.25,Qwen2.5-3B,Apple M1 (Metal)
2601.12150v1_Enhanced Diagnostic Performance via Large-Resoluti.pdf,Enhanced Diagnostic Performance via Large-Resolution Inference Optimization for Pathology Foundation Models,"Mengxuan Hu∗, Zihan Guan⋆, John Kang, Sheng Li‡, Zhongliang Zhou‡",Not found,2601.12150,"Computational pathology, Foundation models, Inference Optimization","Despite their strong performance on tasks such as ROI classification and segmentation, many pathology foundation models remain constrained by a specific input size, creating substantial inefficiencies when applied to whole-slide images (WSIs). This paper proposes an efficient inference strategy that sparsifies attention using spatially aware neighboring blocks and filters out non-informative tokens through global attention scores, enabling inference at higher resolutions under the same GPU budget.",11.93,Qwen2.5-3B,Apple M1 (Metal)
2601.12186v1_Aletheia What Makes RLVR For Code Verifiers Tick.pdf,Exact paper title,"Vatsal Venkatkrishna, Indraneil Paul, Iryna Gurevych",DOI string if found,ID if found,"Reinforcement Learning, Code Verification, Large Language Models, Execution Feedback, Multi-domain Thinking","Multi-domain thinking verifiers trained via Reinforcement Learning from Verifiable Rewards (RLVR) are a prominent fixture in the Large Language Model (LLM) post-training pipeline. This paper explores the creation and open-source release of Aletheia, a controlled testbed for execution-grounded evaluation of code verifiers' robustness across different policy models and covariate shifts. The authors examine the components of the RLVR-based training recipe and uncover opportunities for simplification, particularly highlighting the importance of on-policy learning and thinking-based training at different scales.",11.25,Qwen2.5-3B,Apple M1 (Metal)
2601.12205v1_Do Neural Codecs Generalize A Controlled Study Acr.pdf,Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks,"Shih-Heng Wang1*, Jiatong Shi 2, Jinchuan Tian 2, Haibin Wu3, Shinji Watanabe 2",Not provided,Not provided,"Neural Audio Codecs, Generalization, Unseen Languages, Non-Speech Tasks, Speech-Only Pre-Training, Non-Speech Pre-Training","This paper investigates the generalization capabilities of Neural Audio Codecs (NACs) in three crucial aspects: (i) their ability to generalize to unseen languages during pre-training, (ii) their effectiveness in generalizing to non-speech tasks from speech-only pre-training, and (iii) the impact of including non-speech pre-training data on performance. The study uses strictly controlled configurations and curated pre-trained data to provide fair comparisons and comprehensive evaluations of NACs performance on signal reconstruction quality and downstream applications using 11 metrics. The findings show that NACs can generalize to unseen languages, speech-only pre-trained NACs degrade on non-speech tasks, and including non-speech data during pre-training improves performance on non-speech tasks while maintaining comparable performance on speech tasks.",11.98,Qwen2.5-3B,Apple M1 (Metal)
2601.12212v1_Speculative Sampling with Reinforcement Learning.pdf,Speculative Sampling with Reinforcement Learning,"Chenan Wang, Daniel H. Shi, Haipeng Chen",Not found,Not found,"Reinforcement Learning, Speculative Sampling, Large Language Models, Inference Time Latency, Draft Tree","This paper introduces Re-SpS, a reinforcement learning-based framework for optimizing draft tree hyperparameters in speculative sampling methods for large language models. Re-SpS dynamically adjusts these hyperparameters in real-time, learning context-aware policies that maximize generation speed by balancing speculative aggression with computational overhead. Evaluation results demonstrate consistent improvements over state-of-the-art methods, achieving up to 5.45× speedup over the backbone LLM and up to 1.12× speedup across five diverse benchmarks, with no loss in output fidelity.",11.03,Qwen2.5-3B,Apple M1 (Metal)
2601.12215v1_Wavelet-Driven Masked Multiscale Reconstruction fo.pdf,Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation Models,"Megha Thukral*, Cyrus Tanade, Simon A. Lee, Juhyeon Lee, Hao Zhou, Keum San Chun, Migyeong Gwak, Viswam Nathan, Md Mahbubur Rahman, Li Zhu, Mehrab Bin Morshed, Subramaniam Venkatraman, Sharanya Arcot Desai",10.1101/2601.12215v1,2601.12215v1,"Wearable SSL Method, Wavelet based Modelling, PPG foundation models","This paper introduces Masked Multiscale Reconstruction (MMR) for PPG representation learning, a self-supervised pretraining framework that explicitly learns from hierarchical time–frequency scales of PPG data, improving over or matching state-of-the-art open-source PPG foundation models and other self-supervised baselines on 17 of 19 diverse health-related tasks.",13.29,Qwen2.5-3B,Apple M1 (Metal)
2601.12224v1_Where It Moves It Matters Referring Surgical Instr.pdf,"Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion","Meng Wei, Kun Yuan, Shi Li, Yue Zhou, Long Bai, Nassir Navab, Hongliang Ren, Hong Joo Lee, Tom Vercauteren, Nicolas Padoy",Not provided,Not provided,"surgical segmentation, motion-guided, referring, intuitive interaction, autonomous surgery, surgical instruments, natural language, video analysis","This work introduces SurgRef, a novel motion-guided framework that enables models to understand and segment surgical instruments based on their motion, rather than their appearance. The framework is trained on a diverse, multi-institutional video dataset with dense spatiotemporal masks and rich motion-centric expressions, achieving state-of-the-art accuracy and generalization across surgical procedures.",11.29,Qwen2.5-3B,Apple M1 (Metal)
2601.12234v1_Proc3D Procedural 3D Generation and Parametric Edi.pdf,Proc3D: Procedural 3D Generation and Parametric Editing of 3D Shapes with Large Language Models,"Fadlullah Raji, Stefano Petrangeli, Matheus Gadelha, Yu Shen, Uttaran Bhattacharya, Gang Wu",Not provided,2601.12234v1,"3D modeling, Large Language Models, Procedural Generation, Parametric Editing","Proc3D is a system designed to generate editable 3D models while enabling real-time modifications, using procedural compact graph (PCG) representation and Large Language Models (LLMs).",12.42,Qwen2.5-3B,Apple M1 (Metal)
2601.12242v1_Optimal Power Allocation and Sub-Optimal Channel A.pdf,OPTIMALPOWERALLOCATION ANDSUB-OPTIMALCHANNELASSIGNMENT FORDOWNLINKNOMA SYSTEMUSINGDEEPREINFORCEMENTLEARNING,"WooSeok Kim, Jeonghoon Lee, Sangho Kim, Taesun An, WonMin Lee, Dowon Kim, Kyungseop Shin",Not found,2601.12242,"Non-orthogonal multiple access (NOMA), deep reinforcement learning (DRL), wireless network, resource allocation","In recent years, Non-Orthogonal Multiple Access (NOMA) system has emerged as a promising candidate for multiple access frameworks due to the evolution of deep machine learning, trying to incorporate deep machine learning into the NOMA system. The main motivation for such active studies is the growing need to optimize the utilization of network resources as the expansion of the internet of things (IoT) caused a scarcity of network resources. The NOMA addresses this need by power multiplexing, allowing multiple users to access the network simultaneously. Nevertheless, the NOMA system has few limitations. Several works have proposed to mitigate this, including the optimization of power allocation known as joint resource allocation (JRA) method, and integration of the JRA method and deep reinforcement learning (JRA-DRL). Despite this, the channel assignment problem remains unclear and requires further investigation. In this paper, we propose a deep reinforcement learning framework incorporating replay memory with an on-policy algorithm, allocating network resources in a NOMA system to generalize the learning. Also, we provide extensive simulations to evaluate the effects of varying the learning rate, batch size, type of model, and the number of features in the state.",13.58,Qwen2.5-3B,Apple M1 (Metal)
2601.12243v1_Less is More Label-Guided Summarization of Procedu.pdf,Less is More: Label-Guided Summarization of Procedural and Instructional Videos,"Shreya Rajpal∗, Vellore Institute of Technology, shreyarajpal6@gmail.com, Michal Golovanesky, Brown University, michal_golovanesky@brown.edu, Carsten Eickhoff, University of Tübingen, carsten.eickhoff@uni-tuebingen.de",Not provided,2601.12243,"Video summarization, Procedural videos, Instructional videos, Label-driven summarization, Semantic alignment","This paper proposes a three-stage framework, PRISM, for producing semantically grounded video summaries of procedural and instructional videos. It combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model to ensure that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content.",12.52,Qwen2.5-3B,Apple M1 (Metal)
2601.12247v1_Plan Verify and Fill A Structured Parallel Decodin.pdf,"Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion Language Models","Miao Li * 1, Hanyang Jiang * 1, Sikai Cheng 1, Hengyu Fu 2, Yuhang Cai2, Baihe Huang 2, Tinghan Ye1, Xuanzhou Chen 1, Pascal Van Hentenryck1",,,"Diffusion Language Models, Parallel Decoding, Quantitative Validation, Pragmatic Structural Stopping","This paper proposes Plan-Verify-Fill (PVF), a training-free paradigm that grounds planning via quantitative validation. PVF actively constructs a hierarchical skeleton by prioritizing high-leverage semantic anchors and employs a verification protocol to operationalize pragmatic structural stopping. Extensive evaluations on LLaDA-8B-Instruct and Dream-7B-Instruct demonstrate that PVF reduces the Number of Function Evaluations (NFE) by up to 65% compared to confidence-based parallel decoding across benchmark datasets, unlocking superior efficiency without compromising accuracy.",12.16,Qwen2.5-3B,Apple M1 (Metal)
2601.12248v1_AQUA-Bench Beyond Finding Answers to Knowing When .pdf,AQUA-BENCH: BEYOND FINDING ANSWERS TO KNOWING WHEN THERE ARE NONE,"Chun-Yi Kuan, Hung-yi Lee",Not found,Not found,"Audio question answering, Unanswerable questions, Large language models, Benchmarking","Recent advances in audio-aware large language models have shown strong performance on audio question answering. However, existing benchmarks mainly cover answerable questions and overlook the challenge of unanswerable ones, where no reliable answer can be inferred from the audio. AQUA-Bench, a new benchmark, systematically evaluates three scenarios: Absent Answer Detection, Incompatible Answer Set Detection, and Incompatible Audio Question Detection, offering a rigorous measure of model reliability and promoting the development of audio-language systems that are more robust and trustworthy.",11.7,Qwen2.5-3B,Apple M1 (Metal)
2601.12249v1_An Innovative Framework for Breast Cancer Detectio.pdf,"An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion","Ehsan Sadeghi Pour, Mahdi Esmaeili, Morteza Romoozi",,,"Breast Cancer Detection, Pyramid Adaptive Atrous Convolution (PAAC), Transformer, Multi-Scale Feature Fusion, Self-Attention Mechanism, Medical Image Processing","This thesis presents an innovative framework for detecting malignant masses in mammographic images by integrating Pyramid Adaptive Atrous Convolution (PAAC) and Transformer architectures. The proposed approach utilizes Multi-Scale Feature Fusion to enhance the extraction of features from benign and malignant tissues and combines Dice Loss and Focal Loss functions to improve the model's learning process, effectively reducing errors in binary breast cancer classification and achieving high accuracy and efficiency.",12.07,Qwen2.5-3B,Apple M1 (Metal)
2601.12256v1_Improving Large Molecular Language Model via Relat.pdf,Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration,"Jinyoung Park, Minseong Bae, Jeehye Na, Hyunwoo J. Kim",Not found,Not found,"Large language models, molecular language models, multimodal collaboration, relation-aware attention, hallucination prevention, molecular modeling","This paper proposes CoLLaMo, a large language model-based molecular assistant equipped with a multi-level molecular modality-collaborative projector. The relation-aware modality-collaborative attention mechanism facilitates fine-grained and relation-guided information exchange between atoms, addressing the limitations of existing large molecular language models (LMLMs) in hallucination and robustness. The model enhances the molecular modality generalization capabilities of LMLMs, achieving the best performance on multiple tasks including molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction.",11.59,Qwen2.5-3B,Apple M1 (Metal)
2601.12257v1_Soft Shadow Diffusion SSD Physics-inspired Learnin.pdf,Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy,"Fadlullah Raji, John Murray Bruce",Not found,2601.12257,"Computational imaging, Machine learning, 3D generative models, Diffusion models, Separable non-linear least squares","This paper proposes a novel approach to 3D reconstruction of hidden scenes from ordinary NLOS photographs, using a reformulated light transport model that decomposes the hidden scene into light-occluding and non-light-occluding components. The authors develop two solutions: a gradient-based optimization method and a physics-inspired neural network approach called Soft Shadow diffusion (SSD). SSD is effective on numerous 3D scenes in real experimental scenarios and generalizes well to unseen classes in simulation and real-world NLOS scenes.",12.94,Qwen2.5-3B,Apple M1 (Metal)
2601.12259v1_FutureX-Pro Extending Future Prediction to High-Va.pdf,FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains,"ByteDance, Hong Kong University of Science and Technology, Georgia Institute of Technology, Stanford University, Princeton University",Not found,2601.12259,"FutureX, Future Prediction, High-Value Domains, Financial Markets, Retail Forecasting, Public Health, Natural Disasters, Large Language Models, Domain Grounding, Industrial Deployment","This report introduces FutureX-Pro, a specialized framework extending agentic future prediction to high-value vertical domains, including FutureX-Finance, FutureX-Retail, FutureX-PublicHealth, and FutureX-NaturalDisaster. It benchmarks agentic Large Language Models on foundational prediction tasks and assesses their domain grounding for industrial deployment.",12.67,Qwen2.5-3B,Apple M1 (Metal)
2601.12260v1_Docs2Synth A Synthetic Data Trained Retriever Fram.pdf,Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding,"Yihao Ding, Qiang Sun, Puzhen Wu, Sirui Li, Siwen Luo, Wei Liu",Not found,Not found,"synthetic data, retriever, vision-language pre-trained models, document understanding, OCR, hallucination, domain grounding","Docs2Synth is a synthetic-supervision framework designed to enable retrieval-guided inference for private and low-resource domains. It automatically processes raw document collections, generates and verifies diverse QA pairs via an agent-based system, and trains a lightweight visual retriever to extract domain-relevant evidence. During inference, the retriever collaborates with a multimodal large language model (MLLM) through an iterative retrieval-generation loop, reducing hallucination and improving response consistency. The framework is delivered as an easy-to-use Python package, enabling plug-and-play deployment across diverse real-world scenarios.",11.79,Qwen2.5-3B,Apple M1 (Metal)
2601.12263v1_Multimodal Generative Engine Optimization Rank Man.pdf,Multimodal Generative Engine Optimization: Rank Manipulation for Vision–Language Model Rankers,"Yixuan Du, Chenxiao Yu, Haoyan Xu, Ziyi Wang, Yue Zhao, Xiyang Hu*",Not found,Not found,"Vision-Language Models, Ranking Attacks, Adversarial Manipulation, Multimodal Perturbations","This paper uncovers a vulnerability in Vision-Language Models (VLMs) used in modern retrieval and recommendation systems, revealing that they can be manipulated to unfairly promote target products. The authors present MGEO, a novel adversarial framework that optimizes imperceptible image perturbations and fluent textual suffixes to manipulate VLMs' relevance scoring and rank target items without conventional content filters.",10.43,Qwen2.5-3B,Apple M1 (Metal)
2601.12269v1_Simulated Annealing Enhances Theory-of-Mind Reason.pdf,Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models,"Xucong Hu, Jian-Qiao Zhu",,,"Language Models, Markov Chain Monte Carlo, Simulated Annealing, Power Sampling, Theory of Mind","Autoregressive language models are criticized for optimizing surface plausibility rather than maintaining correct latent-state representations. This paper shows that strong Theory-of-Mind capability can be recovered from the base model without additional weight updates or verifications, using power-sampling methods with simulated annealing.",10.23,Qwen2.5-3B,Apple M1 (Metal)
2601.12276v1_Predictive Prototyping Evaluating Design Concepts .pdf,PREDICTIVE PROTOTYPING: EVALUATING DESIGN CONCEPTS WITH GPT,"Hilsann Yong, Singapore University of Technology & Design",Not found,Not found,"Prototyping, Design Theory, Iteration, Simulation, AI, LLM, GPT, RAG, Crowdsourcing","This work explores whether a GPT can accurately predict information that would be gained during a prototyping effort such as cost, performance, and perceived usability. A novel approach is introduced to emulate design feedback using retrieval augmented generation (RAG) in conjunction with a GPT, specifically OpenAI’s GPT-4o. The method used in this paper leverages prototyping data scraped from the 'Instructables.com' database; thereby increasing the availability of relevant prototyping data to the model. Two efforts are reported. The first is a controlled study where predictions are made about a series of diverse designs. The GPT, and human designers were provided design sketches and asked to predict cost, performance, and usability. Performance of each condition is then compared to ground-truth physical prototyping results. A second effort reports on an experimental application, in which a physical prototype was produced based on recommendations from the GPT-RAG model. The performance of this prototype is compared against a baseline, commercial model, and a topology optimized model. The results indicate that the GPT-RAG predictions are more accurate than individual human or crowd estimations of cost and performance, while offering similar insights in terms of usability; the GPT-RAG inspired prototype also outperformed the commercial, and topology optimized prototypes.",13.63,Qwen2.5-3B,Apple M1 (Metal)
2601.12282v1_CytoCLIP Learning Cytoarchitectural Characteristic.pdf,CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training,"Pralaypati Ta, Sriram Venkatesaperumal, Keerthi Ram, Mohanasankar Sivaprakasam",,,"Cytoarchitecture, Histological Image Processing, Contrastive learning, CLIP","An automated approach to identify brain regions by their cytoarchitecture using vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks. The model comprises two model variants trained on low-resolution whole-region images and high-resolution image tiles, respectively. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.",11.57,Qwen2.5-3B,Apple M1 (Metal)
2601.12286v1_Conversational Context Classification A Representa.pdf,Conversational Context Classification: A Representation Engineering Approach,Jonathan Pan,,,"Large Language Models, Representation Engineering, One-Class Support Vector Machine, In/Out-of-Context, Novelty Detection","This paper explores the use of Representation Engineering and One-Class Support Vector Machine to identify subspaces within the internal states of Large Language Models that represent specific contexts. By training One-Class Support Vector Machine on in-context examples, a robust boundary is established within the LLM's hidden state latent space, enabling the identification of specific context subspace. The approach is evaluated with two open-source LLMs in a specific domain, showing promising results in identifying the subspace for a specific context. The research contributes to better interpreting LLMs and detecting in or out of context conversation threads for AI safety.",11.95,Qwen2.5-3B,Apple M1 (Metal)
2601.12288v1_TimeGMM Single-Pass Probabilistic Forecasting via .pdf,TIMEGMM: SINGLE-PASS PROBABILISTIC FORECASTING VIA ADAPTIVE GAUSSIAN MIXTURE MODELS WITH REVERSIBLE NORMALIZATION,"Lei Liu, Tengyuan Liu, Hongwei Zhao, Jiahui Huang, Ruibo Guo, Bin Li",Not found,Not found,"Probabilistic time series forecasting, Gaussian mixture model, Reversible instance normalization","This paper presents TimeGMM, a novel probabilistic forecasting framework based on Gaussian Mixture Models (GMM) that captures complex future distributions in a single forward pass. Key component is GMM-adapted Reversible Instance Normalization (GRIN), a novel module designed to dynamically adapt to temporal-probabilistic distribution shifts. The framework integrates a dedicated Temporal Encoder (TE-Module) with a Conditional Temporal-Probabilistic Decoder (CTPD-Module) to jointly capture temporal dependencies and mixture distribution parameters. Extensive experiments demonstrate that TimeGMM consistently outperforms state-of-the-art methods, achieving maximum improvements of 22.48% in CRPS and 21.23% in NMAE.",12.32,Qwen2.5-3B,Apple M1 (Metal)
2601.12294v1_ToolPRMBench Evaluating and Advancing Process Rewa.pdf,ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents,"Dawei Li, Yuguang Yao, Zhen Tan, Huan Liu, Ruocheng Guo",Not found,Not found,"process reward models, tool-using agents, reward-guided search, multi-step failures, large language models","Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. This paper introduces ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. Offline and online sampling are used to isolate and capture failures, and a multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. Extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using.",11.82,Qwen2.5-3B,Apple M1 (Metal)
2601.12304v1_A Two-Stage Globally-Diverse Adversarial Attack fo.pdf,A TWO-STAGE GLOBALLY-DIVERSE ADVERSARIAL ATTACK FOR VISION-LANGUAGE PRE-TRAINING MODELS,"Wutao Chen, Huaqin Zou, Chen Wan, Lifeng Huang",,,"Adversarial Attack, Vision-Language Pre-Training, Multi-Modal Retrieval, Transferability","Vision-language pre-training models are vulnerable to adversarial examples, particularly in black-box scenarios. The proposed 2S-GDA framework addresses these challenges by introducing globally-diverse textual perturbations and multi-scale image perturbations. Extensive experiments demonstrate that 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains of up to 11.17% in black-box settings.",11.52,Qwen2.5-3B,Apple M1 (Metal)
2601.12310v1_Survival is the Only Reward Sustainable Self-Train.pdf,Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection,"Jennifer Dodgson, Alfath Daryl Alhajir, Michael Joedhitya, Akira Rafhael Janson Pattirane, Surender Suresh Kumar, Joseph Lim, C.H. Peh, Adith Ramdas, Steven Zhang Zhexu",Not provided,2601.12310v1,"Self-training, Sustainable learning, Environment-mediated selection, Negative-space learning, Robustness, Generalization, Self-improvement","This paper presents a proof-of-concept self-training system architecture that uses environmental viability to select behaviors, leading to stable learning under sparse feedback and bounded memory, and characterizes its learning dynamics and failure modes.",13.88,Qwen2.5-3B,Apple M1 (Metal)
2601.12316v1_GazeFormer-MoE Context-Aware Gaze Estimation via C.pdf,GAZEFORMER-MOE: CONTEXT-AWARE GAZE ESTIMATION VIA CLIP AND MOE,"Xinyuan Zhao, Xianrui Chen, Ahmad Chaddad",Not found,Not found,"Gaze estimation, multi scale fusion, MoE, Transformer","We present a semantics-modulated, multi-scale Transformer for 3D gaze estimation. Our model conditions CLIP global features with learnable prototype banks (illumination, head pose, background, direction), fuses these prototype-enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space, and replaces several FFN blocks with routed/shared Mixture of Experts to increase conditional capacity. Evaluated on MPIIFaceGaze, EYEDIAP, Gaze360 and ETH-XGaze, our model achieves new state-of-the-art angular errors of 2.49°, 3.22°, 10.16°, and 1.44°, demonstrating up to a 64% relative improvement over previously reported results.",12.23,Qwen2.5-3B,Apple M1 (Metal)
2601.12317v1_Explanova Automatically Discover Data Insights in .pdf,Explanova: Automatically Discover Data Insights in N×M Table via XAI Combined LLM Workflow,Yiming Huang,10.1145/nnnnnnn.nnnnnnn,,"Data Analysis, AutoML, LLM, XAI, Feature Analysis, Modeling","This paper proposes Explanova, an automatic LLM workflow designed to discover data insights through explainable AI (XAI) paradigm. It focuses on single-feature statistic analysis, feature-to-feature relation statistic analysis, and feature modeling by all other features in a single data table. The workflow is designed to explore all possible analytical items and is powered by a preset AutoML-like workflow.",10.82,Qwen2.5-3B,Apple M1 (Metal)
2601.12318v1_Beyond Human Annotation Recent Advances in Data Ge.pdf,Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence,"DEHAO YING, FENGCHANG YU, HAIHUA CHEN, CHANGJIANG JIANG, YURONG LI, WEI LU",XXXXXXX.XXXXXXX,,"Document Intelligence, Data Generation, Data Quality Evaluation","This paper surveys recent advances in data generation methods for Document Intelligence, addressing the bottleneck of manual annotation. It introduces a unified taxonomy and four resource-centric paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction, along with a multi-level evaluation framework. The paper aims to position data generation as the central engine for next-generation Document Intelligence.",11.35,Qwen2.5-3B,Apple M1 (Metal)
2601.12323v1_MARO Learning Stronger Reasoning from Social Inter.pdf,MARO: Learning Stronger Reasoning from Social Interaction,"Yin Cai, Zhouhong Gu, JunTao Zhang, Ping Chen*",Not found,Not found,"Large Language Models, Multi-Agent, Social Interaction, Reasoning, Reward Optimization","This paper proposes MARO, a method that enables large language models to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. It addresses the sparse learning signal problem, uneven role distribution problem, and environmental instability issues. Experimental results demonstrate significant improvements in social reasoning capabilities and effective transfer of acquired abilities to other tasks such as mathematical reasoning and instruction following.",11.08,Qwen2.5-3B,Apple M1 (Metal)
2601.12327v1_The Expert Validation Framework EVF Enabling Domai.pdf,The Expert Validation Framework (EVF): Enabling Domain Expert Control in AI Engineering,"Lucas Gren, Felix Dobslaw",10.1145/xxx.xxxx,,"GenAI, expert validation, quality assurance, AI engineering, domain expert control","Presenting an Expert Validation Framework (EVF) that places domain experts at the center of building software with GenAI components, enabling them to maintain authoritative control over system behavior through structured specification, testing, validation, and continuous monitoring processes.",10.53,Qwen2.5-3B,Apple M1 (Metal)
2601.12330v1_IceWatch Forecasting Glacial Lake Outburst Floods .pdf,IceWatch: Forecasting Glacial Lake Outburst Floods (GLOFs) using Multimodal Deep Learning,"Zuha Fatima, Muhammad Anser Sohaib, Muhammad Talha, Ayesha Kanwal, Sidra Sultana, Nazia Perwaiz",,,"CNN, deep learning, glacier monitoring, GLOF detection, LSTM, remote sensing, Sentinel-2, temperature forecasting, transformer, velocity prediction","Glacial Lake Outburst Floods (GLOFs) pose a serious threat in high mountain regions. IceWatch presents a novel deep learning framework for GLOF prediction that incorporates both spatial and temporal perspectives. It uses Sentinel-2 multispectral satellite imagery and NASA ITS_LIVE time series for glacier velocity prediction, and MODIS LST records for near-surface temperature forecasting. The system ensures strong predictive performance, rapid data processing, and robustness to noise and missing information.",11.8,Qwen2.5-3B,Apple M1 (Metal)
2601.12331v1_Efficient Privacy-Preserving Retrieval Augmented G.pdf,Efficient Privacy-Preserving Retrieval Augmented Generation with Distance-Preserving Encryption,"Huanyi Ye, Jiale Guo, Ziyao Liu, Kwok-Yan Lam",Not found,Not found,"Privacy-preserving, Retrieval-Augmented Generation, Distance-Preserving Encryption, Large Language Models, Conditional Approximate Distance-Comparison-Preserving Symmetric Encryption, Differential Privacy","Retrieval-Augmented Generation (RAG) has emerged as a key technique for enhancing response quality of large language models (LLMs) without incurring high computational cost. It works by retrieving knowledge and facts from external databases, augmenting the prompt with the retrieved data, and enabling the LLM to generate more accurate responses. This paper proposes an efficient privacy-preserving RAG framework (ppRAG) tailored for untrusted cloud environments that defends against vector-to-text attack, vector analysis, and query analysis. At its core, the paper introduces Conditional Approximate Distance-Comparison-Preserving Symmetric Encryption (CAPRISE) that encrypts embeddings while still allowing the cloud to compute similarity between an encrypted query embedding and the encrypted database embeddings. CAPRISE preserves only the relative distance ordering between the encrypted query and each encrypted database embedding, without exposing inter-database distances, thereby enhancing both privacy and efficiency. To further mitigate query analysis risks, differential privacy is introduced by perturbing the query embedding prior to encryption, preventing the cloud from inferring sensitive patterns from query frequency. Experimental results show that ppRAG achieves efficient processing throughput, high retrieval accuracy, strong privacy guarantees, making it a practical solution for resource-constrained users seeking secure, cloud-augmented LLMs.",13.27,Qwen2.5-3B,Apple M1 (Metal)
2601.12338v1_Actionable Advice from Reviews via Mixture of LoRA.pdf,Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM Pipeline for Issue Extraction and Business Recommendations,"Kartikey Singh Bhandari, Manav Ganesh, Yashwant Viswanathan, Archit Agrawal, Dhruv Kumar, Pratik Narang",,,"customer reviews, actionable advice, issue extraction, business recommendations, two-LLM pipeline, LoRA experts","This paper studies review-to-action generation, producing concrete, implementable recommendations grounded in review text. It proposes a modular two-LLM framework where an issue model extracts salient issues and assigns coarse themes, and an advice model generates targeted operational fixes conditioned on the extracted issue representation. The advice model is adapted using a mixture-of-LoRA experts strategy to enable specialization without expensive full fine-tuning. Synthetic review-issue-advice triples are constructed from Yelp reviews to supervise training, and recommendations are evaluated using an eight-dimension operational rubric. The approach consistently outperforms prompting-only and single-adapter baselines, yielding higher actionability and specificity while retaining favorable efficiency-quality trade-offs.",12.16,Qwen2.5-3B,Apple M1 (Metal)
2601.12341v1_Time-Continuous Modeling for Temporal Affective Pa.pdf,Time-Continuous Modeling for Temporal Affective Pattern Recognition in LLM’s,"Rezky M. Kam, Coddy N. Siswanto",,,"Affective computing, Language models, Temporal dynamics, Continuous modeling","This paper introduces a hybrid encoder-decoder architecture for text generation models, specifically designed to handle temporal affective dynamics in conversations. It addresses the limitations of current models, which rely on discrete token generation and forward attention weights, by incorporating time-continuous patterns and physics-informed neural networks. This approach aims to improve the understanding and interpretation of affective states in dialogues, making models more psychologically plausible and interpretable.",11.17,Qwen2.5-3B,Apple M1 (Metal)
2601.12343v1_How Well Do LLMs Predict Human Behavior A Measure .pdf,How Well Do LLMs Predict Human Behavior?,"Wayne Gao, Sukjin Han, Annie Liang",Not found,2601.12343,"large language models, human behavior prediction, predictive accuracy, task-specific data, economic variables","This paper proposes a measure to evaluate the knowledge a pretrained LLM brings to predicting human behavior, defined as the equivalent sample size, and applies this method to the Panel Study of Income Dynamics to find that LLMs encode considerable predictive information for some economic variables but much less for others.",13.66,Qwen2.5-3B,Apple M1 (Metal)
2601.12349v1_Zero-Permission Manipulation Can We Trust Large Mu.pdf,Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?,"Yi Qian, Kunwei Qian, Xingbang He, Ligeng Chen, Jikang Zhang, Tiantai Zhang, Haiyang Wei, Linzhang Wang, Hao Wu, Bing Mao",Not provided,Not provided,"Large Multimodal Models, GUI Agents, Android, Security, Manipulation Attacks","This paper presents Action Rebinding, a novel cross-application attack that allows a seemingly-benign application with zero dangerous permissions to rebind an agent's execution. The attack exploits the inherent observation-to-action gap in the agent's reasoning pipeline to trigger foreground transitions and rebind the agent's planned action toward the target application. The authors also introduce an Intent Alignment Strategy (IAS) to manipulate the agent's reasoning process and bypass verification gates. The paper evaluates Action Rebinding attacks on six widely-used Android GUI agents across 15 diverse tasks, demonstrating a 100% success rate for atomic action rebinding and the ability to reliably orchestrate multi-step attack chains. The attacker application requires no sensitive permissions and contains no privileged API calls, achieving a 0% detection rate across malware scanners.",12.32,Qwen2.5-3B,Apple M1 (Metal)
2601.12357v1_SimpleMatch A Simple and Strong Baseline for Seman.pdf,SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence,"Hailong Jin1, Huiying Li1*",Not found,Not found,"semantic correspondence, deep learning, keypoint matching, receptive field, upsampling, supervised learning","Presenting SimpleMatch, a simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions. It addresses the issue of irreversible fusion of adjacent keypoint features caused by deep downsampling operations and introduces a lightweight upsample decoder and a multi-scale supervised loss to ensure upsampled features retain discriminative features across different spatial scales.",11.52,Qwen2.5-3B,Apple M1 (Metal)
2601.12358v1_From Prompts to Pavement LMMs-based Agentic Behavi.pdf,From Prompts to Pavement:LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles,"Omar Y. Goba, Ahmed Y. Gado, Catherine M. Elias, Ahmed Hussein",,,"Large Language Models, Multi-modal Vision Models, Behavior-Tree, Autonomous Vehicles, Adaptive Behavior Planning, SAE Levels 4 and 5","This paper presents an agentic framework that leverages large language models and multi-modal vision models to generate and adapt behavior trees on the fly for autonomous vehicles. The system integrates a Descriptor agent, a Planner agent, and a Generator agent to trigger only upon baseline behavior tree failure, demonstrating successful navigation around unexpected obstacles with no human intervention.",11.3,Qwen2.5-3B,Apple M1 (Metal)
2601.12374v1_A Scalable Entity-Based Framework for Auditing Bia.pdf,A Scalable Entity-Based Framework for Auditing Bias in LLMs,"Akram Elbouanani1, Aboubacar Tuo1, Adrian Popescu1",,,"Large Language Models, Bias Evaluation, Entity Probes, Natural Language Processing, Political Bias","This paper introduces a scalable bias-auditing framework using named entities as probes to measure structural disparities in model behavior. It conducts the largest bias audit to date, comprising 1.9 billion data points across multiple entity types, tasks, languages, models, and prompting strategies. The results reveal systematic biases in models, including penalizing right-wing politicians, favoring left-wing politicians, and preferring Western and wealthy nations over the Global South. Instruction tuning reduces bias, but increasing model scale amplifies it, and prompting in Chinese or Russian does not attenuate Western-aligned preferences. These findings indicate the need for rigorous auditing of LLMs before deployment in high-stakes applications.",11.87,Qwen2.5-3B,Apple M1 (Metal)
2601.12389v1_NADIR Differential Attention Flow for Non-Autoregr.pdf,NADIR: Differential Attention Flow for Non-Autoregressive Transliteration in Indic Languages,"Lakshya Tomar, Vinayak Abrol, Puneet Agarwal",,,"transliteration, non-autoregressive, Indic languages, Differential Transformer, Mixture-of-Experts","This work introduces NADIR, a novel non-autoregressive architecture designed to balance speed and accuracy in multilingual transliteration tasks in Indic languages. NADIR integrates a Differential Transformer and a Mixture-of-Experts mechanism, achieving over a 13× speed-up compared to state-of-the-art autoregressive models while maintaining competitive performance in terms of character error rates.",11.02,Qwen2.5-3B,Apple M1 (Metal)
2601.12392v1_PsychēChat An Empathic Framework Focused on Emotio.pdf,Psych¯eChat: An Empathic Framework Focused on Emotion Shift Tracking and Safety Risk Analysis in Psychological Counseling,"Zhentao Xia, Yongqi Fan, Yuxiang Chu, Yichao Yin, Liangliang Chen, Tong Ruan, Weiyan Zhang",,,"psychological counseling, emotion shift, safety risk, empathy, large language models","Psych¯eChat is an empathic framework designed to track emotion shifts and analyze safety risks in psychological counseling. It employs interactive role-playing to synthesize counselor-seeker dialogues, incorporating modules for emotion management and risk control. Extensive experiments demonstrate its superior performance in emotional insight and safety control compared to existing methods.",11.27,Qwen2.5-3B,Apple M1 (Metal)
2601.12401v1_Beyond the Dirac Delta Mitigating Diversity Collap.pdf,Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement,"Jinmei Liu, Haoru Li, Zhenhong Sun, Chaofeng Chen, Yatao Bian, Bo Wang, Daoyi Dong, Chunlin Chen, Zhi Wang",,,"Reinforcement Learning, Fine-tuning, Versatile Image Generation, Diversity Collapse, Reward Concentration, Stochastic Variations, Potential-based Reward Shaping","Reinforcement learning is used to fine-tune large-scale generative models to align with complex human preferences and user-specified tasks. However, a fundamental limitation is the curse of diversity collapse, where the fine-tuned model achieves high rewards but loses output diversity. This paper proposes DRIFT, an innovative framework that systematically incentivizes output diversity throughout the on-policy fine-tuning process, reconciling strong task alignment with high generation diversity.",11.66,Qwen2.5-3B,Apple M1 (Metal)
2601.12402v1_Weaknesses of Facial Emotion Recognition Systems.pdf,Weaknesses of Facial Emotion Recognition Systems,"Aleksandra Jamróz, Patrycja Wysocka, Piotr Garbat",Not found,2601.12402,"Facial Emotion Recognition, Deep learning, Computer Vision","This study presents a comprehensive analysis of advanced facial emotion recognition solutions, evaluating network performance on different datasets and revealing weaknesses, including differences between datasets, unequal levels of difficulty in recognizing certain emotions, and challenges in differentiating between closely related emotions.",12.45,Qwen2.5-3B,Apple M1 (Metal)
2601.12405v1_Explainable Machine Learning for Pediatric Dental .pdf,Explainable Machine Learning for Pediatric Dental Risk Stratification Using Socio-Demographic Determinants,"Manasi Kanade, Abhi Thakkar, Gabriela Fernandes",Not provided,Not provided,"Machine Learning, Pediatric Dentistry, Risk Stratification, Socio-Demographic Factors, Artificial Intelligence","This study developed and evaluated an explainable artificial intelligence framework for pediatric dental risk stratification, prioritizing interpretability, calibration, and ethical deployment over maximal predictive accuracy.",12.32,Qwen2.5-3B,Apple M1 (Metal)
2601.12410v1_Are LLMs Smarter Than Chimpanzees An Evaluation on.pdf,Are LLMs Smarter Than Chimpanzees?,"Dingyi Yang, Junqi Zhao, Xue Li, Ce Li, Boyang Li*",,2601.12410,"Large Language Models, Chimpanzees, Perspective Taking, Knowledge State Estimation",This paper evaluates the performance of Large Language Models (LLMs) in understanding and predicting the knowledge states and intentions of story characters. The authors design two tasks to test if LLMs can detect when characters demonstrate knowledge they should not possess and if they can predict their next actions based on their own knowledge versus objective truths they do not know. The results show that current state-of-the-art LLMs perform near-randomly on both tasks and are substantially inferior to humans.,11.1,Qwen2.5-3B,Apple M1 (Metal)
2601.12415v1_Orthogonalized Policy OptimizationDecoupling Sampl.pdf,Orthogonalized Policy Optimization,Wang Zixian,Not found,2601.12415,"Large Language Model, Policy Optimization, Alignment Methods, RLHF","This work shows that recent alignment methods for large language models, including PPO, DPO, and IPO, implicitly conflate two fundamental design choices: sampling geometry and optimization geometry. It proposes Orthogonalized Policy Optimization (OPO) to explicitly decouple these choices, leading to stable optimization and avoiding gradient saturation even when model confidence is high.",11.78,Qwen2.5-3B,Apple M1 (Metal)
2601.12436v1_Purification Before Fusion Toward Mask-Free Speech.pdf,PURIFICA TION BEFORE FUSION: TOW ARD MASK-FREE SPEECH ENHANCEMENT FOR ROBUST AUDIO-VISUAL SPEECH RECOGNITION,"Linzhi Wu, Xingyu Zhang∗, Hao Yuan, Yakun Zhang, Changyan Zheng, Liang Xie, Tiejun Liu, Erwei Yin",Not found,Not found,"audio-visual speech recognition, speech feature enhancement, noise-robust, multimodal bottleneck Conformer","This work proposes an end-to-end noise-robust audio-visual speech recognition (AVSR) framework coupled with speech enhancement, eliminating the need for explicit noise mask generation. Leveraging a Conformer-based bottleneck fusion module, it implicitly refines noisy audio features with video assistance, reducing modality redundancy and enhancing inter-modal interactions to preserve speech semantic integrity and achieve robust recognition performance. Experimental evaluations on the public LRS3 benchmark suggest that the method outperforms prior advanced mask-based baselines under noisy conditions.",12.13,Qwen2.5-3B,Apple M1 (Metal)
2601.12442v1_Constraint-Aware Neurosymbolic Uncertainty Quantif.pdf,Constraint-Aware Neurosymbolic Uncertainty Quantification with Bayesian Deep Learning for Scientific Discovery,"Shahnawaz Alam1, Mohammed Mudassir Uddin 1, Mohammed Kaif Pasha 1",,,"Neurosymbolic AI, Uncertainty Quantification, Bayesian Deep Learning, Scientific Constraints, Calibration, Physics-Informed Machine Learning","Scientific Artificial Intelligence applications require models that deliver trustworthy uncertainty estimates while respecting domain constraints. Existing uncertainty quantification methods lack mechanisms to incorporate symbolic scientific knowledge, while neurosymbolic approaches operate deterministically without principled uncertainty modeling. The paper introduces the Constraint-Aware Neurosymbolic Uncertainty Framework (CANUF), unifying Bayesian deep learning with differentiable symbolic reasoning. Experiments on Materials Project, QM9 molecular properties, and climate benchmarks show CANUF reduces Expected Calibration Error by 34.7% versus Bayesian neural networks while maintaining 99.2% constraint satisfaction. Ablations reveal constraint-guided recalibration contributes 18.3% performance gain, with constraint extraction achieving 91.4% precision. CANUF provides the first end-to-end differentiable pipeline addressing uncertainty quantification, constraint satisfaction, and interpretable explanations for scientific predictions.",12.91,Qwen2.5-3B,Apple M1 (Metal)
2601.12443v1_Adversarial Defense in Vision-Language Models An O.pdf,Adversarial Defense in Vision-Language Models: An Overview,"Xiaowei Fu, Lei Zhang*",Not found,Not found,"Vision-Language Models, Adversarial Attacks, Robustness, Training-Time Defense, Test-Time Adaptation Defense, Training-Free Defense","The widespread use of Vision Language Models (VLMs) has raised concerns about their vulnerability to adversarial attacks. Three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting strengths and limitations and discussing ongoing challenges.",11.46,Qwen2.5-3B,Apple M1 (Metal)
2601.12444v1_Large Language Model for OWL Proofs.pdf,Large Language Model for OWL Proofs,"Hui Yang, Jiaoyan Chen, Uli Sattler",10.1145/XXXXXX.XXXXXX,,"Large Language Models, OWL ontologies, proof generation, reasoning, natural language generation, description logics","This work studies proof generation in the context of OWL ontologies, developing an automated dataset construction and evaluation framework for three sequential tasks: Extraction, Simplification, and Explanation, as well as an additional task of assessing Logic Completeness of the premise. Experiments on widely used reasoning LLMs show important findings, including the performance limitations of some models on complex cases, the dominant role of logical complexity in shaping LLM performance, and the impact of noise and incompleteness in input data on LLMs' performance.",11.39,Qwen2.5-3B,Apple M1 (Metal)
2601.12449v1_AgenTRIM Tool Risk Mitigation for Agentic AI.pdf,AgenTRIM: Tool Risk Mitigation for Agentic AI,"Roy Betser1*†, Shamik Bose1†, Amit Giloni1, Chiara Picardi1, Sindhu Padakandla 2, Roman Vainshtein 1",Not found,Not found,"AI agents, tool risk mitigation, agentic agency, least-privilege access, indirect prompt injection, tool misuse, LLM-based agents","AI agents, which combine large language models with external tools, can introduce security risks such as indirect prompt injection and tool misuse. AGENTRIM is a framework designed to detect and mitigate these risks without altering the agent's internal reasoning. It addresses these risks through complementary offline and online phases, reconstructing and verifying the agent's tool interface from code and execution traces, and enforcing least-privilege tool access at runtime through adaptive filtering and status-aware validation of tool calls. Evaluations on the AgentDojo benchmark show that AGENTRIM substantially reduces attack success while maintaining high task performance.",12.07,Qwen2.5-3B,Apple M1 (Metal)
2601.12465v1_Incentivizing In-depth Reasoning over Long Context.pdf,INCENTIVIZINGIN-DEPTHREASONING OVERLONG CONTEXTS WITHPROCESSADVANTAGESHAPING,"Miao Peng, Weizhou Shen, Nuo Chen, Chenliang Li, Ming Yan, Jia Li",Not found,Not found,"Reinforcement Learning, Long Context Reasoning, LLM, Process Advantages Shaping","This paper proposes DEEPREASONQA, a KG-driven synthesis framework that controllably constructs high-difficulty, multi-hop long-context QA pairs with inherent reasoning chains. It introduces Long-context Process Advantage Shaping (LONGPAS), a method that performs fine-grained credit assignment by evaluating reasoning steps along Validity and Relevance dimensions, capturing critical learning signals from 'almost-there' trajectories. Experiments on three long-context reasoning benchmarks show that our approach substantially outperforms RLVR baselines and matches frontier LLMs while using far fewer parameters.",12.04,Qwen2.5-3B,Apple M1 (Metal)
2601.12467v1_Patch-Level Tokenization with CNN Encoders and Att.pdf,Patch-Level Tokenization with CNN Encoders and Attention for Improved Transformer Time-Series Forecasting,"Saurish Nagrath, VIT-AP",,,"Transformer models, multivariate time-series forecasting, financial time-series forecasting, convolutional neural networks, attention mechanisms, representation learning, deep learning for finance","This work proposes a two-stage forecasting framework that explicitly separates local temporal representation learning from global dependency modelling. In the first stage, a convolutional neural network (CNN) operates on fixed-length temporal patches to extract short-range temporal dynamics and non-linear feature interactions, producing compact patch-level token embeddings. Token-level self-attention is subsequently applied during representation learning to refine these embeddings by enabling interactions across temporal patches. In the second stage, a Transformer encoder processes the resulting token sequence to model inter-patch temporal dependencies and generate per-patch forecasts. Experiments conducted on synthetic multivariate time-series data with controlled static and dynamic factors demonstrate that the proposed patch-based tokenization strategy achieves competitive forecasting performance compared to convolutional and patch-based Transformer baselines.",12.91,Qwen2.5-3B,Apple M1 (Metal)
2601.12471v1_Knowing When to Abstain Medical LLMs Under Clinica.pdf,Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty,"Sravanthi Machcha*, Sushrita Yerra*, Sahil Gupta, Aishwarya Sahoo, Sharmin Sultana, Hong Yu, Zonghai Yao",Not found,Not found,"large language models, medical multiple-choice question answering, abstention, conformal prediction, adversarial perturbations, uncertainty","Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. This paper introduces MedAbstain, a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA), integrating conformal prediction, adversarial question perturbations, and explicit abstention options. The systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertainty. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.",11.95,Qwen2.5-3B,Apple M1 (Metal)
2601.12494v1_Harmonizing the Arabic Audio Space with Data Sched.pdf,Harmonizing the Arabic Audio Space with Data Scheduling,"Hunzalah Hassan Bhatti, Firoj Alam, Shammur Absar Chowdhury",,,"Audio Large Language Models, Instruction Tuning, Arabic Speech Summarization, Dialect Identification, Emotion Recognition, Data Scheduling, Parameter-Efficient Fine-Tuning","This paper presents a systematic study of multi-task instruction tuning for an Arabic-centric audio Large Language Model (LLM), covering generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). It introduces a novel dataset for Arabic speech summarization and proposes a strategy to construct information-dense batches by selecting task-and label-balanced examples. The findings reveal a critical efficiency-robustness trade-off and offer practical guidance for the efficient adaptation of Omni models in complex, low-resource multimodal environments.",11.65,Qwen2.5-3B,Apple M1 (Metal)
2601.12499v1_Failure Modes in Multi-Hop QA The Weakest Link Law.pdf,Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck,"Meiru Zhang, Zaiqiao Meng, Nigel Collier",Not found,Not found,"Large Language Models, Multi-Hop Question Answering, Position Bias, Attention Mechanisms, Recognition Failure, Synthesis Failure, Multi-Focus Attention Instruction, Weakest Link Law","Despite scaling to massive context windows, Large Language Models struggle with multi-hop reasoning due to inherent position bias, which causes them to overlook information at certain positions. This paper introduces Multi-Focus Attention Instruction (MFAI) to disentangle the mechanisms of recognition and synthesis failures, and establishes the 'Weakest Link Law' across 5 LLMs on two multi-hop QA tasks. The failure is governed by absolute position rather than the linear distance between facts, and the performance variance is <3%. The paper also identifies a duality in attention steering: while matched MFAI resolves recognition bottlenecks, misleading MFAI triggers confusion in real-world tasks but is successfully filtered in synthetic tasks. Finally, it demonstrates that thinking models that utilize System-2 reasoning effectively locate and integrate the required information, matching gold-only baselines even in noisy, long-context settings.",12.38,Qwen2.5-3B,Apple M1 (Metal)
2601.12518v1_Cooperative Multi-agent RL with Communication Cons.pdf,Cooperative Multi-agent RL with Communication constraints,"Nuoya Xiong ∗, Aarti Singh †",Not found,Not found,"Cooperative Multi-agent Reinforcement Learning, Communication Constraints, Importance Sampling, Base Policy Prediction, ε-Nash Equilibrium","This paper addresses the challenges of cooperative multi-agent reinforcement learning (MARL) in decentralized systems with limited communication. It proposes a technique called base policy prediction to reduce the communication costs by predicting the policy update using old gradients and collecting samples for a sequence of base policies. The approach enables effective learning with significantly fewer communication rounds and improves convergence to an ε-Nash equilibrium in potential games with only O(ε^(-3/4)) communication rounds and O(poly(max i |Ai|)·ε^(-11/4)) samples, compared to existing state-of-the-art results.",12.4,Qwen2.5-3B,Apple M1 (Metal)
2601.12522v1_Improved Bug Localization with AI Agents Leveragin.pdf,Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition,"Asif Mohammed Samir, Mohammad Masudur Rahman",XXXXXXX.XXXXXXX,XXXXXXX,"Bug Localization, Large Language Models, Agentic AI, Cognition, Debugging, Software Engineering, Information Retrieval","This paper presents a novel agentic technique, CogniGent, for bug localization that overcomes limitations of traditional methods and LLMs by leveraging multiple AI agents capable of causal reasoning, call-graph-based root cause analysis, and context engineering. It emulates dynamic cognitive debugging practices and conducts hypothesis testing to support bug localization. Experimental results show significant improvements in performance metrics compared to existing techniques.",11.41,Qwen2.5-3B,Apple M1 (Metal)
2601.12534v1_Encoding Emotion Through Self-Supervised Eye Movem.pdf,ENCODING EMOTION THROUGH SELF-SUPERVISED EYE MOVEMENT RECONSTRUCTION,"Marcus Ma, Jordan Prescott, Emily Zhou, Tiantian Feng, Kleanthis Avramidis, Gabor Mihaly Toth, Shrikanth Narayanan",Not found,Not found,"eye movement, self-supervised learning, emotion prediction, deep learning","The paper investigates how eye movement can predict multimodal markers of emotional expression from naturalistic, low-resolution videos, using a novel gaze detection model developed from a large-scale video dataset of Holocaust survivor interviews.",11.42,Qwen2.5-3B,Apple M1 (Metal)
2601.12535v1_Improving Low-Resource Machine Translation via Rou.pdf,Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning,"Ahmed Attia, Alham Fikri",,2309.14447,"machine translation, low-resource, reinforcement learning, round-trip, self-supervised","This paper investigates a self-supervised reinforcement-learning-based fine-tuning approach for translation in low-resource settings using round-trip bootstrapping with the No Language Left Behind (NLLB) family of models. The authors evaluate both the 600M and 1.3B parameter NLLB models and observe consistent improvements for several target languages, indicating increased fluency and semantic fidelity in translation outputs. The method leverages the NLLB-MD dataset and uses a combination of chrF++ and BLEU as the reward function on reconstructed English sentences. The authors argue that their approach can benefit from scale, enabling models to leverage their pretrained knowledge and continue self-improving.",12.06,Qwen2.5-3B,Apple M1 (Metal)
2601.12538v1_Agentic Reasoning for Large Language Models.pdf,Agentic Reasoning for Large Language Models,"Tianxin Wei1†, Ting-Wei Li1†, Zhining Liu1†, Xuying Ning1, Ze Yang2, Jiaru Zou1, Zhichen Zeng1, Ruizhong Qiu1, Xiao Lin1, Dongqi Fu2, Zihao Li1, Mengting Ai1, Duo Zhou1, Wenxuan Bao1, Yunzhe Li1, Gaotang Li1, Cheng Qian1, Yu Wang5, Xiangru Tang6, Yin Xiao1, Liri Fang1, Hui Liu3, Xianfeng Tang3, Yuji Zhang1, Chi Wang4, Jiaxuan You1, Heng Ji1, Hanghang Tong1, Jingrui He1",Not provided,2601.12538,"Agentic AI, LLM Agent, Agentic Reasoning, Self-evolving","This survey provides a systematic roadmap for agentic reasoning, a paradigm shift that bridges thought and action by reframing large language models as autonomous agents capable of planning, acting, and learning through continual interaction. It covers three complementary dimensions: foundational agentic reasoning, self-evolving agentic reasoning, and collective multi-agent reasoning, analyzing system constraints and optimization settings. The survey also reviews real-world applications and benchmarks in science, robotics, healthcare, autonomous research, and math, illustrating how different reasoning mechanisms are instantiated and evaluated across domains. It offers actionable guidance for agentic systems across environmental dynamics, optimization settings, and agent interaction settings, and outlines open challenges and future directions.",13.6,Qwen2.5-3B,Apple M1 (Metal)
2601.12539v1_MemeLens Multilingual Multitask VLMs for Memes.pdf,MemeLens: Multilingual Multitask VLMs for Memes,"Ali Ezzat Shahroor1*, Mohamed Bayan Kmainasi2∗† , Abul Hasnat3,4 , Dimitar Dimitrov, Giovanni Da San Martino, Preslav Nakov, Firoj Alam1",Not found,Not found,"memes, multimodal, vision language model, multitask, harm, misogyny, sentiment, humor, figurative language, persuasion","Memes are a dominant medium for online communication and manipulation. Existing meme research is distributed across tasks and languages, limiting cross-domain generalization. MEMELENS proposes a unified multilingual and multitask vision language model for meme understanding, consolidating 38 public datasets and presenting a comprehensive empirical analysis across modeling paradigms, task categories, and datasets. The findings suggest that robust meme understanding requires multimodal training, exhibits substantial variation across semantic categories, and remains sensitive to over-specialization when models are fine-tuned on individual datasets.",12.08,Qwen2.5-3B,Apple M1 (Metal)
2601.12542v1_Rethinking the AI Scientist Interactive Multi-Agen.pdf,Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery,"Lukas Weidener*, Marko Brkić*, Mihailo Jovanović*, Ritvik Singh, Chiara Baccin, Emre Ulgac, Alex Dobrin, Aakaash Meduri",Not found,Not found,"Artificial Intelligence, Scientific Discovery, Multi-Agent Systems, Interactive Workflow, Research Automation","This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. The system comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Evaluation on the BixBench benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points.",10.97,Qwen2.5-3B,Apple M1 (Metal)
2601.12547v1_How Clinicians Think and What AI Can Learn From It.pdf,How Clinicians Think—and What AI Can Learn From,"Dr. Dipayan Sengupta, MD (Dermatology), Dr. Saumya Panda, MD (Dermatology)",,2601.12547,"clinical AI, decision-making, ordinal decision-making, non-compensatory, fast-and-frugal heuristics","Most clinical AI systems are built as prediction engines, but real clinical reasoning is a time-bounded, sequential control problem under uncertainty. Clinicians often rely on fast-and-frugal lexicographic heuristics that evaluate a small number of cues in a fixed order and stop early. This paper argues for a shift in the algorithmic center of gravity of clinical AI from more accurate numbers to more robust action selection.",11.77,Qwen2.5-3B,Apple M1 (Metal)
2601.12549v1_Benchmarking Concept-Spilling Across Languages in .pdf,Benchmarking Concept-Spilling Across Languages in LLMs,"Ilia Badanin, Daniil Dzenhaliou, Imanol Schlag",Not found,2601.12549,"Large Language Models, Cross-lingual, Semantic Robustness, Polysemy, Language Spilling","This paper presents a novel comparative framework for evaluating multilingual semantic robustness by systematically measuring how models handle polysemous words across languages. It evaluates a diverse set of open and closed multilingual LLMs using a structured meaning generation task across nine languages, employing a carefully curated benchmark of 100 high-polysemy English words. The findings reveal significant variation in semantic robustness across models and languages, providing a principled ranking system for model comparison without requiring definitive causal attribution of error sources.",12.42,Qwen2.5-3B,Apple M1 (Metal)
2601.12554v1_Artificial Intelligence in Materials Science and E.pdf,"Artificial Intelligence in Materials Science and Engineering: Current Landscape, Key Challenges, and Future Trajectories","Iman Peivaste1,2, Salim Belouettar ∗1, Francesco Mercuri 3, Nicholas Fantuzzi 4, Hamidreza Dehghani 1, Razieh Izadi 1, Halliru Ibrahim 1, Jakub Lengiewicz 1, Maël Belouettar-Mathis5, Kouider Bendine 1, Ahmed Makradi 1, Martin Hörsch 6, Peter Klein7, Mohamed El Hachemi 1, Heinz A. Preisig 8, Yacine Rezgui 9, Natalia Konchakova10, Ali Daouadji 11",Not found,2601.12554,"Artificial Intelligence, Materials Science, Engineering, Machine Learning, Deep Learning, Data-driven, Materials Design, Discovery Acceleration, Optimization","This review provides a comprehensive and structured overview of the current landscape in artificial intelligence applications within materials science and engineering, synthesizing recent advancements and methodologies. It covers a wide range of machine learning approaches, from traditional algorithms to advanced deep learning architectures, including CNNs, GNNs, and Transformers, alongside emerging generative AI and probabilistic models. The review also emphasizes the pivotal role of data in this field and the importance of effective representation and featurization strategies.",13.2,Qwen2.5-3B,Apple M1 (Metal)
2601.12557v1_Life Machine Learning and the Search for Habitabil.pdf,"Life, Machine Learning, and the Search for Habitability: Predicting Biosignature Fluxes for the Habitable Worlds Observatory","Mark Moussa1, Amber V. Young1, Brianna Isola1, Vasuda Trehan1, Michael D. Himes1, Nicholas Wogan2, Giada Arney1",,,"Machine learning, Exoplanets, Biosignatures, Habitable worlds, Direct imaging, Bayesian Convolutional Neural Network, Spectral Query Adaptive Transformer","Future direct-imaging flagship missions, such as NASA’s Habitable Worlds Observatory (HWO), face critical decisions in prioritizing observations due to extremely stringent time and resource constraints. This paper introduces two advanced machine-learning architectures, a Bayesian Convolutional Neural Network (BCNN) and the Spectral Query Adaptive Transformer (SQuAT), for predicting biosignature species fluxes from exoplanetary reflected-light spectra. Both models achieve comparably high predictive accuracy on an augmented dataset spanning a wide range of exoplanetary conditions, highlighting their distinct advantages in uncertainty quantification and spectral interpretability.",12.24,Qwen2.5-3B,Apple M1 (Metal)
2601.12560v1_Agentic Artificial Intelligence AI Architectures T.pdf,"Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents","Arunkumar V, Gangadharan G.R., Rajkumar Buyya",Not found,2601.12560,"Agentic AI, Large Language Models, Autonomous Agents, Multi-Agent Systems, Cognitive Architectures, Tool Use, Planning","This paper investigates the architectures and proposes a unified taxonomy for large language model agents, breaking them into Perception, Brain, Planning, Action, Tool Use, and Collaboration. It explores the transition from linear reasoning procedures to native inference time reasoning models and the shift from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. The paper also reviews current evaluation practices and highlights open challenges such as hallucination in action, infinite loops, and prompt injection.",12.65,Qwen2.5-3B,Apple M1 (Metal)
2601.12577v1_Primate-like perceptual decision making emerges th.pdf,Primate-like perceptual decision making emerges through deep recurrent reinforcement learning,"Nathan J. Wispinski, Scott A. Stone, Anthony Singhal, Patrick M. Pilarski, Craig S. Chapman",Not provided,2601.12577,"primate decision making, reinforcement learning, neural mechanisms, perceptual discrimination, flexible decision making","This study trained an end-to-end deep recurrent neural network using reinforcement learning on a noisy perceptual discrimination task, and found that networks learned key abilities of primate-like decision making, including trading off speed for accuracy and flexibly changing their mind in the face of new information. These results provide experimental support for key pressures that gave rise to the primate ability to make flexible decisions.",12.19,Qwen2.5-3B,Apple M1 (Metal)
2601.12582v1_Ontology-aligned structuring and reuse of multimod.pdf,Ontology-aligned structuring and reuse of multimodal materials data and workflows towards automatic reproduction,"Sepideh Baghaee Ravari, Abril Azocar, Guzman Sarath, Menon Stefan, Sandfeld Tilmann, Hickel Markus, Stricker Markus",Not found,Not found,"text mining, workflow, large language models, stacking fault energy","Reproducibility of computational results in materials science is a challenge due to unstructured reporting of workflows and parameters. An ontology-driven, LLM-assisted framework is introduced for automated extraction and structuring of workflows from literature, focusing on hexagonal close-packed magnesium and its alloys. This framework enables systematic comparison of reported values and supports structured reuse of computational protocols, even though full reproducibility is still constrained by missing metadata.",12.05,Qwen2.5-3B,Apple M1 (Metal)
2601.12585v1_Do MLLMs See What We See Analyzing Visualization L.pdf,Do MLLMs See What We See?,"Mengli (Dawn) Duan*, Yuhe (Sissi) Jiang*, Matthew Varona, Carolina Nobre",Not provided,Not provided,"Visualization Literacy, Multimodal Large Language Model, Evaluation Study","This paper presents the first systematic analysis of barriers to visualization literacy in Multimodal Large Language Models (MLLMs). Using a benchmark with synthetic data, the authors open-coded 309 erroneous responses from four state-of-the-art models and identified two machine-specific barriers to visualization literacy. The findings inform future evaluation and design of reliable AI-driven visualization assistants.",11.07,Qwen2.5-3B,Apple M1 (Metal)
2601.12594v1_SLAP Scalable Language-Audio Pretraining with Vari.pdf,SLAP: SCALABLE LANGUAGE-AUDIO PRETRAINING WITH V ARIABLE-DURATION AUDIO AND MULTI-OBJECTIVE TRAINING,"Xinhao Mei, Gael Le Lan, Haohe Liu, Zhaoheng Ni, V arun Nagaraja, Yang Liu, Yangyang Shi, Vikas Chandra",Not found,Not found,"Multimodal learning, CLAP, self-supervised learning, contrastive learning, multi-objective learning","Contrastive language-audio pretraining (CLAP) has achieved notable success in learning semantically rich audio representations. However, current CLAP models face several key limitations, including training on small datasets, short fixed-duration audio clips, and a single global contrastive training objective. To address these challenges, we introduce Scalable Language-Audio Pretraining (SLAP), which scales CLAP to 109 million audio-text pairs with variable audio durations and incorporates multiple training objectives. SLAP unifies contrastive loss with additional self-supervised and captioning losses, achieving new state-of-the-art performance on audio-text retrieval and zero-shot audio classification tasks.",12.72,Qwen2.5-3B,Apple M1 (Metal)
2601.12607v1_A Cloud-based Multi-Agentic Workflow for Science.pdf,A Cloud-based Multi-Agentic Workflow for Science,"Anurag Acharya, Timothy Vega, Rizwan A. Ashraf, Anshu Sharma, Derek Parker, Robert Rallo",10.1145/nnnnnnn.nnnnnnn,,"Large Language Models, LLMs for Science, LLM Agents, Multi-agent Framework, Catalysis, Chemistry, Material Science, Cloud Computing","This work presents a domain-agnostic, model-independent workflow for an agentic framework that can act as a scientific assistant while being run entirely on cloud. The framework balances models, cloud providers, and external resources, and is validated on synthetic and real-world tasks. It shows high success rates in routing tasks and completing them, while maintaining accuracy comparable to frontier models.",11.51,Qwen2.5-3B,Apple M1 (Metal)
2601.12617v1_Creating Disability Story Videos with Generative A.pdf,"Creating Disability Story Videos with Generative AI: Motivation, Expression, and Sharing","Shuo Niu, Dylan Clements, Hyungsin Kim",10.1145/3772318.3791495,,"Disability, Storytelling, Video, Generative AI, LLM","This research examines how nine people with disabilities used generative AI to create videos sharing their disability experiences. It explores motivations, expression, and sharing of PwD-created GenAI story videos, concluding with a framework of momentous depiction highlighting four core affordances of GenAI. Design implications for GenAI in story completion, media formats, and corrective mechanisms are discussed.",10.97,Qwen2.5-3B,Apple M1 (Metal)
2601.12637v1_Topology-Aware Multiscale Mixture of Experts for E.pdf,TOPOLOGY-AWAREMULTISCALEMIXTURE OFEXPERTS FOR EFFICIENTMOLECULARPROPERTYPREDICTION,"Long D. Nguyen, Kelin Xia, Binh P. Nguyen",Not found,2601.12637,"Graph Neural Networks, Topological Deep Learning, Mixture of Experts, Molecular Representation","Predicting molecular properties is crucial in drug discovery and materials science. This paper proposes Multiscale Interaction Mixture of Experts (MI-MoE) to adapt interaction modeling across geometric regimes, improving multiple strong 3D molecular backbones across diverse benchmark datasets.",11.68,Qwen2.5-3B,Apple M1 (Metal)
2601.12638v1_Mixed Precision PointPillars for Efficient 3D Obje.pdf,Mixed Precision PointPillars for Efficient 3D Object Detection with TensorRT,"1st Ninnart Fuengfusin, 2nd Keisuke Yoneda, 3rd Naoki Suganuma",Not provided,Not provided,"neural networks, quantization, 3D object detection","This paper proposes a mixed precision framework for PointPillars to accelerate 3D LIDAR object detection models. The framework uses post-training quantization to identify sensitive layers and combines them to produce candidate mixed precision models. The methods handle wide numerical distributions and extreme outliers by using a small number of calibration data. With TensorRT deployment, the models offer reduced latency and sizes by up to 2.35 and 2.26 times, respectively.",11.53,Qwen2.5-3B,Apple M1 (Metal)
2601.12641v1_STEP-LLM Generating CAD STEP Models from Natural L.pdf,STEP-LLM: Generating CAD STEP Models from Natural Language with Large Language Models,"Xiangyu Shi, Junyang Ding, Xu Zhao, Sinong Zhan, Payal Mohapatra, Daniel Quispe, Kojo Welbeck, Jian Cao, Wei Chen, Ping Guo, Qi Zhu",Not found,2601.12641,"Computer-aided design, STEP file, large language models, design automation","Computer-aided design (CAD) is vital to modern manufacturing, yet model creation remains labor-intensive and expertise-heavy. This paper presents a novel approach to generate CAD models from natural language using large language models (LLMs), addressing the challenges posed by the graph-structured, cross-referenced nature of the Standard for the Exchange of Product Data (STEP) file. The authors curate a dataset of ∼40K STEP-caption pairs and introduce novel preprocessing techniques tailored for the graph-structured format of STEP, including a depth-first search (DFS)-based reserialization that linearizes cross-references while preserving locality and chain-of-thought (CoT)-style structural annotations. The paper also integrates retrieval-augmented generation (RAG) for supervised fine-tuning (SFT) and uses reinforcement learning (RL) with a specific Chamfer Distance-based geometric reward to refine generation quality. Experiments demonstrate consistent gains in geometric fidelity over the Text2CAD baseline, with improvements arising from multiple stages of the framework.",13.14,Qwen2.5-3B,Apple M1 (Metal)
2601.12646v1_Unbounded Harms Bounded Law Liability in the Age o.pdf,"Unbounded Harms, Bounded Law: Liability in the Age of Borderless AI",Ha-Chi Tran,Not found,2601.12646v1,"artificial intelligence, risk governance, liability, borderless technology, global AI governance","The rapid advancement of artificial intelligence (AI) has exposed fundamental deficiencies in risk governance, particularly in ex post risk management. This paper argues that such harms are structurally inherent to AI supply chains and are likely to increase in frequency and severity due to globalized deployment and asymmetric national capacities. It examines compensation and liability mechanisms from adjacent high-risk and transnational domains to identify transferable legal design principles for AI-related harms.",12.2,Qwen2.5-3B,Apple M1 (Metal)
2601.12648v1_Intelligent Documentation in Medical Education Can.pdf,Intelligent Documentation in Medical Education: Can AI Replace Manual Case Logging?,"Nafiz Imtiaz Khan, MSc∗, Kylie Cleland, BSc, Vladimir Filkov, PhD, Roger Eric Goldman, PhD",,,"artificial intelligence, large language models, radiology, case logs, medical education","This study investigates the feasibility of using large language models (LLMs) to automate procedural case log documentation in radiology training. It evaluates whether AI can replace manual logging, identifies which procedure types are most challenging for extraction, and assesses integration into clinical workflows. Both local and commercial LLMs outperformed the standard benchmark, with Qwen-2.5 achieving F1-scores of 86.66 and Claude-3.5-Haiku reaching an F1-score of 86.89%. Commercial inference delivered sub-2s latency and concise outputs, while local deployment traded speed for lower recurring cost. Automation could save over 35 hours of manual annotation per resident annually. Future work should validate across larger, multi-institution datasets and explore additional prompting strategies.",12.46,Qwen2.5-3B,Apple M1 (Metal)
2601.12654v1_Explanation Multiplicity in SHAP Characterization .pdf,Explanation Multiplicity in SHAP: Characterization and Assessment,"HYUNSEUNG HWANG, KAIST, Republic of Korea, SEUNGEUN LEE, New York University, USA, LUCAS ROSENBLATT, New York University, USA, JULIA STOYANOVICH, New York University, USA, STEVEN EUIJONG WHANG, KAIST, Republic of Korea",Not provided,Not provided,"Explainable AI, SHAP, Post-hoc feature attribution, Explanation multiplicity, Stochasticity, Feature ranking, Model training, Model selection, Explainability","Post-hoc explanations are widely used in high-stakes domains such as lending, employment, and healthcare. SHAP is often used to justify automated decisions, but it can produce different explanations for the same decision, a phenomenon known as explanation multiplicity. This paper presents a methodology to characterize and assess explanation multiplicity in SHAP, and shows that it is widespread and persists even under controlled conditions. This instability poses a normative challenge for responsible AI deployment.",12.39,Qwen2.5-3B,Apple M1 (Metal)
2601.12658v1_Augmenting Question Answering with A Hybrid RAG Ap.pdf,Augmenting Question Answering with A Hybrid RAG Approach,"Tianyi Yang∗, Nashrah Haque ∗, Vaishnave Jonnalagadda∗, Yuya Jeremy Ong †, Zhehui Chen ‡, Yanzhao Wu§, Lei Yu ¶, Divyesh Jadav ∥, Wenqi Wei∗",Not provided,Not provided,"Question-answering, RAG, query processing, LLMs, Factual consistency, Logical reasoning, Self-refinement","This paper introduces Structured-Semantic RAG (SSRAG), a hybrid architecture that enhances QA quality by integrating query augmentation, agentic routing, and a structured retrieval mechanism combining vector and graph based techniques with context unification. The approach improves both answer accuracy and informative-ness over standard RAG implementations, as demonstrated through extensive evaluations on three popular QA datasets.",11.7,Qwen2.5-3B,Apple M1 (Metal)
2601.12661v1_MedConsultBench A Full-Cycle Fine-Grained Process-.pdf,"MedConsultBench: A Full-Cycle, Fine-Grained, Process-Aware Benchmark for Medical Consultation Agents","Chuhan Qiao*, Jianghua Huang*, Daxing Zhao*, Ziding Liu, Yanjun Shen†, Bing Cheng, Wei Lin, Kai Wu",Not provided,2601.12661,"Medical consultation, AI benchmark, Clinical workflow, Information acquisition, Diagnostic accuracy, Medication safety","Current evaluations of medical consultation agents often prioritize outcome-oriented tasks, overlooking the end-to-end process integrity and clinical safety essential for real-world practice. MedConsultBench addresses this gap by introducing Atomic Information Units (AIUs) to track clinical information acquisition at a sub-turn level, enabling precise monitoring of how key facts are elicited through 22 fine-grained metrics. The benchmark evaluates uncertainty-aware yet concise inquiry while emphasizing medication regimen compatibility and the ability to handle realistic post-prescription follow-up Q&A via constraint-respecting plan revisions. Systematic evaluation of 19 large language models reveals significant deficiencies in information-gathering efficiency and medication safety, underscoring a critical gap between theoretical medical knowledge and clinical practice ability.",12.25,Qwen2.5-3B,Apple M1 (Metal)
2601.12664v1_Generalizable Hyperparameter Optimization for Fede.pdf,Generalizable Hyperparameter Optimization for Federated Learning on Non-IID Cancer Images,"Elisa Gonçalves Ribeiro, Rodrigo Moreira, Larissa Ferreira Rodrigues Moreira, André Ricardo Backes",,,"Federated Learning, Hyperparameter Optimization, Non-IID Data, Medical Imaging, Cancer","This paper examines whether hyperparameters optimized on one cancer imaging dataset generalize across non-IID federated scenarios. It considers binary histopathology tasks for ovarian and colorectal cancers and performs centralized Bayesian hyperparameter optimization, transferring dataset-specific optima to the non-IID FL setup. The main contribution is a simple cross-dataset aggregation heuristic by combining configurations by averaging learning rates and considering modal optimizers and batch sizes, achieving competitive classification performance.",10.74,Qwen2.5-3B,Apple M1 (Metal)
2601.12667v1_Empowering All-in-Loop Health Management of Spacec.pdf,Empowering All-in-Loop Health Management of Spacecraft Power System in the Mega-Constellation Era via Human-AI Collaboration,"Yi Di, Zhibin Zhao, Fujin Wang, Xue Liu, Jiafeng Tang, Jiaxin Ren, Zhi Zhai",,,"Large Language Model, Human-AI Collaboration, Spacecraft Power System, All-in-loop Health Management, Satellite Mega-Constellation","This work proposes a principle of aligning underlying capabilities (AUC principle) and develops SpaceHMchat, an open-source Human-AI collaboration (HAIC) framework for all-in-loop health management (AIL HM) of spacecraft power systems (SPS). SpaceHMchat serves across the entire loop of work condition recognition, anomaly detection, fault localization, and maintenance decision making, achieving goals such as conversational task completion, adaptive human-in-the-loop learning, personnel structure optimization, knowledge sharing, efficiency enhancement, transparent reasoning, and improved interpretability. A hardware-realistic fault injection experimental platform is established and its simulation model is built and open-sourced, fully replicating the real SPS. The corresponding experimental results demonstrate excellent performance across 23 quantitative metrics. Another contribution is the release of the first-ever AIL HM dataset of SPS, containing four sub-datasets, involving 4 types of AIL HM sub-tasks, 17 types of faults, and over 700,000 timestamps.",12.22,Qwen2.5-3B,Apple M1 (Metal)
2601.12671v1_Exploiting Test-Time Augmentation in Federated Lea.pdf,Exploiting Test-Time Augmentation in Federated Learning for Brain Tumor MRI Classification,"Thamara Leandra de Deus Melo, Rodrigo Moreira, Larissa Ferreira Rodrigues Moreira, André R. Backes",Not found,Not found,"Brain tumors, Federated Learning, Test-Time Augmentation, Image classification","Efficient brain tumor diagnosis is crucial for early treatment; however, it is challenging due to lesion variability and image complexity. We evaluated convolutional neural networks (CNNs) in a federated learning (FL) setting, comparing models trained on original versus preprocessed MRI images (resizing, grayscale conversion, normalization, filtering, and histogram equalization). Preprocessing alone yielded negligible gains; combined with test-time augmentation (TTA), it delivered consistent, statistically significant improvements in federated MRI classification (p<0.001). In practice, TTA should be the default inference strategy in FL-based medical imaging; when the computational budget permits, pairing TTA with light preprocessing provides additional reliable gains.",11.48,Qwen2.5-3B,Apple M1 (Metal)
2601.12688v1_Logic-Guided Multistage Inference for Explainable .pdf,Logic-Guided Multistage Inference for Explainable Multidefendant Judgment Prediction,"Xu Zhang, Qinghua Wang, Mengyang Zhao, Fang Wang, Cunquan Qu",Not found,Not found,"Multiple defendants, Legal judgment predictions, Label broadcast, Guilt responsibility, Transformer","This work proposes a masked multistage inference (MMSI) framework to enhance intelligent judicial systems, incorporating sentencing logic into a pretrained Transformer encoder framework. The framework clarifies roles through an oriented masking mechanism and improves model sensitivity to culpability distinctions between principals and accomplices. Predicted guilt labels are further incorporated into a regression model through broadcasting, consolidating crime descriptions and court views. The framework achieves significant accuracy improvements in role-based culpability differentiation, outperforming baselines on the custom IMLJP dataset for intentional injury cases. This work offers a robust solution for enhancing intelligent judicial systems, with publicly available code.",11.6,Qwen2.5-3B,Apple M1 (Metal)
2601.12711v1_Neurosymbolic LoRA Why and When to Tune Weights vs.pdf,Neurosymbolic LoRA: Why and When to Tune Weights vs. Rewrite Prompts,"Kevin Wang, Neel P. Bhatt, Cong Liu, Junbo Li, Runjin Chen, Yihan Xi, Timothy Barclay, Alvaro Velasquez, Ufuk Topcu, Zhangyang Wang",,,"Neurosymbolic, LoRA, Fine-tuning, Symbolic, TextGrad, Factual Knowledge, Style Alignment, Mathematical Reasoning","A neurosymbolic LoRA framework that dynamically combines numerical updates and symbolic manipulations to adapt large language models, demonstrating superior adaptability and performance compared to purely numerical or purely symbolic approaches.",10.64,Qwen2.5-3B,Apple M1 (Metal)
2601.12715v1_RSOD Reliability-Guided Sonar Image Object Detecti.pdf,RSOD: Reliability-Guided Sonar Image Object Detection,"Chengzhou Li, Ping Guo, Guanchen Meng, Qi Jia, Jinyuan Liu, Zhu Liu, Xiaokang Liu, Yu Liu, Zhongxuan Luo, Xin Fan",,,"sonar images, object detection, reliability-guided, pseudo-labeling, limited labels","Object detection in sonar images is challenging due to their limited texture details and susceptibility to noise. RSOD proposes a teacher-student framework to address this issue, leveraging a reliability score to develop pseudo-labels and optimize student performance with reliability-guided constraints. This method achieves competitive results even with extremely limited labeled data.",10.82,Qwen2.5-3B,Apple M1 (Metal)
2601.12720v1_Teaching Large Reasoning Models Effective Reflecti.pdf,Teaching Large Reasoning Models Effective Reflection,"Hanbin Wang, Jingwei Song, Jinpeng Li, Qi Zhu, Fei Mi, Ganqu Cui, Yasheng Wang, Lifeng Shang",Not found,Not found,"Large Reasoning Models, Self-Reflective Behaviors, Self-Critique, Verification, Backtracking, Reinforcement Learning, Effective Reflection Rewards","This paper addresses the problem of superficial reflection in Large Reasoning Models (LRMs), proposing Self-Critique Fine-Tuning (SCFT) and Reinforcement Learning with Effective Reflection Rewards (RLERR) to enhance the model's reflective reasoning ability and improve both reasoning accuracy and reflection quality.",10.65,Qwen2.5-3B,Apple M1 (Metal)
2601.12723v1_An Evolutionary Framework for Automatic Optimizati.pdf,An Evolutionary Framework for Automatic Optimization Benchmark Generation via Large Language Models,"Yuhiro Ono, Tomohiro Harada, Yukiya Miura",Not provided,2601.12723,"Optimization benchmarks, Large language models, Evolutionary algorithms, Benchmark generation, Benchmarking","This paper proposes an evolutionary framework for generating optimization benchmarks using a large language model (LLM). The framework leverages the LLM as a generative operator to evolve benchmark problems within a flexible representation space, aiming to address the limitations of existing benchmarks in capturing real-world problem diversity and irregularity.",13.01,Qwen2.5-3B,Apple M1 (Metal)
2601.12727v1_AI-exhibited Personality Traits Can Shape Human Se.pdf,AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations,"Jingshu Li, Tianqi Song, Nattapat Boonprakong, Zicheng Zhu, Yitian Yang, Yi-Chieh Lee",10.1145/3772318.3790654,2601.12727v1,"AI personality traits, self-concept, human-computer interaction, behavioral experiment","This study explores how AI personality traits can shape human self-concept during conversations, using a randomized behavioral experiment.",11.96,Qwen2.5-3B,Apple M1 (Metal)
2601.12731v1_A Shared Geometry of Difficulty in Multilingual La.pdf,A Shared Geometry of Difficulty in Multilingual Language Models,"Stefano Civelli, Pietro Bernardelle, Nicolò Brunello, Gianluca Demartini",,,"multilingual language models, problem difficulty, linear probes, language agnostic, cross-lingual generalization","This work investigates the multilingual geometry of problem difficulty in large language models (LLMs) by training linear probes on the AMC subset of the Easy2Hard benchmark, translated into 21 languages. It finds that difficulty-related signals emerge at two distinct stages of the model internals, corresponding to shallow and deep internal representations. Probes trained on deep representations achieve high accuracy within the same language but exhibit poor cross-lingual generalization, while probes trained on shallow representations generalize better across languages, despite lower within-language performance. These results suggest that LLMs first form a language-agnostic representation of problem difficulty, which subsequently becomes language-specific. This aligns with existing findings in LLM interpretability showing that models tend to operate in an abstract conceptual space before producing language-specific outputs.",12.26,Qwen2.5-3B,Apple M1 (Metal)
2601.12740v1_TreeWriter AI-Assisted Hierarchical Planning and W.pdf,TreeWriter: AI-Assisted Hierarchical Planning and Writing for Long-Form Documents,"Zijian Zhang, Fangshi Du, Xingjian Liu, Pan Chen, Oliver Huang, Runlong Ye, Michael Liut, Alán Aspuru-Guzik",Not provided,Not provided,"AI-assisted writing, long-form documents, hierarchical planning, contextual AI support","TreeWriter is a hierarchical writing system that represents documents as trees and integrates contextual AI support. It allows authors to create, save, and refine document outlines at multiple levels, facilitating drafting, understanding, and iterative editing of long documents. A within-subject study and a two-month field deployment demonstrate its effectiveness in improving idea exploration/development, AI helpfulness, and perceived authorial control.",11.4,Qwen2.5-3B,Apple M1 (Metal)
2601.12742v1_AirHunt Bridging VLM Semantics and Continuous Plan.pdf,AirHunt: Bridging VLM Semantics and Continuous Planning for Efficient Aerial Object Navigation,"Xuecheng Chen, Zongzhuo Liu, Jianfa Ma, Bang Du, Tiantian Zhang, Xueqian Wang, Boyu Zhou",,,"Aerial Object Navigation, Vision-Language Models, Continuous Planning, Zero-Shot Generalization, Geometric Redundancy, Semantic Geometric Coherence","Presenting AirHunt, an aerial object navigation system that efficiently locates open-set objects with zero-shot generalization in outdoor environments by seamlessly fusing Vision-Language Model (VLM) semantic reasoning with continuous path planning.",10.99,Qwen2.5-3B,Apple M1 (Metal)
2601.12744v1_Vision Language Models for Optimization-Driven Int.pdf,Vision Language Models for Optimization-Driven Intent Processing in Autonomous Networks,"Tasnim Ahmed, Yifan Zhu, Salimur Choudhury",Not provided,Not provided,"Vision-Language Models, Optimization, Intent-Based Networking, Code Generation, Model Context Protocol","This paper presents IntentOpt, a benchmark of 85 optimization problems across 17 categories, evaluating four Vision-Language Models (GPT-5-Mini, Claude-Haiku-4.5, Gemini-2.5-Flash, Llama-3.2-11B-Vision) under three prompting strategies on multimodal versus text-only inputs. The evaluation shows that visual parameter extraction reduces execution success by 12–21 percentage points (pp), with GPT-5-Mini dropping from 93% to 72%. Program-of-thought prompting decreases performance by up to 13 pp, and open-source models lag behind closed-source ones, with Llama-3.2-11B-Vision reaching 18% compared to 75% for GPT-5-Mini. These results establish baseline capabilities and limitations of current VLMs for optimization code generation within an IBN system.",12.35,Qwen2.5-3B,Apple M1 (Metal)
2601.12745v1_A Graph Prompt Fine-Tuning Method for WSN Spatio-T.pdf,A GraphA PromptA Fine-TuningA MethodA for Spatio-TemporalACorrelationADetection in Wireless Sensor Networks,"Miao Ye, Jing Cui, Yuan Huang, Yong Wang, Qian He, Jiwen Zhang",,REPLACE_THIS_LINE_WITH_YOUR_MANUSCRIPT_ID_NUMBER,"Anomaly Detection, Graph Neural Networks, Pre-training, Prompt Learning, Wireless Sensor Networks",Anomaly detection of multi-temporal modal data in Wireless Sensor Network (WSN) can provide an important guarantee for reliable network operation. This paper designs a graph neural network anomaly detection backbone network incorporating spatio-temporal correlation features and a multi-task self-supervised training strategy of 'pre-training-graph prompting-fine-tuning' for WSN graph structure data.,11.46,Qwen2.5-3B,Apple M1 (Metal)
2601.12754v1_PAIR-SAFE A Paired-Agent Approach for Runtime Audi.pdf,PAIR-SAFE: A Paired-Agent Approach for Runtime Auditing and Refining,"Jiwon Kim, Violeta J. Rodriguez, Dong Whi Yoo, Eshwar Chandrasekharan, Koustuv Saha",,,"Large language models, Mental health support, Paired-agent, Runtime auditing, Refinement, Motivational Interviewing Treatment Integrity (MITI-4)","Large language models are increasingly used for mental health support, but they can produce responses that are overly directive, inconsistent, or clinically misaligned, particularly in sensitive or high-risk contexts. Existing approaches to mitigating these risks largely rely on implicit alignment through training or prompting, offering limited transparency and runtime accountability. We introduce PAIR-SAFE, a paired-agent framework for auditing and refining AI-generated mental health support that integrates a Responder agent with a supervisory Judge agent grounded in the clinically validated Motivational Interviewing Treatment Integrity (MITI-4) framework. The Judge audits each response and provides structured ALLOW or REVISE decisions that guide runtime response refinement. We simulate counseling interactions using a support-seeker simulator derived from human-annotated motivational interviewing data. We find that Judge-supervised interactions show significant improvements in key MITI dimensions, including Partnership, Seek Collaboration, and overall Relational quality. Our quantitative findings are supported by qualitative expert evaluation, which further highlights the nuances of runtime supervision.",12.54,Qwen2.5-3B,Apple M1 (Metal)
2601.12758v1_VISPA Pluralistic Alignment via Automatic Value Se.pdf,VISPA: Pluralistic Alignment via Automatic Value Selection and Activation,"Shenyan Zheng, Jiayou Zhong, Anudeex Shetty, Heng Ji, Preslav Nakov, Usman Naseem",,,"large language models, pluralistic alignment, value selection, model activation, interpretability","Achieving pluralism in high-stakes domains, such as healthcare, requires models that reflect a range of perspectives rather than a single average preference. VISPA, a training-free framework, enables direct control over value expression by dynamically selecting and activating values internally, demonstrating performance across multiple models and settings.",10.6,Qwen2.5-3B,Apple M1 (Metal)
2601.12762v1_Teaching LLMs to Learn Tool Trialing and Execution.pdf,Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction,"Xingjie Gao, Pengcheng Huang, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, Chen Qian, Ge Yu, Yu Gu",Not found,Not found,"Large Language Models, External Tools, Tool Usage, Trial and Execution, Environment Interaction, Reinforcement Learning","This paper proposes ToolMaster, a framework that enables Large Language Models to learn tool usage through interaction with the environment. It shifts tool use from imitating golden tool-calling trajectories to actively learning through trial and execution, which trains LLMs to first imitate teacher-generated trajectories containing explicit tool trials and self-correction, followed by reinforcement learning to coordinate the trial and execution phases. This process enables agents to autonomously explore correct tool usage by actively interacting with environments and forming experiential knowledge that benefits tool execution. Experimental results demonstrate significant improvements in generalization and robustness across unseen or unfamiliar tools.",12.05,Qwen2.5-3B,Apple M1 (Metal)
2601.12781v1_VIRO Robust and Efficient Neuro-Symbolic Reasoning.pdf,VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension,"Hyejin Park, Junhyuk Kwon, Suha Kwak, Jungseul Ok",,,"Referring Expression Comprehension, Neuro-symbolic Reasoning, Verification-Integrated Reasoning Operators, Robustness, Efficiency","This paper introduces VIRO, a neuro-symbolic framework that integrates lightweight operator-level verifiers within reasoning steps to handle no-target cases robustly. VIRO achieves state-of-the-art performance, demonstrating generalization to real-world egocentric data and superior computational efficiency.",11.17,Qwen2.5-3B,Apple M1 (Metal)
2601.12785v1_Distilling Time Series Foundation Models for Effic.pdf,DISTILLING TIME SERIES FOUNDATION MODELS FOR EFFICIENT FORECASTING,"Yuqi Li, Kuiye Ding, Chuanguang Yang, Szu-Yu Chen, Yingli Tian",Not found,Not found,"Time Series Foundation Model, Knowledge Distillation, Forecasting, Efficient Deployment","Time Series foundation models (TSFMs) deliver strong forecasting performance through large-scale pretraining, but their large parameter sizes make deployment costly. This paper presents DistilTS, the first distillation framework specifically designed for TSFMs, addressing two key challenges: task difficulty discrepancy and architecture discrepancy. DistilTS introduces horizon-weighted objectives and a temporal alignment strategy to balance learning across horizons and reduce architectural mismatch, enabling compact models. Experiments on multiple benchmarks demonstrate that DistilTS achieves forecasting performance comparable to full-sized TSFMs, while reducing parameters by up to 1/150 and accelerating inference by up to 6000×.",12.26,Qwen2.5-3B,Apple M1 (Metal)
2601.12804v1_SL-CBM Enhancing Concept Bottleneck Models with Se.pdf,SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability,"Hanwei Zhang, Luo Cheng, Rui Wen, Yang Zhang, Lijun Zhang, Holger Hermanns",,,"explainable AI, Concept Bottleneck Models, semantic locality, saliency maps, interpretability, spatial explainability","This work proposes SL-CBM, a novel extension of Concept Bottleneck Models (CBMs) that enhances locality faithfulness by generating spatially coherent saliency maps at both concept and class levels. SL-CBM integrates a1×1convolutional layer with a cross-attention mechanism to improve alignment between concepts, image regions, and final predictions. Extensive experiments on image datasets demonstrate that SL-CBM substantially improves locality faithfulness, explanation quality, and intervention efficacy while maintaining competitive classification accuracy.",11.37,Qwen2.5-3B,Apple M1 (Metal)
2601.12805v1_SciHorizon-GENE Benchmarking LLM for Life Sciences.pdf,SciHorizon-Gene: Benchmarking LLM for Life Sciences Inference from Gene Knowledge to Functional Understanding,"Xiaohan Huang, Meng Xiao, Chuan Qin, Qingqing Long, Jinmiao Chen, Yuanchun Zhou, Hengshu Zhu",XXXXXXX.XXXXXXX,,"large language models, benchmarking and evaluation, genomics","Large language models have shown promise in biomedical research, particularly for knowledge-driven interpretation tasks. However, their ability to reliably reason from gene-level knowledge to functional understanding remains largely underexplored. SciHorizon-Gene introduces a large-scale gene-centric benchmark to address this gap, evaluating LLMs along four biologically critical perspectives.",11.29,Qwen2.5-3B,Apple M1 (Metal)
2601.12809v1_Left-Right Symmetry Breaking in CLIP-style Vision-.pdf,Left–Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data,"Takaki Yamamoto1, Chihiro Noguchi 1, Toshihiro Tanizawa1",Not found,Not found,"Vision-Language Models, Transformer, Contrastive Learning, Spatial Understanding, Left-Right Symmetry, Synthetic Data","This paper investigates how vision-language models acquire spatial and relational understanding, specifically focusing on left-right symmetry breaking. It presents a controllable 1D image-text testbed to probe how such understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. The study finds that contrastive training learns left-right relations and that label diversity, more than layout diversity, drives generalization in this setting. Attention decomposition reveals that interactions between positional and token embeddings induce a horizontal attention gradient that breaks left-right symmetry in the encoders. The results provide a mechanistic insight into when and how CLIP-style models acquire relational competence.",11.8,Qwen2.5-3B,Apple M1 (Metal)
2601.12816v1_Fisher-Orthogonal Projected Natural Gradient Desce.pdf,Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning,"Ishir Garg, Neel Kolhe, Andy Peng, Rohan Gopalam",,2601.06287,"Continual Learning, Natural Gradient, Orthogonal Gradient, Fisher Information, Catastrophic Forgetting","Proposes a geometrically principled optimizer, Fisher-Orthogonal Projected Natural Gradient Descent (FOPNG), to address catastrophic forgetting in continual learning by enforcing orthogonality constraints in the Fisher-Riemannian manifold, improving model stability and performance on sequential tasks.",11.4,Qwen2.5-3B,Apple M1 (Metal)
2601.12822v1_MirrorGuard Toward Secure Computer-Use Agents via .pdf,MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real Reasoning Correction,"Wenqi Zhang, Yulin Shen, Changyue Jiang, Jiarun Dai, Geng Hong",XXXXXXX.XXXXXXX,,"Computer Use Agents, Agent Security, Reasoning Correction, Simulation, Vision-Language Models","Large foundation models are integrated into Computer Use Agents (CUAs) to enable autonomous interaction with operating systems through GUIs. This autonomy introduces security risks, and existing defenses often abort tasks prematurely. MirrorGuard is a novel defense framework that uses simulation-based training to improve CUA security in the real world, significantly mitigating security risks.",10.72,Qwen2.5-3B,Apple M1 (Metal)
2601.12837v1_Cognition spaces natural artificial and hybrid.pdf,"Cognition spaces: natural, artificial, and hybrid","Ricard Solé, Luis F Seoane, Jordi Pla-Mauri, Michael Timothy Bennett, Michael E. Hochberg, Michael Levin",,,"Evolved cognition, basal cognition, artificial life, artificial intelligence, synthetic biology, morphospace","Cognitive processes are realized across natural, artificial, and hybrid systems, yet there is no unified framework for comparing their forms, limits, and unrealized possibilities. This paper proposes a cognition space approach that treats cognition as a graded capacity to sense, process, and act upon information, allowing diverse systems to be analyzed within a common conceptual landscape. Three cognition spaces—basal aneural, neural, and human–AI hybrid—are introduced and examined, showing uneven occupation with clusters of realized systems separated by large unoccupied regions. These voids reflect evolutionary contingencies, physical constraints, and design limitations, highlighting hybrid cognition as a promising frontier for exploring novel forms of complexity beyond biological evolution.",11.98,Qwen2.5-3B,Apple M1 (Metal)
2601.12842v1_SCULPT Constraint-Guided Pruned MCTS that Carves E.pdf,SCULPT: Constraint-Guided Pruned MCTS that Carves Efficient Paths for Mathematical Reasoning,"Qitong Fang*, Haotian Li†, Xu Wang‡",,,"Automated agent workflows, Large language models (LLMs), Mathematical reasoning, Constraint-guided exploration, Monte Carlo Tree Search (MCTS), Pruning, Domain-aware scoring","This paper introduces SCULPT, a constraint-guided approach for Monte Carlo Tree Search (MCTS) that integrates domain-aware scoring into selection, expansion, simulation, and backpropagation. SCULPT scores and prunes actions using a combination of symbolic checks and structural pattern guidance, steering the search toward plausible reasoning paths. Under matched LLM configurations, SCULPT yields stable improvements on multiple datasets, and additional results assess executor transferability and performance on frontier reasoning models.",11.43,Qwen2.5-3B,Apple M1 (Metal)
2601.12849v1_The Cost of EFX Generalized-Mean Welfare and Compl.pdf,The Cost of EFX: Generalized-Mean Welfare and Complexity Dichotomies with Few Surplus Items,"Eugene Lim, Tzeh Yuan Neoh, Nicholas Teh",Not found,2601.12849,"Fair division, Envy-freeness, Generalized-mean welfare, NP-hard, Pareto-optimality","This paper studies the interaction between envy-freeness up to any good (EFX) and generalized-mean welfare, focusing on settings with few surplus items. It establishes complexity dichotomies for EFX and welfare optimization, providing both polynomial-time algorithms and NP-hard results.",11.1,Qwen2.5-3B,Apple M1 (Metal)
2601.12856v1_Mining Citywide Dengue Spread Patterns in Singapor.pdf,Mining Citywide Dengue Spread Patterns in Singapore Through Hotspot Dynamics from Open Web Data,"Liping Huang, Gaoxi Xiao, Stefan Ma, Hechang Chen, Shisong Tang, Flora Salim",10.1145/XXXXXX.XXXXXX,,"Dengue Cases, Disease Spreading Pattern, Hotpot Dynamics, Machine Learning","This study introduces a novel framework that uncovers and exploits latent transmission links between urban regions, mined directly from publicly available dengue case data. It models how hotspot formation in one area is influenced by epidemic dynamics in neighboring regions, providing an interpretable explanation for citywide spread. The learned transmission links align with commuting flows, highlighting the interplay between hidden epidemic spread and human mobility. The framework transforms open web-based case data into a predictive and explanatory resource for public health planning and early intervention.",11.49,Qwen2.5-3B,Apple M1 (Metal)
2601.12879v1_Hierarchical Sparse Circuit Extraction from Billio.pdf,Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition,"Mohammed Mudassir Uddin, Shahnawaz Alam, Mohammed Kaif Pasha",Not found,Not found,"Mechanistic interpretability, sparse computational graphs, circuit discovery, transformer architectures, causal inference, attribution methods, hierarchical decomposition","The proposed Hierarchical Attribution Graph Decomposition (HAGD) framework reduces circuit discovery complexity from O(2^n) exhaustive enumeration to O(n^2 log n) through multi-resolution abstraction hierarchies and differentiable circuit search. The methodology integrates cross-layer transcoders for monosemantic feature extraction, graph neural network meta-learning for topology prediction, and causal intervention protocols for validation. Empirical evaluation spans GPT-2 variants, Llama-7B through Llama-70B, and Pythia suite models across algorithmic tasks and natural language benchmarks.",12.59,Qwen2.5-3B,Apple M1 (Metal)
2601.12882v1_YOLO26 An Analysis of NMS-Free End to End Framewor.pdf,YOLO26: ANALYSIS OF NMS-FREE END-TO-END FRAMEWORK FOR REAL-TIME OBJECT DETECTION,Sudip Chakrabarty,Not found,2601.12882v1,"YOLOv26, End-to-End Object Detection, NMS-Free, MuSGD, ProgLoss, Real-Time Computer Vision, You Only Look Once","This paper analyzes YOLO26, a framework that eliminates Non-Maximum Suppression (NMS) in favor of a native end-to-end learning strategy, demonstrating improved performance in inference speed and detection accuracy compared to previous versions and state-of-the-art competitors.",12.71,Qwen2.5-3B,Apple M1 (Metal)
2601.12886v1_Communication Methods in Multi-Agent Reinforcement.pdf,Communication Methods in Multi-Agent Reinforcement Learning,Christoph Wittner,Not found,2601.12886,"Machine learning, MARL, Communication","This work provides an overview of communication techniques in multi-agent reinforcement learning, evaluating the strengths and weaknesses of explicit, implicit, attention-based, graph-based, and hierarchical/role-based communication methods. It highlights the importance of communication methods with low computational overhead for scalability and discusses current research gaps.",11.85,Qwen2.5-3B,Apple M1 (Metal)
2601.12893v1_AdaNODEs Test Time Adaptation for Time Series Fore.pdf,ADANODES: TEST TIME ADAPTATION FOR TIME SERIES FORECASTING USING NEURAL ODES,"Ting Dang, Soumyajit Chatterjee, Hong Jia, Yu Wu, Flora Salim, Fahim Kawsar",Not found,Not found,"test time adaptation, time series forecasting, domain adaptation, neural odes","This paper presents AdaNODEs, an innovative source-free test time adaptation (TTA) method tailored explicitly for time series forecasting. By leveraging Neural Ordinary Differential Equations (NODEs), it proposes a novel adaptation framework that accommodates the unique characteristics of distribution shifts in time series data. The method only requires updating limited model parameters, showing effectiveness in capturing temporal dependencies while avoiding significant memory usage. Extensive experiments demonstrate relative improvements of 5.88% and 28.4% over the state-of-the-art (SOTA) baselines, especially demonstrating robustness across higher severity distribution shifts.",12.21,Qwen2.5-3B,Apple M1 (Metal)
2601.12904v1_From Prefix Cache to Fusion RAG Cache Accelerating.pdf,From Prefix Cache to Fusion RAG Cache: Accelerating LLM Inference in Retrieval-Augmented Generation,"Jiahao Wang, WeiYu Xie, MingXing Zhang, Boxing Zhang, JianWei Dong, Yuening Zhu, Chen Lin, Jinqi Tang, YaoChen Han, ZhiYuan Ai, XiangLin Chen, YongWei Wu, CongFeng Jiang",10.1145/3786655,2601.1290,"Large Language Models, Retrieval-Augmented Generation, Cache Reuse, Inference Optimization","This paper proposes FusionRAG, a novel inference framework that optimizes both the preprocessing and reprocessing stages of Retrieval-Augmented Generation (RAG) to improve generation quality and efficiency. By embedding information from other related text chunks into each chunk and recomputing the KVCache for tokens the model focuses on, FusionRAG achieves a better trade-off between generation quality and efficiency, significantly improving normalized-F1 scores and reducing Time to First Token (TTFT) compared to previous state-of-the-art solutions.",11.77,Qwen2.5-3B,Apple M1 (Metal)
2601.12910v1_SciCoQA Quality Assurance for Scientific Paper--Co.pdf,SCICOQA: Quality Assurance for Scientific Paper–Code Alignment,"Tim Baumgärtner, Iryna Gurevych",,,"reproducibility, scientific paper, code alignment, machine learning, discrepancy detection","We present SCICOQA, a dataset for detecting discrepancies between scientific papers and their codebases to ensure faithful implementations. The dataset consists of 611 paper-code discrepancies, including 81 real and 530 synthetic cases, spanning various computational science disciplines. The evaluation of 21 language models highlights the difficulty of SCICOQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model, GPT-5, can only detect 45.7% of real-world paper-code discrepancies.",11.14,Qwen2.5-3B,Apple M1 (Metal)
2601.12912v1_Human Emotion Verification by Action Languages via.pdf,Human Emotion Verification by Action Languages via Answer Set Programming,"ANDREAS BRÖNNSTRÖM, JUAN CARLOS NIEVES",10.1017/xxxxx,2601.12912,"Action Languages, Answer Set Programming, Theory of Mind","This paper introduces the action language C-MT (Mind Transition Language) built on answer set programming and transition systems to represent human mental states in response to observable actions, formalizing mental states as multi-dimensional configurations and enabling controlled reasoning about the dynamic evolution of human mental states.",12.39,Qwen2.5-3B,Apple M1 (Metal)
2601.12913v1_Actionable Interpretability Must Be Defined in Ter.pdf,Position: Actionable Interpretability Must Be Defined in Terms of Symmetries,"Pietro Barbiero, Mateo Espinosa Zarlenga, Francesco Giannini, Alberto Termine, Mateja Jamnik, Giuseppe Marra",,,"interpretability, symmetries, AI, Bayesian inversions, inference equivariance, information invariance, concept-closure invariance, structural invariance","This paper argues that interpretability research in Artificial Intelligence is fundamentally ill-posed, as existing definitions of interpretability are not actionable. It posits that for a definition of interpretability to be actionable, it must be given in terms of symmetries. The authors hypothesize that four symmetries are sufficient to motivate core interpretability properties, characterize the class of interpretable models, and derive a unified formulation of interpretable inference as a form of Bayesian inversions.",11.37,Qwen2.5-3B,Apple M1 (Metal)
2601.12922v1_Your Privacy Depends on Others Collusion Vulnerabi.pdf,Collusion Vulnerabilities in Individual Differential Privacy,"Johannes Kaiser, Alexander Ziller, Eleni Triantafillou, Daniel Rückert, Georgios Kaissis",Not provided,Not provided,"differential privacy, individual differential privacy, collusion, privacy risk, membership inference","This work reveals a vulnerability in sampling-based Individual Differential Privacy (iDP) mechanisms, where an individual's privacy risk is not solely governed by their own privacy budget but critically depends on the privacy choices of all other data contributors. This mismatch between the promise of individual privacy control and the reality of a system where risk is collectively determined creates an exploitable attack vector. The authors demonstrate empirical attacks against 62% of targeted individuals, increasing their membership inference susceptibility. They propose (εi, δi, ∆)-iDP to provide users with a hard upper bound on excess vulnerability, while offering flexibility to mechanism design.",12.21,Qwen2.5-3B,Apple M1 (Metal)
2601.12925v1_ForeDiffusion Foresight-Conditioned Diffusion Poli.pdf,ForeDiffusion: Foresight-Conditioned Diffusion Policy via Future View for Robot Manipulation,"Weize Xie, Yi Ding, Ying He, Leilei Wang, Binwen Bai, Zheyi Zhao, Chenyang Wang, F. Richard Yu",Not found,Not found,"Diffusion, Robot Manipulation, Foresight, Future View, Policy Optimization","This paper proposes ForeDiffusion, a diffusion-based policy for robot manipulation that incorporates future view predictions into the diffusion process. This approach enables the policy to be forward-looking, correcting trajectory deviations and achieving an average success rate of 80% across various tasks, significantly outperforming existing diffusion methods by 23% in complex tasks.",11.05,Qwen2.5-3B,Apple M1 (Metal)
2601.12929v1_Membership Inference Test Auditing Training Data i.pdf,Membership Inference Test: Auditing Training Data in Object Classification Models,"Gonzalo Mancera, Daniel DeAlcala, Aythami Morales, Ruben Tolosana, Julian Fierrez",Not found,Not found,"Membership Inference, Training Data Auditing, Object Classification, Artificial Intelligence","This research analyzes the performance of Membership Inference Tests (MINT) in determining whether given data were utilized during the training phase, specifically in the domain of object recognition. It proposes and develops architectures tailored for MINT models to optimize performance and efficiency in data utilization.",11.1,Qwen2.5-3B,Apple M1 (Metal)
2601.12931v1_Online Continual Learning for Time Series a Natura.pdf,ONLINECONTINUALLEARNING FORTIMESERIES:A NATURALSCORE-DRIVENAPPROACH,"Edoardo Urettini, Daniele Atzeni, Ioanna-Yvonni Tsaknaki, Antonio Carta",,1909.08874,"online continual learning, time series forecasting, natural gradient descent, robust optimization, replay buffer, dynamic scale heuristic",This paper aims to strengthen the theoretical and practical connections between time series methods and online continual learning (OCL) by redefining neural network optimization as a parameter filtering problem and introducing a robust optimizer called Natural Score-driven Replay (NatSR). Empirical results show that NatSR achieves stronger forecasting performance than more complex state-of-the-art methods.,11.11,Qwen2.5-3B,Apple M1 (Metal)
2601.12937v1_On the Evidentiary Limits of Membership Inference .pdf,On the Evidentiary Limits of Membership Inference for Copyright Auditing,"Murat Bilgehan Ertan, Emirhan Boge, Min Chen, Kaleel Mahmood, Marten van Dijk",Not found,Not found,"Copyright auditing, Membership inference attacks, Large language models, Fine-tuning, Adversarial copyright disputes","As large language models (LLMs) are trained on increasingly opaque corpora, membership inference attacks (MIAs) have been proposed to audit whether copyrighted texts were used during training. This paper asks whether MIAs can serve as admissible evidence in adversarial copyright disputes, formalizing the setting through a judge-prosecutor-accused communication protocol. Experiments show that state-of-the-art MIAs degrade when models are fine-tuned on SAGE-generated paraphrases, indicating that their signals are not robust to semantics-preserving transformations.",11.23,Qwen2.5-3B,Apple M1 (Metal)
2601.12938v1_The Post-Turing Condition Conceptualising Artifici.pdf,The Post-Turing Condition: Conceptualising Artificial Subjectivity and Synthetic Sociality,"Thorsten Jelinek, Patrick Glauner, Alvin Wang Graylin, Yubao Qiu",Not provided,Not provided,"Artificial Intelligence, Social Coordination, Subjectivity, Synthetic Sociality, Human Participation, AI Design, Perception, Representation, Meaning, Real",This paper introduces the PRMO framework to address the structural risk of human exclusion from meaning formation in a technological horizon where artificial agents negotiate coherence and social order primarily among themselves.,13.19,Qwen2.5-3B,Apple M1 (Metal)
2601.12939v1_Active Inference-Driven World Modeling for Adaptiv.pdf,ACTIVE INFERENCE-DRIVEN WORLD MODELING FOR ADAPTIVE UA V SW ARM TRAJECTORY DESIGN,"Kaleem Arshid, Ali Krayani, Lucio Marcenaro, David Martin Gomez, Carlo Regazzoni",,,"Autonomous Systems, World Model, UA V-Swarm, Probabilistic Decision-Making, Active-Inference","This paper proposes an Active Inference-based framework for autonomous trajectory design in UA V swarms. The method integrates probabilistic reasoning and self-learning to enable distributed mission allocation, route ordering, and motion planning. Expert trajectories generated using a Genetic Algorithm with Repulsion Forces (GA-RF) are employed to train a hierarchical World Model capturing swarm behavior across mission, route, and motion levels. During online operation, UA Vs infer actions by minimizing divergence between current beliefs and model-predicted states, enabling adaptive responses to dynamic environments. Simulation results show faster convergence, higher stability, and safer navigation than Q-Learning, demonstrating the scalability and cognitive grounding of the proposed framework for intelligent UA V swarm control.",11.71,Qwen2.5-3B,Apple M1 (Metal)
2601.12946v1_AI-generated data contamination erodes pathologica.pdf,AI-generated data contamination erodes pathological variability and diagnostic reliability,"Hongyu He, Shaowen Xiang, Ye Zhang, Yingtao Zhu, Jin Zhang, Hao Deng, Emily Alsentzer, Qingyu Chen, Kun-Hsing Yu, Andrew Marmenshall, Tingting Chen, Srinivas Anumasa, Daniel Ebner, Dean Ho, Kee Yuan Ngiam, Ching-Yu Cheng, Dianbo Liu",Not provided,Not provided,"AI, medical records, pathological variability, diagnostic reliability, synthetic content, data contamination","Generative artificial intelligence (AI) is rapidly populating medical records with synthetic content, creating a feedback loop where future models are increasingly at risk of training on uncurated AI-generated data. This self-referential cycle drives a rapid erosion of pathological variability and diagnostic reliability, with models progressively converging toward generic phenotypes and failing to detect life-threatening pathology.",13.45,Qwen2.5-3B,Apple M1 (Metal)
2601.12951v1_Beyond Accuracy Characterizing Code Comprehension .pdf,Beyond Accuracy: Characterizing Code Comprehension Capabilities in (Large) Language Models,"Felix Mächtle, Jan-Niclas Serr, Nils Loose, Thomas Eisenbarth",,,"Code Comprehension, Model Evaluation, Machine Learning for Software Engineering","This paper investigates whether Large Language Models' code-comprehension performance aligns with traditional human-centric software metrics or reflects distinct, non-human regularities. It introduces a diagnostic framework that reframes code understanding as a binary input–output consistency task, enabling the evaluation of classification and generative models. Using a large-scale dataset, it correlates model performance with traditional complexity metrics and finds minimal correlation between human-defined metrics and LLM success, while shadow models achieve substantially higher predictive performance.",11.56,Qwen2.5-3B,Apple M1 (Metal)
2601.13007v1_ArchAgent Scalable Legacy Software Architecture Re.pdf,ARCHAGENT: SCALABLE LEGACY SOFTW ARE ARCHITECTURE RECOVERY WITH LLMS,"Rusheng Pan★†, Bingcheng Mao★, Tianyi Ma★, Zhenhua Ling †",Not found,Not found,"Software architecture recovery, legacy software, Large Language Models (LLMs), code repository, cross-repository context","Recovering accurate architecture from large-scale legacy software is hindered by architectural drift, missing relations, and the limited context of Large Language Models (LLMs). ArchAgent, a scalable agent-based framework, combines static analysis, adaptive code segmentation, and LLM-powered synthesis to reconstruct multiview, business-aligned architectures from cross-repository codebases.",11.57,Qwen2.5-3B,Apple M1 (Metal)
2601.13013v1_HT-GNN Hyper-Temporal Graph Neural Network for Cus.pdf,HT-GNN: Hyper-Temporal Graph Neural Network for Customer Lifetime Value Prediction in Baidu Ads,"Xiaohui Zhao, Xinjian Zhao, Jiahui Zhang, Guoyu Liu, Houzhi Wang, Shu Wu",Not provided,Not provided,"Customer Lifetime Value, Advertising Platform, Graph Neural Network, Dynamic Marketing Strategies, Temporal Dynamics, Hypergraph Supervised Module, Transformer-Based Temporal Encoder, Task-Adaptive Mixture-of-Experts","Lifetime value (LTV) prediction is crucial for news feed advertising, enabling platforms to optimize bidding and budget allocation for long-term revenue growth. However, it faces two major challenges: demographic-based targeting creates segment-specific LTV distributions with large value variations across user groups, and dynamic marketing strategies generate irregular behavioral sequences where engagement patterns evolve rapidly. We propose a Hyper-Temporal Graph Neural Network (HT-GNN) which jointly models demographic heterogeneity and temporal dynamics through three key components: a hypergraph-supervised module capturing inter-segment relationships, a transformer-based temporal encoder with adaptive weighting, and a task-adaptive mixture-of-experts with dynamic prediction towers for multi-horizon LTV forecasting. Experiments on Baidu Advertisements with 15 million users demonstrate that HT-GNN consistently outperforms state-of-the-art methods across all metrics and prediction horizons.",12.54,Qwen2.5-3B,Apple M1 (Metal)
2601.13018v1_Bi-Attention HateXplain  Taking into account the s.pdf,Bi-Attention HateXplain : Taking into account the sequential aspect of data during explainability in a multi-task context,Ghislain Dorian Tchuente Mondjo,,,"Multitask learning, Deep Learning, Hate speech, Explainability, Bi-Attention","This article proposes the BiAtt-BiRNN-HateXplain model to improve the reliability of hate speech detection by taking into account the sequential aspect of data during explainability. The model aims to provide more transparent and interpretable explanations compared to traditional black-box models, reducing unintentional bias errors and improving detection performance.",11.36,Qwen2.5-3B,Apple M1 (Metal)
2601.13020v1_PASs-MoE Mitigating Misaligned Co-drift among Rout.pdf,PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway Activation Subspaces for Continual Learning,"Zhiyan Hou, Haiyun Guo, Haokai Ma, Yandu Sun, Yonghui Yang, Jinqiao Wang",Not found,Not found,"Continual Learning, Multi-modal Language Models, Mixture-of-Experts, LoRA, Misaligned Co-drift, Pathway Activation Subspaces","This paper introduces PASs, a LoRA-induced subspace that reflects which low-rank pathway directions an input activates in each expert, to address the Misaligned Co-drift issue in existing LoRA-based Mixture-of-Experts (MoE) methods. The approach uses PASs to propose a fixed-capacity PASs-based MoE-LoRA method with two components: PAS-guided Reweighting and PAS-aware Rank Stabilization. Experiments on a CIT benchmark show that the approach consistently outperforms conventional continual learning baselines and MoE-LoRA variants in both accuracy and anti-forgetting without adding parameters.",11.75,Qwen2.5-3B,Apple M1 (Metal)
2601.13048v1_Analysis of Long Range Dependency Understanding in.pdf,ANALYSIS OF LONG RANGE DEPENDENCY UNDERSTANDING IN STATE SPACE MODELS,"Srividya Ravikumar, Abhinav Anand, Shweta V erma, Mira Mezini",Not provided,Not provided,"Structured state-space models, interpretability, vulnerability detection","This work presents the first systematic kernel interpretability study of the diagonalized state-space model (S4D) trained on a real-world task (vulnerability detection in source code). Through time and frequency domain analysis of the S4D kernel, insights are provided on how the long-range modeling capability of S4D varies under different model architectures, affecting model performance.",11.71,Qwen2.5-3B,Apple M1 (Metal)
2601.13054v1_TinyML-Enabled IoT for Sustainable Precision Irrig.pdf,TinyML-Enabled IoT for Sustainable Precision Irrigation,"Kamogelo Taueatsoala1, Caitlyn Daniels1, Angelina J. Ramsunar1, Petrus Bronkhorst2, Absalom E. Ezugwu1,*",,,"TinyML, edge computing, Internet of Things, precision agriculture, smart irrigation, sustainable water management, embedded machine learning, resource-constrained systems","This paper presents a novel, edge-first IoT framework that integrates Tiny Machine Learning (TinyML) for intelligent, offline-capable precision irrigation. The system utilizes capacitive soil moisture, temperature, humidity, pH, and ambient light sensors for environmental monitoring and predicts irrigation needs with exceptional accuracy. The optimized model was deployed as a lightweight TinyML inference engine on the ESP32, achieving a MAPE < 1%. The system's low-power design and offline functionality confirm its viability for sustainable, scalable deployment in resource-constrained rural settings.",12.58,Qwen2.5-3B,Apple M1 (Metal)
2601.13060v1_MagicGUI-RMS A Multi-Agent Reward Model System for.pdf,MAGICGUI-RMS: A MULTI-AGENTREWARDMODELSYSTEM FORSELF-EVOLVINGGUI AGENTS VIAAUTOMATEDFEEDBACKREFLUX,"Zecheng Li∗, Zhihui Cao ∗ †, Wenke Huang, Yudong Zhang, Keying Qi, Rui Wang, Zeyu Zheng, Jian Zhao, Hao Zhu, Hengxin Wu, Yuran Wang, Guitao Fan, Guokun Wu, Yicong Liu, Zhilin Gao, Haikun Xu, He Yang, Minqi Xiang, Xingyu Liu †, Zuojian Wang †",Not found,Not found,"GUI agents, multi-agent reward model, automated feedback, self-evolving learning, reward-based adaptation","Graphical user interface (GUI) agents are rapidly evolving towards autonomous interaction and reliable task execution. MagicGUI-RMS presents a multi-agent reward model system that delivers adaptive trajectory evaluation, corrective feedback, and self-evolving learning capabilities. It integrates a Domain-Specific Reward Model (DS-RM) with a General-Purpose Reward Model (GP-RM) for fine-grained action assessment and robust generalization across heterogeneous GUI tasks. MagicGUI-RMS supports reward learning at scale through a structured data construction pipeline, reducing annotation costs while maintaining sample fidelity. During execution, it identifies erroneous actions, proposes refined alternatives, and continuously enhances agent behavior through an automated data-reflux mechanism. Extensive experiments demonstrate substantial gains in task accuracy and behavioral robustness.",13.29,Qwen2.5-3B,Apple M1 (Metal)
2601.13075v1_METIS Mentoring Engine for Thoughtful Inquiry  Sol.pdf,METIS: Mentoring Engine for Thoughtful Inquiry & Solutions,"Abhinav Rajeev Kumar, Dhruv Trehan, Paras Chopra",Not provided,Not provided,"AI mentorship, undergraduate research, large language models, literature search, methodology checks, stage-aware","This paper evaluates METIS, an AI research mentor designed to guide undergraduates from initial ideas to publishable conference papers. It compares METIS against GPT-5 and Claude Sonnet 4.5 in terms of single-turn judgments and multi-turn tutoring, demonstrating METIS's superiority in both areas. METIS incorporates tools for literature search, guidelines retrieval, methodology checks, and memory, and it shows improvements in student scores across different stages of writing. The paper also contributes to the field by providing a practical mentoring workflow, a simple system with tools, and open materials for reproducibility.",11.92,Qwen2.5-3B,Apple M1 (Metal)
2601.13111v1_CORE-T COherent REtrieval of Tables for Text-to-SQ.pdf,CORE-T: COherent REtrieval of Tables for Text-to-SQL,"Hassan Soliman1, Vivek Gupta2, Dan Roth3, Iryna Gurevych1",Not found,2601.13111,"Text-to-SQL, Table Retrieval, Open-Book, Heterogeneous Tables, Joint Inference","CORE-T addresses the challenge of retrieving relevant tables for text-to-SQL queries in an open-book setting, where queries must be answered over integrated clusters of tables spanning various domains. It combines dense retrieval with lightweight table compatibility caching and LLM-generated purpose metadata to improve table selection accuracy and reduce the number of tables retrieved.",11.13,Qwen2.5-3B,Apple M1 (Metal)
2601.13114v1_IntAgent NWDAF-Based Intent LLM Agent Towards Adva.pdf,IntAgent: NWDAF-Based Intent LLM Agent,"Abdelrahman Soliman, Ahmed Refaey, Aiman Erbad, Amr Mohamed",Not provided,Not provided,"Intent-based networks, NWDAF, Large Language Models, Intelligent Networks, Closed Loop","IntAgent is an intelligent intent LLM agent that integrates NWDAF analytics and tools to fulfill the network operator's intents. It offers an enriched, 3GPP-compliant data source and an MCP tools server for scheduling, monitoring, and analytics tools, demonstrating the efficacy of its framework through practical use cases.",11.55,Qwen2.5-3B,Apple M1 (Metal)
2601.13122v1_Responsible AI for General-Purpose Systems Overvie.pdf,"Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward","Gourab K. Patro, Himanshi Agrawal, Himanshu Gharat, Supriya Panigrahi, Nim Sherpa, Vishal Vaddina, Dagnachew Birru",Not found,2601.13122,"responsible AI, general-purpose AI, hallucinations, toxicity, stereotypes, Degree of Freedom, AI alignment, retrieval-augmented generation, reasoning enhancements","This paper reviews the risks and vulnerabilities of modern general-purpose AI systems, which are capable of performing various tasks like writing text articles, generating and debugging codes, querying databases, and translating languages. It discusses the challenges and proposes C 2V2 desiderata (Control, Consistency, Value, Veracity) for future general-purpose AI systems, and compares them with traditional task-specific AI systems.",12.87,Qwen2.5-3B,Apple M1 (Metal)
2601.13142v1_TVWorld Foundations for Remote-Control TV Agents.pdf,TVWorld: Foundations for Remote-Control TV Agents,"Zhantao Ma1*, Quanfeng Lu1*, Shuai Zhong1, Dahai Yu3, Ping Luo1, Michael K. Ng 2†",,1234567890,"Large Vision-Language Models, Remote-Control Interaction, TV Navigation, Graph-Based Abstraction, Focus-Awareness, Topology-Awareness","Recent large vision-language models have demonstrated strong potential for device control. However, existing research has primarily focused on point-and-click interaction, while remote-control interaction commonly encountered in everyday TV usage remains largely underexplored. To fill this gap, we introduce TVWorld, an offline graph-based abstraction of real-world TV navigation that enables reproducible and deployment-free evaluation. On this basis, we derive two complementary benchmarks that comprehensively assess TV-use capabilities: TVWorld-N for topology-aware navigation and TVWorld-G for focus-aware grounding. These benchmarks expose a key limitation of existing agents: insufficient topology awareness for focus-based, long-horizon TV navigation. Motivated by this finding, we propose a Topology-Aware Training framework that injects topology awareness into LVLMs. Using this framework, we develop TVTheseus, a foundation model specialized for TV navigation. TVTheseus achieves a success rate of 68.3% on TVWorld-N, surpassing strong closed-source baselines such as Gemini 3 Flash and establishing state-of-the-art (SOTA) performance.",12.92,Qwen2.5-3B,Apple M1 (Metal)
2601.13160v1_Training instability in deep learning follows low-.pdf,Training instability in deep learning follows low-dimensional dynamical principles,"Zhipeng Zhang, Zhenjie Yao, Kai Li, Lei Yang",Not found,2601.13160,"training instability, deep learning, reinforcement learning, large language models, dynamical systems","Deep learning systems achieve remarkable empirical performance, yet the stability of the training process itself remains poorly understood. Training unfolds as a high-dimensional dynamical system in which small perturbations to optimization, data, parameters, or learning signals can induce abrupt and irreversible collapse, undermining reproducibility and scalability. We propose a unified dynamical perspective that characterizes training stability as an intrinsic property of learning systems, organized along four interacting dimensions: optimization, environmental/data, parametric, and learning-signal stability.",13.22,Qwen2.5-3B,Apple M1 (Metal)
2601.13166v1_From 100000 images to winning the first brain MRI .pdf,"From 100,000+ images to winning the first brain MRI foundation model challenges","Pedro  M.  Gordaliza, Jaume  Banus, Benoît  Gérin, Maxence  Wynen, Nataliia  Molchanova, Jonas  Richiardi, Meritxell  Bach  Cuadra",,,"medical image analysis, foundation models, 3D brain MRI, self-supervised learning, fine-tuning, neuroimaging, brain pathology, brain MRI challenges, radiological tasks","Developing Foundation Models for medical image analysis is essential to overcome the unique challenges of radiological tasks. Our solution ranked first in tracks of both contests, relying on a U-Net CNN architecture combined with strategies leveraging anatomical priors and neuroimaging domain knowledge. Notably, our models trained 1-2 orders of magnitude faster and were 10× smaller than competing transformer-based approaches. Models are available here.",10.91,Qwen2.5-3B,Apple M1 (Metal)
2601.13186v1_Prompt Injection Mitigation with Agentic AI Nested.pdf,"Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching","Diego Gosmar, Deborah A. Dahl",Not found,2601.13186,"Prompt Injection, Large Language Models, Multi-Agent Systems, Semantic Caching, Nested Learning, AI Sustainability","This paper extends the evaluation framework for prompt injection mitigation with semantic similarity-based caching, a dedicated fourth-agent rule-based evaluator, and a fifth metric (Observability Score Ratio) to yield TIVS-O. It investigates how defence effectiveness interacts with transparency in a HOPE-inspired Nested Learning architecture.",12.48,Qwen2.5-3B,Apple M1 (Metal)
2601.13187v1_Scientific production in the era of Large Language.pdf,Scientific production in the era of Large Language Models,"Keigo Kusumegi, Xinyu Yang, Paul Ginsparg, Mathijs de Vaan, Toby Stuart, Yian Yin",10.1126/science.adw3000,,"Large Language Models, Scientific research, Paper production, Writing complexity, Scientific evaluation","Large Language Models are reshaping scientific research, leading to increased paper production and changes in manuscript quality and citation patterns.",12.87,Qwen2.5-3B,Apple M1 (Metal)
2601.13197v1_Diffusion-Driven Synthetic Tabular Data Generation.pdf,Diffusion-Driven Synthetic Tabular Data Generation for Enhanced DoS/DDoS Attack Classification,"Aravind B, Anirud R.S., Sai Surya Teja N, Bala Subrahmanya Sriranga Navaneeth A, Karthika R, Mohankumar N",,,"Diffusion models, Tabular data, Synthetic data generation, DoS/DDoS attacks, Class imbalance, Data augmentation, Network intrusion detection","This paper addresses class imbalance in network intrusion detection using Tabular Denoising Diffusion Probability Models (TabDDPM) for data augmentation. It synthesizes high-fidelity minority-class samples from the CIC-IDS2017 dataset through iterative denoising processes, enabling an ANN classifier to achieve near-perfect recall on previously underrepresented attack classes. These results establish diffusion models as an effective solution for tabular data imbalance in security domains, with potential applications in fraud detection and medical diagnostics.",12.02,Qwen2.5-3B,Apple M1 (Metal)
2601.13206v1_Real-Time Deadlines Reveal Temporal Awareness Fail.pdf,Real-Time Deadlines Reveal Temporal Awareness Failures in LLM Strategic Dialogues,"Neil Sehgal, Sharath Chandra Guntuku, Lyle Ungar",,,"Large Language Models, LLMs, Temporal Awareness, Strategic Dialogues, Real-Time Deadlines","Large Language Models (LLMs) generate text token-by-token in discrete time, yet real-world communication critically depends on continuous time constraints. Current LLM architectures and evaluation protocols rarely test for temporal awareness under real-time deadlines. We investigate how LLMs adjust their behavior in time-sensitive settings using simulated negotiations between paired agents under strict deadlines. The results suggest LLMs struggle to internally track elapsed time, but achieve near-perfect deal closure rates under turn-based limits, revealing the failure is in temporal tracking rather than strategic reasoning.",11.77,Qwen2.5-3B,Apple M1 (Metal)
2601.13217v1_Beyond Single-shot Writing Deep Research Agents ar.pdf,Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn Report Revision,"Bingsen Chen1,2,∗, Boyan Li3,†, Ping Nie5, Yuyu Zhang6, Xi Ye3,4, Chen Zhao1,2,‡",Not found,2601.13217,"Deep Research Agents, Multi-turn report revision, Human research practice, Report generation, User feedback","This paper introduces MRDRE, an evaluation suite for Deep Research Agents (DRAs) that establishes multi-turn report revision as a new evaluation axis. It analyzes five diverse DRAs and reveals a critical limitation: while agents can address most user feedback, they also regress on 16–27% of previously covered content and citation quality. The best-performing agents still leave significant headroom, disrupting content outside the feedback's scope and failing to preserve earlier edits. The issues are not easily resolvable through inference-time fixes such as prompt engineering and a dedicated sub-agent for report revision.",11.97,Qwen2.5-3B,Apple M1 (Metal)
2601.13222v1_Incorporating QA Nuggets into Retrieval-Augmented .pdf,Incorporating Q&A Nuggets into Retrieval-Augmented Generation,"Laura Dietz1, Bryan Li2, Gabrielle Liu 3, Jia-Huei Ju 4, Eugene Yang5, Dawn Lawrie5, William Walden5, James Mayfield 5",Not found,2601.13222,"RAG, LLM judge, nugget-based evaluation","This paper presents Crucible, a Nugget-Augmented Generation System that integrates ideas from automatic evaluation into Retrieval-augmented Generation. Crucible constructs a bank of Q&A nuggets from retrieved documents to guide extraction, selection, and report generation, avoiding repeated information through clear and interpretable Q&A semantics. Evaluated on the TREC NeuCLIR 2024 collection, Crucible substantially outperforms Ginger, a recent nugget-based RAG system, in nugget recall, density, and citation grounding.",13.11,Qwen2.5-3B,Apple M1 (Metal)
2601.13227v1_Insider Knowledge How Much Can RAG Systems Gain fr.pdf,Insider Knowledge: How Much Can RAG Systems Gain from Evaluation Secrets?,"Laura Dietz1 , Bryan Li 2 , Eugene Yang3 , Dawn Lawrie3 , William Walden3 , James Mayfield 3",,2601.13227v1,"Retrieval-augmented generation, LLM judge, Nugget evaluation","RAG systems are increasingly evaluated and optimized using LLM judges, an approach that is becoming dominant. This integration can lead to genuine improvements but also to faulty measurements due to circularity. The authors investigate this risk through comparative experiments with nugget-based RAG systems, including Ginger and Crucible, against strong baselines such as GptResearcher. By deliberately modifying Crucible to generate outputs optimized for an LLM judge, they show that near-perfect evaluation scores can be achieved when elements of the evaluation are leaked or can be predicted. The results highlight the importance of blind evaluation settings and methodological diversity to guard against mistaking metric overfitting for genuine system progress.",13.29,Qwen2.5-3B,Apple M1 (Metal)
2601.13228v1_Autoregressive Models Rival Diffusion Models at AN.pdf,AUTOREGRESSIVEMODELSRIVAL DIFFUSIONMODELS ATANY-ORDER GENERATION,"Tianqi Du, Lizhe Fang, Weijie Yang, Chenheng Zhang, Zeming Wei, Yifei Wang, Yisen Wang",Not found,Not found,"Diffusion models, Autoregressive models, Any-order generation, Bidirectional conditioning, Language modeling, Text generation","This work addresses the limitations of diffusion language models by revisiting autoregressive modeling as a foundation and reformulating diffusion-style training into a structured multi-group prediction process. The proposed Any-order Any-subset Autoregressive (A3) framework extends the standard AR factorization to arbitrary token groups and generation orders, preserving the probabilistic rigor and multi-layer dependency modeling of AR while inheriting diffusion models' flexibility for parallel and bidirectional generation. Experiments demonstrate that A3 outperforms diffusion-based models while maintaining flexible decoding.",12.34,Qwen2.5-3B,Apple M1 (Metal)
2601.13233v1_RAG A Random-Forest-Based Generative Design Framew.pdf,RAG: A Random-Forest-Based Generative Design Framework for Uncertainty-Aware Design of Metamaterials with Complex Functional Requirements,"Bolin Chen, Dex Doksoo Lee, Wei “Wayne” Chen, Wei Chen",Not found,2601.13233,"Random forest, Generative design, Functional response, Uncertainty quantification","This paper introduces a RAndom-forest-based Generative approach (RAG) to address the challenges of inverse design for metamaterials with complex functional requirements, focusing on high-dimensional and condition-dependent responses. RAG leverages the small-data compatibility of random forests and reformulates the forward mapping to enable data-efficient predictions of functional responses during inverse design. It estimates the likelihood of solutions conditioned on design requirements and addresses the one-to-many mapping through single-shot design generation. The approach is demonstrated on acoustic and mechanical metamaterials, showcasing its data-efficiency and trustworthiness.",12.44,Qwen2.5-3B,Apple M1 (Metal)
2601.13235v1_RubRIX Rubric-Driven Risk Mitigation in Caregiver-.pdf,RubRIX: Rubric-Driven Risk Mitigation in Caregiver-AI Interactions,"Drishti Goel1, Jeongah Lee2, Qiuyue Joy Zhong2, Violeta J. Rodriguez1, Daniel S. Brown3, Ravi Karkar 2, Dong Whi Yoo 4, Koustuv Saha 1",,,"AI, Caregiving, Risk Mitigation, Large Language Models, Ethics of Care","This paper introduces RubRIX, a theory-driven, clinician-validated framework for evaluating risks in Large Language Model (LLM) caregiving responses. Grounded in the Elements of an Ethic of Care, RubRIX operationalizes five empirically-derived risk dimensions: Attention, Bias & Stigma, Information Inaccuracy, Uncritical Affirmation, and Epistemic Arrogance. The authors evaluate six state-of-the-art LLMs on over 20,000 caregiver queries from Reddit and ALZConnected, demonstrating that Rubric-guided refinement consistently reduces risk-components by 45-98% after one iteration across models. This work contributes a methodological approach for developing domain-sensitive, user-centered evaluation frameworks for high-burden contexts, highlighting the importance of domain-sensitive, interactional risk evaluation for the responsible deployment of LLMs in caregiving support contexts.",12.64,Qwen2.5-3B,Apple M1 (Metal)
2601.13236v1_Pixelwise Uncertainty Quantification of Accelerate.pdf,Pixelwise Uncertainty Quantiﬁcation of Accelerated MRI Reconstruction,"Ilias I. Giannakopoulos, Lokesh B Gautham Muthukumar, Yvonne W. Lui, Riccardo Lattanzi",Not found,Not found,"Conformal Prediction, Magnetic Resonance Imaging, Parallel Imaging, Quantile Regression, Uncertainty Quantification","This work introduces a general framework for pixel-wise uncertainty quantification in parallel MRI reconstructions, enabling automatic identification of unreliable regions without access to any ground-truth reference image. The method integrates conformal quantile regression with image reconstruction methods to estimate statistically rigorous pixel-wise uncertainty intervals. The framework is evaluated on Cartesian undersampled brain and knee data from the fastMRI dataset using acceleration factors ranging from 2 to 10, and an end-to-end Variational Network is used for image reconstruction. Quantitative experiments demonstrate strong agreement between predicted uncertainty maps and true reconstruction error, with a Pearson correlation coefficient higher than 90% at acceleration levels at and above four-fold.",12.05,Qwen2.5-3B,Apple M1 (Metal)
2601.13238v1_A Semantic Decoupling-Based Two-Stage Rainy-Day At.pdf,A Semantic Decoupling–Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision–Language Models,"Chengyin Hu, Xiang Chen, Zhe Jia, Weiwen Shi, Fengyu Zhang, Jiujiang Guo, Yiwei Wei",,,"Vision-Language Models, Rainy-Day Attack, Semantic Decoupling, Weather Robustness, Cross-Modal Semantic Alignment","This paper introduces a two-stage adversarial framework to analyze how Vision-Language Models (VLMs) handle real-world weather conditions, specifically rain. It demonstrates that even physically plausible, highly constrained weather perturbations can lead to substantial semantic misalignment, posing potential safety and reliability risks in real-world deployment.",11.25,Qwen2.5-3B,Apple M1 (Metal)
2601.13240v1_KOCO-BENCH Can Large Language Models Leverage Doma.pdf,KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?,"Xue Jiang, Jiaru Qian, Xianjie Shi, Chenjie Li, Hao Zhu, Ziyu Wang, Jielun Zhang, Zheyu Zhao, Kechi Zhang, Jia Li, Wenpin Jiao, Zhi Jin, Ge Li, Yihong Dong",Not provided,Not provided,"Large Language Models, Domain Knowledge, Software Development, Benchmark, Evaluation","This paper presents KOCO-BENCH, a novel benchmark designed to evaluate domain specialization methods in real-world software development. KOCO-BENCH includes 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora and multi-granularity evaluation tasks. The authors reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs, even with domain specialization methods applied, and highlight the urgent need for more effective domain specialization methods.",12.49,Qwen2.5-3B,Apple M1 (Metal)
2601.13247v1_Aligning Agentic World Models via Knowledgeable Ex.pdf,Aligning Agentic World Models via Knowledgeable Experience Learning,"Baochang Ren, Yunzhi Yao, Rui Sun, Shuofei Qiao, Ningyu Zhang, Huajun Chen",Not found,2601.13247,"Large Language Models, World Models, Embodied Intelligence, Physical Grounding, Process Experience, Goal Experience","Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which struggle to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. WorldMind introduces a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback, unifying Process Experience and Goal Experience to achieve superior performance and cross-model, cross-environment transferability.",12.06,Qwen2.5-3B,Apple M1 (Metal)
2601.13260v1_Stop Taking Tokenizers for Granted They Are Core D.pdf,Stop Taking Tokenizers for Granted: They Are Core Design Decisions in Large Language Models,"Sawsan Alqahtani†♠*, Mir Tafseer Nayeem♣*, Md Tahmid Rahman Laskar♦♡, Tasnim Mohiuddin♢, M Saiful Bari♥",Not found,Not found,"Large Language Models, Tokenization, Subword Tokenization, Byte Pair Encoding, Fairness, Efficiency, Adaptability","This paper reframes tokenization as a core modeling decision rather than a preprocessing step, arguing for a context-aware framework that integrates tokenizer and model co-design, guided by linguistic, domain, and deployment considerations. Standardized evaluation and transparent reporting are essential to make tokenization choices accountable and comparable.",11.19,Qwen2.5-3B,Apple M1 (Metal)
2601.13262v1_CURE-Med Curriculum-Informed Reinforcement Learnin.pdf,CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning,"Eric Onyame∗, Akash Ghosh∗, Subhadip Baidya, Sriparna Saha, Xiuying Chen, Chirag Agarwal",Not found,Not found,"Curriculum-Informed Reinforcement Learning, Multilingual Medical Reasoning, Code-Switching, Group Relative Policy Optimization, Language Consistency, Logical Correctness","This paper introduces CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries. It proposes CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to improve logical correctness and language stability across thirteen languages.",12.04,Qwen2.5-3B,Apple M1 (Metal)
2601.13268v1_Improving the Safety and Trustworthiness of Medica.pdf,Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation Loops,"Zainab Ghafoor, Md Shafiqul Islam, Koushik Howlader, Md Rasel Khondokar, Tanusree Bhattacharjee, Sayantan Chakraborty, Adrito Roy, Ushashi Bhattacharjee, Tirtho Roy",Not found,Not found,"Medical AI, Large Language Models, Multi-Agent Systems, Ethical Compliance, Safety Assessment","This work introduces a multi-agent refinement framework to enhance the safety and reliability of medical Large Language Models (LLMs) through structured, iterative alignment. The system combines two generative models—DeepSeek R1 and Med-PaLM—with two evaluation agents, LLaMA 3.1 and Phi-4, which assess responses using the American Medical Association’s (AMA) Principles of Medical Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol. The study evaluates performance across 900 clinically diverse queries spanning nine ethical domains, measuring convergence efficiency, ethical violation reduction, and domain-specific risk behavior. Results demonstrate that DeepSeek R1 achieves faster convergence (mean 2.34 vs. 2.67 iterations), while Med-PaLM shows superior handling of privacy-sensitive scenarios. The iterative multi-agent loop achieved an 89% reduction in ethical violations and a 92% risk downgrade rate, underscoring the effectiveness of the approach.",12.86,Qwen2.5-3B,Apple M1 (Metal)
2601.13295v1_CooperBench Why Coding Agents Cannot be Your Teamm.pdf,CooperBench: Why Coding Agents Cannot be Your Teammates Yet,"CooperBench, Arpandeep Khatua, Hao Zhu, Peter Tran, Arya Prabhudesai, Frederic Sadrieh, Johann K. Lieberwirth, Xinkai Yu, Yicheng Fu, Michael J. Ryan, Jiaxin Pei, Diyi Yang",2026-1-21,CooperBench,"CooperBench, coding agents, teamwork, coordination, conflicts, compatibility, expert execution, virtual machines, chat between agents, agent execution, agent communication, agent compatibility, agent tasks, agent performance","CooperBench is a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages, designed to test the coordination and compatibility of AI agents. The benchmark reveals that agents achieve on average 30% lower success rates when working together compared to performing both tasks individually, highlighting the challenges of coordination and compatibility in AI teamwork.",10.74,Qwen2.5-3B,Apple M1 (Metal)
2601.13317v1_Paid Voices vs. Public Feeds Interpretable Cross-P.pdf,Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme Modeling of Climate Discourse,"Samantha Sudhoff*, Pranav Perumal, Zhaoqing Wu*, Tunazzina Islam*",Not provided,Not provided,"Climate communication, Paid advertising, Public social media, Thematic analysis, Natural language processing","Climate discourse online plays a crucial role in shaping public understanding of climate change and influencing political and policy outcomes. However, climate communication unfolds across structurally distinct platforms with fundamentally different incentive structures. This work presents a comparative analysis of climate discourse across paid advertisements on Meta (previously known as Facebook) and public posts on Bluesky from July 2024 to September 2025, using an interpretable, end-to-end thematic discovery and assignment framework.",11.51,Qwen2.5-3B,Apple M1 (Metal)
2601.13327v1_PepEDiff Zero-Shot Peptide Binder Design via Prote.pdf,PepEDiﬀ: Zero-Shot Peptide Binder Design via Protein Embedding Diffusion,"Po-Yu Liang, Tibo Duran, Jun Bai",,2601.13327,"Deep Learning, Drug Discovery, Protein Design","We present PepEDiﬀ, a novel peptide binder generator that designs binding sequences given a target receptor protein sequence and its pocket residues, improving structural and sequence diversity by generating sequences directly in a continuous latent space derived from a pretrained protein embedding model, without relying on predicted structures.",12.54,Qwen2.5-3B,Apple M1 (Metal)
2601.13348v1_The AI Genie Phenomenon and Three Types of AI Chat.pdf,"The AI Genie Phenomenon and Three Types of AI Chatbot Addiction: Escapist Roleplays, Pseudosocial Companions, and Epistemic Rabbit Holes","M. Karen Shen, Jessica Huang, Olivia Liang, IG-JAE KIM, DONGWOOK YOON",Not found,Not found,"AI chatbot, addiction, addictive use, technology addiction, escapist roleplay, pseudosocial companion, epistemic rabbit hole","Recent reports on generative AI chatbot use raise concerns about its addictive potential. This study examines how to characterize AI chatbot addiction—why users become addicted, the symptoms commonly reported, and the distinct types it comprises.",11.15,Qwen2.5-3B,Apple M1 (Metal)
2601.13352v1_LLM-as-RNN A Recurrent Language Model for Memory U.pdf,LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction,"Yuxing Lu*12, J. Ben Tamo*1, Weichen Zhao3, Nan Sun4, Yishan Zhong1, Wenqi Shi5, Jinzhuo Wang†2, May D. Wang†1",,,"Large language models, Recurrent Neural Networks, Long Short-Term Memory, Transformer architectures, In-Context Learning, Memory updates, Sequence prediction, Adaptive prediction","This paper proposes LLM-as-RNN, an inference-only framework that turns a frozen Large Language Model (LLM) into a recurrent predictor by representing its hidden state as natural-language memory. This state, updated via feedback-driven text rewrites, enables learning without parameter updates, effectively performing online learning through language. The method is evaluated on three sequential benchmarks in healthcare, meteorology, and finance across different model families, significantly outperforming baseline methods.",11.55,Qwen2.5-3B,Apple M1 (Metal)
2601.13358v1_The Geometry of Thought How Scale Restructures Rea.pdf,The Geometry of Thought: How Scale Restructures Reasoning in Large Language Models,Samuel Cyrenius Anderson,Not found,2601.13358,"Large Language Models, Neural Scaling Laws, Reasoning Tasks, Chain-of-Thought Prompts, Model Parameters, Parameter Increase, Geometric Reorganizations, Domain-Specific Effects","This paper investigates how scale affects reasoning in large language models. It reveals that scale restructures reasoning, triggering domain-specific phase transitions rather than uniform capability gains. The findings suggest that the geometry of thought determines the cost of thought, offering a blueprint for inference acceleration.",11.92,Qwen2.5-3B,Apple M1 (Metal)
2601.13376v1_Bounded Minds Generative Machines Envisioning Conv.pdf,"Bounded Minds, Generative Machines: Envisioning Conversational AI that Works with Human Heuristics and Reduces Bias Risk",JIQUN LIU,XXXXXXX,XXXXXXX,"Bounded Rationality, Heuristics, Conversational AI, GenAI, Evaluation","This article outlines a research pathway grounded in bounded rationality, arguing that conversational AI should be designed to work with human heuristics rather than against them, and identifies key directions for detecting cognitive vulnerability, supporting judgment under uncertainty, and evaluating conversational systems beyond factual accuracy, toward decision quality and cognitive robustness.",11.51,Qwen2.5-3B,Apple M1 (Metal)
2601.13383v1_A Lightweight Modular Framework for Constructing A.pdf,"A LIGHTWEIGHTMODULARFRAMEWORK FORCONSTRUCTING AUTONOMOUSAGENTSDRIVEN BYLARGELANGUAGE MODELS: DESIGN, IMPLEMENTATION,ANDAPPLICATIONS IN AGENTFORGE","A. A. Jafari, C. Ozcinar, G. Anbarjafari",Not found,2601.13383,"autonomous agents, large language models, modular architecture, natural language processing, software framework, task automation, artificial intelligence, open-source software","The paper presents AgentForge, a lightweight, open-source Python framework designed to democratize the construction of LLM-driven autonomous agents through a principled modular architecture. It introduces three key innovations: a composable skill abstraction, a unified LLM backend interface, and a declarative YAML-based configuration system. Comprehensive experimental evaluation demonstrates competitive task completion rates and reduced development time compared to existing frameworks.",12.77,Qwen2.5-3B,Apple M1 (Metal)
2601.13385v1_Organ-Aware Attention Improves CT Triage and Class.pdf,Organ-Aware Attention Improves CT Triage and Classification,"Lavsen Dahal, Yubraj Bhandari, Geoffrey D. Rubin, Joseph Y. Lo",,,"CT triage, classification, organ-aware attention, Vision-Language Models, 3D anatomy, radiologist burnout","This study presents ORACLE-CT, an encoder-agnostic, organ-aware head that pairs Organ-Masked Attention with Organ-Scalar Fusion, achieving state-of-the-art supervised classification performance across chest and abdomen CT under a unified evaluation protocol.",10.35,Qwen2.5-3B,Apple M1 (Metal)
2601.13392v1_Beyond Memorization Testing LLM Reasoning on Unsee.pdf,Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation Tasks,"Shlok Shelat, Jay Raval, Souvik Roy, Manas Gaur",,,"Large language models, Formal language tasks, Deterministic finite automata, Pattern matching, Symbolic reasoning, Theory of Computation","Large language models have demonstrated strong performance on formal language tasks, yet whether this reflects genuine symbolic reasoning or pattern matching on familiar constructions remains unclear. We introduce a benchmark for DFA construction from regular languages, comprising factual knowledge questions, seen construction problems from public sources, and two types of unseen problems: hand-crafted instances with multiple interacting constraints and systematically generated problems via Arden's theorem. Models achieve perfect accuracy on factual questions and 84-90% on seen tasks, but accuracy drops sharply on unseen problems by 30-64%, with failures stemming from systematic misinterpretation of language constraints, incorrect handling of Kleene-star semantics, and a failure to preserve global consistency. We evaluate a three-stage hint protocol that enables correction of shallow errors but does not reliably resolve globally inconsistent or structurally flawed automata. Our analysis across multiple prompting strategies reveals that errors persist regardless of prompting approach, exposing a fundamental gap between LLMs' ability to generate syntactically plausible DFAs and their capacity for semantically correct formal reasoning.",12.63,Qwen2.5-3B,Apple M1 (Metal)
2601.13398v1_Can LLMs Compress and Decompress Evaluating Code U.pdf,LLMs Compress (and Decompress): Evaluating Code Understanding and Execution via Invertibility,"Nickil Maveli, Antonio Vergari, Shay B. Cohen",Not provided,Not provided,"LLMs, Code Understanding, Execution Invertibility, Robust Reasoning, Forward Execution, Backward Inversion, Semantic Code Understanding","Recent progress in Code-LLMs has demonstrated remarkable performance across various software engineering applications. However, evaluating the reasoning ability of Code-LLMs requires going beyond isolated input–output predictions. This paper presents ROUNDTRIPCODEEVAL (RTCE), a comprehensive benchmark consisting of four distinct code execution reasoning tasks designed to rigorously test round-trip consistency. RTCE provides an execution-free, exact-match evaluation of bijection fidelity, assessing whether models preserve a consistent one-to-one mapping between encoding and decoding operations across various algorithms and directions. The authors systematically evaluate state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms. Each method yields modest improvements, but none closes the gap, indicating that current LLMs struggle with true round-trip consistency, which demonstrates that they lack the internal coherence required for trustworthy code reasoning. RTCE surfaces several new and previously unmeasured insights that are not captured by existing benchmarks.",12.65,Qwen2.5-3B,Apple M1 (Metal)
2601.13400v1_Deep Image Prior with L0 Gradient Regularizer for .pdf,DEEP IMAGE PRIOR WITH L0 GRADIENT REGULARIZER FOR IMAGE SMOOTHING,"Nhat Thanh Tran, Kevin Bui, Jack Xin",Not found,Not found,"image smoothing, optimization, ADMM, deep image prior, ℓ 0 gradient","Image smoothing is a fundamental image processing operation that preserves the underlying structure, such as strong edges and contours, and removes minor details and textures in an image. This paper proposes DIP-ℓ 0, a deep image prior framework that incorporates the ℓ 0 gradient regularizer, enabling high-quality image smoothing without requiring a training dataset.",11.07,Qwen2.5-3B,Apple M1 (Metal)
2601.13401v1_Reasoning with Pixel-level Precision QVLM Architec.pdf,Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics,"Peter A. Massih1,2 & Eric Cosatto1",,,"Vision-Language Models, Quantitative Spatial Reasoning, Satellite Image Analysis, Geospatial Analytics, Quantitative Geospatial Analytics","Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning due to their architectures destroying pixel-level information required for counting and measurements. This paper introduces SQuID, a benchmark dataset of 2,000 satellite image Question-Answer pairs, and QVLM, a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis.",11.3,Qwen2.5-3B,Apple M1 (Metal)
2601.13404v1_Local-to-Global Logical Explanations for Deep Visi.pdf,Local-to-Global Logical Explanations for Deep Vision Models,"Bhavan Vasu, Giuseppe Raffa, Prasad Tadepalli",Not found,2601.13404,"Explainable AI, Neurosymbolic AI, Monotone DNF, Deep Learning","While deep neural networks are extremely effective at classifying images, they remain opaque and hard to interpret. We introduce local and global explanation methods for black-box models that generate explanations in terms of human-recognizable primitive concepts.",11.9,Qwen2.5-3B,Apple M1 (Metal)
2601.13406v1_Integrating Virtual Reality and Large Language Mod.pdf,Integrating Virtual Reality and Large Language Models for Team-Based Non-Technical Skills Training and Evaluation in the Operating Room,"Jacob Barker, Doga Demirel, Cullen Jackson, Anna Johansson, Robbin Miraglia, Darian Hoagland, Stephanie B. Jones, John Mitchell, Daniel B. Jones, Suvranu De",,,"Virtual Reality, Large Language Models, Team-Based Training, Non-Technical Skills, Operating Room, Simulation","This study introduces VORTeX, a multi-user virtual reality platform integrating immersive team simulation with large language model analytics to train and evaluate communication, decision-making, teamwork, and leadership in surgical professionals during laparoscopic emergencies.",13.4,Qwen2.5-3B,Apple M1 (Metal)
2601.13412v1_Using deep learning for predicting cleansing quali.pdf,Using deep learning for predicting cleansing quality of colon capsule endoscopy images,"Puneet Sharma, Kristian Dalsbø Hindberg, Benedicte Schelde-Olesen, Ulrik Deding, Esmaeil S. Nadimi, Jan-Matthias Braun, on behalf of the AICE consortium",Not found,2601.13412,"deep learning, colon capsule endoscopy, image prediction, cleansing quality","In this study, we explore the application of deep learning techniques for predicting cleansing quality in colon capsule endoscopy (CCE) images using a dataset of 500 images labeled by 14 clinicians on the Leighton–Rex scale.",13.12,Qwen2.5-3B,Apple M1 (Metal)
2601.13422v1_TrustEnergy A Unified Framework for Accurate and R.pdf,TrustEnergy: A Unified Framework for Accurate and Reliable User-level Energy Usage Prediction,"Dahai Yu, Rongchao Xu, Dingyi Zhuang, Yuheng Bu, Shenhao Wang, Guang Wang",,,"Energy prediction, Deep learning, Hierarchical spatiotemporal representation, Sequential conformalized quantile regression, User-level prediction",This paper proposes a unified framework called TrustEnergy for accurate and reliable user-level energy usage prediction. It includes two key technical components: a Hierarchical Spatiotemporal Representation module and an innovative Sequential Conformalized Quantile Regression module. The framework achieves a 5.4% increase in prediction accuracy and 5.7% improvement in uncertainty quantification compared to state-of-the-art baselines.,11.38,Qwen2.5-3B,Apple M1 (Metal)
2601.13435v1_A Learnable Wavelet Transformer for Long-Short Equ.pdf,A Learnable Wavelet Transformer for Long-Short Equity Trading and Risk-Adjusted Return Optimization,"Shuozhe Li ∗, Du Cheng ∗, Leqi Liu",Not found,Not found,"Neural wavelet regularization, wavelet-transformer network, low-guided high-frequency injection, return optimization","Learning profitable intraday trading policies from financial time series is challenging due to heavy noise, non-stationarity, and strong cross-sectional dependence among related assets. We propose WaveLSFormer, a learnable wavelet-based long-short Transformer that jointly performs multi-scale decomposition and return-oriented decision learning. The model outputs a portfolio of long/short positions that is rescaled to satisfy a fixed risk budget, and is optimized directly with a trading objective and risk-aware regularization. Extensive experiments demonstrate that WaveLSFormer consistently outperforms MLP, LSTM, and Transformer backbones, with and without fixed discrete wavelet front-ends.",12.26,Qwen2.5-3B,Apple M1 (Metal)
2601.13437v1_MOSLD-Bench Multilingual Open-Set Learning and Dis.pdf,MOSLD-Bench: Multilingual Open-Set Learning and Discovery,"Adriana-Valentina Costache, Daria-Nicoleta Dragomir, Silviu-Florin Gheorghe, Eduard Poesina, Paul Irofti, Radu Tudor Ionescu",Not found,Not found,"multilingual, open-set learning, discovery, text categorization","The paper introduces the first multilingual open-set learning and discovery (MOSLD) benchmark for text categorization, comprising 960K data samples across 12 languages. It proposes a novel framework for the OSLD task, integrating multiple stages to continuously discover and learn new classes. The benchmark is constructed by rearranging existing datasets and collecting new data samples from the news domain. The authors evaluate several language models, including their own, to obtain results that can serve as a reference for future work.",11.34,Qwen2.5-3B,Apple M1 (Metal)
2601.13443v1_Explicit Cognitive Allocation A Principle for Gove.pdf,Explicit Cognitive Allocation: A Principle for Governed and Auditable Inference in Large Language Models,"Héctor Manuel Manzanilla-Granados, Zaira Navarrete-Cazales, Miriam Pescador-Rojas, Tonahtiu Ramírez-Romero",Not found,Not found,"Large Language Models, AI-assisted reasoning, Cognitive allocation, Epistemic functions, Cognitive Universal Agent, Universal Cognitive Instruments","The rapid adoption of large language models has enabled new forms of AI-assisted reasoning. However, prevailing modes of LLM use remain cognitively unstructured, leading to limitations in traceability, epistemic control, and reproducibility. This paper introduces Explicit Cognitive Allocation as a principle for structuring AI-assisted inference through the explicit separation and orchestration of epistemic functions, formalizing abstract inquiries into investigable forms. Controlled comparisons between CUA-orchestrated inference and baseline LLM inference demonstrate the benefits of explicit cognitive and instrumental allocation, including earlier and structurally governed epistemic convergence, higher epistemic alignment under semantic expansion, and systematic exposure of the instrumental landscape of inquiry.",13.53,Qwen2.5-3B,Apple M1 (Metal)
2601.13458v1_Labels or Preferences Budget-Constrained Learning .pdf,Budget-Constrained Learning with Human Judgments over AI-Generated Outputs,"Zihan Dong, Ruijia Wu, Linjun Zhang",Not found,2601.13458,"Budget-Constrained Learning, Human Judgments, AI-generated Outputs, Preference Data, Active Learning","This work addresses the need for principled, budget-conscious data acquisition strategies in AI, particularly in the context of combining ground-truth labels and human preference data. It introduces Preference-Calibrated Active Learning (PCAL), a method that optimally allocates a fixed annotation budget between ground-truth labels and pairwise preferences, ensuring robust performance even with poorly estimated nuisance models.",12.64,Qwen2.5-3B,Apple M1 (Metal)
2601.13462v1_SpatialBench-UC Uncertainty-Aware Evaluation of Sp.pdf,SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt,Amine Rostane,Not found,2601.13462,"Text-to-Image Generation, Spatial Evaluation, Uncertainty, Reproducibility","Evaluating whether text to image models follow explicit spatial instructions is difficult to automate. Object detectors may miss targets or return multiple plausible detections, and simple geometric tests can become ambiguous in borderline cases. Spatial evaluation is naturally a selective prediction problem, a checker should be allowed to abstain when evidence is weak and should report confidence so results can be interpreted as a risk–coverage trade-off rather than a single score.",12.01,Qwen2.5-3B,Apple M1 (Metal)
2601.13464v1_Context and Transcripts Improve Detection of Deepf.pdf,Context and Transcripts Improve Detection of Deepfake Audios,"Chongyang Gao, Marco Postiglione, Julian Baldwin, Natalia Denisenko, Isabel Gortner, Luke Fosdick, Chiara Pulice, Sarit Kraus, V. S. Subrahmanian",not found,not found,"deepfake, audio, context, transcript, journalist, public figures","Humans use context to assess the veracity of information. However, current audio deepfake detectors only analyze the audio file without considering either context or transcripts. We create and analyze a Journalist-provided Deepfake Dataset (JDD) of 255 public deepfakes which were primarily contributed by over 70 journalists since early 2024. We also generate a synthetic audio dataset (SYN) of dead public figures and propose a novel Context-based Audio Deepfake Detector (CADD) architecture. In addition, we evaluate performance on two large-scale datasets: ITW and P2V. We show that sufficient context and/or the transcript can significantly improve the efficacy of audio deepfake detectors. Performance (measured via F1 score, AUC, and EER) of multiple baseline audio deepfake detectors and traditional classifiers can be improved by 5%-37.58% in F1-score, 3.77%-42.79% in AUC, and 6.17%-47.83% in EER. We additionally show that CADD, via its use of context and/or transcripts, is more robust to 5 adversarial evasion strategies, limiting performance degradation to an average of just -0.71% across all experiments. Code, models, and datasets are available at our project page: https://sites.northwestern.edu/nsail/cadd-context-based-audio-deepfake-detection/(access restricted during review).",13.1,Qwen2.5-3B,Apple M1 (Metal)
2601.13465v1_Graph Neural Networks are Heuristics.pdf,Graph Neural Networks are Heuristics,"Yimeng Min, Carla P. Gomes",Not found,2601.13465,"Graph Neural Networks, Heuristics, Combinatorial Optimization, Travelling Salesman Problem, Neural Networks, Machine Learning","This paper demonstrates that a single training trajectory can transform a graph neural network into an unsupervised heuristic for combinatorial optimization, focusing on the Travelling Salesman Problem. It shows that encoding global structural constraints as an inductive bias enables a non-autoregressive model to generate solutions via direct forward passes, without search, supervision, or sequential decision-making.",11.46,Qwen2.5-3B,Apple M1 (Metal)
2601.13474v1_Preconditioning Benefits of Spectral Orthogonaliza.pdf,Preconditioning Benefits of Spectral Orthogonalization in Muon,"Jianhao Ma, Yu Huang, Yuejie Chi, Yuxin Chen",Not provided,2601.13474,"Muon optimizer, matrix optimization, spectral orthogonalization, preconditioning","The Muon optimizer, a matrix-structured algorithm leveraging spectral orthogonalization of gradients, demonstrates superior convergence properties compared to gradient descent and Adam, especially in matrix factorization and in-context learning of linear transformers.",11.86,Qwen2.5-3B,Apple M1 (Metal)
2601.13476v1_A Unified Variational Imputation Framework for Ele.pdf,A Unified Variational Imputation Framework for Electric Vehicle Charging Data,"Jinhao Li, Hao Wang",Not found,Not found,"Electric vehicle, data imputation, charging demand, large language model, retrieval-augmented generation","The reliability of data-driven applications in electric vehicle (EV) infrastructure, such as charging demand forecasting, hinges on the availability of complete, high-quality charging data. However, real-world EV datasets are often plagued by missing records, and existing imputation methods are ill-equipped for the complex, multimodal context of charging data, often relying on a restrictive one-model-per-station paradigm that ignores valuable inter-station correlations. To address these gaps, we develop a novel probabilistic variational imputation framework that leverages the power of large language models and retrieval-augmented memory (PRAIM). PRAIM employs a pre-trained language model to encode heterogeneous data, spanning time-series demand, calendar features, and geospatial context, into a unified, semantically rich representation. This is dynamically fortified by retrieval-augmented memory that retrieves relevant examples from the entire charging network, enabling a single, unified imputation model empowered by variational neural architecture to overcome data sparsity. Extensive experiments on four public datasets demonstrate that PRAIM significantly outperforms established baselines in both imputation accuracy and its ability to preserve the original data’s statistical distribution, leading to substantial improvements in downstream forecasting performance.",13.14,Qwen2.5-3B,Apple M1 (Metal)
2601.13481v1_Towards Efficient and Robust Linguistic Emotion Di.pdf,Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement,"Jian Zhang, Zhangqi Wang, Zhiyuan Wang, Weiping Fu, Yu He, Haiping Zhu∗, Qika Lin∗, Jun Liu",Not found,Not found,"Linguistic Emotion Diagnosis, Emotional Comorbidity, Inefficient Exploration, Automated Prompt Optimization, Multi-Agent Collaboration, Medical Language Processing, Trustworthy Artificial Intelligence","This paper proposes APOLO, a framework that systematically explores a broader and finer-grained prompt space to enhance diagnostic efficiency and robustness for linguistic emotion diagnosis in mental health applications. It addresses issues of emotional comorbidity and inefficient exploration, using a multi-agent collaboration mechanism to refine prompts and optimize diagnostic accuracy and reliability.",12.39,Qwen2.5-3B,Apple M1 (Metal)
2601.13487v1_The Hidden Toll of Social Media News Causal Effect.pdf,The Hidden Toll of Social Media News: Causal Effects on Psychosocial Wellbeing,"Olivia Pal, Agam Goyal, Eshwar Chandrasekharan, Koustuv Saha",,,"social media, news consumption, psychosocial wellbeing, depression, stress, anxiety, lone-liness, social interaction","This study examines the psychosocial effects of news consumption on social media, revealing that while news engagement can lead to increased depression, stress, and anxiety, it also decreases loneliness and increases social interaction. The findings suggest that news feed bookmarking is associated with greater psychosocial deterioration compared to commenting or quoting, with magnitude differences exceeding tenfold. These per-engagement effects accumulate with repeated exposure, indicating significant psychosocial impacts.",11.71,Qwen2.5-3B,Apple M1 (Metal)
2601.13508v1_CatMaster An Agentic Autonomous System for Computa.pdf,CatMaster: An Agentic Autonomous System for Computational Heterogeneous Catalysis Research,"Honghao Chen, Jiangjie Qiu, Yi Shen Tew, Xiaonan Wang ∗",,2601.13508,"autonomous system, computational chemistry, heterogeneous catalysis, density functional theory, machine learning","CatMaster is an LLM-driven agent system designed to streamline computational heterogeneous catalysis studies, addressing issues of long workflows, consistency, and reproducibility.",13.31,Qwen2.5-3B,Apple M1 (Metal)
2601.13515v1_Automatic Adjustment of HPA Parameters and Attack .pdf,Automatic Adjustment of HPA Parameters and Attack Prevention in Kubernetes Using Random Forests,"Huah Yong Chan, Hanlin Zhou, Jingfei Ni, Mengchun Wu, Qing Deng",Not found,2601.13515,"Kubernetes, HPA, Security, Random Forest","In this paper, HTTP status codes are used as custom metrics within the HPA to dynamically adjust the maximum pod parameter in response to attacks, managed by integrating the Random Forest classification algorithm from machine learning. This approach enables targeted adjustment of HPA parameters using machine learning scripts, effectively managing attack traffic and redirecting all access from attacking IPs to honeypot pods to lower the incidence of 5XX status codes under high load conditions. The method also ensures effective isolation of attack traffic and prevents excessive HPA expansion due to attacks. Experiments demonstrate the importance of setting appropriate thresholds for HPA adjustments.",13.44,Qwen2.5-3B,Apple M1 (Metal)
2601.13518v1_AgenticRed Optimizing Agentic Systems for Automate.pdf,AGENTICRED: Optimizing Agentic Systems for Automated Red-teaming,"Jiayi Yuan*, Jonathan Nöther, Natasha Jaques, Goran Radanović",,,"Automated red-teaming, Agentic systems, Large Language Models (LLMs), System design, Evolutionary selection, Transferability","This paper introduces AGENTICRED, an automated pipeline that leverages LLMs' in-context learning to iteratively design and refine red-teaming systems without human intervention. It aims to address the limitations of existing human-specified workflows by treating red-teaming as a system design problem and using evolutionary selection to evolve agentic systems. AGENTICRED outperforms state-of-the-art approaches, achieving high attack success rates on proprietary models.",11.32,Qwen2.5-3B,Apple M1 (Metal)
2601.13528v1_Eliciting Harmful Capabilities by Fine-Tuning On S.pdf,ELICITINGHARMFULCAPABILITIES BYFINE-TUNING ONSAFEGUARDEDOUTPUTS,"Jackson Kaunismaa∗, MATS, Avery Griffin, Anthropic, John Hughes, Anthropic, Christina Q Knight, Scale AI, Mrinank Sharma†, Anthropic, Erik Jones†",Not found,2601.13528,"AI safety, safeguards, elicitation attacks, hazardous chemical synthesis, open-source models, frontier models","This work demonstrates that even robustly safeguarded models can be used to elicit harmful capabilities in open-source models through elicitation attacks. The attacks consist of three stages: constructing prompts in adjacent domains to a target harmful task, obtaining responses from safeguarded frontier models, and fine-tuning open-source models on these prompt-output pairs. The attacks are evaluated within the domain of hazardous chemical synthesis and processing, showing that they can recover approximately 40% of the capability gap between the base open-source model and an unrestricted frontier model. The efficacy of the attacks scales with the capability of the frontier model and the amount of generated fine-tuning data.",12.79,Qwen2.5-3B,Apple M1 (Metal)
2601.13533v1_Reasoning While Recommending Entropy-Guided Latent.pdf,Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models,Changshuo Zhang,Not found,Not found,"Generative Re-ranking, Latent Reasoning, Reinforcement Learning","This paper introduces an entropy-guided latent reasoning mechanism to improve the accuracy of capturing complex preferences in generative re-ranking scenarios. It proposes the EGLR recommendation model, which combines real-time reasoning during list generation and entropy-guided variable-length reasoning, leading to a more precisely adapted exploration-exploitation trade-off. The model is lightweight and compatible with existing generative re-ranking models, enhancing their performance.",11.41,Qwen2.5-3B,Apple M1 (Metal)
2601.13534v1_MN-TSGContinuous Time Series Generation with Irreg.pdf,MN-TSG: CONTINUOUSTIMESERIESGENERATION WITH IRREGULAROBSERVATIONS,"Xu Zhang, Junwei Deng, Chang Xu, Hao Li, Jiang Bian",Not found,2601.13534,"Irregular time series, continuous time series generation, deep learning architecture","Time series generation (TSG) plays a critical role in various domains. Most existing methods assume regularly sampled observations, which often misalign with real-world scenarios. MN-TSG proposes a novel framework integrating Mixture-of-Experts (MoE)–based Neural Controlled Differential Equations (NCDEs) with existing TSG models for irregular and continuous generation tasks.",11.78,Qwen2.5-3B,Apple M1 (Metal)
2601.13537v1_When Wording Steers the Evaluation Framing Bias in.pdf,When Wording Steers the Evaluation: Framing Bias in LLM judges,"Yerin Hwang, Dongryeol Lee, Taegwan Kang, Minwoo Lee, Kyomin Jung",Not provided,Not provided,"Large language models, Framing bias, LLM evaluation, Predicate-positive, Predicate-negative, Phishing email","This paper investigates how subtle prompt phrasing can steer the judgments of large language models (LLMs) in evaluation tasks, revealing significant discrepancies in model outputs and suggesting that framing bias is a structural property of current LLM-based evaluation systems.",10.74,Qwen2.5-3B,Apple M1 (Metal)
2601.13545v1_TruthTensor Evaluating LLMs Human Imitation throug.pdf,TRUTHTENSOR: EVALUATINGLLMSHUMANIMITATION THROUGH PREDICTIONMARKETDRIFT ANDHOLISTICREASONING,"Shirin Shahabi, Spencer Graham, Haruna Isah",[DOI string if found],ID if found,"Large Language Models, LLMs, Human Imitation, Prediction Markets, Drift, Holistic Evaluation","This paper introduces TruthTensor, a novel evaluation paradigm for Large Language Models (LLMs) that assesses them not only as prediction engines but as human-imitation systems operating in socially-grounded, high-entropy environments. It combines probabilistic scoring and live prediction markets to provide a holistic view of model behavior, complementing traditional correctness metrics with drift-centric diagnostics and robustness checks for reproducibility.",10.74,Qwen2.5-3B,Apple M1 (Metal)
2601.13546v1_ChatAD Reasoning-Enhanced Time-Series Anomaly Dete.pdf,ChatAD: Reasoning-Enhanced Time-Series Anomaly Detection with Multi-Turn Instruction Evolution,"Hui Sun1*, Chang Xu2†, Haonan Xie3, Hao Li4, Yuhao Huang5, Chuheng Zhang2, Ming Jin6, Xiaoguang Liu1, Gang Wang1, Jiang Bian2",,,"Anomaly Detection, Time Series, Multi-Turn Dialogue, LLM-driven, Reasoning, Cross-task Generalization","This paper proposes ChatAD, a multi-agent-based Time Series Evolution algorithm named TSEvol, and introduces the AD reasoning & multi-turn dialogue Dataset TSEData-20K. It also proposes the TS Kahneman-Tversky Optimization (TKTO) to enhance cross-task generalization and introduces aLLM-driven Learning-based AD Benchmark LLADBench to evaluate ChatAD and nine baselines. The three ChatAD models achieve substantial gains in accuracy and F1 score, reducing false positives by 37.42%.",11.68,Qwen2.5-3B,Apple M1 (Metal)
2601.13547v1_HateXScore A Metric Suite for Evaluating Reasoning.pdf,HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations,"Yujia Hu, Roy Ka-Wei Lee",,,"Hate Speech, Explanations, Reasoning Quality, Transparency, Automated Moderation","HateXScore is a four-component metric suite designed to evaluate the reasoning quality of model explanations in hate speech detection. It assesses conclusion explicitness, faithfulness and causal grounding of quoted spans, protected group identification, and logical consistency among these elements. Evaluated on six diverse hate speech datasets, HateXScore is intended as a diagnostic complement to reveal interpretability failures and annotation inconsistencies that are invisible to standard metrics like Accuracy or F1. Human evaluation shows strong agreement with HateXScore, validating it as a practical tool for trustworthy and transparent moderation.",11.63,Qwen2.5-3B,Apple M1 (Metal)
2601.13558v1_Leveraging ChatGPT and Other NLP Methods for Ident.pdf,Leveraging ChatGPT and Other NLP Methods for Identifying Risk and Protective Behaviors in MSM: Social Media and Dating Apps Text Analysis,"Mehrab Beikzadeh, Chenglin Hong, Cory J Cascalheira, Callisto Boka, Majid Sarrafzadeh, Ian W Holloway",Not provided,Not provided,"Men who have sex with men, MSM, Social media, Dating apps, Text analysis, Risk behaviors, Protective behaviors, HIV, Alcohol use, Harmful drinking, Machine learning, Public health, Pre-exposure prophylaxis, ChatGPT, BERT, LIWC","This study aims to determine if text data from social media and dating apps can predict risk and protective behaviors among men who have sex with men (MSM). The authors collected textual data with user consent and trained machine learning models to identify various risk behaviors such as condomless anal sex, number of sexual partners, binge drinking, and heavy drinking. The model was highly predictive of monthly binge drinking and having over 5 sexual partners, but slightly less predictive of PrEP use and heavy drinking. ChatGPT embeddings were found to be highly informative in prediction, and combining ChatGPT embeddings with LIWC and BERT and using the most correlated features improved performance.",12.87,Qwen2.5-3B,Apple M1 (Metal)
2601.13559v1_AgentGC Evolutionary Learning-based Lossless Compr.pdf,AgentGC: Evolutionary Learning-based Lossless Compression for Genomics Data,"Hui Sun, Yanfeng Ding, Huidong Ma, Chang Xu, Keyan Jin, Lizheng Zu, Cheng Zhong, Xiaoguang Liu, Gang Wang, Wentong Cai",Not found,Not found,"genomics, compression, evolutionary learning, LLM, multiple agents","AgentGC is a first evolutionary agent-based genomics data compressor consisting of three layers: User, Cognitive, and Compression. It uses a user-friendly interface via Leader combined with LLM, integrates LLM to consider joint optimization of algorithm-dataset-system, and performs compression & decompression via an automated multi-knowledge learning-based framework. It supports three modes: CP, TP, and BM. Compared to 14 baselines on 9 datasets, it achieves significant compression ratios and throughput gains.",11.6,Qwen2.5-3B,Apple M1 (Metal)
2601.13562v1_Reasoning is a Modality.pdf,Reasoning is a Modality,"Zhiguang Liu, Yi Shang",Not found,Not found,"AI, Abstract Reasoning, Transformer, Visual Reasoning","The paper proposes that reasoning is a distinct modality, separate from the low-level workspace on which rules are applied. It introduces a novel role-separated transformer block to solve visual reasoning tasks in the Abstraction and Reasoning Corpus (ARC), achieving 62.6% accuracy, surpassing human performance.",10.73,Qwen2.5-3B,Apple M1 (Metal)
2601.13563v1_ButterflyMoE Sub-Linear Ternary Experts via Struct.pdf,ButterflyMoE: Sub-Linear Ternary Experts via Structured Butterfly Orbits,Aryan Karmore,Not found,Not found,"Machine learning, Mixture of Experts, Quantization, Compression, Memory efficiency","ButterflyMoE is a method that treats experts not as independent weight matrices but as geometric reorientations of a unified shared quantized substrate. It achieves sub-linear memory scaling, reducing memory usage from 256 MB to 1.9 MB for a 64 expert model on a Jetson Nano, enabling deployments that were previously unattainable.",10.66,Qwen2.5-3B,Apple M1 (Metal)
2601.13564v1_Multi-objective fluorescent molecule design with a.pdf,Multi-objective fluorescent molecule design with a data-physics dual-driven generative framework,"Yanheng Li, Zhichen Pu, Lijiang Yang, Zehao Zhou, Yi Qin Gao",,,"fluorescent molecules, multi-objective optimization, data-driven design, generative models",This paper presents a novel approach to design fluorescent molecules using a dual-driven generative framework that integrates data science and physical principles.,13.3,Qwen2.5-3B,Apple M1 (Metal)
2601.13566v1_Self-Improvement as Coherence Optimization A Theor.pdf,Self-Improvement as Coherence Optimization: A Theoretical Account,"Tianyi Qiu, Ahmed Hani Ismail, Zhonghao He, Shi Feng",Not found,2601.13566,"Language models, Coherence optimization, Semi-supervised learning, Self-improvement","This paper explores how language models can improve their accuracy without external supervision, presenting methods such as debate, internal coherence maximization, iterative bootstrap, and Metropolis-Hastings sampling. It shows that these methods all optimize the same objective: coherence, defined as the joint likelihood of the model's behaviors across all contexts. The paper proves that coherence optimization is equivalent to description-length regularization and is optimal for semi-supervised learning when the regularizer is derived from a pretrained model.",12.54,Qwen2.5-3B,Apple M1 (Metal)
2601.13570v1_GeoDynamics A Geometric State-Space Neural Network.pdf,GeoDynamics: A Geometric State-Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds,"Tingting Dan, Jiaqi Ding, Guorong Wu",Not found,2601.13570,"Geometric state-space, Neural network, Brain dynamics, Riemannian manifold, Functional connectivity, Alzheimer's, Parkinson's, Autism","GeoDynamics is a geometric state-space neural network designed to model brain dynamics on Riemannian manifolds, capturing the trajectories of symmetric positive-definite (SPD) matrices that represent functional connectivity at each time point. This approach reveals task-driven state changes and early markers of brain disorders, offering insights into the underlying neurobiological processes.",12.43,Qwen2.5-3B,Apple M1 (Metal)
2601.13580v1_Neural Organ Transplantation NOT Checkpoint-Based .pdf,NEURALORGANTRANSPLANTATION(NOT): CHECKPOINT-BASEDMODULARADAPTATION FOR TRANSFORMERMODELS,Ahmad Al-Zuraiqi,Not found,2601.13580v1,"Modular Deep Learning, Transfer Learning, Checkpoint Transfer, Domain Adaptation, Large Language Model","We introduce Neural Organ Transplantation (NOT), a modular adaptation framework that enables trained transformer layers to function as reusable transferable checkpoints for domain adaptation, achieving an order-of-magnitude improvement in perplexity over LoRA while training significantly faster.",13.56,Qwen2.5-3B,Apple M1 (Metal)
2601.13581v1_SCRIPTMIND Crime Script Inference and Cognitive Ev.pdf,SCRIPTMIND: Crime Script Inference and Cognitive Evaluation for LLM-based Social Engineering Scam Detection System,"Heedou Kim, Changsik Kim, Sanghwa Shin, Jaewoo Kang",,anonymous/ScriptMind,"Social Engineering, Large Language Models, Cognitive Evaluation, Scam Detection","SCRIPTMIND is an integrated framework for LLM-based scam detection that bridges automated reasoning and human cognition. It comprises three components: the Crime Script Inference Task (CSIT) for scam reasoning, the Crime Script–Aware Inference Dataset (CSID) for fine-tuning small LLMs, and the Cognitive Simulation-based Evaluation of Social Engineering Defense (CSED) for assessing real-time cognitive impact. Using 571 Korean phone scam cases, it achieved superior performance over commercial models in detection accuracy, false-positive reduction, scammer utterance prediction, and rationale quality.",12.13,Qwen2.5-3B,Apple M1 (Metal)
2601.13588v1_TREX Tokenizer Regression for Optimal Data Mixture.pdf,TREX: Tokenizer Regression for Optimal Data Mixture,"Inho Won, Hangyeol Yoo, Minkyung Cho, Jungyeul Park, Hoyun Song, KyungTae Lim",Not found,Not found,"tokenizer, data mixture, multilingual, compression efficiency, language ratios","Building effective tokenizers for multilingual Large Language Models (LLMs) requires careful control over language-specific data mixtures. Existing approaches rely on heuristics or costly large-scale searches to determine optimal language ratios, leading to accuracy-cost trade-offs. This work introduces TREX, a regression-based framework that efficiently predicts the optimal data mixture for tokenizer training, enabling scalable mixture search before large-scale tokenizer training. The learned model demonstrates strong scalability, robustness, and practical effectiveness in both in- and out-of-distribution compression efficiency.",11.65,Qwen2.5-3B,Apple M1 (Metal)
2601.13589v1_Motion-to-Response Content Generation via Multi-Ag.pdf,MOTION-TO-RESPONSECONTENTGENERATION VIA MULTI-AGENTAI SYSTEM WITHREAL-TIMESAFETYVERIFICATION,HyeYoung Lee,Not found,2601.13589v1,"Speech Emotion Recognition, Multi-Agent Systems, Content Generation, Safety Verification, On-Device AI","This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals, emphasizing the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents.",12.17,Qwen2.5-3B,Apple M1 (Metal)
2601.13590v1_Vulnerability of LLMs Belief Systems LLMs Belief R.pdf,Vulnerability of LLMs’ Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions,"Fan Huang, Haewoon Kwak, Jisun An",,,"Large Language Models, persuasion, belief stability, LLM susceptibility, meta-cognition, adversarial fine-tuning","This study systematically evaluates the susceptibility of five mainstream Large Language Models (LLMs) to persuasion under the SMCR communication framework. It analyzes how different persuasive strategies influence belief stability over multiple interaction turns and examines whether meta-cognition prompting affects resistance to persuasion. Results show that smaller models exhibit extreme compliance, while meta-cognition prompting increases vulnerability by accelerating belief erosion. The study also evaluates adversarial fine-tuning as a defense mechanism, finding that while GPT-4o-mini achieves near-complete robustness, Llama models remain highly susceptible even when fine-tuned on their own failure cases. These findings highlight substantial model-dependent limits of current robustness interventions and offer guidance for developing more trustworthy LLMs.",12.13,Qwen2.5-3B,Apple M1 (Metal)
2601.13591v1_DSAEval Evaluating Data Science Agents on a Wide R.pdf,DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems,"Maojun Sun1*, Yifei Xie1*, Yue Wu1*, Ruijian Han1†, Binyan Jiang1, Defeng Sun2, Yancheng Yuan2†, Jian Huang1,2†",Not found,Not found,"Data Science, Agents, Evaluation, Real-World Problems, Multi-Modal Perception, Multi-Query Interactions, Multi-Dimensional Evaluation","Recent advances in Large Language Model-based data science agents have significantly promoted the automation of data science, encompassing a wide range of tasks from exploratory data analysis and traditional machine learning to complex deep learning workflows. However, evaluating the efficacy of these agents remains a formidable challenge. Real-world data science problems are inherently open-ended and exploratory, often lacking unique, standardized solutions, which renders traditional exact-match evaluation metrics insufficient. DSAEval, a comprehensive benchmark, addresses these challenges by evaluating data science agents using large-scale, real-world problems spanning broad domains of data science problems, including Statistical Testing & Inference (STI), Data Mining, and Data Visualization.",12.36,Qwen2.5-3B,Apple M1 (Metal)
2601.13592v1_Machine learning based radiative parameterization .pdf,Machine learning based radiative parameterization scheme and its performance in operational reforecast experiments,"Jing Hao, Xiao Sa, Li Haoyu, Xiao Huadong, Xue Wei",Not provided,Not provided,"Machine learning, Radiation, Hybrid model, Operational reforecast experiments","This study investigates critical limitations in hybrid forecasting frameworks that embed deep neural networks into numerical prediction models, focusing on coupling compatibility and long-term integration stability. A residual convolutional neural network is employed to approximate the Rapid Radiative Transfer Model for General Circulation Models (RRTMG) within the global operational system of China Meteorological Administration, demonstrating comparable accuracy to traditional physical schemes while accelerating computation speed by approximately eightfold.",12.52,Qwen2.5-3B,Apple M1 (Metal)
2601.13599v1_Diffusion In Diffusion Breaking the Autoregressive.pdf,Diffusion In Diffusion: Breaking the Autoregressive Bottle-neck in Block Diffusion Models,"Linrui Ma, Yufei Cui, Kai Han, Yunhe Wang",,2601.13599,"language modeling, block diffusion, discrete diffusion, autoregressive models, diffusion models, semi-AR architecture","Proposes DIFFUSION INDIFFUSION—a 'draft-then-refine' framework to overcome irreversibility and myopia problems in block diffusion models, using snapshot confidence remasking and mix-scale training to improve performance on the OpenWebText dataset.",11.13,Qwen2.5-3B,Apple M1 (Metal)
2601.13600v1_Foundations of Global Consistency Checking with No.pdf,Foundations of Global Consistency Checking with Noisy LLM Oracles,"Paul He*, Elke Kirschbaum, Shiva Kasiviswanathan",,,"Global Consistency, Natural Language Facts, Large Language Models, Verification, Consistency Checking","Ensuring the global consistency of sets of natural-language facts is essential for tasks such as fact-checking, summarization, and knowledge base construction. While Large Language Models (LLMs) can assess the consistency of small subsets of facts, their judgments are noisy, and pairwise checks are insufficient to guarantee global coherence. This paper formalizes the problem and proposes an adaptive divide-and-conquer algorithm to identify minimal inconsistent subsets (MUSes) of facts and optionally compute minimal repairs through hitting-sets. The approach has low-degree polynomial query complexity and efficiently detects and localizes inconsistencies with both synthetic and real LLM oracles.",11.82,Qwen2.5-3B,Apple M1 (Metal)
2601.13614v1_CauScientist Teaching LLMs to Respect Data for Cau.pdf,CauScientist: Teaching LLMs to Respect Data for Causal Discovery,"Bo Peng, Sirui Chen, Lei Xu, Chaochao Lu",Not found,2601.13614,"causal discovery, large language models, statistical indistinguishability, modeling assumptions, distribution shift","Causal discovery is fundamental to scientific understanding and decision-making. Existing methods face limitations, and CauScientist proposes a collaborative framework that integrates LLMs as hypothesis-generating 'data scientists' with probabilistic statistics as verifiers. It employs hybrid initialization, iterative refinement, and error memory to improve performance and reduce structural hamming distance compared to standalone LLMs.",11.41,Qwen2.5-3B,Apple M1 (Metal)
2601.13622v1_CARPE Context-Aware Image Representation Prioritiz.pdf,CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models,"Donghee Lee, Rui Cai, Zhe Zhao",Not found,2601.13622,"Large Vision-Language Models, Context-Aware Image Representation, Ensemble Learning, Vision-Text Integration, Image Classification","This paper proposes CARPE, a novel, model-agnostic framework that introduces vision-integration layers and a context-aware ensemble strategy to enhance the classification performance of Large Vision-Language Models (LVLMs) on image benchmarks.",11.66,Qwen2.5-3B,Apple M1 (Metal)
2601.13632v1_Resilient Routing Risk-Aware Dynamic Routing in Sm.pdf,Resilient Routing: Risk-Aware Dynamic Routing in Smart Logistics via Spatiotemporal Graph Learning,"Zhiming Xue, Sichen Zhao, Yalun Qi, Xianling Zeng, Zihan Yu",Not found,Not found,"Smart Logistics, Graph Neural Network, Dynamic Routing, Spatiotemporal Modeling, Supply Chain Resilience","This paper proposes a Risk-Aware Dynamic Routing (RADR) framework that integrates Spatiotemporal Graph Neural Networks (ST-GNN) with combinatorial optimization to address the challenges of traffic congestion and fluctuating retail demand in smart logistics. The framework constructs a logistics topology graph using discrete GPS data and a hybrid deep learning model combining Graph Convolutional Network (GCN) and Gated Recurrent Unit (GRU) to predict future congestion risks and integrate these predictions into a dynamic edge weight mechanism for path planning. The experimental results show that the RADR algorithm significantly enhances supply chain resilience, particularly in high congestion scenarios.",12.69,Qwen2.5-3B,Apple M1 (Metal)
2601.13645v1_Quadratic Upper Bound for Boosting Robustness.pdf,Quadratic Upper Bound for Boosting Robustness,"Euijin Y ou, Hyang-Won Lee",Not found,Not found,"Adversarial training, Robustness, Fast adversarial training, Loss function, Robustness improvement","This paper develops a loss function to improve robustness in Fast Adversarial Training (FAT) without requiring stronger inner maximization. Specifically, it derives a quadratic upper bound (QUB) on the adversarial training loss function and proposes to utilize the bound with existing FAT methods. Experimental results show significant improvement in robustness.",11.16,Qwen2.5-3B,Apple M1 (Metal)
2601.13647v1_Fusion Segment Transformer Bi-Directional Attentio.pdf,FUSION SEGMENT TRANSFORMER: BI-DIRECTIONAL A TTENTION GUIDED FUSION NETWORK FOR AI GENERA TED MUSIC DETECTION,"Yumin Kim, Seonghyeon Go",Not found,githubCode/Demo,"AI-generated music detection, Full-audio segment detection, Musical structure analysis, Cross-modal fusion layer, Music representation","With the rise of generative AI technology, the Fusion Segment Transformer addresses the challenge of full-audio detection for AI-generated music, integrating content and structural information through a Gated Fusion Layer. Experiments on SONICS and AIME datasets show superior performance compared to previous models and baselines.",11.79,Qwen2.5-3B,Apple M1 (Metal)
2601.13649v1_Fairness or Fluency An Investigation into Language.pdf,Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge,"Xiaolin Zhou, Zheng Luo, Yicheng Gao, Qixuan Chen, Xiyang Hu, Yue Zhao, Ruishan Liu",Not provided,Not provided,"Large Language Models, LLM-as-a-judge, Language bias, Performance disparity, Inter-language judging, Same-language judging, Low-perplexity bias","This paper investigates two types of language bias in pairwise LLM-as-a-judge: performance disparity between languages when the judge is prompted to compare options from the same language, and bias towards options written in major languages when the judge is prompted to compare options of two different languages. The study finds significant performance disparities across language families, with European languages consistently outperforming African languages, and that this bias is more pronounced in culturally-related subjects. For inter-language judging, most models favor English answers, and this preference is influenced more by answer language than question language. The paper also explores whether language bias is caused by low-perplexity bias, finding that while perplexity is slightly correlated with language bias, language bias cannot be fully explained by perplexity only.",12.48,Qwen2.5-3B,Apple M1 (Metal)
2601.13655v1_Why Does the LLM Stop Computing An Empirical Study.pdf,Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs,"GUANGBA YU, Zirui Wang, Yujie Huang, Renyi Zhong, Yuedong Zhong, Yilun Wang, Michael R. Lyu",Not provided,Not provided,"Large Language Models, Failure Analysis, Empirical Study","This study conducts the first large-scale empirical study of 705 real-world failures from the open-source DeepSeek, Llama, and Qwen ecosystems, revealing a paradigm shift in the reliability of user-managed orchestration of open-source LLMs.",11.46,Qwen2.5-3B,Apple M1 (Metal)
2601.13657v1_Communication-Free Collective Navigation for a Swa.pdf,Communication-Free Collective Navigation for a Swarm of UA Vs via LiDAR-Based Deep Reinforcement Learning,"Myong-Yol Choi, Hankyoul Ko, Hanse Cho, Changseung Kim, Seunghwan Kim, Jaemin Seo, Hyondong Oh",Not found,Not found,"deep reinforcement learning, collective navigation, unmanned aerial vehicle, swarm, sensor-based control, LiDAR, leader-follower, robust navigation, perception, occlusion, field-of-view","This paper presents a deep reinforcement learning (DRL) based controller for collective navigation of unmanned aerial vehicle (UA V) swarms in communication-denied environments, enabling robust operation in complex, obstacle-rich environments. The system uses LiDAR point clustering and an extended Kalman filter for stable neighbor tracking, and a DRL controller trained in GPU-accelerated Nvidia Isaac Sim enables followers to learn complex emergent behaviors—balancing flocking and obstacle avoidance—using only local perception.",12.44,Qwen2.5-3B,Apple M1 (Metal)
2601.13659v1_Temporal-Spatial Decouple before Act Disentangled .pdf,TEMPORAL-SPA TIAL DECOUPLE BEFORE ACT: DISENTANGLED REPRESENTA TION LEARNING FOR MULTIMODAL SENTIMENT ANALYSIS,"Chunlei Meng, Ziyang Zhou, Lucas He, Xiaojing Du, Chun Ouyang†, Zhongxue Gan",Not found,Not found,"Multimodal Sentiment Analysis, Temporal-Spatial Decoupling, Representation Learning","Proposes TSDA, Temporal–Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction. Achieves better performance compared to baselines.",11.45,Qwen2.5-3B,Apple M1 (Metal)
2601.13671v1_The Orchestration of Multi-Agent Systems Architect.pdf,"The Orchestration of Multi-Agent Systems: Architectures, Protocols, and Enterprise Adoption","Apoorva Adimulam, Rajesh Gupta, Sumit Kumar",,,"Agent orchestration, Agent-to-Agent protocol, dynamic task allocation, Model Context Protocol (MCP), multi-agent systems, observability, state management, system governance","This paper consolidates and formalizes the technical composition of orchestrated multi-agent systems, presenting a unified architectural framework that integrates planning, policy enforcement, state management, and quality operations into a coherent orchestration layer. It also details two complementary communication protocols—the Model Context Protocol and the Agent-to-Agent protocol, which establish an interoperable communication substrate for scalable, auditable, and policy-compliant reasoning across distributed agent collectives.",12.19,Qwen2.5-3B,Apple M1 (Metal)
2601.13684v1_HeteroCache A Dynamic Retrieval Approach to Hetero.pdf,HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache,"Zhiyuan Shi, Qibo Qiu, Feng Xue, Zhonglin Jiang, Li Yu, Jian Jiang, Xiaofei He, Wenxiao Wang",Not found,Not found,"HeteroCache, Dynamic Retrieval, KV Cache, Attention Drift, Spatial Redundancy, Fine-Grained Weighting, Hierarchical Storage, Long-Context Inference","The linear memory growth of the KV cache poses a significant bottleneck for LLM inference in long-context tasks. Existing static compression methods often fail to preserve globally important information due to their coarse-grained caching strategies and high I/O overhead. HeteroCache, a training-free dynamic compression framework, addresses these limitations by categorizing attention heads based on stability and redundancy, applying a fine-grained weighting strategy, and employing a hierarchical storage mechanism to hide I/O latency.",12.29,Qwen2.5-3B,Apple M1 (Metal)
2601.13687v1_Understanding Mental States to Guide Social Influe.pdf,Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue,"Zhichao Liang, Satoshi Nakamura",,,"Social Influence, Theory of Mind, Multi-Person Dialogue, Language Models","This paper introduces SocialMindChange, a benchmark that moves from tracking minds to changing minds in social interaction. Each instance defines a social context with 4 characters and five connected scenes. The model plays one character and generates dialogue across the five scenes to reach the target while remaining consistent with the evolving states of all participants. The benchmark includes selected higher-order states and covers 6000 scenarios and over 90,000 questions, each validated for realism and quality. Evaluations on ten state-of-the-art LLMs show that their average performance is 54.2% below human performance, suggesting current LLMs struggle to maintain and change mental-state representations across long, linked interactions.",12.12,Qwen2.5-3B,Apple M1 (Metal)
2601.13693v1_End-to-End Reverse Screening Identifies Protein Ta.pdf,End-to-End Reverse Screening Identifies Protein Targets of Small Molecules Using HelixFold3,"Shengjie Xu, Xianbin Ye, Mengran Zhu, Xiaonan Zhang, Shanzhuo Zhang, Xiaomin Fang",Not provided,2601.13693,"Reverse screening, Target identification, Biomolecular structure prediction, HelixFold3","Identifying protein targets for small molecules, or reverse screening, is essential for understanding drug action, guiding compound repurposing, predicting off-target effects, and elucidating the molecular mechanisms of bioactive compounds. This paper presents an end-to-end reverse screening strategy leveraging HelixFold3, a high-accuracy biomolecular structure prediction model, to improve screening accuracy and demonstrate enhanced structural fidelity, binding-site precision, and target prioritization.",13.89,Qwen2.5-3B,Apple M1 (Metal)
2601.13697v1_Uncertainty-Aware Gradient Signal-to-Noise Data Se.pdf,Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning,"Zhihang Yuan, Chengyu Yue, Long Huang, Litu Ou, Lei Shi",Not found,Not found,"Instruction tuning, Large language models, Data selection, Gradient signal-to-noise ratio, Epistemic uncertainty","Instruction tuning is a standard paradigm for adapting large language models to follow human instructions. Modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning expensive and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. This paper proposes GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations and in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.",12.52,Qwen2.5-3B,Apple M1 (Metal)
2601.13698v1_Does Privacy Always Harm Fairness Data-Dependent T.pdf,Privacy Always Harm Fairness? Data-Dependent Trade-offs via Chernoff Information Neural Estimation,"Arjun Nichani, Hsiang Hsu, Chun-Fu (Richard) Chen, Haewon Jeong",Not found,2601.13698,"Privacy, Fairness, Chernoff Information, Machine Learning, Differential Privacy","This paper explores the relationship between fairness and privacy in machine learning, using the information-theoretic measure Chernoff Information. It defines Noisy Chernoff Difference and analyzes the data-dependent nature of the relationship among fairness, privacy, and accuracy. The authors highlight the implications of different data distributions on fairness and privacy and propose a method for estimating Chernoff Information on data from unknown distributions.",11.89,Qwen2.5-3B,Apple M1 (Metal)
2601.13704v1_Performance and Complexity Trade-off Optimization .pdf,Performance and Complexity Trade-off Optimization of Speech Models During Training,"Esteban Gómez, Tom Bäckström",Not found,Not found,"Speech machine learning, low-complexity, voice activity detection, deep fake detection","This paper proposes a reparameterization technique based on feature noise injection that enables joint optimization of performance and computational complexity during training using SGD-based methods. Unlike traditional pruning methods, our approach allows the model size to be dynamically optimized for a target performance-complexity trade-off, without relying on heuristic criteria to select which weights or structures to remove. The effectiveness of the method is demonstrated through three case studies, including a synthetic example and two practical real-world applications: voice activity detection and audio anti-spoofing. The code related to our work is publicly available.",12.26,Qwen2.5-3B,Apple M1 (Metal)
2601.13707v1_Attention-space Contrastive Guidance for Efficient.pdf,Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs,"Yujin Jo, Sangyoon Bae, Taesup Kim*",Not found,Not found,"Large Vision-Language Models, Hallucination Mitigation, Contrastive Guidance, Attention Space, Efficient Computation","This paper addresses the issue of hallucinations in large vision-language models (LVLMs) by framing hallucination mitigation as contrastive guidance, steering generation toward visually grounded and semantically faithful text. The proposed Attention-space Contrastive Guidance (ACG) is a single-pass mechanism that operates within self-attention layers to construct both vision-language and language-only attention paths in a single forward computation, enabling computationally efficient guidance directly embedded in the model's representation contextualization. Experiments on the CHAIR and POPE benchmarks show that ACG achieves state-of-the-art faithfulness and caption quality while significantly reducing computational cost.",12.45,Qwen2.5-3B,Apple M1 (Metal)
2601.13709v1_Hidden in Plain Text Measuring LLM Deception Quali.pdf,Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games,"Christopher Kao, Vanshika Vats, James Davis",,,"large language models, natural language processing, autonomous game players, social deduction games","This paper studies deception in the Social Deduction Game (SDG) Mafia, using an asynchronous multi-agent framework to simulate realistic social contexts. It compares the prediction accuracy of a Mafia Detector using GPT-4-Turbo to that of 28 human games and a random baseline, finding that LLMs blend in better and deceive more effectively. The results underscore the sophistication and risks of LLM deception in social contexts.",11.75,Qwen2.5-3B,Apple M1 (Metal)
2601.13710v1_Who Should Have Surgery A Comparative Study of Gen.pdf,Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction,"Sayeed Shafayet Chowdhury, Snehasis Mukhopadhyay, Shiaofen Fang, Vijay R. Ramakrishnan",,,"Chronic Rhinosinusitis, clinical decision support, generative artificial intelligence, large language models, SNOT-22, surgical outcome prediction, tabular clinical data","This study compares generative artificial intelligence (GenAI) models, including ChatGPT, Claude, Gemini, and Perplexity, with supervised machine learning (ML) models (logistic regression, tree ensembles, and an in-house MLP) in predicting clinically meaningful improvement in chronic rhinosinusitis (CRS) outcomes. The models were benchmarked on a prospectively collected cohort where all patients underwent surgery, and the study aims to identify those who should have avoided surgery based on pre-operative clinical data. The findings support an ML-first, GenAI-augmented workflow for surgical candidacy triage and shared decision-making.",12.38,Qwen2.5-3B,Apple M1 (Metal)
2601.13717v1_Simulated Ignorance Fails A Systematic Study of LL.pdf,Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff,"Zehan Li, Yuxuan Wang, Ali El Lahib, Ying-Jieh Xia, Xinyu Pi",Not provided,Not provided,"Forecasting, LLMs, Simulated Ignorance, Retrospective Forecasting, Knowledge Cutoff","This study evaluates the effectiveness of Simulated Ignorance (SI) in approximating True Ignorance (TI) for forecasting models. Across 477 competition-level questions and 9 models, it finds that SI fails systematically, leaving a 52% performance gap between SI and TI. It also demonstrates that chain-of-thought reasoning and reasoning-optimized models do not reliably suppress prior knowledge, even when reasoning traces contain no explicit post-cutoff references. These findings suggest that prompts cannot reliably rewind model knowledge, and recommend against using SI-based retrospective setups to benchmark forecasting capabilities.",11.72,Qwen2.5-3B,Apple M1 (Metal)
2601.13719v1_Hierarchical Long Video Understanding with Audiovi.pdf,Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search,"Xinlei Yin1*, Xiulian Peng2, Xiao Li2, Zhiwei Xiong1, Yan Lu2",,,"long video understanding, audiovisual entity cohesion, agentic search, semantic continuity, temporal grounding, adaptive sampling, token compression, memory-based approaches","Long video understanding presents significant challenges due to extremely long context windows. Existing solutions relying on naive chunking strategies with retrieval-augmented generation typically suffer from information fragmentation and a loss of global coherence. This paper presents HAVEN, a unified framework for long-video understanding that integrates audiovisual entity cohesion and hierarchical video indexing with agentic search, enabling coherent and comprehensive reasoning. The framework preserves semantic consistency by integrating entity-level representations across visual and auditory streams and organizes content into a structured hierarchy spanning global summary, scene, segment, and entity levels. An agentic search mechanism is employed to enable dynamic retrieval and reasoning across these layers, facilitating coherent narrative reconstruction and fine-grained entity tracking. Extensive experiments demonstrate that the method achieves good temporal coherence, entity consistency, and retrieval efficiency, establishing a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably, it achieves outstanding performance in the challenging reasoning category, reaching 80.1%. These results highlight the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.",13.38,Qwen2.5-3B,Apple M1 (Metal)
2601.13722v1_OP-Bench Benchmarking Over-Personalization for Mem.pdf,OP-Bench: Benchmarking Over-Personalization for Memory-Augmented Personalized Conversational Agents,"Yulin Hu, Zimo Long, Jiahe Guo, Xingyu Sui, Xing Fu, Weixiang Zhao, Yanyan Zhao, Bing Qin",Not provided,2601.13722,"Memory-augmented, Personalized Conversational Agents, Over-Personalization, Memory Filtering, Large Language Models","This work formalizes over-personalization into three types: Irrelevance, Repetition, and Sycophancy, and introduces OP-Bench, a benchmark of 1,700 verified instances constructed from long-horizon dialogue histories. Using OP-Bench, the authors evaluate multiple large language models and memory-augmentation methods, finding that over-personalization is widespread when memory is introduced. They propose Self-ReCheck, a lightweight, model-agnostic memory filtering mechanism to mitigate over-personalization while preserving personalization performance.",12.66,Qwen2.5-3B,Apple M1 (Metal)
2601.13734v1_Towards robust long-context understanding of large.pdf,TOW ARDS ROBUST LONG-CONTEXT UNDERSTANDING OF LARGE LANGUAGE MODEL VIA ACTIVE RECAP LEARNING,Chenyu Hui,,,"Large Language Model, Long-Context Understanding, Active Recap Learning, Recap Supervision, Recap Agent","This paper proposes active recap learning (ARL), a framework for enhancing large language model (LLM) in understanding long contexts. ARL enables models to revisit and summarize earlier content through targeted sequence construction during continued pretraining and retrospective summarization at inference, leading to substantial improvements in performance.",11.72,Qwen2.5-3B,Apple M1 (Metal)
2601.13735v1_Reasoning or Fluency Dissecting Probabilistic Conf.pdf,Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection,"Hojin Kim, Jaehyung Kim",,,"Large Language Models, Chain-of-Thought, Best-of-N, Probabilistic Confidence, Reasoning Fidelity, Inter-step Causality, Fluency, Logical Structure","This work challenges the assumption that probabilistic confidence metrics capture inter-step causal dependencies necessary for valid reasoning. By introducing three classes of inter-step causality perturbations, the authors find that selection accuracy degrades only marginally under these disruptions, even severe interventions do not substantially reduce selection performance. These findings suggest that current probabilistic metrics are largely insensitive to logical structure and primarily capture surface-level fluency or in-distribution priors instead. The authors propose a contrastive causality metric to explicitly isolate inter-step causal dependencies and demonstrate that it yields more faithful output selection than existing probability-based approaches.",12.13,Qwen2.5-3B,Apple M1 (Metal)
2601.13749v1_Pro-AI Bias in Large Language Models.pdf,Pro-AI Bias in Large Language Models,"Benaya Trabelsi, Jonathan Shaki, Sarit Kraus",,,"Large Language Models, Bias, Artificial Intelligence, Salary Estimation, Decision Support","This study investigates whether large language models display a systematic preferential bias in favor of artificial intelligence (AI) itself. Across three experiments, consistent evidence of pro-AI bias is found, including disproportionate recommendation of AI-related options and systematic overestimation of AI-related job salaries.",11.19,Qwen2.5-3B,Apple M1 (Metal)
2601.13752v1_Finding RELIEF Shaping Reasoning Behavior without .pdf,Finding RELIEF: Shaping Reasoning Behavior without Reasoning,"Chak Tou Leong, Dingwei Chen, Heming Xia, Sunbowen Lee, Jian Wang, Wenjie Li",Not found,Not found,"Large reasoning models, reinforcement learning, fine-tuning, belief engineering, reasoning behavior, computational redundancy, unfaithful explanations","This paper reveals that large reasoning models (LRMs) possess latent reasoning beliefs that can be captured through simple logit probing. Building upon this insight, the authors propose RELIEF, a framework that shapes LRM behavior by aligning the model's self-concept with a target belief blueprint. Crucially, RELIEF bypasses the need for reasoning-trace supervision, fine-tuning on synthesized, self-reﬂective question-answering pairs that affirm the target belief. Extensive experiments demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs.",12.14,Qwen2.5-3B,Apple M1 (Metal)
2601.13761v1_DARC Decoupled Asymmetric Reasoning Curriculum for.pdf,DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution,"Shengda Fan1*, Xuyan Ye1*, Yankai Lin1†",Not provided,2601.13761,"Large Language Models, Self-Play, Optimization Stability, Self-Evolution, Pseudo-Labeled Data","DARC introduces a two-stage framework to stabilize the self-evolution process of large language models, addressing challenges such as non-stationary objectives and bootstrapping errors. It trains a Questioner to synthesize difficulty-calibrated questions and a Solver with an asymmetric self-distillation mechanism to generate high-quality pseudo-labels.",11.58,Qwen2.5-3B,Apple M1 (Metal)
2601.13768v1_vLinear A Powerful Linear Model for Multivariate T.pdf,vLinear: A Powerful Linear Model for Multivariate Time Series Forecasting,"Wenzhen Yue, Ruohao Guo, Ji Shi, Zihan Hao, Shiyu Hu, Xianghua Ying",Not found,Not found,"Time series forecasting, Multivariate time series, Linear model, Transformer, Self-attention, Efficiency, Forecasting accuracy","This paper presents vLinear, a powerful yet efficient linear-based multivariate time series forecaster. It introduces vecTrans, a lightweight module that models multivariate correlations using a learnable rank-1 matrix, reducing computational complexity from O(N^2) to O(N). vLinear achieves state-of-the-art performance across 22 benchmarks and 124 forecasting settings, and it introduces WFM-Loss as the objective, which focuses on more reliable paths and horizons. The code is available at https://anonymous.4open.science/r/vLinear.",11.67,Qwen2.5-3B,Apple M1 (Metal)
2601.13770v1_Look-Ahead-Bench a Standardized Benchmark of Look-.pdf,Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance,Mostapha Benhenda *,,arXiv:2601.13770v1,"look-ahead bias, point-in-time models, large language models, financial applications, temporal causality","This paper introduces Look-Ahead-Bench, a standardized benchmark to measure look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q&A, this benchmark evaluates model behavior in practical scenarios. The study reveals significant lookahead bias in standard LLMs, unlike PiT-Inference models which demonstrate improved generalization and reasoning abilities as they scale in size.",11.45,Qwen2.5-3B,Apple M1 (Metal)
2601.13798v1_Insight Interpretable Semantic Hierarchies in Visi.pdf,INSIGHT: Interpretable Semantic Hierarchies in Vision-Language Encoders,"Kai Wittenmayer, Sukrut Rao, Amin Parchami-Araghi, Bernt Schiele, Jonas Fischer",Not found,2601.13798,"Interpretable, Semantic, Hierarchies, Vision-Language, Encoders",INSIGHT provides rich conceptual explanations for vision foundation model tasks by providing fine-grained concepts that are human-interpretable and spatially grounded in the input image.,11.43,Qwen2.5-3B,Apple M1 (Metal)
2601.13809v1_DroneVLA VLA based Aerial Manipulation.pdf,DroneVLA: VLA based Aerial Manipulation,"Fawad Mehboob∗, Monijesu James∗, Amir Habel∗, Jeffrin Sam, Miguel Altamirano Cabrera, Dzmitry Tsetserukou",,,"Aerial Manipulation, Vision-Language-Action Models, Human-Robot Interaction, Robotic Fetch-and-Carry","This work introduces a novel concept of autonomous aerial manipulator capable of interpreting high-level natural language commands to retrieve objects and deliver them to a human user. The system integrates a MediaPipe based on Grounding DINO and a Vision-Language-Action (VLA) model with a custom-built drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. VLA performs semantic reasoning to interpret the intent of a user prompt and generates a prioritized task queue for grasping of relevant objects in the scene. Grounding DINO and dynamic A* planning algorithm are used to navigate and safely relocate the object. To ensure safe and natural interaction during the handover phase, the system employs a human-centric controller driven by MediaPipe. This module provides real-time human pose and orientation estimation, allowing the drone to employ visual servoing to maintain a stable, distinct position directly in front of the user, facilitating a comfortable handover.",12.77,Qwen2.5-3B,Apple M1 (Metal)
2601.13846v1_Virtual Urbanism An AI-Driven Framework for Quanti.pdf,Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity,"Glinskaya, Maria",,,"generative artificial intelligence, latent diffusion model, low-rank adaptation model, urban perception, urban identity","This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through synthetic urban replicas. The pilot study demonstrates the feasibility of the framework by producing dynamic synthetic urban sequences of nine Tokyo areas using Stable Diffusion and LoRA models, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments assessed perceptual legitimacy, quantified area-level identity, and derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. The Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-assisted urban analysis.",11.83,Qwen2.5-3B,Apple M1 (Metal)
2601.13864v1_HardSecBench Benchmarking the Security Awareness o.pdf,HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware,"Qirui Chen, Jingxian Shuai, Shuangwu Chen, Shenghao Ye, Zijian Wen, Xufei Su, Jie Jin, Jiangming Li, Jun Chen, Xiaobin Tan, Jian Yang",Not found,Not found,"Large language models, security, benchmark, hardware, code generation, LLM-generated code, security flaws, functional correctness, security awareness","This work introduces HardSecBench, a benchmark with 924 tasks spanning Verilog Register Transfer Level (RTL) and firmware-level C, covering 76 hardware-relevant Common Weakness Enumeration (CWE) entries. The benchmark evaluates security awareness under realistic specifications and finds that models often satisfy functional requirements while still leaving security risks. Security results vary with prompting, highlighting pressing challenges and offering actionable insights for future advancements in LLM-assisted hardware design.",12.24,Qwen2.5-3B,Apple M1 (Metal)
2601.13880v1_LifeAgentBench A Multi-dimensional Benchmark and A.pdf,LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health,"Ye Tian, Zihao Wang, Onat Gungor, Xiaoran Fan, Tajana Rosing",10.10000/anonymous.4open.science/r/LifeAgentBench-CE7B,1234567890,"digital health, personalized health support, long-horizon reasoning, mobile sensing, large language models","This paper introduces LifeAgentBench, a large-scale QA benchmark for long-horizon, cross-dimensional, and multi-user lifestyle health reasoning, containing 22,573 questions spanning from basic retrieval to complex reasoning. It also presents LifeAgent, a strong baseline agent for health assistants that integrates multi-step evidence retrieval with deterministic aggregation, achieving significant improvements compared to widely used baselines. The benchmark and agent are publicly available.",12.09,Qwen2.5-3B,Apple M1 (Metal)
2601.13885v1_Confident Rankings with Fewer Items Adaptive LLM E.pdf,Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores,"Esma Balkır1, Alice Pernthaller1, Marco Basaldella1, José Hernández-Orallo2, Nigel Collier 1",Not provided,Not provided,"Computerized Adaptive Testing, Large Language Models, Continuous Scores, Item Response Theory, Ranking, Uncertainty","This paper presents a principled extension of Item Response Theory-based adaptive testing to continuous bounded scores, replacing the Bernoulli response distribution with a heteroskedastic normal distribution. It introduces an uncertainty-aware ranker with adaptive stopping criteria that achieves reliable model ranking while testing as few items and as cheaply as possible. The method is validated on five benchmarks spanning n-gram-based, embedding-based, and LLM-as-judge metrics, using 2% of the items while improving ranking correlation by 0.12 τ over random sampling, with 95% accuracy on confident predictions.",12.16,Qwen2.5-3B,Apple M1 (Metal)
2601.13887v1_Human Simulation Computation A Human-Inspired Fram.pdf,Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems,Hong Su,Not found,Not found,"Human Simulation Computation, Environment Interaction, Adaptive Artificial Intelligence, Human-Inspired Reasoning","Large language models (LLMs) have demonstrated strong capabilities in knowledge representation and reasoning based on textual data. However, their reliance on language material alone limits their ability to adapt, verify reasoning outcomes, and operate effectively in open and dynamic real-world environments. This paper proposes Human Simulation Computation (HSC), a human-inspired computational framework that models intelligence as a continuous, closed-loop process involving thinking, action, learning, reflection, and activity scheduling.",12.16,Qwen2.5-3B,Apple M1 (Metal)
2601.13895v1_OmniOVCD Streamlining Open-Vocabulary Change Detec.pdf,OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3,"Xu Zhang, Danyang Li, Yingjie Xia, Xiaohang Dong, Hualong Yu, Jianye Wang, Qicheng Li",Not found,Not found,"Change Detection, Open-Vocabulary, SAM 3, Segment Anything Model, Synergistic Fusion, Instance Decoupling","This paper proposes OmniOVCD, a new framework for Open-Vocabulary Change Detection (OVCD) that utilizes the Segment Anything Model 3 (SAM 3) to streamline the process. By leveraging the decoupled output heads of SAM 3, the authors introduce a Synergistic Fusion to Instance Decoupling (SFID) strategy, which fuses semantic, instance, and presence outputs to construct land-cover masks and decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images, resulting in accurate change masks. Experiments on four public benchmarks demonstrate SOTA performance.",12.03,Qwen2.5-3B,Apple M1 (Metal)
2601.13897v1_TractRLFusion A GPT-Based Multi-Critic Policy Fusi.pdf,TRACTRLFUSION: A GPT-BASED MULTI-CRITIC POLICY FUSION FRAMEWORK FOR FIBER TRACTOGRAPHY,"Ankita Joshi, Ashutosh Sharma, Anoushkrit Goel, Ranjeet Ranjan Jha, Chirag Ahuja, Arnav Bhavsar, Aditya Nigam",,,"Diffusion MRI, Tractography, Reinforcement Learning, Transformers","Tractography plays a pivotal role in the non-invasive reconstruction of white matter fiber pathways, providing vital information on brain connectivity and supporting precise neurosurgical planning. TractRLFusion, a novel GPT-based policy fusion framework, integrates multiple RL policies through a data-driven fusion strategy, demonstrating improved accuracy and anatomical reliability compared to individual RL policies and state-of-the-art methods.",12.06,Qwen2.5-3B,Apple M1 (Metal)
2601.13904v1_PREFAB PREFerence-based Affective Modeling for Low.pdf,PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation,"Jaeyoung Moon, Youjin Choi, Yucheon Park, David Melhart, Georgios N. Yannakakis, Kyung-Joong Kim",10.1145/3675094.3678379,2601.13904,"Affective Computing, Preference Learning, Self-Annotation, User Modeling, Ordinal Representation, Peak-End Rule","PREFAB is a low-budget retrospective self-annotation method that targets affective inflection regions rather than full annotation, employing a preference-learning model to detect relative affective changes and directing annotators to label only selected segments while interpolating the remainder of the stimulus.",11.54,Qwen2.5-3B,Apple M1 (Metal)
2601.13920v1_Asymmetric regularization mechanism for GAN traini.pdf,Asymmetric regularization mechanism for GAN training with Variational Inequalities,"Spyridon C. Giagtzoglou, Mark H.M. Winands, Barbara Franci",,,"GANs, Variational Inequalities, Nash Equilibrium, Regularization, Extrapolation-from-the-Past (EFTP)","We propose an asymmetric regularization mechanism for stabilizing the training of generative adversarial networks (GANs) by formulating the training as a Nash equilibrium seeking problem. The mechanism is based on Tikhonov step and a novel zero-centered gradient penalty, ensuring last-iterate linear convergence of a single-call Extrapolation-from-the-Past (EFTP) method. The regularization is applied only on the discriminator side, adding curvature where it is most needed while preserving the target equilibrium point.",11.94,Qwen2.5-3B,Apple M1 (Metal)
2601.13938v1_IF-GEO Conflict-Aware Instruction Fusion for Multi.pdf,IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization,"Heyang Zhou, JiaJia Chen, Xiaolu Chen, Jie Bao, Zhen Chen, Yong Liao",Not provided,Not provided,"Generative Search Engines, Conflict-Aware Instruction Fusion, Multi-Query Optimization, Large Language Models, Source Visibility","As Generative Engines revolutionize information retrieval by synthesizing direct answers from retrieved sources, ensuring source visibility becomes a significant challenge. Improving it through targeted content revisions is termed Generative Engine Optimization (GEO). However, optimizing a document for diverse queries presents a constrained optimization challenge where heterogeneous queries often impose conflicting and competing revision requirements under a limited content budget. To address this challenge, we propose IF-GEO, a 'diverge-then-converge' framework comprising two phases: (i) mining distinct optimization preferences from representative latent queries; (ii) synthesizing a Global Revision Blueprint for guided editing by coordinating preferences via conflict-aware instruction fusion. To explicitly quantify IF-GEO’s objective of cross-query stability, we introduce risk-aware stability metrics. Experiments on multi-query benchmarks demonstrate that IF-GEO achieves substantial performance gains while maintaining robustness across diverse retrieval scenarios.",12.76,Qwen2.5-3B,Apple M1 (Metal)
2601.13942v1_Glance-or-Gaze Incentivizing LMMs to Adaptively Fo.pdf,Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning,"Hongbo Bai, Yujin Zhou, Yile Wu, Chi-Min Chan, Pengcheng Wen, Kunhao Pan, Sirui Han, Yike Guo",Not found,Not found,"Large Multimodal Models, Reinforcement Learning, Visual Question Answering, Knowledge Intensive Queries, Long-Tail Entities, Static Parametric Knowledge","This paper proposes Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. The dual-stage training strategy includes Reflective GoG Behavior Alignment via supervised fine-tuning and Complexity-Adaptive Reinforcement Learning, enhancing the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance.",12.22,Qwen2.5-3B,Apple M1 (Metal)
2601.13948v1_Stream-Voice-Anon Enhancing Utility of Real-Time S.pdf,Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models,"Nikita Kuzmin, Songting Liu, Kong Aik Lee, Eng Siong Chng",Not provided,Not provided,"speaker anonymization, neural audio codec, language models, privacy preservation, disentanglement","This paper presents Stream-Voice-Anon, a system that enhances the utility of real-time speaker anonymization by adapting modern causal LM-based neural audio codec architectures specifically for streaming anonymization. The anonymization approach incorporates pseudo-speaker representation sampling, speaker embedding mixing, and diverse prompt selection strategies to prevent speaker information leakage. The system achieves substantial improvements in intelligibility and emotion preservation compared to the previous state-of-the-art streaming method DarkStream while maintaining comparable latency and privacy protection against lazy-informed attackers.",12.11,Qwen2.5-3B,Apple M1 (Metal)
2601.13964v1_RL-BioAug Label-Efficient Reinforcement Learning f.pdf,RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning,"Cheol-Hui Lee, Hwa-Yeon Lee, Dong-Joo Kim",,,"Reinforcement Learning, Self-Supervised Learning, Contrastive Learning, EEG Representation Learning, Data Augmentation, Healthcare","The paper proposes RL-BioAug, a framework that uses a label-efficient reinforcement learning agent to autonomously determine optimal augmentation policies for self-supervised EEG representation learning, significantly outperforming random selection strategies in terms of performance metrics.",11.52,Qwen2.5-3B,Apple M1 (Metal)
2601.13969v1_Autonomous Knowledge Graph Exploration with Adapti.pdf,Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval,"Joaquín Polonuer, Lucas Vittor, Iñaki Arango, Ayush Noori, David A. Clifton, Luciano Del Corro, Marinka Zitnik",Not found,Not found,"Knowledge Graphs, Knowledge Retrieval, Language Models, Adaptive Retrieval, Breadth-Depth Tradeoff","ARK: Adaptive Retriever of Knowledge is an agent-based KG retriever that balances breadth and depth in search using a two-operation toolset. It improves retrieval performance on STaRK and distills tool-use trajectories into an 8B model, enhancing performance on Amazon, MAG, and Prime datasets.",10.98,Qwen2.5-3B,Apple M1 (Metal)
2601.13992v1_The Whole Is Greater Than the Sum of Its Parts A C.pdf,The Whole Is Greater Than the Sum of Its Parts: A Compatibility-Aware Multi-Teacher CoT Distillation Framework,"Jin Cui, Jiaqi Guo, Jiepeng Zhou, Ruixuan Yang, Jiayi Lu, Jiajun Xu, Jiangcheng Song, Boran Zhao, Pengju Ren",,,"CoT reasoning, Large Language Models, Student Models, Distillation, Compatibility, Multi-Teacher, Chain-of-Thought, Catastrophic Forgetting, Reasoning Path Diversity","This paper introduces COMPACT, a framework that adaptively fuses supervisions from different teachers to transfer reasoning strategies of Large Language Models into compact Student Models. It addresses the challenges of teacher-student incompatibility and passive supervision, aiming to integrate diverse reasoning capabilities without damaging the model's original knowledge structure.",11.94,Qwen2.5-3B,Apple M1 (Metal)
2601.13994v1_torch-sla Differentiable Sparse Linear Algebra wit.pdf,torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch,Mingyuan Chi,https://doi.org/10.13140/RV/%s,2601.13994,"PyTorch, sparse linear algebra, differentiable computation, adjoint methods, tensor parallelism","torch-sla is an open-source PyTorch library that enables GPU-accelerated, scalable, and differentiable sparse linear algebra, addressing three fundamental challenges: GPU acceleration for sparse linear solves, nonlinear solves, and eigenvalue computation; multi-GPU scaling via domain decomposition with halo exchange; and adjoint-based differentiation for efficient gradient computation.",11.8,Qwen2.5-3B,Apple M1 (Metal)
2601.13999v1_DAME Duration-Aware Matryoshka Embedding for Durat.pdf,Duration-Aware Matryoshka Embedding for Short-Speaker Verification,"Youngmoon Jung*, Joon-Young Yang*, Ju-ho Kim, Jaeyoung Roh, Chang Woo Han, Hoon-Young Cho",Not found,Not found,"Speaker Verification, Short Utterance, Duration-Aware, Matryoshka Embedding, Multi-Scale Aggregation","Short-utterance speaker verification remains challenging due to limited speaker-discriminative cues in short speech segments. The proposed Duration-Aware Matryoshka Embedding (DAME) builds a nested hierarchy of sub-embeddings aligned to utterance durations, capturing compact speaker traits from short utterances and richer details from longer speech. DAME supports both training from scratch and fine-tuning, improving performance across durations and reducing equal error rate on 1-s and other short-duration trials.",12.63,Qwen2.5-3B,Apple M1 (Metal)
2601.14012v1_MATE Matryoshka Audio-Text Embeddings for Open-Voc.pdf,MATE: MATRYOSHKA AUDIO–TEXT EMBEDDINGS FOR OPEN-VOCABULARY KEYWORD SPOTTING,"Youngmoon Jung, Myunghun Jung, Joon-Young Yang, Yong-Hyeok Lee, Jaeyoung Roh, Hoon-Young Cho",,,"Keyword spotting, open-vocabulary, text enrollment, audio–text embedding, deep metric learning","This work proposes Matryoshka Audio–Text Embeddings (MATE), a dual-encoder framework that encodes multiple embedding granularities within a single vector via nested sub-embeddings. MATE is trained with standard deep metric learning objectives for audio–text keyword spotting, achieving state-of-the-art results on WSJ and LibriPhrase without any inference overhead.",11.94,Qwen2.5-3B,Apple M1 (Metal)
2601.14022v1_Credible CO2 Comparisons A Machine Learning Approa.pdf,Credible CO2 Comparisons: A Machine Learning Approach to Vehicle Powertrain Assessment,"Rodrigo Pereira David, David1, Luciano Araujo Dourado Filho, Daniel Marques da Silva, João Alfredo Cal-Braz",xxx/xxxx,,"machine learning, vehicle emissions, electric vehicles","This paper proposes a machine learning-based framework for like-for-like operational assessment of internal combustion engine vehicles (ICEVs) and electric vehicles (EVs) under identical, real-world driving conditions. It isolates technology-specific effects by holding the observed speed profile and environmental context fixed, enabling direct comparison of powertrain performance. The approach uses recurrent neural network models trained independently for each domain to learn the mapping from contextual driving variables to internal actuation variables and instantaneous CO2-equivalent emission rates. This structure allows the construction of counterfactual scenarios that answer: 'What emissions would an EV have generated if it had followed the same driving profile as an ICEV?' By aligning both vehicle types on a unified instantaneous emissions metric, the framework enables fair and reproducible evaluation of powertrain technologies.",12.78,Qwen2.5-3B,Apple M1 (Metal)
2601.14027v1_Numina-Lean-Agent An Open and General Agentic Reas.pdf,Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics,"Junqi Liu, Zihao Zhou, Zekai Zhu, Marco Dos Santos, Weikun He, Jiawei Liu, Ran Wang, Lihong Zhi, Jia Li, Wenda Li, Yunzhou Xie, Junqiao Zhao, Qiufeng Wang",Not provided,Not provided,"Formal theorem proving, Agentic systems, Neural theorem proving, Lean theorem prover, Mathematical reasoning","This paper introduces Numina-Lean-Agent, a general agentic reasoning system that combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean. The system is designed to improve performance by simply replacing the underlying base model, without training, and to enable flexible extension and autonomous calling of specialized tools. It solves all problems in Putnam 2025 and successfully formalizes the Brascamp–Lieb theorem, demonstrating its generality and flexibility.",12.51,Qwen2.5-3B,Apple M1 (Metal)
2601.14039v1_Generalizing Abstention for Noise-Robust Learning .pdf,Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation,"Wesam Moustafa, Hossam Elsafty, Helen Schneider, Lorenz Sparrenberg, Rafet Sifa",Not found,2601.14039,"Abstention, Medical Image Segmentation, Label Noise, Noise-Robust Learning, Loss Functions","This paper addresses the problem of label noise in medical image segmentation, a critical issue often arising from the inherent difficulty of manual annotation. It introduces a universal and modular abstention framework capable of enhancing the noise-robustness of a diverse range of loss functions, demonstrating its versatility and effectiveness through experiments on the CaDIS and DSAD medical datasets.",11.71,Qwen2.5-3B,Apple M1 (Metal)
2601.14041v1_Top 10 Open Challenges Steering the Future of Diff.pdf,Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants,"Yunhe Wang, Kai Han, Huiling Zhen, Yuchuan Tian, Hanting Chen, Yongbing Huang, Yufei Cui, Yingte Shu, Shan Gao, Ismail Elezi, Roy Vaughan Miles, Songcen Xu, Feng Wen, Chao Xu, Sinan Zeng, Dacheng Tao",,2601.14041,"Large Language Models, Diffusion Models, Transformers","This Perspective identifies ten fundamental challenges for Diffusion Language Models (DLMs), including architectural inertia and gradient sparsity, and proposes a strategic roadmap to overcome these limitations and enable next-generation AI capable of complex structural reasoning and dynamic self-correction.",13.48,Qwen2.5-3B,Apple M1 (Metal)
2601.14047v1_Collective intelligence in science direct elicitat.pdf,COLLECTIVE INTELLIGENCE IN SCIENCE: DIRECT ELICITATION OF DIVERSE INFORMATION FROM EXPERTS WITH UNKNOWN INFORMATION STRUCTURE,"ALEXEY V. OSIPOV, NIKOLAY N. OSIPOV",Not found,2601.14047,"interpretability, wisdom of crowd, play money, prediction market, information pooling, information elicitation, rational expectation equilibrium, direct communication, large language models, scientific collaboration","Proposes a simple mechanism based on a self-resolving play-money prediction market entangled with a chat to aggregate relevant private information from a large group of experts on an open scientific problem, even if the ground truth cannot be established and experts initially know nothing about each other.",12.13,Qwen2.5-3B,Apple M1 (Metal)
2601.14051v1_Kakugo Distillation of Low-Resource Languages into.pdf,Kakugo: Distillation of Low-Resource Languages into Small Language Models,"Peter Devine, Mardhiyah Sanni, Farid Adilazuarda, Julieta Gil Loizaga, Barry Haddow",Not found,Not found,"Small Language Models, Low-Resource Languages, Model Distillation, Synthetic Data, Teacher-Student Model, Natural Language Processing","We present Kakugo, a novel and cost-effective pipeline designed to train general-purpose Small Language Models (SLMs) for low-resource languages using only the language name as input. By using a large teacher model to generate synthetic prompts and translate instruction datasets, we produced training data and SLMs for 54 low-resource languages. Evaluations across a diverse set of general natural language processing tasks demonstrate that our pipeline consistently improves performance over base models. With a total generation and training cost of under $50 per language, Kakugo offers an accessible method for communities to develop language-specific AI.",11.61,Qwen2.5-3B,Apple M1 (Metal)
2601.14053v1_LLMOrbit A Circular Taxonomy of Large Language Mod.pdf,LLMOrbit: A Circular Taxonomy of Large Language Models,"Badri N. Patro, Vijay S. Agneeswaran",,2601.14053,"large language models, artificial intelligence, generative AI, agentic systems, scaling wall, efficiency, post-training techniques, model merging, quantization, distributed edge computing","This paper presents LLMOrbit, a comprehensive circular taxonomy of large language models spanning 2019-2025, examining over 50 major models across 15 organizations through eight interconnected orbital dimensions. It identifies three critical crises threatening AI progress and six different paradigms to break the scaling wall, including post-training gains, efficiency revolution, and democratization. The paper provides insight into key techniques and analyzes the emergence of reasoning in the post-scaling era.",11.74,Qwen2.5-3B,Apple M1 (Metal)
2601.14055v1_Decoder-Free Supervoxel GNN for Accurate Brain-Tum.pdf,Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI,"Andrea Protani, Marc Molina Van Den Bosch, Lorenzo Giusti, Heloisa Barbosa Da Silva, Paolo Cacace, Albert Sund Aillet, Miguel Angel Gonzalez Ballester, Friedhelm Hummel, Luigi Serio",Not found,2601.14055,"BrainTumorLocalization, GraphNeuralNetworks, Multi-modal MRI, Supervoxel, Regression","Our approach introduces SVGFormer, a decoder-free pipeline that partitions the volume into a semantic graph of super-voxels. Its hierarchical encoder learns rich node representations by combining patch-level Transformer with supervoxel-level GraphAttention Network, achieving strong performance in node-level classification and tumor proportion regression.",13.21,Qwen2.5-3B,Apple M1 (Metal)
2601.14056v1_POCI-Diff Position Objects Consistently and Intera.pdf,POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion,"Andrea Rigo, Luca Stornaiuolo, Weijie Wang, Mauro Martino, Bruno Lepri, Nicu Sebe",Not found,2601.14056,"Diffusion, Image Generation, 3D Layout","We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, supporting one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. Our method preserves object identity and consistency across edits by conditioning the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.",13.7,Qwen2.5-3B,Apple M1 (Metal)
2601.14063v1_XCR-Bench A Multi-Task Benchmark for Evaluating Cu.pdf,XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs,"Mohsinul Kabir, Tasnim Ahmed, Md Mezbaur Rahman, Shaoxiong Ji, Hassan Alhuzali, Sophia Ananiadou",,,"large language models, cross-cultural competence, cultural reasoning, language models, cross-cultural sentence pairs, CSI annotation, social etiquette, cultural reference, cultural adaptation, linguistic bias","XCR-Bench is a benchmark for evaluating cross-cultural reasoning in large language models (LLMs), consisting of 4.9k parallel sentences and 1,098 unique Culture-Specific Items (CSIs). It integrates Newmark's CSI framework with Hall's Triad of Culture, enabling systematic analysis of cultural reasoning beyond surface-level artifacts. The benchmark highlights weaknesses in identifying and adapting CSIs related to social etiquette and cultural reference, and evidence of regional and ethno-religious biases in LLMs during cultural adaptation. The corpus and code are released to facilitate future research on cross-cultural NLP.",12.64,Qwen2.5-3B,Apple M1 (Metal)
2601.14069v1_Unsupervised Video Class-Incremental Learning via .pdf,Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management,"Nattapong Kurpukdee, Adrian G. Bors",Not provided,Not provided,"Unsupervised, Video Class-Incremental, Video Continual Learning, Deep Embedded Clustering","This paper proposes a simple yet effective approach to address unsupervised video class incremental learning (uVCIL), focusing on learning video information without forgetting and without considering any data labels. The approach involves a deep feature extractor network providing representative video features during each task and progressively building a series of deep clusters from the extracted features. During successive task learning, the model updated from the previous task is used as an initial state to transfer knowledge to the current learning task. The approach is evaluated on three standard video action recognition datasets, significantly outperforming other baselines.",12.44,Qwen2.5-3B,Apple M1 (Metal)
2601.14084v1_DermaBench A Clinician-Annotated Benchmark Dataset.pdf,DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning,"Abdurrahim Yilmaz, Ozan Erdem, Ece Gokyayla, Ayda Acar, Burc Bugra Dagtas, Dilara Ilhan Erdil, Gulsum Gencoglan, Burak Temelkuran",,,"Dermatology, Visual Question Answering, Multimodal Learning, Clinician Annotation, Benchmark Dataset","DermaBench is a clinician-annotated dermatology Visual Question Answering (VQA) benchmark dataset built on the Diverse Dermatology Images (DDI) dataset. It comprises 656 clinical images from 570 unique patients, covering Fitzpatrick skin types I–VI. The dataset includes annotations for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, along with open-ended narrative descriptions and summaries. It is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.",12.87,Qwen2.5-3B,Apple M1 (Metal)
2601.14086v1_Two-Stream temporal transformer for video action c.pdf,TWO-STREAM TEMPORAL TRANSFORMER FOR VIDEO ACTION CLASSIFICATION,"Nattapong Kurpukdee, Adrian G. Bors",Not found,Not found,"Video Transformer, Optical Flow, Two-Stream video processing, Video Action Classification","Motion representation plays an important role in video understanding and has many applications including action recognition, robot and autonomous guidance or others. Lately, transformer networks, through their self-attention mechanism capabilities, have proved their efficiency in many applications. In this study, we introduce a new two-stream transformer video classifier, which extracts spatio-temporal information from content and optical flow representing movement information.",12.0,Qwen2.5-3B,Apple M1 (Metal)
2601.14087v1_1-bit Count-based Sorting Unit to Reduce Link Powe.pdf,'1'-bit Count-based Sorting Unit to Reduce Link Power in DNN Accelerators,"Ruichi Han, Yizhi Chen, Tong Lei, Jordi Altayo Gonzalez, Ahmed Hemani",Not provided,Not provided,"'1'-bit count, approximate computing, bit transition reduction, link power","This work proposes the hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks (CNN). By leveraging approximate computing to group population counts into coarse-grained buckets, our design achieves hardware area reductions while preserving the link power benefits of data reordering. Our approximate sorting unit achieves up to 35.4% area reduction while maintaining 19.50% BT reduction compared to 20.42% of precise implementation.",12.18,Qwen2.5-3B,Apple M1 (Metal)
2601.14091v1_Zero-shot adaptable task planning for autonomous c.pdf,Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems,"Hossein Naderi1, Alireza Shojaei2, Lifu Huang3, Philip Agee4, Kereshmeh Afsari5, Abiola Akanmu6",,,"Construction robotics, quadruped robots, robot task planning, multi-AI agent, LLMs, VLMs, GPT4o","This study explores the potential of foundation models to enhance the adaptability and generalizability of task planning in construction robots. Four models are proposed and implemented using lightweight, open-source large language models (LLMs) and vision language models (VLMs). These models include one single agent and three multi-agent teams that collaborate to create robot action plans. The models are evaluated across three construction roles: Painter, Safety Inspector, and Floor Tiling. Results show that the four-agent team outperforms the state-of-the-art GPT-4o in most metrics while being ten times more cost-effective. Additionally, teams with three and four agents demonstrate improved generalizability. By discussing how agent behaviors influence outputs, this study enhances the understanding of AI teams and supports future research in diverse unstructured environments beyond construction.",13.2,Qwen2.5-3B,Apple M1 (Metal)
2601.14096v1_Remapping and navigation of an embedding space via.pdf,Remapping and navigation of an embedding space via error minimization: a fundamental organizational principle of cognition in natural and artificial systems,"Benedikt Hartl, Léo Pio-Lopez, Chris Fields, Michael Levin",Not found,2601.14096,"Evolution, Development, Intelligence, Active Inference, Navigation Policy, Nested Embedding Spaces",A fundamental organizational principle of cognition in natural and artificial systems is the remapping and navigation of an embedding space via error minimization.,15.08,Qwen2.5-3B,Apple M1 (Metal)
2601.14099v1_Causal feature selection framework for stable soft.pdf,Causal Feature Selection Framework for Stable Soft Sensor Modeling based on Time-Delayed Cross Mapping,"Shi-Shun Chen, Xiao-Yang Li, Enrico Zio",,,"Causal Feature Selection, Soft Sensor Modeling, Time-Delayed Cross Mapping, Stability",This paper presents a causal feature selection framework for developing stable soft sensors using time-delayed cross mapping.,14.52,Qwen2.5-3B,Apple M1 (Metal)
2601.14115v1_Riemannian Liquid Spatio-Temporal Graph Network.pdf,Riemannian Liquid Spatio-Temporal Graph Network,"Liangsi Lu, Jingchao Wang, Zhaorong Dai, Hanqian Liu, Yang Shi",10.1145/3774904.3792090,,"Riemannian Manifolds, Neural ODEs, Spatio-Temporal Graphs","This paper introduces the Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), a framework that combines continuous-time liquid dynamics with the geometric inductive biases of Riemannian manifolds. RLSTG models graph evolution through an Ordinary Differential Equation (ODE) formulated directly on a curved manifold, enabling it to faithfully capture the intrinsic geometry of both structurally static and dynamic spatio-temporal graphs. The paper provides rigorous theoretical guarantees for RLSTG, extending stability theorems of Liquid Time-Constant (LTC) networks to the Riemannian domain and quantifying its expressive power via state trajectory analysis. Extensive experiments on real-world benchmarks demonstrate that RLSTG achieves superior performance on graphs with complex structures.",12.4,Qwen2.5-3B,Apple M1 (Metal)
2601.14124v1_Style Transfer as Bias Mitigation Diffusion Models.pdf,Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic,"Saad Mankarious, Ayah Zirikly",Not provided,2601.14124,"Synthetic Data, Bias Mitigation, Diffusion Models, Style Transfer, Mental Health, Arabic Language","This work proposes a pretraining-free diffusion-based approach for synthetic text generation in Arabic mental health, focusing on male-to-female style transfer to mitigate gender bias. Using the CARMA corpus, it constructs five datasets capturing varying linguistic and semantic aspects of gender expression and trains separate diffusion models for each setting. Quantitative evaluations show high semantic fidelity and meaningful stylistic divergence, while qualitative analysis confirms plausible gender transformations.",12.23,Qwen2.5-3B,Apple M1 (Metal)
2601.14152v1_Lost in the Prompt Order Revealing the Limitations.pdf,Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models,"Hyunjong Ok, Jaeho Lee",,,"language models, prompt sensitivity, causal attention, multiple-choice question answering","Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. This work investigates a striking case in multiple-choice question answering, where placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%. Through systematic architectural analysis, causal attention is identified as the core mechanism responsible for this phenomenon.",10.83,Qwen2.5-3B,Apple M1 (Metal)
2601.14154v1_LLM Augmented Intervenable Multimodal Adaptor for .pdf,LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery,"Shubham Pandey†, Bhavin Jawade†, Srirangaraj Setlur†, Venu Govindaraju†, Kenneth Seastedt⋆",Not found,Not found,"deep learning, lung cancer surgery, postoperative complications, interventional deep learning, hyperspherical embedding, radiomics, machine learning","We present MIRA-CLE, a deep learning architecture for predicting postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRA-CLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. We incorporate an interventional deep learning module to enhance transparency and clinical utility, providing interpretable and actionable insights for domain experts. Our approach is validated on a real-world dataset of 3,094 lung cancer patients, demonstrating superior performance compared to traditional machine learning models and contemporary large language models (LLM) variants.",12.6,Qwen2.5-3B,Apple M1 (Metal)
2601.14157v1_ConceptCaps -- a Distilled Concept Dataset for Int.pdf,ConceptCaps - a Distilled Concept Dataset for Interpretability in Music Models,"Bruno Sienkiewicz, Łukasz Neumann, Mateusz Modrzejewski",Not found,Not found,"Music, Interpretability, Concept Activation Vectors, Variational Autoencoder, Language Model, Music Generation, Audio-Text Alignment, Linguistic Quality Metrics","This paper introduces ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. The pipeline separates semantic modeling from text generation, improving coherence and controllability over end-to-end approaches. The dataset is validated through audio-text alignment, linguistic quality metrics, and TCA V analysis.",11.9,Qwen2.5-3B,Apple M1 (Metal)
2601.14160v1_Domain-Adaptation through Synthetic Data Fine-Tuni.pdf,Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law,"Ali Hamza Bashir, Muhammad Rehan Khalid, Kostadin Cvejoski, Jana Birr, Armin Berger, Sandra Halscheidt, Christian Temath, Rafet Sifa, David Berghaus",,,"domain adaptation, large language models, synthetic data, German law, legal question answering","This paper presents an effective method for adapting advanced Large Language Models (LLMs) to German legal question answering through a novel synthetic data generation approach. The approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes, using rigorous automated filtering methods and parameter-efficient fine-tuning techniques. The results demonstrate significant outperformance of LLMs adapted with the synthetic dataset on German legal question answering tasks.",12.2,Qwen2.5-3B,Apple M1 (Metal)
2601.14171v1_Paper2Rebuttal A Multi-Agent Framework for Transpa.pdf,Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance,"Qianli Ma*, Chang Guo*, Zhiheng Tian*, Siyu Wang, Jipeng Xiao, Yuanhao Yue, Zhipeng Zhang†",,,"peer review, rebuttal generation, multi-agent systems, transparent assistance","This paper introduces REBUTTALAGENT, a multi-agent framework designed to assist authors in generating transparent and evidence-backed rebuttals to peer reviews. The system reframes rebuttal generation as an evidence-centric planning task, decomposing complex feedback into atomic concerns and dynamically constructing hybrid contexts. It integrates an autonomous external search module to resolve concerns requiring outside literature, ensuring that every argument is explicitly anchored in internal or external evidence. The authors validate their approach on the proposed REBUTTALBENCH, demonstrating superior performance in coverage, faithfulness, and strategic coherence compared to strong baselines. The system offers a transparent and controllable assistant for the peer review process.",12.15,Qwen2.5-3B,Apple M1 (Metal)
2601.14172v1_Human Values in a Single Sentence Moral Presence H.pdf,"Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum","Víctor Yeste, Paolo Rosso",Not found,2601.14172,"Human values, Schwartz motivational continuum, Transformer ensembles, Sentence-level identification, Moral presence","The study focuses on sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. It addresses the challenges posed by sparse moral cues and class imbalance in out-of-context sentences from news and political manifestos, using a binary moral presence task and comparing presence-gated hierarchies to direct multi-label classifiers.",13.31,Qwen2.5-3B,Apple M1 (Metal)
2601.14175v1_A model of errors in transformers.pdf,A model of errors in transformers,"Suvrat Raju, Praneeth Netrapalli",,,"transformers, LLMs, errors, attention mechanism, quantitative model","We study the error rate of LLMs on tasks like arithmetic and dynamic programming, and derive a two-parameter relationship between accuracy and task complexity. Our model provides an alternative to suggestions of collapse of reasoning or inability to express compositional functions.",9.84,Qwen2.5-3B,Apple M1 (Metal)
2601.14192v1_Toward Efficient Agents Memory Tool learning and P.pdf,"Toward Efficient Agents: A Survey of Memory, Tool learning, and Planning","Xiaofang Yang1,2,†, Lijun Li1,2,†, Heng Zhou1,3,†, Tong Zhu1,†, Xiaoye Qu1, Yuchen Fan1, Qianshan Wei5, Rui Ye4, Li Kang1,4, Yiran Qin6, Zhiqiang Kou7, Daizong Liu8, Qi Li5, Ning Ding9, Siheng Chen4, Jing Shao1",,2601.14192v1,"Agents, Efficiency, Agent Memory, Tool Learning, Planning","This paper investigates the efficiency of agents in three core components: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. It reviews recent approaches that differ in implementation yet converge on shared high-level principles, including bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency. The trade-off between effectiveness and cost is also discussed through the Pareto frontier.",13.78,Qwen2.5-3B,Apple M1 (Metal)
2601.14209v1_InT Self-Proposed Interventions Enable Credit Assi.pdf,InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning,"Matthew Y. R. Yang1, Hao Bai2, Ian Wu1, Gene Yang1, Amrith Setlur1, Aviral Kumar1",Not found,Not found,"Reinforcement Learning, Large Language Models, Credit Assignment, Outcome-Based Reinforcement Learning, Intervention Training","This paper introduces Interventions Training (InT), a method for improving the reasoning capabilities of large language models (LLMs) using reinforcement learning. InT enables fine-grained credit assignment by proposing short, targeted corrections to the model's reasoning traces, which are then fine-tuned to improve accuracy.",12.22,Qwen2.5-3B,Apple M1 (Metal)
2601.14230v1_MASCOT Towards Multi-Agent Socio-Collaborative Com.pdf,MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems,"Yiyang Wang, Yiqiao Jin, Alex Cabral, Josiah Hester",,,"Multi-agent systems, Socio-collaborative companions, Persona collapse, Social sycophancy, Persona-aware behavioral alignment, Collaborative dialogue optimization","Multi-agent systems (MAS) have emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse and social sycophancy, leading to redundant, non-constructive dialogue. This paper proposes MASCOT, a generalizable framework that introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors, including Persona-Aware Behavioral Alignment and Collaborative Dialogue Optimization. Extensive evaluations across psychological support and workplace domains demonstrate significant improvements in Persona Consistency and Social Contribution.",12.14,Qwen2.5-3B,Apple M1 (Metal)
2601.14232v1_KAGE-Bench Fast Known-Axis Visual Generalization E.pdf,KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning,"Egor Cherepannov, Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov",Not found,Not found,"Reinforcement Learning, Pixel-based RL, Visual Generalization, Known-Axis, Environment Factorization, Pixel Policy, Known-Axis Suites, Known-Axis Shifts, Known-Axis Evaluation, Known-Axis Benchmark","This paper introduces KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Using a standard PPO-CNN baseline, the authors observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. The fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors.",12.65,Qwen2.5-3B,Apple M1 (Metal)
2601.14234v1_Q-learning with Adjoint Matching.pdf,Q-learning with Adjoint Matching,"Qiyang Li, Sergey Levine",Not found,2601.14234,"Q-learning, reinforcement learning, continuous-action RL, flow policies, diffusion policies, adjoint matching","Q-learning with Adjoint Matching (QAM) is a novel TD-based reinforcement learning algorithm that tackles the challenge of optimizing expressive diffusion or flow-matching policies with respect to a parameterized Q-function. It sidesteps the numerical instability of direct gradient-based optimization through a multi-step denoising process by leveraging adjoint matching, a technique from generative modeling. QAM combines temporal-difference backup for critic learning and consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.",11.93,Qwen2.5-3B,Apple M1 (Metal)
2601.14235v1_Opportunities in AIML for the Rubin LSST Dark Ener.pdf,Opportunities in AI/ML for the Rubin LSST,"Dark Energy Science Collaboration, Eric Aubourg, Camille Avestruz, Matthew R. Becker, Biswajit Biswas, Rahul Biswas, Boris Bolliet, Adam S. Bolton, Clecio R. Bom, Raphaël Bonnet-Guerrini, Alexandre Boucaud, Jean-Eric Campagne, Chihway Chang, Aleksandra Ciprijanović, Johann Cohen-Tanugi, Michael W. Coughlin, John Franklin Crenshaw, Juan C. Cuevas-Tello, Juan de Vicente, Seth W. Digel, Steven Dillmann, Alex Drlica-Wagner, Sydney Erickson, Alexander T. Gagliano, Christos Georgiou, Aritra Ghosh, Matthew Grayling, Kirill A. Grishin, Alan Heavens, Lindsay R. House, Mustapha Ishak, Wassim Kabalan, Arun Kannawadi, Francis Lanusse, C. Danielle Leonard, Pierre-Franc François Léget, Michelle Lochner, Yao-Yuan Mao, Peter Melchior, Grant Merz, Martin Millon, Anais Moller, Gautham Narayan, Yuuki Omori, Hiranya Peiris, Andrés A. Plazas Malagón, Nesar Ramachandra, Benjamin Remy, Cécile Roucelle, Jaime Ruiz-Zapatero, Stefan Schuldt, Ignacio Sevilla-Noarbe, Ved G. Shah, Tjitske Starkenburg, Stephen Thorp, Laura Toribio San Cipriano, Tilman Tröster, Roberto Trotta, Padma Venkatraman, Amanda Wasserman, Tim White, Justine Zeghal, Tianqing Zhang, Yuanyuan Zhang",,,"AI, Machine Learning, Dark Energy, LSST",This paper discusses opportunities for applying AI and machine learning techniques to enhance the Dark Energy Science Collaboration's research using the Large Synoptic Survey Telescope (LSST).,12.25,Qwen2.5-3B,Apple M1 (Metal)
2601.14255v1_VideoMaMa Mask-Guided Video Matting via Generative.pdf,VideoMaMa: Mask-Guided Video Matting via Generative Prior,"Sangbeom Lim, Seoung Wug Oh, Jiahui Huang, Heeji Yoon, Seungryong Kim, Joon-Young Lee",https://doi.org/10.1101/2601.14255v1,2601.14255v1,"video matting, diffusion models, pseudo-labeling, real-world footage","VideoMaMa is a diffusion-based model that generates high-quality alpha mattes from input binary segmentation masks, demonstrating strong zero-shot generalization to real-world footage.",13.54,Qwen2.5-3B,Apple M1 (Metal)
