filename,title,authors,doi,arxiv_id,keywords,summary,tps,model,platform
2601.07192v1_Relink Constructing Query-Driven Evidence Graph On.pdf,Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG,"Manzong Huang, Chenyang Bu, Yi He, Xingrui Zhuo, Xindong Wu",,2309.14426,"GraphRAG, Relink, Query-Driven Evidence Graph, Open-Domain Question Answering, Large Language Models, Knowledge Graph, Factual Errors, Retrieval-Augmented Generation","Graph-based Retrieval-Augmented Generation (GraphRAG) mitigates hallucinations in Large Language Models (LLMs) by grounding them in structured knowledge. However, current GraphRAG methods are constrained by a prevailing build-then-reason paradigm, which relies on a static, pre-constructed Knowledge Graph (KG). This paradigm faces two critical challenges: the inherent incompleteness of the KG often breaks reasoning paths, and the low signal-to-noise ratio introduces distractor facts that disrupt the reasoning process. To address these challenges, the authors propose Relink, a framework that dynamically builds a query-specific evidence graph. Relink instantiates required facts from a latent relation pool derived from the original text corpus, repairing broken paths on the fly. It employs a unified, query-aware evaluation strategy to select the most useful facts for answering the query. Extensive experiments on five Open-Domain Question Answering benchmarks show that Relink achieves significant improvements in EM and F1 scores over leading GraphRAG baselines, demonstrating the superiority of the proposed framework.",27.77,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07197v1_Beyond Variance Knowledge-Aware LLM Compression vi.pdf,Beyond Variance: Knowledge-Aware LLM Compression via Fisher-Aligned Subspace Diagnostics,"Ibne Farabi Shihab*, Sanjeda Akter*, Anuj Sharma",,2405.16017,"Large Language Models, Knowledge Compression, Fisher Information Matrix, Transformer Architectures, Activation-Gradient Coupling, Dependence Violation Score","Post-training activation compression is essential for deploying Large Language Models (LLMs) on resource-constrained hardware. Standard methods like Singular Value Decomposition (SVD) are gradient-blind, preserving high-variance dimensions regardless of their impact on factual knowledge preservation. The paper introduces Fisher-Aligned Subspace Compression (FASC), a knowledge-aware compression framework that selects subspaces by directly modeling activation-gradient coupling, minimizing a second-order surrogate of the loss function. FASC leverages the Fisher Information Matrix to identify dimensions critical for factual knowledge, often residing in low-variance but high-gradient-sensitive subspaces. The Dependence Violation Score (ρ) is proposed as a general-purpose diagnostic metric to quantify activation-gradient coupling, revealing where factual knowledge is stored within transformer architectures. Extensive experiments on Mistral-7B and Llama-3-8B demonstrate that FASC preserves 6-8% more accuracy on knowledge-intensive benchmarks (MMLU, LAMA) compared to variance-based methods at 50% rank reduction, effectively enabling a 7B model to match the factual recall of a 13B uncompressed model. The analysis reveals that ρ serves as a fundamental signal of stored knowledge, with high-ρ layers emerging only when models internalize factual associations during training.",28.87,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07199v1_Forward versus Backward Comparing Reasoning Object.pdf,Forward versus Backward: Comparing Reasoning Objectives in Direct Preference Optimization,"Murtaza Nikazad, Raghuram Ramanujan",,1909.06853,"Direct Preference Optimization, Reasoning, Training Objectives, GSM8K, Large Language Models","This paper investigates the effect of training objective composition on reasoning reliability through Direct Preference Optimization. Two complementary training signals are examined: forward chain-of-thought generation, which trains the model to produce correct reasoning traces, and backward verification, which trains the model to verify and acknowledge errors in candidate solutions. Experiments on GSM8K reveal a fundamental trade-off between these objectives. Forward-only DPO training achieves the highest accuracy improvement, increasing from 83.1% to 86.6% (+3.5 percentage points), while backward-only training yields minimal accuracy gains but substantially reduces the false positive rate from 13.4% to 4.3%. Notably, both training variants reduce acknowledgement rate compared to the baseline, suggesting that preference optimization increases model confidence in its outputs. These findings indicate that forward and backward reasoning objectives provide distinct and complementary learning signals: forward training improves problem-solving capability, while backward training improves verification calibration. The complete training and evaluation pipeline, implemented efficiently through Low-Rank Adaptation, is released to facilitate further research.",29.62,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07200v1_Safeguarding LLM Fine-tuning via Push-Pull Distrib.pdf,Safeguarding LLM Fine-tuning via Push-Pull Distributional Alignment,"Haozhong Wang, Zhuo Li, Yibo Yang, He Zhao, Hongyuan Zha, Dandan Guo",,2310.14237,"Large Language Models, Fine-tuning, Safety Alignment, Optimal Transport, Push-Pull Mechanism","The paper introduces Safety Optimal Transport (SOT), a novel framework that reframes safe fine-tuning from an instance-level filtering challenge to a distribution-level alignment task grounded in Optimal Transport (OT). SOT optimizes sample importance by actively pulling the downstream distribution towards a trusted safe anchor while simultaneously pushing it away from a general harmful reference. This establishes a robust geometric safety boundary that effectively purifies the training data. Extensive experiments across diverse model families and domains demonstrate that SOT significantly enhances model safety while maintaining competitive downstream performance, achieving a superior safety-utility trade-off compared to baselines.",28.25,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07201v1_CalPro Prior-Aware Evidential--Conformal Predictio.pdf,CalPro: Prior-Aware Evidential–Conformal Prediction with Structure-Aware Guarantees for Protein Structures,"Ibne Farabi Shihab * 1, Sanjeda Akter * 1, Anuj Sharma 2",,2601.03567,"protein structure prediction, uncertainty quantification, conformal prediction, deep learning, graph-based architecture, domain priors","Deep protein structure predictors like AlphaFold provide non-calibrated confidence estimates that degrade under distribution shifts. CalPro introduces a prior-aware evidential-conformal framework for robust uncertainty quantification. It combines a geometric evidential head, a differentiable conformal layer, and domain priors. Theoretical guarantees show near-nominal coverage while tighter intervals than vanilla conformal. Empirically, CalPro achieves ≤5% coverage degradation, reduces calibration error by 30–50%, and improves downstream success by 25%. The framework applies to structured regression tasks, validated on non-biological benchmarks.",28.52,Qwen2.5-3B,AMD RX 6800 (Vulkan)
