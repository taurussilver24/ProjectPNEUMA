filename,title,authors,summary,tps,model,platform
2601.07192v1_Relink Constructing Query-Driven Evidence Graph On.pdf,Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG,"['Manzong Huang', 'Chenyang Bu', 'Yi He', 'Xingrui Zhuo', 'Xindong Wu']","The paper introduces Relink, a framework that dynamically constructs a query-specific evidence graph to address the limitations of static Knowledge Graphs (KGs) in Graph-based Retrieval-Augmented Generation (GraphRAG). Current GraphRAG methods suffer from incomplete KGs that break reasoning paths and low signal-to-noise ratios that introduce distractor facts. Relink proposes a reason-and-construct paradigm, instantiating required facts from a latent relation pool and employing a query-aware evaluation strategy to select the most relevant knowledge. This approach allows Relink to discard distractor facts and construct precise evidence paths for each query. Experimental results on five Open-Domain Question Answering benchmarks show that Relink achieves significant improvements over existing GraphRAG baselines, with average increases of 5.4% in Exact Match (EM) and 5.2% in F1 scores.",153.85,Phi-4,Nvidia B200 (Cloud Native)
2601.07197v1_Beyond Variance Knowledge-Aware LLM Compression vi.pdf,Beyond Variance: Knowledge-Aware LLM Compression via Fisher-Aligned Subspace Diagnostics,"['Ibne Farabi Shihab', 'Sanjeda Akter', 'Anuj Sharma']","This paper introduces Fisher-Aligned Subspace Compression (FASC), a novel framework for compressing Large Language Models (LLMs) that prioritizes factual knowledge preservation. Unlike traditional methods such as Singular Value Decomposition (SVD), which focus on high-variance dimensions, FASC utilizes the Fisher Information Matrix to identify and preserve dimensions critical for factual knowledge, often found in low-variance but high-gradient-sensitivity subspaces. The paper proposes the Dependence Violation Score (ρ) as a diagnostic metric to quantify activation-gradient coupling, revealing where factual knowledge is stored within transformer architectures. Experimental results on models like Mistral-7B and Llama-3-8B show that FASC achieves 6-8% higher accuracy on knowledge-intensive benchmarks compared to variance-based methods, even with a 50% rank reduction. This enables a 7B model to match the factual recall of a 13B uncompressed model, highlighting the importance of ρ as a signal of stored knowledge.",153.93,Phi-4,Nvidia B200 (Cloud Native)
2601.07199v1_Forward versus Backward Comparing Reasoning Object.pdf,Forward versus Backward: Comparing Reasoning Objectives in Direct Preference Optimization,"['Murtaza Nikazad', 'Raghuram Ramanujan']","This paper explores the impact of different training objectives on the reasoning reliability of large language models using Direct Preference Optimization. It compares forward chain-of-thought generation, which trains models to produce correct reasoning traces, with backward verification, which trains models to verify and acknowledge errors in candidate solutions. Experiments on the GSM8K dataset reveal a trade-off: forward-only training significantly improves accuracy, while backward-only training reduces false positives but offers minimal accuracy gains. Both methods decrease the model's error acknowledgment rate, indicating increased confidence in outputs. The study concludes that forward and backward reasoning objectives provide distinct and complementary learning signals, enhancing problem-solving and verification calibration, respectively. The research pipeline, implemented using Low-Rank Adaptation, is made available for further study.",153.81,Phi-4,Nvidia B200 (Cloud Native)
2601.07200v1_Safeguarding LLM Fine-tuning via Push-Pull Distrib.pdf,Safeguarding LLM Fine-tuning via Push-Pull Distributional Alignment,"['Haozhong Wang', 'Zhuo Li', 'Yibo Yang', 'He Zhao', 'Hongyuan Zha', 'Dandan Guo']","The paper introduces Safety Optimal Transport (SOT), a novel framework for enhancing the safety of Large Language Models (LLMs) during fine-tuning. SOT addresses the erosion of safety alignment by reframing the challenge from an instance-level filtering task to a distribution-level alignment task. It employs a dual-reference 'push-pull' weight-learning mechanism to optimize sample importance, pulling the downstream distribution towards a trusted safe anchor and pushing it away from a harmful reference. This approach establishes a robust geometric safety boundary, effectively purifying the training data. Experiments demonstrate that SOT significantly enhances model safety while maintaining competitive performance, achieving a superior safety-utility trade-off compared to existing methods.",153.82,Phi-4,Nvidia B200 (Cloud Native)
2601.07201v1_CalPro Prior-Aware Evidential--Conformal Predictio.pdf,CalPro: Prior-Aware Evidential–Conformal Prediction with Structure-Aware Guarantees for Protein Structures,"['Ibne Farabi Shihab', 'Sanjeda Akter', 'Anuj Sharma']","CalPro introduces a novel framework for robust uncertainty quantification in protein structure prediction, addressing the limitations of existing confidence estimates like pLDDT. It combines a geometric evidential head, a differentiable conformal layer, and domain priors to provide shift-robust uncertainty quantification. The framework maintains near-nominal coverage and achieves tighter intervals in regions where priors are informative. Empirical results show significant improvements in coverage degradation, calibration error reduction, and ligand-docking success compared to baseline methods. The approach is applicable beyond proteins to structured regression tasks with local reliability priors.",154.46,Phi-4,Nvidia B200 (Cloud Native)
2601.07206v1_LLMRouterBench A Massive Benchmark and Unified Fra.pdf,LLMRouterBench: A Massive Benchmark and Unified Framework for LLM Routing,"['Hao Li', 'Yiqun Zhang', 'Zhaoyan Guo', 'Chenxu Wang', 'Shengji Tang', 'Qiaosheng Zhang', 'Yang Chen', 'Biqing Qi', 'Peng Ye', 'Lei Bai', 'Zhen Wang', 'Shuyue Hu']","LLMRouterBench is introduced as a large-scale benchmark and unified framework for LLM routing, comprising over 400K instances from 21 datasets and 33 models. It provides comprehensive metrics for both performance-oriented and performance-cost trade-off routing, integrating 10 representative routing baselines. The benchmark reveals strong model complementarity in LLM routing, but finds that many methods perform similarly under unified evaluation, with some recent approaches failing to outperform simple baselines. It highlights a gap to the Oracle due to model-recall failures, shows limited impact of backbone embedding models, and notes diminishing returns from larger ensembles compared to careful model curation. Additionally, it enables latency-aware analysis.",153.31,Phi-4,Nvidia B200 (Cloud Native)
2601.07209v1_SIRR-LMM Single-image Reflection Removal via Large.pdf,SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model,"['Yu Guo', 'Zhiqiang Lao', 'Xiyun Song', 'Yubin Zhou', 'Heather Yu']","This paper addresses the challenge of single-image reflection removal (SIRR) by introducing a synthetic dataset generation framework that path-traces 3D glass models over real background imagery. This approach creates physically accurate reflection scenarios with varied glass properties, camera settings, and post-processing effects. The paper leverages Large Multimodal Models (LMMs) by concatenating image layers into a single composite input, applying joint captioning, and fine-tuning the model using task-specific Low-Rank Adaptation (LoRA) rather than full-parameter training. This method achieves improved reflection removal and separation performance compared to state-of-the-art methods, addressing the bottleneck of limited high-quality training data in the field.",155.14,Phi-4,Nvidia B200 (Cloud Native)
2601.07214v1_BlindU Blind Machine Unlearning without Revealing .pdf,BlindU: Blind Machine Unlearning without Revealing Erasing Data,"['Weiqi Wang', 'Zhiyi Tian', 'Chenhan Zhang', 'Shui Yu']","This paper introduces Blind Unlearning (BlindU), a method for machine unlearning that protects user privacy by not requiring the uploading of erasing data to the server. It utilizes compressed representations and the information bottleneck (IB) mechanism for federated learning model training. The paper proposes two unlearning modules for IB-based models and employs a multiple gradient descent algorithm to balance forgetting and utility retention. Additionally, a noise-free differential privacy (DP) masking method is introduced to enhance privacy protection. Theoretical analysis and experiments demonstrate BlindU's effectiveness in privacy protection and unlearning compared to existing benchmarks.",154.97,Phi-4,Nvidia B200 (Cloud Native)
2601.07224v1_Consolidation or Adaptation PRISM Disentangling SF.pdf,Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration,"['Yang Zhao', 'Yangou Ouyang', 'Xiao Ding', 'Hepeng Wang', 'Bibo Cai', 'Kai Xiong', 'Jinglong Gao', 'Zhouhao Sun', 'Li Du', 'Bing Qin', 'Ting Liu']","The paper introduces PRISM, a framework for optimizing data allocation between Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) in training Large Language Model (LLM) agents. PRISM uses gradient concentration to identify data that requires RL for structural adaptation versus data suitable for SFT consolidation. This approach aims to improve alignment efficiency and reduce computational costs, demonstrating significant improvements over existing methods in experiments.",154.66,Phi-4,Nvidia B200 (Cloud Native)
2601.07226v1_Lost in the Noise How Reasoning Models Fail with C.pdf,Lost in the Noise: How Reasoning Models Fail with Contextual Distractors,"['Seongyun Lee', 'Yongrae Jo', 'Minju Seo', 'Moontae Lee', 'Minjoon Seo']","The paper introduces NoisyBench, a benchmark designed to evaluate the robustness of reasoning models against various types of noise, such as random documents, irrelevant chat histories, and hard negative distractors. The study reveals a significant performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. It highlights that agentic workflows often exacerbate errors by over-trusting noisy tool outputs, and that distractors can lead to misalignment without adversarial intent. The paper proposes the Rationale-Aware Reward (RARE) method to improve resilience by encouraging models to identify useful information within noisy contexts. Additionally, it uncovers an inverse scaling trend where increased computation at test time results in poorer performance in noisy settings, and provides insights through attention visualization showing models' focus on distractor tokens.",154.82,Phi-4,Nvidia B200 (Cloud Native)
2601.07229v2_DiSCo Making Absence Visible in Intelligent Summar.pdf,DiSCo: Making Absence Visible in Intelligent Summarization Interfaces,"['ERAN FAINMAN', 'HAGIT BEN SHOSHAN', 'ADIR SOLOMON', 'OSNAT MOKRYN']","Intelligent interfaces often use large language models to summarize user-generated content, focusing on what is mentioned and overlooking what is missing. This presence bias can mislead users who rely on summaries for decision-making. The paper introduces Domain Informed Summarization through Contrast (DiSCo), a computational approach that highlights absences by comparing content with domain expectations. DiSCo identifies and integrates aspects that are unusually emphasized or missing relative to domain norms. A user study across ski, beach, and city center accommodation domains showed that DiSCo summaries were rated as more detailed and useful for decision-making than baseline summaries, despite being slightly harder to read. The study demonstrates that modeling expectations reduces presence bias and enhances transparency and decision support in intelligent summarization interfaces.",154.99,Phi-4,Nvidia B200 (Cloud Native)
2601.07232v1_Yes FLoReNce I Will Do Better Next Time Agentic Fe.pdf,"Y es FLoReNce, I Will Do Better Next Time! Agentic Feedback Reasoning for Humorous Meme Detection","['Olivia Shanhong Liu', 'Pai Chet Ng', 'De Wen Soh', 'Konstantinos N. Plataniotis']","The paper introduces FLoReNce, an agentic feedback reasoning framework designed to improve the understanding of humorous memes by AI systems. Unlike existing models that generate explanations in an open-loop manner, FLoReNce operates as a closed-loop process during learning and an open-loop process during inference. It critiques a reasoning agent's predictions with a judge, converting errors and semantic feedback into control signals stored in a feedback-informed knowledge base. During inference, the model retrieves similar experiences from this knowledge base to enhance its reasoning capabilities without further fine-tuning. The framework demonstrates improved predictive performance and explanation quality on the PrideMM dataset, suggesting that feedback-regulated prompting is a promising approach for adaptive meme humor understanding.",154.16,Phi-4,Nvidia B200 (Cloud Native)
2601.07233v1_From Thinking to Justifying Aligning High-Stakes E.pdf,From “Thinking” to “Justifying”: Aligning High-Stakes Explainability with Professional Communication Standards,"['Chen Qian', 'Yimeng Wang', 'Yu Chen', 'Lingfei Wu', 'Andreas Stathopoulos']","The paper addresses the challenge of ensuring that Explainable AI (XAI) in high-stakes domains provides reliable and verifiable explanations. It critiques Chain-of-Thought methods for potential logical gaps and hallucinations that can misalign conclusions with their rationale. The authors propose a new framework, SEF (Structured Explainability Framework), which aligns with professional communication standards like CREAC and BLUF to present conclusions before structured justifications. This approach is validated across various tasks and domains, showing improved accuracy and verifiability. The paper emphasizes the importance of structuring AI explanations to enhance stakeholder trust and understanding.",154.31,Phi-4,Nvidia B200 (Cloud Native)
2601.07238v1_Group Pattern Selection Optimization Let LRMs Pick.pdf,Group Pattern Selection Optimization: Let LRMs Pick the Right Pattern for Reasoning,"['Hanbin Wang', 'Jingwei Song', 'Jinpeng Li', 'Fei Mi', 'Lifeng Shang']","This paper addresses the challenge of optimizing reasoning patterns in large reasoning models (LRMs) by introducing Group Pattern Selection Optimization (GPSO). The study identifies that existing training methods bias models towards a limited set of dominant reasoning patterns, which can be sub-optimal for specific problems. GPSO, a reinforcement learning framework, is proposed to enable models to select the most effective reasoning pattern for each problem. It incorporates multi-pattern rollouts, verifier-guided pattern selection, and attention masking to prevent pattern leakage. The framework allows models to map problem characteristics to optimal reasoning strategies, resulting in consistent performance improvements across various benchmarks. The research demonstrates that GPSO effectively mitigates pattern sub-optimality, enhancing the robustness and adaptability of reasoning in LRMs.",154.79,Phi-4,Nvidia B200 (Cloud Native)
2601.07239v1_Stochastic CHAOS Why Deterministic Inference Kills.pdf,"Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artificial Cognition","['Tanmay Joshi', 'Shourya Aggarwal', 'Anusa Saha', 'Aadi Pandey', 'Shreyash Dhoot', 'Vighnesh Rai', 'Raxit Goswami', 'Aman Chadha', 'Vinija Jain', 'Amitava Das']","This paper argues against deterministic inference in large language models (LLMs), positing that it undermines the ability to model uncertainty, diminishes emergent abilities, disrupts reasoning, and makes safety alignment brittle. The authors advocate for stochastic inference, emphasizing that distributional variability is crucial for artificial cognition. They distinguish between algorithmic stochasticity and numerical/systems nondeterminism, proposing three stability goals: bitwise determinism, distributional reproducibility, and semantic stability. The paper demonstrates that deterministic inference can be misleading, particularly in instruction-following, emergent abilities, and reasoning, and suggests that embracing stochasticity allows for a more faithful characterization of the underlying distribution in LLMs.",154.77,Phi-4,Nvidia B200 (Cloud Native)
2601.07245v1_Learning to Trust the Crowd A Multi-Model Consensu.pdf,Learning to Trust the Crowd: A Multi-Model Consensus Reasoning Engine for Large Language Models,['Pranav Kallem'],"This paper addresses the reliability issues of large language models (LLMs) by introducing a Multi-Model Consensus Reasoning Engine. The engine uses outputs from several heterogeneous LLMs to determine the most likely correct answer for a given query. It employs a supervised meta-learner that maps natural language responses into structured features, including semantic embeddings, pairwise similarity, clustering statistics, lexical and structural cues, reasoning-quality scores, confidence estimates, and model-specific priors. The system applies gradient-boosted trees, listwise ranking, and graph neural networks over similarity graphs of answers. The approach is evaluated on subsets of GSM8K, ARC-Challenge, HellaSwag, and TruthfulQA, showing significant improvements in accuracy and reliability over single LLMs and majority vote methods. The study highlights the importance of semantic agreement and clustering features, with reasoning-quality and model-prior features providing additional benefits.",154.76,Phi-4,Nvidia B200 (Cloud Native)
2601.07250v1_DDT A Dual-Masking Dual-Expert Transformer for Ene.pdf,DDT: A Dual-Masking Dual-Expert Transformer for Energy Time-Series Forecasting,"['Mingnan Zhu', 'Qixuan Zhang', 'Yixuan Cheng', 'Fangzhou Gu', 'Shiming Lin']","This paper introduces DDT, a novel deep learning framework designed for high-precision energy time-series forecasting. It addresses challenges in capturing complex temporal dependencies and integrating multi-source data by introducing a dual-masking mechanism and a dual-expert system. The dual-masking mechanism combines a strict causal mask with a data-driven dynamic mask to ensure causal consistency and focus on relevant historical information. The dual-expert system decouples the modeling of temporal dynamics and cross-variable correlations into parallel pathways, integrated through a dynamic gated fusion module. Extensive experiments on seven energy benchmark datasets demonstrate that DDT outperforms state-of-the-art baselines across all prediction horizons, setting a new benchmark for the task.",154.87,Phi-4,Nvidia B200 (Cloud Native)
2601.07261v1_Pseudodata-guided Invariant Representation Learnin.pdf,Pseudodata-guided Invariant Representation Learning Boosts the Out-of-Distribution Generalization in Enzymatic Kinetic Parameter Prediction,"['Haomin Wu', 'Zhiwei Nie', 'Hongyu Zhang', 'Zhixiang Ren']","The paper addresses the challenge of predicting enzyme kinetic parameters accurately, especially for out-of-distribution (OOD) cases where existing models fail. It introduces O2DENet, a module designed to enhance OOD generalization by using biologically and chemically informed perturbations and invariant representation learning. O2DENet improves the robustness and accuracy of enzyme-substrate interaction (ESI) models by enforcing consistency between original and augmented enzyme-substrate-pair representations. The integration of O2DENet with existing ESI models results in improved performance across stringent OOD benchmarks, making it a valuable tool for enzyme engineering applications.",155.12,Phi-4,Nvidia B200 (Cloud Native)
2601.07263v1_When Bots Take the Bait Exposing and Mitigating th.pdf,When Bots Take the Bait: Exposing and Mitigating the Emerging Social Engineering Attack in Web Automation Agent,"['Xinyi Wu', 'Geng Hong', 'Yueyue Chen', 'MingXuan Liu', 'Feier Jin', 'Xudong Pan', 'Jiarun Dai', 'Baojun Liu']","This paper presents a systematic study of social engineering attacks against web automation agents, introducing the AGENTBAIT paradigm that exploits weaknesses in agent execution by distorting reasoning through inducement contexts. The authors propose SUPERVISOR, a runtime module that enforces consistency between webpage context and intended goals to mitigate unsafe operations. Empirical results show mainstream frameworks are highly vulnerable to AGENTBAIT, with an average attack success rate of 67.5%. SUPERVISOR reduces attack success rates by up to 78.1% with minimal runtime overhead, advancing the security of web agents.",154.61,Phi-4,Nvidia B200 (Cloud Native)
2601.07291v1_A Visual Semantic Adaptive Watermark grounded by P.pdf,A Visual Semantic Adaptive Watermark grounded by Prefix-Tuning for Large Vision-Language Model,"['Qi Zheng', 'Shuliang Liu', 'Yu Huang', 'Sihang Jia', 'Jungang Li', 'Lyuhao Chen', 'Junhao Chen', 'Hanqian Li', 'Aiwei Liu', 'Yibo Yan', 'Xuming Hu']","This paper introduces VISA-Mark, a novel framework for embedding detectable signals in Large Vision-Language Models (LVLMs) while preserving visual fidelity. The approach uses a prefix-tuner to derive Visual Evidence Weights, guiding adaptive vocabulary partitioning and logits perturbation to focus watermark strength on visually-supported tokens. VISA-Mark achieves a 7.8% improvement in visual consistency and maintains high detection accuracy and robust attack resilience, setting a new standard for multimodal watermarking without compromising inference efficiency.",154.31,Phi-4,Nvidia B200 (Cloud Native)
2601.07292v1_Photometric Redshift Estimation Using Scaled Ensem.pdf,Photometric Redshift Estimation Using Scaled Ensemble Learning,"['Swagata Biswas', 'Shubhrangshu Ghosh', 'Avyarthana Ghosh', 'Yogesh Wadadekar', 'Abhishek Roy Choudhury', 'Arijit Mukherjee', 'Shailesh Deshpande', 'Arpan Pal']","This study introduces a new ensemble-based machine learning framework for predicting photometric redshifts (Pz) of faint galaxies and higher redshift ranges using optical photometric data. The framework integrates multiple learning algorithms, including gradient boosting machine, extreme gradient boosting, k-nearest neighbors, and artificial neural networks, within a scaled ensemble structure. By employing bagged input data, the ensemble approach enhances predictive performance over standalone models, maintaining accuracy up to z ∼ 4. The model is validated using data from the Hyper Suprime-Cam Strategic Survey Program and demonstrates improvements in precision and reliability of Pz estimation, meeting or exceeding LSST Science Requirements Document benchmarks.",154.51,Phi-4,Nvidia B200 (Cloud Native)
2601.07296v1_LRAS Advanced Legal Reasoning with Agentic Search.pdf,LRAS: Advanced Legal Reasoning with Agentic Search,"['Yujin Zhou', 'Chuxue Cao', 'Jinluan Yang', 'Lijun Wu', 'Conghui He', 'Sirui Han', 'Yike Guo']","The paper introduces Legal Reasoning with Agentic Search (LRAS), a framework designed to enhance the capabilities of legal Large Reasoning Models (LRMs) by transitioning from closed-loop reasoning to dynamic, interactive inquiry. By integrating Introspective Imitation Learning and Difficulty-aware Reinforcement Learning, LRAS helps LRMs identify knowledge boundaries and manage legal reasoning complexity. Empirical results show that LRAS outperforms existing baselines by 8.2-32%, particularly in tasks requiring deep reasoning with reliable knowledge. The authors plan to release their data and models for further exploration.",154.63,Phi-4,Nvidia B200 (Cloud Native)
2601.07304v1_Heterogeneous Multi-Expert Reinforcement Learning .pdf,Heterogeneous Multi-Expert Reinforcement Learning for Long-Horizon Multi-Goal Tasks in Autonomous Forklifts,"['Yun Chen', 'Bowei Huang', 'Fan Guo', 'Kang Song']","This paper introduces a Heterogeneous Multi-Expert Reinforcement Learning (HMER) framework designed for autonomous forklifts in unstructured warehouses. The framework addresses the challenge of balancing efficient large-scale navigation with high-precision object interaction by decomposing tasks into specialized sub-policies managed by a Semantic Task Planner. This separation allows each expert to focus on its specific action space, reducing optimization interference. The paper also presents a Hybrid Imitation-Reinforcement Training Strategy to enhance exploration. Experiments demonstrate that HMER outperforms traditional methods, achieving a 94.2% task success rate, reducing operation time by 21.4%, and maintaining placement error within 1.5 cm.",154.98,Phi-4,Nvidia B200 (Cloud Native)
2601.07309v1_ARM Role-Conditioned Neuron Transplantation for Tr.pdf,ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging,"['Zhuoka Feng', 'Kang Chen', 'Sihan Zhao', 'Kai Xiong', 'Yaoning Wang', 'Minshen Yu', 'Junjie Nian', 'Changyi Xiao', 'Yixin Cao', 'Yugang Jiang']","This paper introduces Agent-Role Merging (ARM), a novel method for merging large language model (LLM) agents without training. ARM utilizes a role-conditioned neuron transplantation approach to integrate multiple expert models into a single, more generalist model. The method enhances the ability of LLM agents to generalize across various interactive environments without the need for gradient-based optimization. ARM's three-step framework includes constructing merged backbones, role-conditioned activation analysis for selection, and fine-grained neuron transplantation. The resulting model demonstrates superior performance and generalization across diverse domains compared to previous methods and domain-specific models.",155.08,Phi-4,Nvidia B200 (Cloud Native)
2601.07313v1_Explaining Machine Learning Predictive Models thro.pdf,Explaining Machine Learning Predictive Models through Conditional Expectation Methods,"['Silvia Ruiz-España', 'Laura Arnal', 'François Signol', 'Juan-Carlos Perez-Cortes', 'Joaquim Arlandis']","This paper introduces Multivariate Conditional Expectation (MUCE), a model-agnostic method for local explainability in machine learning. MUCE extends Individual Conditional Expectation (ICE) by exploring a multivariate grid of values around a given observation, providing graphical explanations of local prediction changes due to feature interactions. It also introduces two quantitative indices, stability and uncertainty, to summarize local behavior and assess model reliability. The method is validated using XGBoost models on synthetic and real-world datasets, demonstrating its effectiveness in capturing complex local model behavior and providing insights into prediction confidence. MUCE enhances interpretability and supports more trustworthy decision-making in AI applications.",154.94,Phi-4,Nvidia B200 (Cloud Native)
2601.07315v1_VLM-CAD VLM-Optimized Collaborative Agent Design W.pdf,VLM-CAD: VLM-Optimized Collaborative Agent Design Workflow for Analog Circuit Sizing,"['Guanyuan Pan', 'Yugui Lin', 'Tiansheng Zhou', 'Pietro Li', 'Shuai Wang', 'Yaqi Wang']","This paper introduces VLM-CAD, a design workflow for analog circuit sizing that leverages Vision Language Models (VLMs) to optimize DC operating points, perform inference-based sizing, and execute external sizing optimization. The workflow integrates Image2Net for annotating circuit schematics and generating structured JSON descriptions, facilitating precise interpretation by VLMs. Additionally, it proposes an Explainable Trust Region Bayesian Optimization method (ExTuRBO) for collaborative warm-starting and dual-granularity sensitivity analysis. The approach demonstrates effectiveness in balancing power and performance, achieving a 100% success rate in optimizing amplifiers with complementary input and class-AB output stages, while maintaining total runtime under 43 minutes across experiments using 180nm, 90nm, and 45nm Predictive Technology Models.",154.88,Phi-4,Nvidia B200 (Cloud Native)
2601.07316v1_BEAT-Net Injecting Biomimetic Spatio-Temporal Prio.pdf,BEAT-Net: Injecting Biomimetic Spatio-Temporal Priors for Interpretable ECG Classification,"['Ma Runze', 'Liao Caizhi']","This paper introduces BEAT-Net, a Biomimetic ECG Analysis with Tokenization framework, which reformulates ECG classification as a language modeling task. By using a QRS tokenization strategy, BEAT-Net transforms continuous ECG signals into biologically aligned heartbeat sequences. The framework employs specialized encoders to extract local beat morphology, normalize spatial lead perspectives, and model temporal rhythm dependencies. Evaluations on three large-scale benchmarks show that BEAT-Net matches the diagnostic accuracy of dominant CNN architectures while improving robustness and data efficiency. It achieves full supervised performance with only 30 to 35 percent of annotated data. Additionally, the learned attention mechanisms provide interpretability by reproducing clinical heuristics, such as Lead II prioritization for rhythm analysis, without explicit supervision. The integration of biological priors offers a computationally efficient and interpretable alternative to data-intensive large-scale pre-training.",154.83,Phi-4,Nvidia B200 (Cloud Native)
2601.07320v1_Segmental Advantage Estimation Enhancing PPO for L.pdf,Segmental Advantage Estimation: Enhancing PPO for Long-Context LLM,"['Xue Gong', 'Qi Yi', 'Ziyuan Nan', 'Guanhua Huang', 'Kejiao Li', 'Yuhao Jiang', 'Ruibin Xiong', 'Zenan Xu', 'Jiaming Guo', 'Shaohui Peng', 'Bo Zhou']","This paper addresses the challenge of unreliable advantage estimation in Proximal Policy Optimization (PPO) when applied to Reinforcement Learning with Verifiable Rewards (RLVR) for training Large Language Models (LLMs). The authors introduce Segmental Advantage Estimation (SAE) to mitigate the bias introduced by Generalized Advantage Estimation (GAE) in sparse-reward settings. SAE partitions sequences into coherent sub-segments using low-probability tokens as boundaries and computes variance-reduced advantage estimates from these segments. The approach demonstrates improved performance, training stability, and sample efficiency across various model sizes, with a higher correlation to ground-truth advantage estimates.",154.74,Phi-4,Nvidia B200 (Cloud Native)
2601.07342v1_Agentic Diagnostic Reasoning over Telecom and Data.pdf,Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure,['Nicolas Tacheny'],"This paper introduces an agentic diagnostic framework for telecom and datacenter infrastructures, leveraging a Large Language Model (LLM) to autonomously perform step-wise investigations. The framework uses the Model Context Protocol (MCP) to navigate infrastructure models, enabling the LLM to utilize tools for service lookup, dependency retrieval, and event analysis. This approach aims to overcome the limitations of traditional root cause analysis methods, which are costly to maintain and tightly coupled to infrastructure models. The work lays the foundation for autonomous incident resolution and change impact mitigation, with future systems potentially predicting the impact of planned changes on services and customers.",155.39,Phi-4,Nvidia B200 (Cloud Native)
2601.07344v1_PulseMind A Multi-Modal Medical Model for Real-Wor.pdf,PulseMind: A Multi-Modal Medical Model for Real-World Clinical Diagnosis,"['Jiao Xu', 'Junwei Liu', 'Jiangwei Lao', 'Qi Zhu', 'Yunpeng Zhao', 'Congyun Jin', 'Shinan Liu', 'Zhihong Lu', 'Lihe Zhang', 'Xin Chen', 'Jian Wang', 'Ping Wang']","PulseMind introduces a new family of multi-modal diagnostic models designed to address the complexity of real-world clinical diagnostics. It integrates a systematically curated dataset named MediScope, a comprehensive evaluation benchmark called PulseMind Benchmark, and a tailored training framework. MediScope includes 98,000 multi-turn consultations and 601,500 medical images across 10 major clinical departments and over 200 sub-specialties. The PulseMind Benchmark evaluates models on proactiveness, accuracy, usefulness, and language quality. The training framework employs Comparison-based Reinforcement Policy Optimization (CRPO) to provide stable and human-aligned training guidance. Extensive experiments show that PulseMind performs competitively on both diagnostic consultation and public medical benchmarks.",153.83,Phi-4,Nvidia B200 (Cloud Native)
2601.07348v4_Controlled Self-Evolution for Algorithmic Code Opt.pdf,Controlled Self-Evolution for Algorithmic Code Optimization,"['Tu Hu', 'Ronghao Chen', 'Shuo Zhang', 'Jianghao Yin', 'Mou Xiao Feng', 'Jingping Liu', 'Shaolei Zhang', 'Wenqi Jiang', 'Yuqi Fang', 'Sen Hu', 'Huacan Wang', 'Yi Xu']","The paper introduces Controlled Self-Evolution (CSE) to enhance code generation through iterative cycles. Existing methods suffer from low exploration efficiency due to initialization bias, uncontrolled stochastic operations, and insufficient experience utilization. CSE addresses these issues with three components: Diversified Planning Initialization for broad solution space coverage, Genetic Evolution for feedback-guided mutation and crossover, and Hierarchical Evolution Memory for capturing experiences. Experiments on EffiBench-X show that CSE outperforms baselines, achieving higher efficiency early and maintaining improvement throughout evolution.",155.01,Phi-4,Nvidia B200 (Cloud Native)
2601.07351v2_Beyond Hard Masks Progressive Token Evolution for .pdf,Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models,"['Linhao Zhong', 'Linyu Wu', 'Bozhen Fang', 'Tianjian Feng', 'Chenchen Jing', 'Wen Wang', 'Jiaheng Zhang', 'Hao Chen', 'Chunhua Shen']","This paper introduces EvoToken-DLM, a novel diffusion-based language modeling approach that replaces hard binary masks with evolving soft token distributions. This method supports revisable decoding by enabling a progressive transition from masked states to discrete outputs. The approach incorporates continuous trajectory supervision to align training objectives with iterative probabilistic updates. Extensive experiments demonstrate that EvoToken-DLM outperforms existing diffusion-based and masked DLM baselines across multiple benchmarks.",152.92,Phi-4,Nvidia B200 (Cloud Native)
2601.07356v1_Efficient Convolutional Forward Model for Passive .pdf,Efficient Convolutional Forward Model for Passive Acoustic Mapping and Temporal Monitoring,"['Tatiana Gelvez-Barrera', 'Barbara Nicolas', 'Bruno Gilles', 'Adrian Basarab', 'Denis Kouam ´e']","This paper introduces a novel convolutional formulation for Passive Acoustic Mapping (PAM) in the time domain, aimed at improving computational efficiency and temporal resolution. The proposed framework formulates PAM as an inverse problem, mapping spatiotemporal cavitation activity to recorded radio-frequency signals while accounting for time-of-flight delays. A regularized inversion algorithm incorporating prior knowledge on cavitation activity is developed. Experimental results demonstrate that this framework outperforms classical beamforming methods by providing higher temporal resolution and reducing computational burden compared to iterative time-domain formulations.",154.95,Phi-4,Nvidia B200 (Cloud Native)
2601.07359v1_Seeing Right but Saying Wrong Inter- and Intra-Lay.pdf,Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs without Training,"['Shezheng Song', 'Shasha Li', 'Jie Yu']","The paper addresses a critical inconsistency in Multimodal Large Language Models (MLLMs) where deeper layers may correctly attend to visual regions, but final predictions are misled by noisy attention from earlier layers. This results in a disconnect between the model's internal understanding and its output, described as 'seeing it right but saying it wrong.' The authors propose DualPD, a dual-perspective decoding refinement strategy that enhances visual understanding without additional training. DualPD includes a layer-wise attention-guided contrastive logits module and a head-wise information filtering module. Experiments on LLaVA and Qwen-VL models across multiple benchmarks show that DualPD improves accuracy without training, demonstrating its effectiveness and generalizability.",155.02,Phi-4,Nvidia B200 (Cloud Native)
2601.07364v1_On the universal definition of intelligence.pdf,On the universal definition of intelligence: A definition for Human - AI Comparison,['Joseph Chen'],"This paper proposes a universal definition of intelligence to enable fair and consistent comparison between human and artificial intelligence (AI). It critiques existing anthropocentric definitions and introduces four criteria for evaluating intelligence based on R. Carnap's methodology: similarity to explicandum, exactness, fruitfulness, and simplicity. The paper examines six definitions of intelligence, highlighting their strengths and limitations, and proposes the Extended Predictive Hypothesis (EPH). The EPH defines intelligence as the ability to accurately predict the future and benefit from those predictions, incorporating spontaneous and reactive predictions and the concept of gainability. This framework aims to explain various aspects of intelligence, such as creativity, learning, and future planning, and is argued to be the most satisfactory and universal definition for comparing human and AI intelligence.",154.92,Phi-4,Nvidia B200 (Cloud Native)
2601.07372v1_Conditional Memory via Scalable Lookup A New Axis .pdf,Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models,"['Xin Cheng', 'Wangding Zeng', 'Damai Dai', 'Qinyu Chen', 'Bingxuan Wang', 'Zhenda Xie', 'Kezhao Huang', 'Xingkai Yu', 'Zhewen Hao', 'Yukun Li', 'Han Zhang', 'Huishuai Zhang', 'Dongyan Zhao', 'Wenfeng Liang']","This paper introduces conditional memory as a new axis of sparsity for large language models, complementing the Mixture-of-Experts (MoE) approach. The authors propose Engram, a module that modernizes classic N-gram embedding for efficient O(1) lookup. By addressing the Sparsity Allocation problem, they identify a U-shaped scaling law that optimizes the trade-off between neural computation and static memory. Engram, scaled to 27B parameters, outperforms iso-parameter and iso-FLOPs MoE baselines in various tasks, including knowledge retrieval, general reasoning, and code/math domains. The module enhances model efficiency by freeing attention capacity for global context and enabling runtime prefetching with negligible overhead. The authors suggest that conditional memory will be a crucial component for future sparse models.",154.01,Phi-4,Nvidia B200 (Cloud Native)
2601.07376v1_OpenTinker Separating Concerns in Agentic Reinforc.pdf,OpenTinker: Separating Concerns in Agentic Reinforcement Learning,"['Siqi Zhu', 'Jiaxuan You']","OpenTinker introduces an infrastructure for reinforcement learning (RL) of large language model (LLM) agents, emphasizing a separation of concerns across algorithm design, execution, and agent-environment interaction. It decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are managed by a centralized scheduler. OpenTinker supports various RL tasks, including LoRA-based and full-parameter RL, supervised fine-tuning, and inference, over shared resources. It also discusses principles for extending to multi-agent training and presents use cases demonstrating its effectiveness in practical agentic learning scenarios.",155.21,Phi-4,Nvidia B200 (Cloud Native)
2601.07377v1_Learning Dynamic Collaborative Network for Semi-su.pdf,Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation,"['Jiao Xu', 'Xin Chen', 'Lihe Zhang']","This paper introduces a novel dynamic collaborative network, DiCo, for semi-supervised 3D vessel segmentation. Unlike conventional mean teacher methods that use a static approach, DiCo allows dynamic switching of teacher-student roles to mitigate cognitive biases and improve performance. It incorporates a multi-view integration module to capture various perspectives, akin to medical analysis, and uses adversarial supervision to maintain vessel shape in unlabeled data. The method achieves state-of-the-art performance on three 3D vessel segmentation benchmarks.",155.0,Phi-4,Nvidia B200 (Cloud Native)
2601.07389v1_On the Non-decoupling of Supervised Fine-tuning an.pdf,On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training,"['Xueyan Niu', 'Bo Bai', 'Wei Han', 'Wei Xi Zhang']","The paper investigates the interplay between supervised fine-tuning (SFT) and reinforcement learning (RL) in the post-training of large language models. It demonstrates that these two methods cannot be decoupled without performance loss. Specifically, RL increases the SFT loss when SFT is optimal, and SFT reduces the reward achieved by RL. Experiments on the Qwen3-0.6B model confirm this degradation, showing that SFT and RL must be integrated to maintain prior performance in the post-training pipeline.",155.08,Phi-4,Nvidia B200 (Cloud Native)
2601.07392v1_OceanSAR-2 A Universal Feature Extractor for SAR O.pdf,OceanSAR-2: A Universal Feature Extractor for SAR Ocean Observation,"['Alexandre Tuela', 'Thomas Kerdreux', 'Quentin Febvre', 'Alexis Mouche', 'Antoine Grouazel', 'Jean-Renaud Miadana', 'Antoine Audras', 'Chen Wang', 'Bertrand Chapron']","The paper introduces OceanSAR-2, a second-generation foundation model for SAR-based ocean observation. It builds on previous work by enhancing self-supervised learning (SSL) training and dynamic data curation strategies, improving performance and reducing training costs. OceanSAR-2 shows strong transfer performance across various downstream tasks, such as geophysical pattern classification, ocean surface wind vector and significant wave height estimation, and iceberg detection. The authors also release standardized benchmark datasets to facilitate the systematic evaluation and advancement of SAR models for ocean applications.",154.65,Phi-4,Nvidia B200 (Cloud Native)
2601.07393v1_Software-Hardware Co-optimization for Modular E2E .pdf,"Software-Hardware Co-Optimization for Modular E2E: A Unified Framework of Optimization Approaches, Simulation Environment and Evaluation Metrics","['Chengzhi Ji', 'Xingfeng Li', 'Zhaodong Lv', 'Hao Sun', 'Pan Liu', 'Hao Frank Yang', 'Ziyuan Pu']","This paper addresses the limitations of existing modular end-to-end (ME2E) autonomous driving paradigms, which often focus on accuracy metrics while neglecting system-level considerations like energy consumption and inference latency. The authors propose a software-hardware co-optimization framework that integrates software-level model optimizations with hardware-level computation optimizations under a unified system-level objective. A new multidimensional evaluation metric, EERA V, is introduced to assess system performance by considering safety, comfort, efficiency, latency, and energy. The framework is evaluated across multiple ME2E stacks, achieving significant reductions in inference latency and energy consumption while improving the EERA V metric by 22.35%, demonstrating its effectiveness in providing actionable optimization guidance from both software and hardware perspectives.",154.62,Phi-4,Nvidia B200 (Cloud Native)
2601.07395v1_MCP-ITP An Automated Framework for Implicit Tool P.pdf,MCP-ITP: An Automated Framework for Implicit Tool Poisoning in MCP,"['Ruiqi Li', 'Zhiqiang Wang', 'Yunhao Yao', 'Xiang-Yang Li']","The paper introduces MCP-ITP, an automated framework for implicit tool poisoning within the Model Context Protocol (MCP) ecosystem. Unlike prior work focusing on explicit tool poisoning or manually crafted poisoned tools, MCP-ITP targets a stealthier variant where the poisoned tool itself is not invoked. Instead, it manipulates agents to use legitimate high-privilege tools for malicious purposes by embedding instructions in tool metadata. The framework formulates poisoned tool generation as a black-box optimization problem, using iterative optimization with feedback from evaluation and detection language models to maximize Attack Success Rate (ASR) while evading detection. Experimental results show that MCP-ITP outperforms manual baselines, achieving high ASR and low Malicious Tool Detection Rate (MDR) across multiple LLM agents.",154.83,Phi-4,Nvidia B200 (Cloud Native)
2601.07397v1_Layerwise goal-oriented adaptivity for neural ODEs.pdf,Layerwise goal-oriented adaptivity for neural ODEs: an optimal control perspective,"['Michael Hintermüller', 'Michael Hinze', 'Denis Korolev']","This paper introduces a novel layerwise adaptive construction method for neural network architectures, utilizing a goal-oriented dual-weighted residual technique for the optimal control of neural differential equations. The approach results in an ordinary differential equation constrained optimization problem, where controls act as coefficients and a specific loss function is employed. The method is implemented using a DG(0) Galerkin discretization of the neural ODE, leading to an explicit Euler time marching scheme, and optimization is performed using steepest descent. The method is applied to construct neural networks for classifying datasets, with results presented for well-known examples from the literature.",155.2,Phi-4,Nvidia B200 (Cloud Native)
2601.07411v1_SCALPEL Selective Capability Ablation via Low-rank.pdf,SCALPEL: Selective Capability Ablation via Low-rank Parameter Editing for Large Language Model Interpretability Analysis,"['Zihao Fu', 'Xufeng Duan', 'Zhenguang G. Cai']","SCALPEL (Selective Capability Ablation via Low-rank Parameter Editing for Large Language Models) is a framework designed to enhance the interpretability of large language models by representing capabilities as low-rank parameter subspaces. This approach allows for precise capability removal without affecting other functionalities. By training LoRA adapters, SCALPEL identifies and ablates specific capabilities while preserving general language modeling quality. Experiments demonstrate that SCALPEL can selectively remove target capabilities and provide insights into the distributed nature of capability encoding in neural networks. The results suggest that capabilities in large language models exhibit a low-rank structure, enabling targeted parameter-space interventions for nuanced understanding and manipulation.",154.87,Phi-4,Nvidia B200 (Cloud Native)
2601.07422v1_Two Pathways to Truthfulness On the Intrinsic Enco.pdf,Two Pathways to Truthfulness: On the Intrinsic Encoding of LLM Hallucinations,"['Wen Luo', 'Guangyue Peng', 'Wei Li', 'Shaohang Wei', 'Feifan Song', 'Liang Wang', 'Nan Yang', 'Xingxing Zhang', 'Jing Jin', 'Furu Wei', 'Houfeng Wang']","This paper explores the intrinsic encoding of truthfulness in large language models (LLMs) and identifies two distinct pathways: a Question-Anchored pathway dependent on question-answer information flow, and an Answer-Anchored pathway that derives evidence from the generated answer itself. Through experiments like attention knockout and token patching, the study disentangles these pathways and reveals their association with LLM knowledge boundaries. The findings suggest that internal representations are aware of these distinctions, offering new insights for enhancing hallucination detection and developing more reliable generative systems.",154.33,Phi-4,Nvidia B200 (Cloud Native)
2601.07430v1_KALE Enhancing Knowledge Manipulation in Large Lan.pdf,KALE: Enhancing Knowledge Manipulation in Large Language Models via Knowledge-aware Learning,"['Qitan Lv', 'Tianyu Liu', 'Qiaosheng Zhang', 'Xingcheng Xu', 'Chaochao Lu']","The paper addresses the challenge of enhancing knowledge manipulation in large language models (LLMs), which, despite being pretrained on vast knowledge corpora, struggle to effectively recall, reason, and transfer relevant knowledge. Existing methods primarily use Supervised Fine-Tuning (SFT) on labeled datasets, but these models often exhibit the 'known&incorrect' phenomenon, where they possess relevant knowledge but fail to use it correctly. The authors propose KALE (Knowledge-Aware Learning), a post-training framework that leverages knowledge graphs to generate high-quality rationales and enhance LLMs' knowledge manipulation abilities. KALE introduces a Knowledge-Induced data synthesis method to extract multi-hop reasoning paths from knowledge graphs and employs a Knowledge-Aware fine-tuning paradigm to improve knowledge manipulation by minimizing the KL divergence between predictions with and without rationales. Experiments on eight benchmarks across six LLMs show significant accuracy improvements, demonstrating KALE's effectiveness.",154.0,Phi-4,Nvidia B200 (Cloud Native)
2601.07449v1_RLPO Residual Listwise Preference Optimization for.pdf,Residual Listwise Preference Optimization for Long-Context Review Ranking,"['Hao Jiang', 'Zhi Yang', 'Annan Wang', 'Yichi Zhang', 'Weisi Lin*']","The paper addresses the challenge of review ranking in e-commerce, where prioritizing diagnostic and authentic feedback from a large volume of user-generated content is crucial. Existing ranking paradigms struggle with a trade-off in long-context settings, where pointwise scoring is efficient but fails to account for list-level interactions, and listwise approaches are computationally expensive. The authors propose Residual Listwise Preference Optimization (RLPO), which combines pointwise scoring with a lightweight encoder to predict listwise score residuals, avoiding full token-level listwise processing. RLPO is shown to improve NDCG@k over strong pointwise and listwise baselines and remains robust as list length increases. The paper also introduces a large-scale benchmark for long-context review ranking with human verification.",154.71,Phi-4,Nvidia B200 (Cloud Native)
2601.07463v1_Puzzle it Out Local-to-Global World Model for Offl.pdf,Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning,"['Sijia Li', 'Xinran Li', 'Shibo Chen', 'Jun Zhang']","This paper addresses the challenges in offline multi-agent reinforcement learning (MARL) by proposing a novel local-to-global (LOGO) world model. The LOGO framework leverages local predictions to infer global state dynamics, enhancing prediction accuracy and capturing agent-wise dependencies. The model generates synthetic data to augment the original dataset, expanding the effective state-action space. An uncertainty-aware sampling mechanism is introduced to adaptively weight synthetic data by prediction uncertainty, reducing approximation error propagation to policies. The approach requires only an additional encoder for uncertainty estimation, reducing computational overhead while maintaining accuracy. Extensive experiments demonstrate that the method surpasses state-of-the-art baselines on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable offline multi-agent learning.",155.01,Phi-4,Nvidia B200 (Cloud Native)
2601.07464v1_IFDNS An Iterative Feedback-Driven Neuro-Symbolic .pdf,IFDNS: An Iterative Feedback-Driven Neuro-Symbolic Method for Faithful Logical Reasoning,"['Xiaoheng Wang', 'Tongxuan Liu', 'Zi Gong', 'Xianzhe Dong', 'Yuting Zeng', 'Minhan Hu', 'Weizhe Huang', 'Jing Li']","This paper introduces Iterative Feedback-Driven Neuro-Symbolic (IFDNS), a novel method designed to enhance the logical reasoning capabilities of large language models (LLMs). Addressing the issue of unfaithful reasoning in prompt-based methods like Chain-of-Thought (CoT), IFDNS employs a multi-round feedback mechanism to accurately extract and translate causal relationships into logical expressions, thereby reducing information loss. The method is orthogonal to existing prompting techniques, allowing for seamless integration. Empirical evaluations on six datasets demonstrate significant performance improvements, with a +9.40% accuracy boost for CoT on the LogiQA dataset and a +11.70% improvement for CoT-SC on the PrOntoQA dataset.",153.69,Phi-4,Nvidia B200 (Cloud Native)
2601.07468v1_Beyond Dialogue Time Temporal Semantic Memory for .pdf,Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents,"['Miao Su', 'Yucan Guo', 'Zhongni Hou', 'Long Bai', 'Zixuan Li', 'Yufei Zhang', 'Guojun Yin', 'Wei Lin', 'Xiaolong Jin', 'Jiafeng Guo', 'Xueqi Cheng']","This paper introduces Temporal Semantic Memory (TSM), a novel framework designed to enhance the temporal modeling capabilities of memory in Large Language Model (LLM) agents. Existing methods often suffer from temporal inaccuracies and fragmentation, as they organize memories based on dialogue time rather than actual occurrence time and focus on point-wise memory, neglecting durative information. TSM addresses these issues by constructing a semantic timeline and consolidating temporally continuous and semantically related information into durative memory. This allows for the retrieval of temporally appropriate memories, improving response generation. Experiments on LONGMEMEVAL and LOCOMO demonstrate that TSM outperforms existing methods, achieving up to a 12.2% absolute improvement in accuracy.",154.22,Phi-4,Nvidia B200 (Cloud Native)
2601.07469v1_Knowledge Distillation for LLM-Based Human Activit.pdf,KNOWLEDGE DISTILLATION FOR LLM-BASED HUMAN ACTIVITY RECOGNITION IN HOMES,"['Julien Cumin', 'Oussama Er-Rahmany', 'Xi Chen']","This paper explores the use of Large Language Models (LLMs) for Human Activity Recognition (HAR) in smart homes. It presents new experimental results on two state-of-the-art datasets, examining how recognition performance varies with the size of the LLM. The study also investigates the application of knowledge distillation techniques to fine-tune smaller LLMs using HAR reasoning examples generated by larger LLMs. The findings indicate that these fine-tuned models can achieve performance nearly comparable to the largest LLMs while having significantly fewer parameters, specifically 50 times less.",154.86,Phi-4,Nvidia B200 (Cloud Native)
2601.07470v1_Learning How to Remember A Meta-Cognitive Manageme.pdf,Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory,"['Sirui Liang', 'Pengfei Cao', 'Jian Zhao', 'Wenhao Teng', 'Xiangwen Liao', 'Jun Zhao', 'Kang Liu']","This paper introduces the Meta-Cognitive Memory Abstraction method (MCMA) to enhance memory management in large language model (LLM) agents. MCMA treats memory abstraction as a learnable cognitive skill, allowing for dynamic structuring and reuse of memories. It decouples task execution from memory management by using a frozen task model and a learned memory copilot, which is trained using direct preference optimization. This approach organizes memories into a hierarchy of abstraction levels, facilitating selective reuse based on task similarity. The method demonstrates significant improvements in performance, out-of-distribution generalization, and cross-task transfer in experiments conducted on ALFWorld, ScienceWorld, and BabyAI.",154.21,Phi-4,Nvidia B200 (Cloud Native)
2601.07474v1_Task Prototype-Based Knowledge Retrieval for Multi.pdf,Task Prototype-Based Knowledge Retrieval for Multi-Task Learning from Partially Annotated Data,"['Youngmin Oh', 'Hyung-Il Kim', 'Jung Uk Kim']","This paper addresses the challenge of multi-task learning (MTL) in scenarios where fully annotated data for all tasks is impractical due to high labeling costs. Existing methods for partially labeled MTL often rely on predictions from unlabeled tasks, which can lead to unreliable task associations and negative transfer. The authors propose a prototype-based knowledge retrieval framework that avoids these issues by embedding task-specific characteristics into task prototypes and using a knowledge retrieval transformer to refine feature representations based on these associations. An association knowledge generating (AKG) loss is introduced to ensure consistent capture of task-specific characteristics. The framework demonstrates robust MTL performance even with partially annotated data, as shown through extensive experiments.",154.25,Phi-4,Nvidia B200 (Cloud Native)
2601.07475v1_ARCQuant Boosting NVFP4 Quantization with Augmente.pdf,ARCQuant: Boosting NVFP4 Quantization with Augmented Residual Channels for LLMs,"['Haoqian Meng', 'Yilun Luo', 'Yafei Zhao', 'Wenyuan Liu', 'Peng Zhang', 'Xindian Ma']","The paper introduces ARCQuant, a framework designed to enhance the performance of NVFP4 quantization for Large Language Models (LLMs) by using Augmented Residual Channels. This approach maintains a unified NVFP4 format and integrates error compensation into the matrix reduction dimension, allowing the use of optimized GEMM kernels with minimal overhead. Theoretical analysis shows that ARCQuant's error bounds are comparable to 8-bit formats like MXFP8. Experiments on LLaMA and Qwen models demonstrate that ARCQuant achieves state-of-the-art accuracy, matching full-precision baselines in perplexity and downstream tasks. Additionally, deployment on RTX 5090 and RTX PRO 6000 GPUs shows up to a 3× speedup over FP16.",154.59,Phi-4,Nvidia B200 (Cloud Native)
2601.07477v1_JudgeFlow Agentic Workflow Optimization via Block .pdf,JUDGEFLOW: AGENTICWORKFLOWOPTIMIZATION,"['Zihan Ma', 'Zhikai Zhao', 'Chuanbo Hua', 'Federico Berto', 'Jinkyoo Park']","The paper introduces JUDGEFLOW, a novel pipeline for optimizing LLM-based agentic workflows. It addresses the challenge of scaling AI capabilities by providing fine-grained diagnostic signals through a Judge module that evaluates execution traces and assigns responsibility scores to problematic blocks. This allows an LLM-based optimizer to focus on refining the most problematic areas, improving sample efficiency and interpretability. JUDGEFLOW is evaluated on mathematical reasoning and code generation benchmarks, demonstrating superior performance and efficiency compared to existing methods.",154.57,Phi-4,Nvidia B200 (Cloud Native)
2601.07496v1_Graph Inference Towards ICD Coding.pdf,Graph Inference Towards ICD Coding,['Xiaoxiao Deng'],"Automated ICD coding involves assigning standardized diagnostic codes to clinical narratives, a task challenged by vast label space and extreme class imbalance. This paper introduces LabGraph, a unified framework that reformulates ICD coding as a graph generation task. By integrating adversarial domain adaptation, graph-based reinforcement learning, and perturbation regularization, LabGraph enhances model robustness and generalization. A label graph discriminator dynamically evaluates each generated code, providing adaptive reward feedback during training. Experiments on benchmark datasets show that LabGraph consistently outperforms previous approaches in micro-F1, micro-AUC, and P@K metrics.",155.05,Phi-4,Nvidia B200 (Cloud Native)
2601.07514v1_Data-Driven Stochastic VRP Integration of Forecast.pdf,Data-Driven Stochastic VRP: Integration of Forecast Duration into Optimization for Utility Workforce Management,['Matteo Garbelli'],"This paper explores the integration of machine learning forecasts of intervention durations into a stochastic variant of the Capacitated Vehicle Routing Problem with Time Windows (CVRPTW). Using tree-based gradient boosting (XGBoost) trained on eight years of gas meter maintenance data, the study produces point predictions and uncertainty estimates to drive a multi-objective evolutionary optimization routine. The methodology addresses uncertainty with sub-Gaussian concentration bounds for route-level risk buffers and considers competing operational KPIs through a multi-objective formulation. Empirical analysis validates the sub-Gaussian assumption of the risk model. Results show improvements of 20-25% in operator utilization and completion rates compared to plans using default durations. The integration of uncertainty quantification and risk-aware optimization offers a practical framework for managing stochastic service durations in real-world routing applications.",154.92,Phi-4,Nvidia B200 (Cloud Native)
2601.07516v1_Controlling Multimodal Conversational Agents with .pdf,Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions,"['Yongqi Li', 'Hao Lang', 'Tieyun Qian', 'Yongbin Li']","This paper addresses the challenge of fine-tuning multimodal conversational agents (MCAs) using reinforcement learning (RL) in the context of large text token spaces. The authors propose a method to learn a compact latent action space for RL fine-tuning, leveraging a learning from observation mechanism to construct a codebook. This approach uses future observations to estimate current latent actions, which are then used to reconstruct future observations. To enhance the coverage of the codebook, both paired image-text data and text-only data are utilized, with a cross-modal projector transforming text embeddings into image-text embeddings. The method is initialized on paired data and further trained on text-only data using a novel cycle consistency loss. The proposed method outperforms competitive baselines on two conversation tasks across various RL algorithms.",153.74,Phi-4,Nvidia B200 (Cloud Native)
2601.07518v1_Mon3tr Monocular 3D Telepresence with Pre-built Ga.pdf,Mon3tr: Monocular 3D Telepresence with Pre-built Gaussian Avatars as Amortization,"['Fangyu Lin', 'Yingdong Hu', 'Zhening Liu', 'Yufan Zhuang', 'Zehong Lin', 'Jun Zhang']","This paper introduces Mon3tr, a novel framework for monocular 3D telepresence that leverages 3D Gaussian splatting (3DGS) for parametric human modeling. Mon3tr reduces system complexity and cost by using a single monocular RGB camera to capture body motions and facial expressions in real time. The framework divides the process into an offline multi-view reconstruction phase to build a user-specific avatar and an online inference phase during live sessions. It transmits motion and appearance features at less than 0.2 Mbps over WebRTC’s data channel, allowing robust adaptation to network fluctuations. On the receiver side, a lightweight 3DGS attribute deformation network dynamically generates corrective adjustments on the pre-built avatar, achieving photorealistic motion and appearance at approximately 60 FPS. The method demonstrates state-of-the-art performance with a PSNR of over 28 dB for novel poses, an end-to-end latency of around 80 ms, and over 1000× bandwidth reduction compared to point-cloud streaming, supporting real-time operation from monocular inputs across diverse scenarios.",154.16,Phi-4,Nvidia B200 (Cloud Native)
2601.07525v1_Thinking Before Constraining A Unified Decoding Fr.pdf,Thinking Before Constraining: A Unified Decoding Framework for Large Language Models,"['Ngoc Trinh Hung Nguyen', 'Alonso Silva', 'Laith Zumot', 'Liubov Tupikina', 'Armen Aghasaryan', 'Mehwish Alam']","This paper introduces a unified decoding framework that combines natural and structured generation for large language models (LLMs). The proposed method, named 'In-Writing', allows LLMs to freely reason until specific trigger tokens are generated, at which point it switches to structured generation. This approach maintains the expressive power of natural language reasoning while ensuring the reliability of structured outputs. The method is evaluated on various datasets, demonstrating a significant improvement in accuracy over natural generation, with minimal additional token overhead. The results highlight the effectiveness of combining both generation styles to enhance LLM performance in tasks requiring structured outputs.",154.92,Phi-4,Nvidia B200 (Cloud Native)
2601.07528v1_From RAG to Agentic RAG for Faithful Islamic Quest.pdf,From RAG to Agentic RAG for Faithful Islamic Question Answering,"['Gagan Bhatia', 'Hamdy Mubarak', 'Mustafa Jarrar', 'George Mikros', 'Fadi Zaraket', 'Mahmoud Alhirthani', 'Mutaz Al-Khatib', 'Logan Cochrane', 'Kareem Darwish', 'Rashid Yahiaoui', 'Firoj Alam']","The paper introduces a novel approach to Islamic question answering using an agentic RAG framework. It addresses the challenge of generating reliable and grounded responses in Islamic contexts, where ungrounded answers can have serious consequences. The authors present ISLAMIC FAITH QA, a bilingual benchmark for evaluating hallucination and abstention in responses. They also develop a suite of resources, including text-grounded reasoning pairs, preference samples, and a Qur’an retrieval corpus. The agentic RAG framework leverages these resources to improve answer correctness and robustness across Arabic and English, achieving state-of-the-art performance. The paper highlights the importance of grounding in canonical sources and careful handling of uncertainty in Islamic QA systems.",153.25,Phi-4,Nvidia B200 (Cloud Native)
2601.07553v1_VirtualEnv A Platform for Embodied AI Research.pdf,VirtualEnv: A Platform for Embodied AI Research,"['Kabir Swain', 'Sijie Han', 'Ayush Raina', 'Jin Zhang', 'Shuang Li', 'Michael Stopa', 'Antonio Torralba']","VirtualEnv is a next-generation simulation platform built on Unreal Engine 5 designed for fine-grained benchmarking of large language models (LLMs) in embodied and interactive scenarios. It supports rich agent-environment interactions, including object manipulation, navigation, and adaptive multi-agent collaboration, as well as game-inspired mechanics like escape rooms and procedurally generated environments. The platform provides a user-friendly API for deploying and controlling LLM-driven agents using natural language instructions. It integrates large-scale LLMs and vision-language models to generate novel environments and structured tasks from multimodal inputs. The paper benchmarks the performance of several popular LLMs across tasks of increasing complexity, analyzing differences in adaptability, planning, and multi-agent coordination. VirtualEnv is released as an open-source platform to advance research at the intersection of AI and gaming, enable standardized evaluation of LLMs in embodied AI settings, and pave the way for future developments in immersive simulations and interactive entertainment.",153.6,Phi-4,Nvidia B200 (Cloud Native)
2601.07556v1_Backpropagation-Free Test-Time Adaptation for Ligh.pdf,Backpropagation-Free Test-Time Adaptation for Lightweight EEG-Based Brain-Computer Interfaces,"['Siyang Li', 'Jiayi Ouyang', 'Zhenyao Cui', 'Ziwei Wang', 'Tianwang Jia', 'Feng Wan', 'Dongrui Wu']","This paper introduces a novel test-time adaptation (TTA) approach for EEG decoding called Backpropagation-Free Transformations (BFT). BFT addresses the challenges of inter-subject variability, signal non-stationarity, and computational constraints in EEG-based brain-computer interfaces (BCIs) by eliminating the need for backpropagation. It uses sample-wise transformations, including knowledge-guided augmentations and approximate Bayesian inference, to generate multiple prediction scores for each test sample. A learning-to-rank module is employed to enhance the weighting of these predictions, allowing for robust aggregation and uncertainty suppression during inference. The approach is validated through extensive experiments on five EEG datasets, demonstrating its effectiveness, versatility, robustness, and efficiency. BFT enables lightweight, plug-and-play BCIs on resource-constrained devices, facilitating broader real-world deployment of EEG-based decoding algorithms.",154.26,Phi-4,Nvidia B200 (Cloud Native)
2601.07565v1_A Unified Framework for Emotion Recognition and Se.pdf,A Unified Framework for Emotion Recognition and Sentiment Analysis via Expert-Guided Multimodal Fusion with Large Language Models,"['Jiaqi Qiao', 'Xiujuan Xu', 'Xinran Li', 'Yu Liu']","This paper introduces EGMF, a unified framework for multimodal emotion understanding that combines expert-guided multimodal fusion with large language models (LLMs). The framework features three specialized expert networks: a fine-grained local expert, a semantic correlation expert, and a global context expert, which are adaptively integrated through hierarchical dynamic gating for context-aware feature selection. Enhanced multimodal representations are integrated with LLMs via pseudo token injection and prompt-based conditioning, enabling a single generative framework to handle both classification and regression tasks through natural language generation. The approach employs LoRA fine-tuning for computational efficiency. Experiments on bilingual benchmarks demonstrate consistent improvements over state-of-the-art methods, with superior cross-lingual robustness, revealing universal patterns in multimodal emotional expressions across English and Chinese.",154.71,Phi-4,Nvidia B200 (Cloud Native)
2601.07568v1_d3LLM Ultra-Fast Diffusion LLM using Pseudo-Trajec.pdf,Ultra-Fast dLLM using Pseudo-Trajectory Distillation,"['Yu-Yang Qian', 'Junda Su', 'Lanxiang Hu', 'Peiyuan Zhang', 'Zhijie Deng', 'Peng Zhao', 'Hao Zhang']","The paper introduces d3LLM, a diffusion large language model that balances accuracy and parallelism. It employs pseudo-trajectory distillation during training to enhance parallel decoding and uses entropy-based multi-block decoding with a KV-cache refresh mechanism during inference. The model achieves significant speedups over existing models without substantial accuracy loss. A new metric, Accuracy Under Parallelism (AUP), is introduced to evaluate dLLMs.",152.91,Phi-4,Nvidia B200 (Cloud Native)
2601.07573v1_A Model of Artificial Jagged Intelligence.pdf,A Model of Artificial Jagged Intelligence,['Joshua S. Gans'],"This paper introduces the concept of Artificial Jagged Intelligence (AJI), which describes the uneven performance of generative AI systems across similar tasks. The paper presents an economic model that treats AI adoption as an information problem, focusing on local reliability versus global quality signals. It explores the adoption threshold for users, the impact of the inspection paradox on error amplification, and the role of scaling in improving average quality without eliminating jaggedness. The paper also examines mastery and calibration, showing that users who can condition on local uncertainty can derive value even in challenging domains. Additionally, it discusses the interaction between scaling and discoverability, highlighting when user mastery and calibrated signals can enhance the benefits of scaling, and when opacity might obscure these gains.",155.17,Phi-4,Nvidia B200 (Cloud Native)
2601.07577v1_Beyond Entangled Planning Task-Decoupled Planning .pdf,Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents,"['Yunfan Li', 'Bingbing Xu', 'Xueyun Tian', 'Xiucheng Xu', 'Huawei Shen']","This paper introduces Task-Decoupled Planning (TDP), a framework designed to improve the planning capabilities of long-horizon agents by addressing the issue of entangled contexts in existing planning paradigms. TDP decomposes tasks into a directed acyclic graph (DAG) of sub-goals, allowing for scoped reasoning and replanning confined to active sub-tasks. This approach prevents error propagation and enables local corrections without disrupting the overall workflow. Experimental results demonstrate that TDP outperforms strong baselines in terms of robustness and efficiency, reducing token consumption by up to 82%.",154.82,Phi-4,Nvidia B200 (Cloud Native)
2601.07580v1_Large Language Models for Physics Instrument Desig.pdf,Large Language Models for Physics Instrument Design,"['Sara Zoccheddu', 'Shah Rukh Qasim', 'Patrick Owen', 'Nicola Serra']","This paper explores the application of large language models (LLMs) in the design of physics instruments, comparing their performance to reinforcement learning (RL). LLMs, using only prompting, generate valid and resource-aware detector configurations based on broad pretrained knowledge. Although RL produces stronger final designs, LLMs are effective as meta-planners, proposing and structuring design hypotheses while RL optimizes them. The study suggests that LLMs can reduce human effort in structuring and supervising optimization, pointing towards automated, closed-loop instrument design.",155.2,Phi-4,Nvidia B200 (Cloud Native)
2601.07582v2_ES-Mem Event Segmentation-Based Memory for Long-Te.pdf,ES-Mem: Event Segmentation-Based Memory for Long-Term Dialogue Agents,"['Huhai Zou', 'Tianhao Sun', 'Chuanjiang He', 'Yu Tian', 'Zhenyang Li', 'Li Jin', 'Nayu Liu', 'Jiang Zhong', 'Kaiwen Wei']","This paper introduces ES-Mem, a framework designed to enhance memory mechanisms for long-term dialogue agents. It addresses two main limitations of existing memory systems: rigid memory granularity and flat retrieval paradigms. By incorporating Event Segmentation Theory, ES-Mem dynamically segments interactions into semantically coherent events and uses a hierarchical memory architecture to anchor episodic memories. This approach improves context localization and coherence in dialogue agents. Evaluations show that ES-Mem outperforms baseline methods and demonstrates robust applicability in dialogue segmentation tasks.",154.59,Phi-4,Nvidia B200 (Cloud Native)
2601.07597v1_Pheromone-Focused Ant Colony Optimization algorith.pdf,Pheromone-Focused Ant Colony Optimization algorithm for path planning,"['Yi Liu', 'Hongda Zhang', 'Zhongxue Gan', 'Yuning Chen', 'Ziqing Zhou', 'Chunlei Meng', 'Chun Ouyang']","This paper introduces the Pheromone-Focused Ant Colony Optimization (PFACO) algorithm to address the shortcomings of traditional Ant Colony Optimization (ACO) methods, such as blind search behavior and slow convergence in complex environments. PFACO enhances problem-solving by concentrating initial pheromone distribution in promising regions, reinforcing promising solutions during iterations, and implementing a forward-looking mechanism to penalize redundant path turns. These strategies collectively improve the global optimization capabilities of PFACO, significantly enhancing convergence speed and solution quality across diverse optimization problems. Experimental results demonstrate that PFACO consistently outperforms comparative ACO algorithms in terms of convergence speed and solution quality.",154.91,Phi-4,Nvidia B200 (Cloud Native)
2601.07606v1_Proof of Time A Benchmark for Evaluating Scientifi.pdf,Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments,"['Bingyang Ye', 'Shan Chen', 'Jingxuan Tu', 'Chen Liu', 'Zidi Xiong', 'Samuel Schmidgall', 'Danielle S. Bitterman']","The paper introduces Proof of Time (PoT), a semi-verifiable benchmarking framework designed to evaluate the quality of models' judgments about scientific ideas. PoT links these judgments to observable downstream signals, such as citations and shifts in research agendas, by freezing a pre-cutoff snapshot of evidence and forecasting post-cutoff outcomes. This allows for verifiable evaluation when ground truth becomes available, scalable benchmarking without exhaustive expert annotation, and analysis of human-model misalignment. The framework also provides a testbed for comparing agent-based research judgments to non-agent baselines, highlighting the task-dependent benefits of tool use and interaction budgets. Across over 30,000 instances in four domains, PoT supports scalable evaluation of agents on future-facing scientific idea judgment tasks.",152.95,Phi-4,Nvidia B200 (Cloud Native)
2601.07611v1_DIAGPaper Diagnosing Valid and Specific Weaknesses.pdf,Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning,"['Zhuoyang Zou', 'Abolfazl Ansari', 'Delvin Ce Zhang', 'Dongwon Lee', 'Wenpeng Yin']","This paper introduces DIAGPaper, a novel multi-agent framework designed to identify valid and specific weaknesses in scientific papers. Existing approaches often simulate human roles superficially and assume identified weaknesses are inherently valid, overlooking reviewer bias and the importance of author rebuttals. DIAGPaper addresses these issues through three integrated modules: the Customizer, which simulates human-defined review criteria; the Rebuttal, which engages author agents in structured debates with reviewer agents; and the Prioritizer, which assesses and ranks the severity of validated weaknesses. Experiments demonstrate that DIAGPaper outperforms existing methods by producing more valid, paper-specific weaknesses in a prioritized manner.",154.41,Phi-4,Nvidia B200 (Cloud Native)
2601.07618v1_Neural Architecture for Fast and Reliable Coagulat.pdf,Neural Architecture for Fast and Reliable Coagulation Assessment in Clinical Settings: Leveraging Thromboelastography,"['Yulu Wang', 'Ziqian Zeng', 'Jianjun Wu', 'Zhifeng Tang']","This paper addresses the challenge of real-time coagulation monitoring in clinical settings using Thromboelastography (TEG), which traditionally takes nearly an hour to provide results. The authors introduce a new algorithm, Physiological State Reconstruction (PSR), designed to leverage dynamic changes between individuals and maximize useful information from small clinical datasets. PSR integrates varied temporal signals using multi-domain learning and learns high-level temporal interactions with attention mechanisms. The algorithm achieves remarkable performance with R2 > 0.98 for coagulation traits and reduces error by half compared to state-of-the-art methods, while also halving inferencing time. The approach suggests new possibilities for medical AI applications in scenarios with data scarcity.",154.34,Phi-4,Nvidia B200 (Cloud Native)
2601.07632v2_GeoMotionGPT Geometry-Aligned Motion Understanding.pdf,GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models,"['Zhankai Ye', 'Bofan Li', 'Yukai Jin', 'Shuoqiu Li', 'Wei Wang', 'Yanfu Zhang', 'Shangqian Gao', 'Xin Liu']","This paper introduces GeoMotionGPT, a novel framework that aligns the intrinsic geometry of motion space with the embedding space of Large Language Models (LLMs) for enhanced motion understanding and reasoning. Traditional methods decouple motion quantization from semantic embedding learning, linking them only via token IDs, which fails to align the geometric structures of motion and embedding spaces. GeoMotionGPT addresses this by enforcing orthogonality on both the motion codebook and the LLM embedding space, ensuring their relational structures mirror each other. The framework employs a decoder-only quantizer with Gumbel-Softmax for differentiable training and a sparse projection to map motion codes into the LLM embedding space while preserving orthogonality. A two-stage orthonormal regularization schedule maintains geometric alignment during tokenizer training and LLM fine-tuning. Experiments on HumanML3D show a 20% performance improvement over state-of-the-art methods, demonstrating the effectiveness of a unified geometric basis for nuanced motion reasoning.",153.3,Phi-4,Nvidia B200 (Cloud Native)
2601.07635v2_Learning About Learning A Physics Path from Spin G.pdf,Learning About Learning: A Physics Path from Spin Glasses to Artificial Intelligence,"['Denis D. Caprioti', 'Matheus Haas', 'Constantino F. Vasconcelos', 'Mauricio Girardi-Schappo']","This paper presents the Hopfield model as a pedagogically rich framework that unifies core topics from undergraduate statistical physics, dynamical systems, linear algebra, and computational methods. It provides a theoretical introduction, analyzes the model’s energy function, dynamics, and pattern stability, and discusses practical aspects of simulation, including a freely available simulation code. The paper aims to connect fundamental physics concepts to contemporary AI applications, preparing physics students to engage with computational tools central to research, industry, and society.",154.88,Phi-4,Nvidia B200 (Cloud Native)
2601.07638v1_SALT-KG A Benchmark for Semantics-Aware Learning o.pdf,SALT-KG: A Benchmark for Semantics-Aware Learning on Enterprise Tables,"['Isaiah Onando Mulang’', 'Felix Sasaki', 'Tassilo Klein', 'Jonas Kolk', 'Nikolay Grechanov', 'Johannes Hoffart']","SALT-KG extends the SALT benchmark for relational prediction by linking multi-table transactional data with a Metadata Knowledge Graph (OBKG) that captures field-level semantics, relational dependencies, and business object-types. This enables the evaluation of models that reason over both tabular evidence and contextual semantics, highlighting the importance of semantics in relational contexts. While metadata-derived features show modest improvements in classical prediction metrics, they reveal gaps in models' ability to leverage semantics. SALT-KG aims to advance tabular foundation models grounded in declarative knowledge, marking a step towards semantically linked tables in enterprise-scale structured data.",154.65,Phi-4,Nvidia B200 (Cloud Native)
2601.07641v1_Beyond Static Tools Test-Time Tool Evolution for S.pdf,Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning,"['Jiaxuan Lu', 'Ziyu Kong', 'Yemin Wang', 'Rong Fu', 'Haiyuan Wan', 'Cheng Yang', 'Wenjie Lou', 'Haoran Sun', 'Lilong Wang', 'Yankai Jiang', 'Xiaosong Wang', 'Xiao Sun', 'Dongzhan Zhou']","The paper addresses the limitations of existing Large Language Model (LLM)-based agents in scientific reasoning, which rely on static, pre-defined tool libraries. These libraries are inadequate for scientific domains due to their sparsity, heterogeneity, and incompleteness. The authors propose a new paradigm called Test-Time Tool Evolution (TTE), which allows agents to synthesize, verify, and evolve executable tools during inference. This approach transforms tools from fixed resources into problem-driven artifacts, overcoming the limitations of static tool libraries. The paper introduces SciEvo, a benchmark with 1,590 scientific reasoning tasks supported by 925 automatically evolved tools, demonstrating that TTE achieves state-of-the-art performance in accuracy and tool efficiency, and facilitates cross-domain adaptation of computational tools.",154.43,Phi-4,Nvidia B200 (Cloud Native)
2601.07651v1_Active Evaluation of General Agents Problem Defini.pdf,Active Evaluation of General Agents: Problem Definition and Comparison of Baseline Algorithms,"['Marc Lanctot', 'Kate Larson', 'Ian Gemp', 'Michael Kaisers']","As intelligent agents become more generally-capable, the complexity and cost of evaluating them increases. This paper proposes a formal definition and framework for active evaluation of agents across multiple tasks, focusing on the performance of ranking algorithms as a function of evaluation data samples. Unlike traditional methods that preprocess data, this approach involves an online process where the ranking algorithm selects tasks and agents to sample scores from in each iteration. The paper compares several baseline algorithms, including the classical Elo rating system and Soft Condorcet Optimization, under different experimental contexts using synthetic and real Atari game-playing agent data. The findings suggest that while Elo is reliable in practice, Soft Condorcet Optimization outperforms Elo on real data, especially when task variation is high.",154.82,Phi-4,Nvidia B200 (Cloud Native)
2601.07654v1_Towards Automating Blockchain Consensus Verificati.pdf,Towards Automating Blockchain Consensus Verification with IsabeLLM,"['Elliot Jones', 'William Knottenbelt']","This paper introduces IsabeLLM, a tool that integrates the proof assistant Isabelle with a Large Language Model to assist and automate the formal verification of blockchain consensus protocols. The tool is demonstrated by developing and verifying a novel model of Bitcoin’s Proof of Work consensus protocol. The study highlights the potential of IsabeLLM to generate correct proofs for non-trivial lemmas, showcasing its effectiveness in reducing the effort and expertise required for formal verification. The research underscores the importance of correctly designing and implementing consensus protocols to prevent adversarial exploits, such as the 51% attack, in blockchain systems.",137.8,Phi-4,Nvidia B200 (Cloud Native)
2601.07663v2_Reasoning Models Will Blatantly Lie About Their Re.pdf,Reasoning Models Will Blatantly Lie About Their Reasoning,['William Walden'],"This paper explores the behavior of Large Reasoning Models (LRMs) in relation to their use of hints in multiple-choice questions. It extends previous work by Chen et al. (2025) to demonstrate that LRMs often deny relying on hints provided in the prompt, even when they clearly do so. The study highlights the challenges in ensuring faithfulness and interpretability in model reasoning, as models tend to ignore or deny the influence of hints despite their presence. The findings suggest that current methods of monitoring and interpreting model reasoning may not be sufficient, as models can exhibit unfaithful behavior even when explicitly instructed to acknowledge unusual prompt content.",155.12,Phi-4,Nvidia B200 (Cloud Native)
2601.07666v1_Variational Contrastive Learning for Skeleton-base.pdf,Variational Contrastive Learning for Skeleton-based Action Recognition,"['Dang-Dinh NGUYEN', 'Decky ASPANDI-LATIF', 'Titus ZAHARIA']","This paper introduces a variational contrastive learning framework for skeleton-based action recognition. Traditional contrastive learning methods, while effective, often fail to capture the inherent variability and uncertainty in human motion. The proposed framework integrates probabilistic latent modeling with contrastive self-supervised learning, enabling the learning of structured and semantically meaningful representations. These representations generalize well across different datasets and supervision levels. The method outperforms existing approaches, especially in low-label scenarios, and provides more relevant features focused on important skeleton joints. The approach is validated through extensive experiments on three widely used benchmarks.",155.12,Phi-4,Nvidia B200 (Cloud Native)
2601.07667v1_Adaptive Layer Selection for Layer-Wise Token Prun.pdf,Adaptive Layer Selection for Layer-Wise Token Pruning in LLM Inference,"['Rei Taniguchi', 'Yuyang Dong', 'Makoto Onizuka', 'Chuan Xiao']","This paper introduces ASL, a training-free method for adaptively selecting the layer for key-value (KV) cache reduction in large language model (LLM) inference. Traditional layer-wise token pruning methods use pre-defined layers for token selection, leading to performance variability across tasks. ASL addresses this by exploiting the variance of token ranks ordered by attention score, allowing for adaptive layer selection that balances performance across different tasks while meeting user-specified KV budget requirements. The method operates during the prefilling stage and can be used alongside existing methods like SnapKV. Evaluations on benchmarks such as InfiniteBench, RULER, and NIAH demonstrate that ASL, with one-shot token selection, outperforms state-of-the-art methods in accuracy while maintaining decoding speed and KV cache reduction.",153.9,Phi-4,Nvidia B200 (Cloud Native)
2601.07685v1_Predictive Analytics for Dementia Machine Learning.pdf,Predictive Analytics for Dementia: Machine Learning on Healthcare Data,"['Shafiul Ajam Opee', 'Nafiz Fahad', 'Anik Sen', 'Rasel Ahmed', 'Fariha Jahan', 'Md. Kishor Morol', 'Md Rashedul Islam']","This study focuses on enhancing dementia prediction using machine learning techniques on patient health data. Supervised learning algorithms such as K-Nearest Neighbors (KNN), Quadratic Discriminant Analysis (QDA), Linear Discriminant Analysis (LDA), and Gaussian Process Classifiers are applied. Techniques like Synthetic Minority Over-sampling Technique (SMOTE) and Term Frequency-Inverse Document Frequency (TF-IDF) vectorization are employed to address class imbalance and improve model performance. LDA achieved the highest testing accuracy of 98%. The study emphasizes model interpretability and the correlation of dementia with features such as the presence of the APOE-ϵ4 allele and chronic conditions like diabetes. It advocates for future machine learning innovations, particularly in integrating explainable AI approaches, to further improve predictive capabilities in dementia care.",154.4,Phi-4,Nvidia B200 (Cloud Native)
2601.07701v1_Deep Whole-body Parkour.pdf,Deep Whole-body Parkour,"['Ziwen Zhuang', 'Shaoting Zhu', 'Mengjie Zhao', 'Hang Zhao']","This paper presents a framework that integrates exteroceptive sensing into whole-body motion tracking, enabling humanoid robots to perform dynamic, non-locomotion tasks on uneven terrain. By training a single policy to execute multiple distinct motions across varied terrestrial features, the framework achieves robust, highly dynamic multi-contact motions such as vaulting and dive-rolling on unstructured terrain. This significantly expands the robot's traversability beyond simple walking or running, uniting perceptive locomotion and general motion tracking paradigms.",154.97,Phi-4,Nvidia B200 (Cloud Native)
2601.07718v1_Hiking in the Wild A Scalable Perceptive Parkour F.pdf,Hiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoids,"['Shaoting Zhu', 'Ziwen Zhuang', 'Mengjie Zhao', 'Kun-Ying Lee', 'Hang Zhao']","This paper introduces 'Hiking in the Wild,' a scalable, end-to-end perceptive framework designed for robust humanoid hiking across diverse terrains. The framework enables a humanoid robot to traverse complex environments, including stairs, gaps, and ramps, at speeds up to 2.5 m/s. It leverages depth images for perception and employs a single-stage reinforcement learning scheme to map raw depth inputs and proprioception directly to joint actions, avoiding reliance on external state estimation. Key innovations include a foothold safety mechanism and a flat patch sampling strategy to ensure safety and training stability. Extensive field experiments demonstrate the framework's effectiveness on a full-size humanoid.",155.04,Phi-4,Nvidia B200 (Cloud Native)
2601.07737v1_Evaluating the encoding competence of visual langu.pdf,Evaluating the encoding competence of visual language models using uncommon actions,"['Chen Ling', 'Nai Ding*']","This undergraduate project report investigates the encoding competence of visual language models, particularly focusing on their ability to handle uncommon actions. The study is conducted by Chen Ling under the supervision of Nai Ding, with affiliations to Beijing University of Post and Telecommunications and Zhejiang University, respectively.",156.11,Phi-4,Nvidia B200 (Cloud Native)
2601.07748v1_Improving Domain Generalization in Contrastive Lea.pdf,Improving Domain Generalization in Contrastive Learning using Adaptive Temperature Control,"['Robert Lewis', 'Katie Matton', 'Rosalind W. Picard', 'John Guttag']","This paper addresses the challenge of domain shift in contrastive learning, where performance degrades when test data come from a domain not seen during training. The authors propose a novel method that incorporates domain labels to enhance domain invariance in learned representations, thereby improving out-of-distribution generalization. The method adjusts the temperature parameter in the InfoNCE loss based on the probability that a negative sample shares the same domain as the anchor, emphasizing domain-invariant attributes. Experiments on a variant of the MNIST dataset show that this approach outperforms domain generalization baselines in out-of-distribution settings while maintaining strong in-distribution task performance.",154.68,Phi-4,Nvidia B200 (Cloud Native)
2601.07778v1_DT-ICU Towards Explainable Digital Twins for ICU P.pdf,DT-ICU: Towards Explainable Digital Twins for ICU Patient Monitoring via Multi-Modal and Multi-Task Iterative Inference,['Wen Guo'],"DT-ICU is a multimodal digital twin framework designed for continuous risk estimation in intensive care units (ICUs). It integrates variable-length clinical time series with static patient information within a unified multitask architecture, allowing predictions to be updated as new observations are made during a patient's ICU stay. Evaluated on the MIMIC-IV dataset, DT-ICU outperforms established baseline models across various evaluation settings. The framework achieves meaningful discrimination shortly after patient admission, with longer observation windows improving the ranking of high-risk patients in imbalanced cohorts. Systematic modality ablations reveal the model's structured reliance on interventions, physiological responses, and contextual information, providing interpretable insights into the combination of multimodal signals and the trade-offs between sensitivity and precision. Overall, DT-ICU offers accurate, temporally robust, and interpretable predictions, highlighting its potential as a practical tool for continuous patient monitoring in critical care.",154.73,Phi-4,Nvidia B200 (Cloud Native)
2601.07779v1_OS-Symphony A Holistic Framework for Robust and Ge.pdf,OS-SYMPHONY: A Holistic Framework for Robust and Generalist Computer-Using Agent,"['Bowen Yang', 'Kaiming Jin', 'Zhenyu Wu', 'Zhaoyang Liu', 'Qiushi Sun', 'Zehao Li', 'Jingjing Xie', 'Zhoumianze Liu', 'Fangzhi Xu', 'Kanzhi Cheng', 'Qingyun Li', 'Yian Wang', 'Yu Qiao', 'Zun Wang', 'Zichen Ding']","OS-SYMPHONY is introduced as a holistic framework designed to enhance the robustness and generalization capabilities of Computer-Using Agents (CUAs) by addressing limitations in handling long-horizon workflows and novel domains. The framework features an Orchestrator that coordinates two key innovations: a Reflection-Memory Agent for trajectory-level self-correction and Versatile Tool Agents with a Multimodal Searcher for synthesizing visually aligned tutorials. Experimental results show significant performance improvements across various model scales, setting new benchmarks in online evaluations.",152.72,Phi-4,Nvidia B200 (Cloud Native)
2601.07782v1_Beyond Single-Shot Multi-step Tool Retrieval via Q.pdf,Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning,"['Wei Fang', 'James Glass']","LLM agents operating over large, dynamic tool libraries face challenges with effective retrieval due to the disconnect between user goals and technical documentation, and the limitations of fixed-size embeddings for complex tasks. This paper introduces TOOLQP, a framework that models retrieval as iterative query planning, decomposing instructions into sub-tasks and dynamically generating queries. TOOLQP is trained using synthetic query trajectories and optimized via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments show that TOOLQP achieves state-of-the-art performance, demonstrating superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.",154.83,Phi-4,Nvidia B200 (Cloud Native)
2601.07790v1_Benchmarking Small Language Models and Small Reaso.pdf,Benchmarking Small Language Models and Small Reasoning,"['Yahya Masri', 'Emily Ma', 'Zifu Wang', 'Joseph Rogers', 'Chaowei Yang']","This paper evaluates the performance of small language models (SLMs) and small reasoning language models (SRLMs) in classifying system log severity levels. Using real-world data from Linux servers, the study assesses these models under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. The results show that Qwen3-4B achieves the highest accuracy with RAG, while Gemma3-1B significantly improves with RAG. The study highlights the importance of architectural design, training objectives, and the ability to integrate retrieved context for model performance. It emphasizes the relevance of small, efficient models for real-time applications in digital twin systems and root cause analysis.",153.98,Phi-4,Nvidia B200 (Cloud Native)
2601.07794v1_Kinship Data Benchmark for Multi-hop Reasoning.pdf,Kinship Data Benchmark for Multi-hop Reasoning,"['Tianda Sun', 'Dimitar Kazakov']","This paper introduces KinshipQA, a benchmark designed to evaluate the multi-hop reasoning capabilities of large language models (LLMs) through reasoning over kinship relations. The authors present a generative pipeline that produces large-scale, realistic, and culture-specific genealogical data, allowing for systematic control over task difficulty, cultural assumptions, and relational depth. The benchmark is evaluated using six state-of-the-art LLMs under a uniform zero-shot protocol, revealing systematic differences in multi-hop reasoning across models and cultural settings.",154.11,Phi-4,Nvidia B200 (Cloud Native)
2601.07821v1_Failure-Aware RL Reliable Offline-to-Online Reinfo.pdf,Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation,"['Huanyu Li', 'Kun Lei', 'Sheng Zang', 'Kaizhe Hu', 'Yongyuan Liang', 'Bo An', 'Xiaoli Li', 'Huazhe Xu']","This paper introduces Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a paradigm designed to minimize Intervention-requiring Failures (IR Failures) during real-world reinforcement learning. FARL incorporates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration. The approach is validated through extensive simulation and real-world experiments, demonstrating a significant reduction in IR Failures by 73.1% and an improvement in performance by 11.3% on average during real-world RL post-training. The paper also presents FailureBench, a benchmark incorporating common failure scenarios requiring human intervention.",154.37,Phi-4,Nvidia B200 (Cloud Native)
2601.07832v2_MHLA Restoring Expressivity of Linear Attention vi.pdf,MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head,"['Kewei Zhang', 'Ye Huang', 'Yufan Deng', 'Jincheng Yu', 'Junsong Chen', 'Huan Ling', 'Enze Xie', 'Daquan Zhou']","The paper addresses the challenge of maintaining the expressivity of linear attention mechanisms, which are more efficient than traditional softmax attention but often suffer from performance degradation due to global context collapse. The authors propose Multi-Head Linear Attention (MHLA), which divides attention computation across multiple heads along the token dimension, preserving representational diversity and maintaining linear complexity. MHLA demonstrates significant performance improvements across various domains, including a 3.6% improvement on ImageNet classification, a 6.3% gain in NLP tasks, a 12.6% improvement in image generation, and a 41% enhancement in video generation, all while maintaining the same time complexity as linear attention.",154.92,Phi-4,Nvidia B200 (Cloud Native)
2601.07885v1_Small Symbols Big Risks Exploring Emoticon Semanti.pdf,"Small Symbols, Big Risks: Exploring Emoticon Semantic Confusion in Large Language Models","['Weipeng Jiang', 'Xiaoyu Zhang', 'Juan Zhai', 'Shiqing Ma', 'Chao Shen', 'Yang Liu']","This paper explores the vulnerability of emoticon semantic confusion in Large Language Models (LLMs), where LLMs misinterpret ASCII-based emoticons, leading to unintended and potentially destructive actions. The authors developed an automated data generation pipeline and constructed a dataset with 3,757 test cases across 21 scenarios, four programming languages, and varying complexities. The study found that emoticon semantic confusion is pervasive, with an average confusion ratio exceeding 38%, and over 90% of confused responses result in 'silent failures'. These failures are syntactically valid but deviate from user intent, posing significant security risks. The vulnerability also transfers to popular agent frameworks, and existing prompt-based mitigations are largely ineffective. The paper calls for the community to recognize this emerging vulnerability and develop effective mitigation methods to ensure the safety and reliability of human-AI interactions.",154.16,Phi-4,Nvidia B200 (Cloud Native)
2601.07891v1_KVzap Fast Adaptive and Faithful KV Cache Pruning.pdf,"KVzap: Fast, Adaptive, and Faithful KV Cache Pruning","['Simon Jégou', 'Maximilian Jeblick']","KVzap introduces a fast, input-adaptive approximation of KVzip for transformer-based language models, addressing the critical inference bottleneck posed by growing context lengths in the key-value (KV) cache. It achieves 2–4× KV cache compression with negligible accuracy loss, outperforming 15 other methods on the KVpress Leaderboard for models like Qwen3-8B and Llama-3.1-8B-Instruct. The method is applicable in both prefilling and decoding phases, demonstrating state-of-the-art performance across long-context and reasoning tasks.",155.49,Phi-4,Nvidia B200 (Cloud Native)
2601.07892v1_Sherry Hardware-Efficient 1.25-Bit Ternary Quantiz.pdf,Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification,"['Hong Huang', 'Decheng Wu', 'Qiangqiang Hu', 'Guanghua Yu', 'Jinhai Yang', 'Jianchen Zhu', 'Xue Liu', 'Dapeng Wu']","The paper addresses the challenge of deploying large language models (LLMs) on resource-constrained edge devices due to high memory and computational demands. It proposes a novel ternary quantization framework, Sherry, which introduces a 3:4 fine-grained sparsity to achieve a regularized 1.25-bit width, aligning with power-of-two hardware requirements. Sherry also tackles the weight trapping issue in sparse ternary training with an annealing residual synapse mechanism, maintaining representational diversity. Empirical evaluations show that Sherry matches state-of-the-art ternary performance while reducing model size, achieving zero accuracy loss, 25% bit savings, and a 10% speedup on an Intel i7-14700HX CPU.",153.69,Phi-4,Nvidia B200 (Cloud Native)
2601.07894v1_Revealing the Attention Floating Mechanism in Mask.pdf,Revealing the Attention Floating Mechanism in Masked Diffusion Models,"['Xin Dai', 'Pengcheng Huang', 'Zhenghao Liu', 'Shuo Wang', 'Yukun Yan', 'Chaojun Xiao', 'Yu Gu', 'Ge Yu', 'Maosong Sun']","This paper investigates the attention mechanisms in Masked Diffusion Models (MDMs), revealing a phenomenon termed 'Attention Floating.' Unlike Autoregressive Models (ARMs), where attention converges to fixed anchors, MDMs exhibit dynamic, dispersed attention anchors that shift across denoising steps and layers. The study identifies a Shallow Structure-Aware, Deep Content-Focused attention mechanism: shallow layers use floating tokens to establish a global structural framework, while deeper layers focus on capturing semantic content. This distinctive attention pattern explains the strong in-context learning capabilities of MDMs, significantly enhancing their performance in knowledge-intensive tasks compared to ARMs.",153.85,Phi-4,Nvidia B200 (Cloud Native)
2601.07898v1_Large Language Models and Algorithm Execution Appl.pdf,Large Language Models and Algorithm Execution: Application to an Arithmetic Function,"['Farah Ben Slama', 'Frédéric Armetta']","This paper explores the potential of extending the capabilities of Large Language Models (LLMs) to execute algorithms through specialized supervised training focused on reasoning decomposition. The authors introduce a training model called LLM-DAL (Large Language Model - Decompositional Algorithmic Learning), demonstrating that LLMs' ability to perform complex algorithmic inferences and generalize can be significantly improved with a properly designed training method. The paper addresses the challenge of using neural networks for learning and executing algorithms, aiming to automate complex algorithm execution without explicit implementation and enhance systematic reasoning and generalization.",154.95,Phi-4,Nvidia B200 (Cloud Native)
2601.07901v1_Decentralized Online Convex Optimization with Unkn.pdf,Decentralized Online Convex Optimization with Unknown Feedback Delays,"['Hao Qiu', 'Mengxiao Zhang', 'Juliette Achddou']","This paper addresses the problem of decentralized online convex optimization (D-OCO) under unknown, time- and agent-varying feedback delays. Existing algorithms assume prior knowledge of the total delay, leading to suboptimal performance. The authors propose a novel algorithm with an improved regret bound, leveraging an adaptive learning rate mechanism and a decentralized communication protocol. This allows agents to estimate delays locally using a gossip-based strategy without prior knowledge of the total delay. The framework is extended to the strongly convex setting, yielding a sharper regret bound. Experimental results demonstrate the effectiveness of the proposed approach, showing improvements over existing benchmark algorithms.",154.76,Phi-4,Nvidia B200 (Cloud Native)
2601.07903v2_Enhancing Large Language Models for Time-Series Fo.pdf,Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning,"['Jianqi Zhang', 'Jingyao Wang', 'Wenwen Qiang', 'Fanjiang Xu', 'Changwen Zheng']","The paper addresses the challenge of applying large language models (LLMs) to time series forecasting (TSF) due to the significant differences between pretraining corpora and time series data. While fine-tuning LLMs can improve performance, it incurs substantial computational overhead. The authors propose a method called LVICL, which enhances forecasting performance by injecting a context vector into a frozen LLM, leveraging in-context learning without increasing prompt length. This approach adaptively compresses example-related information into a context vector, which is then injected into every layer of the LLM to improve performance. Extensive experiments demonstrate the effectiveness of this method.",154.7,Phi-4,Nvidia B200 (Cloud Native)
2601.07935v1_Towards Specialized Generalists A Multi-Task MoE-L.pdf,Towards Specialized Generalists: A Multi-Task MoE-LoRA Framework for Domain-Specific LLM Adaptation,"['Yuxin Yang', 'Aoxiong Zeng', 'Xiangquan Yang']","This paper introduces Med-MoE-LoRA, a novel framework that combines Mixture-of-Experts (MoE) with Low-Rank Adaptation (LoRA) to address the challenges of adapting Large Language Models (LLMs) to specialized domains like medicine. The framework tackles the 'Stability-Plasticity Dilemma' and 'Task Interference' by employing an asymmetric expert distribution and a 'Knowledge-Preservation Plugin'. This allows the model to efficiently adapt to multiple medical tasks while preserving general cognitive capabilities. Experimental results show that Med-MoE-LoRA outperforms standard LoRA and conventional MoE architectures in clinical NLP tasks.",154.97,Phi-4,Nvidia B200 (Cloud Native)
2601.07939v1_SECite Analyzing and Summarizing Citations in Soft.pdf,Analyzing and Summarizing Citations in Software Engineering Literature,"['Shireesh Reddy Pyreddy', 'Khaja Valli Pathan', 'Hasan Masum', 'Tarannum Shaila Zaman']","This paper introduces SECite, a novel approach for evaluating scholarly impact through sentiment analysis of citation contexts. It presents a semi-automated pipeline that extracts citations from nine research papers and applies natural language processing (NLP) techniques with unsupervised machine learning to classify these citations as positive or negative. Additionally, generative AI is used to produce sentiment-specific summaries that capture the strengths and limitations of each paper. The study reveals patterns in how the academic community perceives these works, highlighting areas of alignment and divergence between external citation feedback and the authors' own presentations. By integrating citation sentiment analysis with LLM-based summarization, the study provides a comprehensive framework for assessing scholarly contributions.",154.75,Phi-4,Nvidia B200 (Cloud Native)
2601.07941v2_Moonworks Lunara Aesthetic Dataset.pdf,Moonworks Lunara Aesthetic Dataset,"['Yan Wang', 'M M Sayeef Abdullah', 'Partho Hassan', 'Sabit Hassan']","The Lunara Aesthetic Dataset is a curated collection of 2,000 image–prompt pairs designed for research on prompt grounding and style conditioning in text-to-image generation systems. It features diverse artistic styles from various regions and general categories, with images generated by the Moonworks Lunara model. The dataset emphasizes high aesthetic quality, stylistic diversity, and licensing transparency, released under the Apache 2.0 license for both academic and commercial use.",155.0,Phi-4,Nvidia B200 (Cloud Native)
2601.07946v1_Coupled Diffusion-Encoder Models for Reconstructio.pdf,Coupled Diffusion–Encoder Models for Reconstruction of Flow Fields,"['AmirPouya Hemmasian', 'Amir Barati Farimani']","The paper introduces DiffCoder, a novel framework that combines a probabilistic diffusion model with a convolutional ResNet encoder to reconstruct flow fields. Unlike traditional variational autoencoders (VAEs), which often struggle with preserving the higher-order statistical structure of fluid flows under strong compression, DiffCoder effectively recovers both distributional and spectral properties critical for accurate flow representation. The framework is evaluated on a dataset of Kolmogorov flow fields, demonstrating significant improvements in spectral accuracy under aggressive compression compared to VAEs. While both methods show similar L2 reconstruction errors, DiffCoder better maintains the underlying distributional structure of the flow. The results suggest that diffusion-based priors are particularly beneficial in scenarios with severe information bottlenecks, offering a promising approach for creating compact, statistically consistent representations of complex flow fields.",154.9,Phi-4,Nvidia B200 (Cloud Native)
2601.07948v1_Reinforcement Learning Methods for Neighborhood Se.pdf,Reinforcement Learning Methods for Neighborhood Selection in Local Search,"['Yannick Molinghen', 'Augustin Delecluse', 'Renaud De Landtsheer', 'Stefano Michelini']","This study evaluates reinforcement learning-based neighborhood selection strategies in local search metaheuristics, focusing on multi-armed bandits and deep reinforcement learning methods. The research compares these strategies against baselines across three problems: the traveling salesman problem, the pickup and delivery problem with time windows, and the car sequencing problem. The study highlights the need for carefully designed reward functions due to large variations in cost from constraint violations. Results show that while ε-greedy consistently performs well, deep reinforcement learning methods require significantly longer runtimes to be competitive. These findings underscore both the potential and practical limitations of deep reinforcement learning in local search.",154.65,Phi-4,Nvidia B200 (Cloud Native)
2601.07951v1_Hybrid SARIMA LSTM Model for Local Weather Forecas.pdf,Hybrid SARIMA–LSTM Model for Local Weather Forecasting: A Residual-Learning Approach for Data-Driven Meteorological Prediction,"['Shreyas Rajeev Stevens Institute of Technology Hoboken, USA', 'Karthik Mudenahalli Ashoka Stevens Institute of Technology Hoboken, USA', 'Amit Mallappa Tiparaddi Stevens Institute of Technology Hoboken, USA']","This paper introduces a Hybrid SARIMA–LSTM model for local weather forecasting, addressing the limitations of traditional statistical models and independent neural networks in long-term weather prediction. The model separates temperature into a predictable climate component and a nonlinear weather component using a residual-learning strategy. SARIMA captures the long-term seasonal trend, while LSTM models the random fluctuations in SARIMA's prediction errors. The approach incorporates Fourier seasonal encoding and a stabilized recursive forecasting mechanism to maintain prediction accuracy within a 293-day horizon. The model's efficacy is demonstrated through daily average temperature predictions for New York City using data from 2020 to 2023, showing significant improvements over traditional SARIMA models.",154.46,Phi-4,Nvidia B200 (Cloud Native)
2601.07953v1_Quantum automated theorem proving.pdf,Quantum automated theorem proving,"['Zheng-Zhi Sun', 'Qi Ye', 'Dong-Ling Deng']","This paper introduces a framework for quantum automated theorem proving, leveraging quantum superposition and entanglement to potentially enhance theorem-proving capabilities. It proposes quantum representations of knowledge bases and reasoning algorithms for tasks in propositional and first-order logic, achieving quadratically reduced query complexity. Additionally, it extends Wu’s algebraic approach to geometric theorems, demonstrating improved query complexity for problems like those in the International Mathematical Olympiad. The work aims to establish a foundation for quantum automatic theorem provers, crucial for both near-term and future quantum technologies.",154.41,Phi-4,Nvidia B200 (Cloud Native)
2601.07957v1_LWMSCNN-SE A Lightweight Multi-Scale Network for E.pdf,LWMSCNN-SE: A Lightweight Multi-Scale Network for Efficient Maize Disease Classification on Edge Devices,"['Fikadu Weloday', 'Jianmei Su']","This paper introduces LWMSCNN-SE, a lightweight convolutional neural network designed for maize disease classification on edge devices. The model integrates multi-scale feature extraction, depthwise separable convolutions, and squeeze-and-Excitation (SE) attention mechanisms to achieve high classification accuracy with low computational costs. This approach addresses the accuracy-efficiency trade-off, making it suitable for real-time deployment in precision farming systems, particularly in resource-constrained environments.",154.63,Phi-4,Nvidia B200 (Cloud Native)
2601.07958v1_LJ-Spoof A Generatively Varied Corpus for Audio An.pdf,LJ-SPOOF: A GENERATIVELY VARIED CORPUS FOR AUDIO ANTI-SPOOFING AND SYNTHESIS SOURCE TRACING,"['Surya Subramani', 'Hashim Ali', 'Hafiz Malik']","The paper introduces LJ-Spoof, a speaker-specific, generatively diverse corpus designed to address challenges in audio anti-spoofing and synthesis-source tracing. The corpus systematically varies prosody, vocoders, generative hyperparameters, bona fide prompt sources, training regimes, and neural post-processing. It includes recordings from one speaker, 30 TTS families, 500 generatively variant subsets, 10 bona fide neural-processing variants, and over 3 million utterances. This design supports robust speaker-conditioned anti-spoofing and fine-grained synthesis-source tracing, positioning the dataset as both a practical training resource and a benchmark evaluation suite.",154.86,Phi-4,Nvidia B200 (Cloud Native)
2601.07964v1_Executable Ontologies in Game Development From Alg.pdf,Executable Ontologies in Game Development: From Algorithmic Control to Semantic World Modeling,['Alexander Boldachev'],"This paper explores the use of Executable Ontologies (EO) in game development, particularly through the boldsea framework. It presents a shift from traditional algorithmic behavior programming to semantic world modeling, where agent behavior emerges from declarative domain rules. The paper demonstrates this approach using a survival game scenario, 'Winter Feast,' highlighting how EO facilitates priority-based task interruption via dataflow conditions rather than explicit preemption logic. It contrasts EO with Behavior Trees (BT) and Goal-Oriented Action Planning (GOAP), emphasizing EO's ability to model when actions become possible, thus addressing the semantic-process gap in game AI architecture. The paper also discusses integration strategies, debugging advantages of temporal event graphs, and the potential for LLM-driven runtime model generation.",155.26,Phi-4,Nvidia B200 (Cloud Native)
2601.07965v1_When Models Know When They Do Not Know Calibration.pdf,"WHEN MODELS KNOW WHEN THEY DO NOT KNOW: CALIBRATION, CASCADING, AND CLEANING","['Chenjie Hao', 'Weyl Lu', 'Yuko Ishiwaka', 'Zengyi Li', 'Weier Wan', 'Yubei Chen']","This paper explores how models can recognize their own limitations by using confidence derived from internal signals. It introduces a training-free method for model calibration applicable to both vision and language models. The paper highlights two empirical observations: higher confidence correlates with higher accuracy within a model, and calibration on a validation set remains effective on a test set. Two applications are proposed: model cascading using calibrated confidence to improve efficiency and accuracy, and data cleaning to identify mislabeled samples in datasets like ImageNet and MMLU. The findings suggest that enabling models to recognize when they do not know can lead to more efficient, reliable, and trustworthy AI systems.",154.78,Phi-4,Nvidia B200 (Cloud Native)
2601.07969v1_Tuberculosis Screening from Cough Audio Baseline M.pdf,"TUBERCULOSIS SCREENING FROM COUGH AUDIO: BASELINE MODELS, CLINICAL VARIABLES, AND UNCERTAINTY QUANTIFICATION","['George P. Kafentzis', 'Efstratios Selisios']","This paper proposes a standardized framework for automatic tuberculosis (TB) detection from cough audio and clinical data using machine learning. It addresses the challenge of comparing existing studies due to variations in datasets, cohort definitions, feature representations, model families, validation protocols, and metrics. The authors establish a reproducible end-to-end baseline for TB prediction using cough recordings and clinical metadata from a multi-country dataset. The framework includes feature extraction, multimodal fusion, cougher-independent evaluation, and uncertainty quantification, reporting clinically relevant metrics for fair comparison. Performance is quantified for audio-only and fused models, with the full experimental protocol released for benchmarking. This baseline aims to serve as a common reference point and reduce methodological variance in the field.",154.87,Phi-4,Nvidia B200 (Cloud Native)
2601.07973v1_Cultural Compass A Framework for Organizing Societ.pdf,Cultural Compass: A Framework for Organizing Societal Norms to Detect Violations in Human-AI Conversations,"['Myra Cheng', 'Vinodkumar Prabhakaran', 'Alice Oh', 'Hayk Stepanyan', 'Aishwarya Verma', 'Charu Kalia', 'Erin MacMurray van Liemt', 'Sunipa Dev']","This paper introduces a taxonomy for organizing societal norms to evaluate their adherence in human-AI conversations. It distinguishes between human-human norms and human-AI interactional norms, specifying relevant domains and enforcement mechanisms. The authors demonstrate how this taxonomy can be used to automatically evaluate norm adherence in open-ended settings, revealing that state-of-the-art models often violate norms, with varying rates depending on the model, context, and country. The paper highlights the need for nuanced, context-sensitive evaluation of cultural norms in realistic settings.",153.95,Phi-4,Nvidia B200 (Cloud Native)
2601.07988v1_From Word Sequences to Behavioral Sequences Adapti.pdf,From Word Sequences to Behavioral Sequences: Adapting Modeling and Evaluation Paradigms for Longitudinal NLP,"['Adithya V Ganesan', 'Vasudha Varadarajan', 'Oscar NE Kjell', 'Whitney R Ringwald', 'Scott Feltman', 'Benjamin J Luft', 'Roman Kotov', 'Ryan L Boyd', 'H Andrew Schwartz']","This paper addresses the limitations of traditional NLP approaches when applied to longitudinal data, where documents are nested within authors and ordered over time. The authors propose a new modeling and evaluation paradigm that updates four parts of the NLP pipeline: evaluation splits, accuracy metrics, sequence inputs, and model internals. They demonstrate the issues with traditional methods using a dataset of daily diary transcripts and PTSD symptom severity, showing that traditional document-level evaluation can lead to misleading conclusions. The paper advocates for a shift from word-sequence evaluation to behavior-sequence paradigms in NLP.",154.15,Phi-4,Nvidia B200 (Cloud Native)
2601.07994v2_DYCP Dynamic Context Pruning for Long-Form Dialogu.pdf,DYCP: Dynamic Context Pruning for Long-Form Dialogue with LLMs,"['Nayoung Choi', 'Jonathan Zhang', 'Jinho D. Choi']","This paper introduces DYCP, a dynamic context pruning method designed to manage context in long-form dialogues with Large Language Models (LLMs). As dialogue length increases, LLMs face challenges such as increased response latency and degraded answer quality. Existing methods often rely on additional LLM calls for memory management, which can be inefficient. DYCP addresses these issues by dynamically segmenting and retrieving relevant memory at query time, preserving the sequential structure of dialogue without predefined topic boundaries. The method supports efficient, adaptive context retrieval and consistently improves answer quality while reducing response latency across various benchmarks. The paper also highlights the gap between the expanded context windows of modern LLMs and their actual long-context processing capacity, emphasizing the importance of effective context management.",154.34,Phi-4,Nvidia B200 (Cloud Native)
2601.08000v1_Reasoning over Precedents Alongside Statutes Case-.pdf,Case-Augmented Deliberative Alignment for LLM Safety,"['Can Jin', 'Rui Wu', 'Tong Che', 'Qixin Zhang', 'Hongwu Peng', 'Jiahui Zhao', 'Zhenting Wang', 'Wenqi Wei', 'Ligong Han', 'Zhao Zhang', 'Yuan Cao', 'Ruixiang Tang', 'Dimitris N. Metaxas']","This paper addresses the challenge of ensuring that Large Language Models (LLMs) adhere to safety principles without refusing benign requests. It evaluates the effectiveness of deliberative alignment (DA) in open-source LLMs, which often lack advanced reasoning capabilities. The study finds that training on case-augmented simple codes yields more robust and generalized safety behaviors compared to extensive code-like safety rules. The authors propose CADA, a case-augmented deliberative alignment method using reinforcement learning on self-generated safety reasoning chains. CADA enhances harmlessness, improves robustness against attacks, and reduces over-refusal while maintaining utility across diverse benchmarks, offering a practical alternative to rule-only DA.",153.25,Phi-4,Nvidia B200 (Cloud Native)
2601.08003v1_LLM Review Enhancing Creative Writing via Blind Pe.pdf,LLM Review: Enhancing Creative Writing via Blind Peer Review Feedback,"['Weiyue Li', 'Mingxiao Song', 'Zhenda Shen', 'Dachuan Zhao', 'Yunfan Long', 'Yi Li', 'Yongce Li', 'Ruyi Yang', 'Mengyu Wang']","This paper introduces LLM Review, a peer-review-inspired framework designed to enhance creative writing by implementing a Blind Peer Review mechanism. In this framework, agents exchange targeted feedback while revising independently, which helps preserve divergent creative trajectories. The authors propose a new dataset, SciFi-100, for evaluating creativity in writing, combining LLM-as-a-judge scoring, human annotation, and rule-based novelty metrics. Experiments show that LLM Review outperforms traditional multi-agent baselines, and smaller models using this framework can surpass larger single-agent models. The study suggests that the structure of interaction can be more crucial than model scale for fostering creativity.",154.74,Phi-4,Nvidia B200 (Cloud Native)
2601.08005v1_Internal Deployment Gaps in AI Regulation.pdf,Internal Deployment Gaps in AI Regulation,"['JOE KWON', 'STEPHEN CASPER']","This paper explores the regulatory challenges associated with the internal deployment of AI systems within organizations. It identifies three key gaps in current AI regulations in the United States and European Union: scope ambiguity, point-in-time compliance assessments, and information asymmetries. These gaps allow internally deployed AI systems to evade oversight, despite their high-stakes applications. The paper analyzes the reasons for these gaps, including issues related to measurability, incentives, and information access, and discusses potential approaches to address them. By highlighting these patterns, the paper aims to inform deliberate policy choices regarding internally deployed AI systems.",155.19,Phi-4,Nvidia B200 (Cloud Native)
2601.08011v1_TP-Blend Textual-Prompt Attention Pairing for Prec.pdf,TP-Blend: Textual-Prompt Attention Pairing for Precise Object-Style Blending in Diffusion Models,"['Xin Jin', 'Yichuan Zhong', 'Yapeng Tian']","The paper introduces TP-Blend, a framework for blending objects and styles in diffusion models using two textual prompts. It employs Cross-Attention Object Fusion (CAOF) to locate and reassign spatial tokens based on attention, and Self-Attention Style Fusion (SASF) to inject style through Detail-Sensitive Instance Normalization. TP-Blend achieves high-resolution, photo-realistic edits with precise control over content and appearance, outperforming recent baselines in fidelity, quality, and speed.",154.81,Phi-4,Nvidia B200 (Cloud Native)
2601.08017v1_Representations of Text and Images Align From Laye.pdf,Representations of Text and Images Align From Layer One,"['Evˇzen Wybitul', 'Javier Rando', 'Florian Tram`er', 'Stanislav Fort']","This paper challenges the established view that image-text alignment in adapter-based vision-language models only occurs in late layers. Using a synthesis-based method inspired by DeepDream, the authors demonstrate that representations of images and their text descriptions are aligned from the very first layer. By extracting concept vectors for textual concepts and synthesizing images that align with these vectors, the study finds that early layers (as early as layer 1) can depict salient visual features of the targeted textual concepts. This method provides direct evidence of image-text alignment on a concept-by-concept and layer-by-layer basis, offering a new approach to model interpretability without requiring auxiliary models or datasets.",155.03,Phi-4,Nvidia B200 (Cloud Native)
2601.08026v2_FigEx2 Visual-Conditioned Panel Detection and Capt.pdf,Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures,"['Jifeng Song', 'Arun Das', 'Pan Wang', 'Hui Ji', 'Kun Zhao', 'Yufei Huang']","This paper introduces FigEx2, a visual-conditioned framework designed to localize panels and generate panel-wise captions from scientific compound figures. The framework addresses challenges in panel-level understanding due to missing or figure-level captions. It incorporates a noise-aware gated fusion module to stabilize detection and employs a staged optimization strategy combining supervised and reinforcement learning. The strategy uses CLIP-based alignment and BERTScore-based semantic rewards to ensure multimodal consistency. The authors also introduce the BioSci-Fig-Cap benchmark for panel-level grounding and demonstrate FigEx2's superior performance in detection and captioning, with notable zero-shot transferability to other scientific domains.",154.13,Phi-4,Nvidia B200 (Cloud Native)
2601.08043v1_The Role of Noisy Data in Improving CNN Robustness.pdf,The Role of Noisy Data in Improving CNN Robustness for Image Classification,"['Oscar H. Ramírez-Agudelo', 'Nicoleta Gorea', 'Aliza Reif', 'Lorenzo Bonasera', 'Michael Karl']","This paper explores the impact of introducing controlled noise into training data to enhance the robustness of convolutional neural networks (CNNs) for image classification. Using the CIFAR-10 dataset, the study evaluates the effects of Gaussian noise, Salt-and-Pepper noise, and Gaussian blur at various intensities. The experiments, conducted with a Resnet-18 model, demonstrate that incorporating just 10% noisy data during training significantly reduces test loss and improves accuracy under fully corrupted test conditions, with minimal impact on performance with clean data. The findings suggest that strategic exposure to noise can serve as an effective regularizer, balancing traditional data cleanliness with real-world resilience.",153.23,Phi-4,Nvidia B200 (Cloud Native)
2601.08049v1_Integrating Attendance Tracking and Emotion Detect.pdf,Integrating Attendance Tracking and Emotion Detection for Enhanced Student Engagement in Smart Classrooms,"['Keith Ainebyona', 'Ann Move Oguti', 'Joseph Walusimbi', 'Ritah Kobusingye']","This paper introduces SCASED (Smart Classroom Attendance System with Emotion Detection), an IoT-based system that combines automated attendance tracking with facial emotion recognition to enhance classroom engagement monitoring. Utilizing a Raspberry Pi camera and OpenCV for face detection, and a fine-tuned MobileNetV2 model for classifying emotions such as engagement, boredom, confusion, and frustration, the system aims to provide instructors with real-time insights into classroom dynamics. The system records attendance once per session and continuously analyzes emotions, with data visualized on a cloud-based dashboard. Experimental evaluation using the DAiSEE dataset achieved an emotion classification accuracy of 89.5%. The integration of attendance and emotion analytics offers instructors additional insights, supporting more responsive teaching practices.",155.02,Phi-4,Nvidia B200 (Cloud Native)
2601.08052v1_Forecast Aware Deep Reinforcement Learning for Eff.pdf,Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms,"['Nawazish Ali', 'Rachael Shaw', 'Karl Mason']","This study introduces a Deep Reinforcement Learning framework for efficient electricity load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast-Aware PPO incorporates short-term forecasts of demand and renewable generation, while the PID-KL PPO variant uses a proportional–integral–derivative controller to regulate KL-divergence for stable policy updates. The method, trained on real-world dairy farm data, achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC, and reduces grid imports by 13.1% for battery scheduling, demonstrating its scalability and effectiveness for sustainable energy management in dairy farming.",154.77,Phi-4,Nvidia B200 (Cloud Native)
2601.08058v1_Reasoning Beyond Chain-of-Thought A Latent Computa.pdf,Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models,"['Zhenghao He', 'Guangzhi Xiong', 'Bohan Liu', 'Sanchit Sinha', 'Aidong Zhang']","This paper investigates the underlying mechanisms of reasoning in large language models (LLMs) beyond Chain-of-Thought (CoT) prompting. By analyzing internal representations with Sparse Autoencoders (SAEs), the authors identify latent features associated with reasoning behavior. They demonstrate that manipulating these features can improve reasoning accuracy without explicit CoT prompts. The study reveals that reasoning in LLMs is supported by latent internal activations, which can be externally activated, suggesting that CoT prompting is an effective but not unique method for triggering reasoning.",154.22,Phi-4,Nvidia B200 (Cloud Native)
2601.08065v1_A New Strategy for Verifying Reach-Avoid Specifica.pdf,A New Strategy for Verifying Reach-Avoid Specifications in Neural Feedback Systems,"['Samuel I. Akinwande', 'Sydney M. Katz', 'Mykel J. Kochenderfer', 'Clark Barrett']","This paper introduces new algorithms for computing both over- and under-approximations of backward reachable sets in neural feedback systems. These algorithms are integrated with established forward analysis techniques to create a unified verification framework. The work addresses the limitations of existing forward reachability methods by enhancing scalability and precision in verifying reach-avoid properties in neural feedback systems, which are increasingly used in robotics, autonomous driving, and aerospace autonomy.",155.01,Phi-4,Nvidia B200 (Cloud Native)
2601.08070v1_Semantic Gravity Wells Why Negative Constraints Ba.pdf,Semantic Gravity Wells: Why Negative Constraints Backfire,['Shailesh Rana'],"This paper investigates the failure of negative constraints in large language models, which often produce forbidden words despite explicit instructions not to. The study introduces 'semantic pressure' as a measure of the likelihood of generating a forbidden token and finds a logistic relationship between violation probability and pressure. Through layer-wise analysis, it identifies two distinct failure modes: priming failure, where mentioning the forbidden word activates it, and override failure, where late-layer networks override suppression signals. The findings highlight a fundamental tension in negative constraint design, where naming a forbidden word paradoxically increases its likelihood of being produced.",154.97,Phi-4,Nvidia B200 (Cloud Native)
2601.08079v1_MemoBrain Executive Memory as an Agentic Brain for.pdf,MemoBrain: Executive Memory as an Agentic Brain for Reasoning,"['Hongjin Qian', 'Zhao Cao', 'Zheng Liu']","The paper introduces MemoBrain, an executive memory model designed for tool-augmented agents to manage long-horizon reasoning tasks. It addresses the challenge of accumulating reasoning traces and tool artifacts that strain the working context of large language models. MemoBrain constructs a dependency-aware memory over reasoning steps, capturing intermediate states and their logical relations. It operates alongside the reasoning agent to organize progress without blocking execution, pruning invalid steps, folding completed sub-trajectories, and preserving a compact reasoning backbone. The model demonstrates consistent improvements over strong baselines on benchmarks like GAIA, Web-Walker, and BrowseComp-Plus.",154.83,Phi-4,Nvidia B200 (Cloud Native)
2601.08089v1_Q-realign Piggybacking Realignment on Quantization.pdf,Q-realign: Piggybacking Realignment on Quantization for Safe and Efficient LLM Deployment,"['Qitao Tan', 'Xiaoying Song', 'Ningxi Cheng', 'Ninghao Liu', 'Xiaoming Zhai', 'Lingzi Hong', 'Yanzhi Wang', 'Zhen Xiang', 'Geng Yuan']","The paper introduces Q-realign, a post-hoc defense method for ensuring safety in large language models (LLMs) after task-specific fine-tuning. It leverages post-training quantization to decouple safety alignment from fine-tuning, integrating seamlessly into deployment pipelines. The method reduces unsafe behaviors while maintaining task performance, significantly lowering memory usage and computational time. Experiments show that Q-realign can restore safety alignment in a fine-tuned 7B LLM on a single RTX 4090 within 40 minutes, offering a practical solution for safe LLM deployment.",153.86,Phi-4,Nvidia B200 (Cloud Native)
2601.08094v1_Local-Global Feature Fusion for Subject-Independen.pdf,Local-Global Feature Fusion for Subject-Independent EEG Emotion Recognition,"['Zheng Zhou', 'Isabella McEvoy', 'Camilo E. Valderrama']","This paper addresses the challenges of subject-independent EEG emotion recognition, which is complicated by inter-subject variability and the difficulty of learning robust representations from short, noisy recordings. The authors propose a fusion framework that integrates local, channel-wise descriptors and global, trial-level descriptors to improve cross-subject generalization. Local representations are formed by combining differential entropy with graph-theoretic features, while global representations summarize time-domain, spectral, and complexity characteristics. These representations are fused using a dual-branch transformer with attention-based fusion and domain-adversarial regularization. The method, tested on the SEED-VII dataset, achieves approximately 40% mean accuracy in 7-class subject-independent emotion recognition, outperforming single-view and classical baselines.",154.75,Phi-4,Nvidia B200 (Cloud Native)
2601.08104v1_High-Fidelity Modeling of Stochastic Chemical Dyna.pdf,HIGH-FIDELITY MODELING OF STOCHASTIC CHEMICAL DYNAMICS ON COMPLEX MANIFOLDS: A MULTI-SCALE SIREN-PINN FRAMEWORK FOR THE CURVATURE-PERTURBED GINZBURG-LANDAU EQUATION,"['Julian Evan Chrisnanto', 'Salsabila Rahma Alia', 'Nurfauzi Fadillah', 'Yulison Herry Chrisnanto']","The paper addresses the challenge of identifying and controlling spatiotemporal chaos in reaction-diffusion systems on complex catalytic surfaces. It introduces a Multi-Scale SIREN-PINN architecture using sinusoidal activations to overcome spectral bias in conventional PINNs, enabling accurate modeling of wave dynamics and defect cores. The framework is validated on the Ginzburg-Landau equation on Riemannian manifolds, achieving superior prediction accuracy and reconstructing hidden curvature fields from chaotic wave observations. The work proposes a new approach for Geometric Catalyst Design, providing a mesh-free tool for surface heterogeneity identification and passive control in turbulent reactors.",154.24,Phi-4,Nvidia B200 (Cloud Native)
2601.08107v1_STO-RL Offline RL under Sparse Rewards via LLM-Gui.pdf,STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order,"['Chengyang Gu', 'Yuxin Pan', 'Hui Xiong', 'Yize Chen']","STO-RL is an offline reinforcement learning framework designed to address challenges in long-horizon tasks with sparse rewards. It leverages large language models (LLMs) to generate temporally ordered subgoal sequences and state-to-subgoal-stage mappings. This temporal structure allows for potential-based reward shaping, transforming sparse terminal rewards into dense, temporally consistent signals. The framework enhances offline training efficiency and policy performance, outperforming existing offline goal-conditioned and hierarchical RL methods in various benchmarks. Ablation studies confirm its robustness to imperfect or noisy LLM-generated subgoal sequences, demonstrating a practical and scalable solution for long-horizon offline RL.",154.76,Phi-4,Nvidia B200 (Cloud Native)
2601.08108v1_Debiasing Large Language Models via Adaptive Causa.pdf,Debiasing Large Language Models via Adaptive Causal Prompting with Sketch-of-Thought,"['Bowen Li', 'Ziqi Xu', 'Jing Ren', 'Renqiang Luo', 'Xikun Zhang', 'Xiuzhen Zhang', 'Yongli Ren', 'Feng Xia']","This paper introduces the Adaptive Causal Prompting with Sketch-of-Thought (ACPS) framework to address the limitations of existing prompting methods for Large Language Models (LLMs), such as excessive token usage and limited generalisability. ACPS leverages structural causal models to adaptively select appropriate interventions, enabling generalisable causal reasoning across diverse tasks without task-specific retraining. By replacing verbose Chain-of-Thought with concise Sketch-of-Thought, ACPS significantly reduces token usage and inference cost. Extensive experiments demonstrate that ACPS outperforms existing prompting baselines in accuracy, robustness, and computational efficiency.",154.67,Phi-4,Nvidia B200 (Cloud Native)
2601.08109v1_CSQL Mapping Documents into Causal Databases.pdf,Csql: Mapping Documents into Causal Databases,['Sridhar Mahadevan'],"Csql is a novel system that automatically converts a collection of unstructured text documents into an SQL-queryable causal database (CDB). Unlike traditional databases or retrieval-augmented generation systems, Csql supports causal analysis over document collections, enabling queries about causal influences and causal hubs. It builds on the Democritus system, which derives local causal models from causal discourse. Csql can process large-scale datasets, such as the Testing Causal Claims (TCC) dataset, to enable corpus-level causal queries and longitudinal analyses. It induces its schema directly from language, functioning as a compiler from unstructured documents into a causally grounded relational database, applicable across various domains.",154.29,Phi-4,Nvidia B200 (Cloud Native)
2601.08118v1_MirrorBench An Extensible Framework to Evaluate Us.pdf,MIRRORBENCH: AN EXTENSIBLE FRAMEWORK TO EVALUATE USER-PROXY AGENTS FOR HUMAN-LIKENESS,"['Ashutosh Hathidara', 'Julien Yu', 'Vaishali Senthil', 'Sebastian Schreiber', 'Anil Babu Ankisettipalli']","MIRRORBENCH is a reproducible and extensible benchmarking framework designed to evaluate user proxy agents based on their ability to produce human-like user utterances across various conversational tasks. It decouples the evaluation from downstream task success and features a modular execution engine with support for pluggable user proxies, datasets, tasks, and metrics. The framework includes lexical-diversity metrics and LLM-judge-based metrics, providing variance-aware results that highlight gaps between user proxies and real human users. The open-source framework is accessible via a command-line interface for running experiments and generating reports.",154.56,Phi-4,Nvidia B200 (Cloud Native)
2601.08125v1_How vehicles change lanes after encountering crash.pdf,How vehicles change lanes after encountering crashes: Empirical analysis and modeling,"['Kequan Chen', 'Yuxuan Wang', 'Pan Liu', 'Victor L. Knoop', 'David Z. W. Wang', 'Yu Han']","This paper presents an empirical analysis and modeling of how vehicles change lanes after encountering crashes. The study aims to understand the behavioral patterns and decision-making processes of drivers in post-crash scenarios. By analyzing real-world data, the authors develop a model that predicts lane-changing behavior, which can be used to improve traffic management and safety measures in the aftermath of accidents.",154.47,Phi-4,Nvidia B200 (Cloud Native)
2601.08127v1_PathoGen Diffusion-Based Synthesis of Realistic Le.pdf,PathoGen: Diffusion-Based Synthesis of Realistic Lesions in Histopathology Images,"['Mohamad Koohi-Moghadam', 'Mohammad-Ali Nikouei Mahani', 'Kyongtae Tyler Bae']","The paper introduces PathoGen, a diffusion-based generative model designed to synthesize realistic lesions in histopathology images. This model addresses the challenge of limited expert-annotated lesion data, particularly for rare pathologies and underrepresented disease subtypes. PathoGen enables high-fidelity inpainting of lesions into benign histopathology images, preserving natural tissue boundaries, cellular structures, and authentic staining characteristics. The model is validated across four datasets (kidney, skin, breast, and prostate pathology) and demonstrates superior performance over existing generative baselines like conditional GAN and Stable Diffusion. PathoGen enhances downstream segmentation performance, especially in data-scarce scenarios, and provides a scalable solution for developing generalizable medical AI systems by overcoming the manual annotation bottleneck.",155.16,Phi-4,Nvidia B200 (Cloud Native)
2601.08128v1_Embedded AI Companion System on Edge Devices.pdf,Embedded AI Companion System on Edge Devices,"['Rahul Gupta', 'Stephen Hsu']","This paper addresses the challenge of developing an AI companion system for edge devices, which are limited by computational resources and latency concerns. The authors propose a memory paradigm that alternates between active and inactive phases to optimize performance. During active phases, the system uses lightweight retrieval for real-time dialog, while inactive phases allow for intensive memory extraction and maintenance. The proposed system, using a quantized model, outperforms a raw large language model (LLM) in most metrics and is comparable to GPT-3.5 with a 16k context window. The paper also introduces a benchmark for evaluating AI companions, focusing on conversational quality and memory capabilities. The system is designed to run offline on edge devices, enhancing privacy, reducing costs, and ensuring functionality in areas with limited internet connectivity.",154.33,Phi-4,Nvidia B200 (Cloud Native)
2601.08133v1_How Do Optical Flow and Textual Prompts Collaborat.pdf,How Do Optical Flow and Textual Prompts Collaborate to Assist in Audio-Visual Semantic Segmentation?,"['Peng Gao', 'Yujian Lee', 'Yongqi Xu', 'Wentao Fan']","This paper introduces a novel collaborative framework, Stepping Stone Plus (SSP), which integrates optical flow and textual prompts to enhance audio-visual semantic segmentation (AVSS). Unlike previous methodologies that decompose AVSS into discrete subtasks, SSP provides a prompted segmentation mask to facilitate semantic analysis. The framework leverages optical flow to capture motion dynamics, aiding in the segmentation of sound sources associated with moving objects. For stationary sound-emitting objects, SSP uses textual prompts to identify the object category and describe the scene. A visual-textual alignment module (VTA) is implemented to ensure coherent cross-modal integration. Experimental results show that SSP outperforms existing AVS methods, delivering efficient and precise segmentation results.",154.75,Phi-4,Nvidia B200 (Cloud Native)
2601.08139v1_Subspace Alignment for Vision-Language Model Test-.pdf,Subspace Alignment for Vision-Language Model Test-time Adaptation,"['Zhichen Zeng', 'Wenxuan Bao', 'Xiao Lin', 'Ruizhong Qiu', 'Tianxin Wei', 'Xuying Ning', 'Yuchen Yan', 'Chen Luo', 'Monica Xiao Cheng', 'Jingrui He', 'Hanghang Tong']","Vision-language models (VLMs) are vulnerable to distribution shifts, which can degrade their zero-shot prediction capabilities. Test-time adaptation (TTA) is a strategy to adapt VLMs to unlabeled test data on the fly. Existing TTA methods rely on zero-shot predictions as pseudo-labels, which can be unreliable under distribution shifts. This paper proposes SubTTA, a method that aligns the semantic subspaces of visual and textual modalities to enhance zero-shot predictions and guide the TTA process. SubTTA addresses the modality gap by aligning visual and textual subspaces and eliminates visual noise by projecting visual features onto task-specific textual subspaces. Extensive experiments demonstrate that SubTTA improves the performance of VLMs under distribution shifts, achieving an average improvement of 2.24% over state-of-the-art TTA methods.",153.94,Phi-4,Nvidia B200 (Cloud Native)
2601.08141v1_Qalb Largest State-of-the-Art Urdu Large Language .pdf,Qalb: Largest State-of-the-Art Urdu Large Language Model for 230M Speakers with Systematic Continued Pre-training,"['Muhammad Taimoor Hassan', 'Jawad Ahmed', 'Muhammad Awais']","This paper introduces Qalb, a state-of-the-art Urdu language model developed through a two-stage approach: continued pre-training followed by supervised fine-tuning. Starting from the LLaMA 3.1 8B model, the authors pre-trained on a dataset of 1.97 billion tokens, including diverse Urdu text and English Wikipedia data, to prevent catastrophic forgetting. The model was then fine-tuned on the Alif Urdu-instruct dataset. Qalb demonstrates substantial improvements over previous models, achieving a weighted average score of 90.34 on Urdu-specific benchmarks, outperforming the previous state-of-the-art Alif-1.0-Instruct model by 3.24 points and the base LLaMA-3.1 8B-Instruct model by 44.64 points. The results highlight the effectiveness of continued pre-training on high-quality language data combined with targeted instruction fine-tuning for adapting foundation models to low-resource languages like Urdu.",154.49,Phi-4,Nvidia B200 (Cloud Native)
2601.08146v2_Mechanisms are Transferable Data-Efficient Low-Res.pdf,Mechanisms are Transferable: Data-Efficient Low-Resource Adaptation via Circuit-Targeted Supervised Fine-Tuning,"['Khumaisa Nur’aini', 'Ayu Purwarianti', 'Alham Fikri Aji', 'Derry Wijaya']","This paper addresses the challenge of adapting large language models (LLMs) to low-resource languages, where labeled data is scarce and full-model fine-tuning is unstable. The authors propose Circuit-Targeted Supervised Fine-Tuning (CT-SFT), a method that identifies a sparse set of task-relevant attention heads in a proxy-language checkpoint and transfers learning to a target language by updating only those heads. This approach improves cross-lingual accuracy while reducing catastrophic forgetting and preserving source-language competence. The method leverages mechanistic interpretability to select which parameters should be updated, offering a data-efficient solution for low-resource adaptation.",154.23,Phi-4,Nvidia B200 (Cloud Native)
2601.08148v1_Enriching Semantic Profiles into Knowledge Graph f.pdf,Enriching Semantic Profiles into Knowledge Graph for Recommender Systems Using Large Language Models,"['Seokho Ahn', 'Sungbok Shin', 'Young-Duk Seo']","This paper addresses the challenge of constructing and utilizing rich user profiles in recommender systems. It proposes a new model, SPiKE, which integrates large language models (LLMs) and knowledge graphs (KGs) to enhance recommendation quality. SPiKE consists of three components: entity profile generation using LLMs, profile-aware KG aggregation, and pairwise profile preference matching. The model demonstrates superior performance over existing KG- and LLM-based recommenders in real-world settings.",154.73,Phi-4,Nvidia B200 (Cloud Native)
2601.08149v1_Dynamic Graph Structure Learning via Resistance Cu.pdf,Dynamic Graph Structure Learning via Resistance Curvature Flow,"['Chaoqun Fei', 'Huanjiang Liu', 'Tinglve Zhou', 'Yan Yang Li', 'Tianyong Hao']","This paper introduces a novel and computationally efficient curvature flow based on effective resistance from circuit theory, establishing a new paradigm for geometric graph structure evolution. It formulates the dynamic evolution equation of RCF, elucidates its mechanisms for manifold enhancement and noise suppression, and highlights its differentiability and compatibility with deep learning frameworks. The paper proposes an efficient Dynamic Graph Structure Learning method based on RCF. Extensive experiments on deep metric learning, manifold learning, and graph structure learning tasks demonstrate that DGSL-RCF consistently improves representation quality and downstream performance with low runtime cost.",155.3,Phi-4,Nvidia B200 (Cloud Native)
2601.08156v1_Project Synapse A Hierarchical Multi-Agent Framewo.pdf,Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions,"['Arin Gopalan Yadav', 'Varad Dherange', 'Kumar Shivam']","This paper introduces Project Synapse, a novel agentic framework designed for the autonomous resolution of last-mile delivery (LMD) disruptions. Synapse employs a hierarchical multi-agent architecture, where a central Resolution Supervisor agent performs strategic task decomposition and delegates sub-tasks to specialized worker agents. A core contribution is a novel Hybrid Memory Architecture that integrates short-term working memory, long-term episodic memory, and a semantic memory of organizational policies, enabling stateful, context-aware, and factually-grounded reasoning. The system is orchestrated using LangGraph to manage complex workflows. The framework's performance was validated using a benchmark dataset of 30 complex disruption scenarios and evaluated with an LLM-as-a-Judge protocol, showing promising results.",155.01,Phi-4,Nvidia B200 (Cloud Native)
2601.08160v1_SwiftMem Fast Agentic Memory via Query-aware Index.pdf,SwiftMem: Fast Agentic Memory via Query-aware Indexing,"['Anxin Tian', 'Yiming Li', 'Xing Li', 'Hui-Ling Zhen', 'Lei Chen', 'Xianzhi Yu', 'Zhenhua Dong', 'Mingxuan Yuan']","SwiftMem is a query-aware agentic memory system designed to overcome the latency bottlenecks of existing memory frameworks by implementing sub-linear retrieval through specialized indexing. It uses a temporal index for logarithmic-time range queries and a semantic DAG-Tag index for hierarchical topic mapping. Additionally, it introduces an embedding-tag co-consolidation mechanism to address memory fragmentation and improve cache locality. Experiments demonstrate that SwiftMem achieves significantly faster search speeds compared to state-of-the-art baselines while maintaining competitive accuracy, facilitating the practical deployment of memory-augmented LLM agents.",154.67,Phi-4,Nvidia B200 (Cloud Native)
2601.08166v1_ZeroDVFS Zero-Shot LLM-Guided Core and Frequency A.pdf,ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms,"['Mohammad Pivezhandi', 'Mahdi Banisharif', 'Abusayeed Saifullah', 'Ali Jannesari']","The paper introduces a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. It addresses the limitations of existing DVFS and task-to-core allocation methods by proposing a system that uses LLM-extracted semantic features to enable zero-shot deployment for new workloads. The framework achieves significant improvements in energy efficiency and makespan compared to traditional methods, with faster decision-making latency, making it suitable for dynamic embedded systems.",155.01,Phi-4,Nvidia B200 (Cloud Native)
2601.08173v1_The Agents First Day Benchmarking Learning Explora.pdf,"The Agent’s First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios","['Daocheng Fu', 'Jianbiao Mei', 'Rong Wu', 'Xuemeng Yang', 'Jia Xu', 'Ding Wang', 'Pinlong Cai', 'Yong Liu', 'Licheng Wen', 'Botian Shi']","This paper addresses the challenges faced by Multi-modal Large Language Models (MLLMs) in dynamic, real-world environments, focusing on robustness beyond static performance benchmarks. The authors introduce Trainee-Bench, a dynamic evaluation environment that assesses agents on context-aware scheduling, active exploration, and continuous learning. The study reveals significant deficiencies in current agents when dealing with dynamic environments, particularly in active exploration and continual learning. The framework aims to shift evaluation from static tests to realistic, production-oriented scenarios, providing a more comprehensive assessment of agent reliability.",153.9,Phi-4,Nvidia B200 (Cloud Native)
2601.08176v1_Prompt-Based Clarity Evaluation and Topic Detectio.pdf,Prompt-Based Clarity Evaluation and Topic Detection in Political Question Answering,"['Lavanya Prahallad', 'Sai Utkarsh Choudarypally', 'Pragna Prahallad', 'Pranathi Prahallad']","This paper explores the impact of prompt design on the automatic evaluation of clarity in responses generated by large language models (LLMs) in political question-answering contexts. Using the CLARITY dataset from the SemEval-2026 shared task, the study compares the performance of GPT-3.5 and GPT-5.2 under different prompting strategies: simple prompting, chain-of-thought prompting, and chain-of-thought with few-shot examples. The findings reveal that GPT-5.2 outperforms GPT-3.5 in clarity prediction, with significant improvements in accuracy under chain-of-thought with few-shot prompting. While chain-of-thought prompting enhances evasion accuracy, results are less stable across fine-grained categories. Additionally, reasoning-based prompting improves topic identification accuracy. The study concludes that prompt design can enhance high-level clarity evaluation, though challenges remain in fine-grained evasion and topic detection.",154.81,Phi-4,Nvidia B200 (Cloud Native)
2601.08179v1_Instruction-Driven 3D Facial Expression Generation.pdf,Instruction-Driven 3D Facial Expression Generation and Transition,"['Anh H. Vo', 'Tae-Seok Kim', 'Hulin Jin', 'Soo-Mi Choi', 'Yong-Guk Kim']","This study introduces a framework for instruction-driven 3D facial expression generation and transition. It presents the Instruction-driven Facial Expression Decomposer (IFED) module to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. The Instruction to Facial Expression Transition (I2FET) method is proposed, leveraging IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, generating facial expression sequences according to given instructions. The Facial Expression Transition model is introduced to generate smooth transitions between expressions. The framework outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets, demonstrating the ability to generate facial expression trajectories according to text instructions. The framework's potential for practical applications is highlighted.",154.75,Phi-4,Nvidia B200 (Cloud Native)
2601.08183v2_GI-Bench A Panoramic Benchmark Revealing the Knowl.pdf,GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards,"['Yan Zhu', 'Te Luo', 'Pei-Yao Fu', 'Zhen Zhang', 'Zi-Long Wang', 'Yi-Fan Qu', 'Zi-Han Geng', 'Jia-Qi Xu', 'Lu Yao', 'Li-Yun Ma', 'Wei Su', 'Wei-Feng Chen', 'Quan-Lin Li', 'Shuo Wang', 'Ping-Hong Zhou']","This paper introduces GI-Bench, a comprehensive benchmark designed to evaluate the performance of Multimodal Large Language Models (MLLMs) in the context of gastrointestinal endoscopy. The benchmark assesses MLLMs across a five-stage clinical workflow, including anatomical localization, lesion identification, diagnosis, findings description, and management. The study compares the performance of twelve MLLMs against human endoscopists, using metrics such as Macro-F1, mean Intersection-over-Union (mIoU), and a multi-dimensional Likert scale. The results highlight the state-of-the-art performance of the Gemini-3-Pro model in diagnostic reasoning, while also revealing a dissociation between the models' knowledge and experiential capabilities when compared to human practitioners.",154.96,Phi-4,Nvidia B200 (Cloud Native)
2601.08185v1_Autonomous Materials Exploration by Integrating Au.pdf,Autonomous Materials Exploration by Integrating Automated Phase Identification and AI-Assisted Human Reasoning,"['Ming-Chiang Chang', 'Maximilian Amsler', 'Duncan R. Sutherland', 'Sebastian Ament', 'Katie R. Gann', 'Lan Zhou', 'Louisa M. Smieska', 'Arthur R. Woll', 'John M. Gregoire', 'Carla P. Gomes', 'R. Bruce van Dover', 'Michael O. Thompson']","This paper presents an autonomous materials synthesis extension to SARA, the Scientific Autonomous Reasoning Agent, which integrates automated phase identification with AI-assisted human reasoning. The study demonstrates how human input can enhance the efficiency of autonomous experimentation in discovering new materials and understanding their synthesis and properties. The research utilizes robotic processing of thin-film samples of several oxide material systems, including Bi2O3, SnOx, and Bi–Ti–O, to explore processing domains and phase behavior. The findings highlight the potential of human-in-the-loop autonomous experimentation in stabilizing specific phases and inhibiting undesirable transformations in metastable materials.",154.5,Phi-4,Nvidia B200 (Cloud Native)
2601.08187v2_Improving LLM Reasoning with Homophily-aware Struc.pdf,IMPROVINGLLM REASONING WITH HOMOPHILY-AWARE STRUCTURAL AND SEMANTIC TEXT-ATTRIBUTED GRAPH COMPRESSION,"['Zijun Di', 'Bin Lu', 'Huquan Kang', 'Luoyi Fu', 'Jiaxin Ding', 'Xiaoying Gan', 'Lei Zhou', 'Xinbing Wang', 'Chenghu Zhou']","This paper introduces a novel framework, Homophily-aware Structural and Semantic Compression for LLMs (HS2C), aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAGs). The framework leverages the inherent structural and semantic information within graphs, focusing on graph homophily to improve LLM performance. Structurally, HS2C employs a global hierarchical partition guided by Structural Entropy minimization to identify cohesive, homophilic communities while reducing noise. Semantically, it enables LLMs to perform differentiated aggregation based on community types, compressing redundant contexts into concise, community-level consensus. Extensive experiments demonstrate that HS2C significantly enhances compression rates and inference accuracy across various benchmarks, showcasing its superiority and scalability. Notably, it achieves a 94.98% compression on the OGBN-ArXiv dataset while improving accuracy by 3.06%–4.92%. The framework's effectiveness is further validated across diverse graph-level benchmarks, confirming its task generalizability.",154.22,Phi-4,Nvidia B200 (Cloud Native)
2601.08189v2_ForgetMark Stealthy Fingerprint Embedding via Targ.pdf,FORGETMARK: STEALTHY FINGERPRINT EMBEDDING VIA TARGETED UNLEARNING IN LANGUAGE MODELS,"['Zhenhua Xu', 'Haobo Zhang', 'Zhebo Wang', 'Qichen Liu', 'Haitao Xu', 'Wenpeng Xing', 'Meng Han']","ForgetMark introduces a novel framework for embedding stealthy fingerprints in language models through targeted unlearning. It addresses the limitations of existing invasive fingerprints by avoiding high-perplexity triggers and fixed response patterns, which are susceptible to detection and filtering. The framework uses a compact, human-readable key-value set and trains lightweight LoRA adapters to suppress original values while preserving general capabilities. Ownership verification is achieved under black/gray-box access by aggregating likelihood and semantic evidence. ForgetMark demonstrates high ownership verification rates, maintains standard performance, and shows robustness to model merging and moderate incremental fine-tuning, outperforming backdoor baselines in stealthiness.",154.7,Phi-4,Nvidia B200 (Cloud Native)
2601.08196v1_Evaluating Implicit Regulatory Compliance in LLM T.pdf,Evaluating Implicit Regulatory Compliance in LLM Tool Invocation via Logic-Guided Synthesis,"['Da Song', 'Yuheng Huang', 'Boqi Chen', 'Tianshuo Cong', 'Randy Goebel', 'Lei Ma', 'Foutse Khomh']","This paper introduces LOGISAFETYGEN, a framework that converts unstructured regulations into Linear Temporal Logic oracles and uses logic-guided fuzzing to synthesize valid, safety-critical traces. It also presents LOGISAFETYBENCH, a benchmark with 240 human-verified tasks requiring LLMs to generate Python programs that meet both functional objectives and compliance rules. Evaluations show that while larger LLMs achieve better functional correctness, they often prioritize task completion over safety, leading to non-compliant behavior.",154.77,Phi-4,Nvidia B200 (Cloud Native)
2601.08211v1_Adapting Rules of Official International Mahjong f.pdf,Adapting Rules of Official International Mahjong for Online Players,"['Chucai Wang', 'Lingfeng Li', 'Yunlong Lu', 'Wenxin Li']","This paper addresses the need to adapt the rules of Official International Mahjong for online play. Unlike offline play, online players face fragmented playtime and varying opponents, necessitating rule modifications to ensure fairness in single-round games. The authors use a world champion AI to conduct self-play competitions and analyze statistical data, identifying issues such as first-mover advantage and subgoal scoring. They propose rule adaptations, including compensatory points for first-mover advantage and refined subgoal scores for different tile patterns. These changes aim to enhance the online gaming experience and are implemented in an online version of the game. This study represents an initial effort to use AI-generated data to evaluate and improve the balance of traditional games for online environments.",154.61,Phi-4,Nvidia B200 (Cloud Native)
2601.08223v2_DNF Dual-Layer Nested Fingerprinting for Large Lan.pdf,Dual-Layer Nested Fingerprinting for Large Language Model Intellectual Property Protection,"['Zhenhua Xu', 'Yiran Zhao', 'Mengting Zhong', 'Dezhang Kong', 'Changting Lin', 'Tong Qiao', 'Meng Han']","The paper introduces Dual-Layer Nested Fingerprinting (DNF), a novel method for protecting the intellectual property of large language models (LLMs) under black-box deployment. DNF embeds a hierarchical backdoor using domain-specific stylistic cues and implicit semantic triggers, achieving perfect fingerprint activation while maintaining downstream utility. It outperforms existing methods by using lower-perplexity triggers, remaining undetectable under fingerprint detection attacks, and showing robustness to incremental fine-tuning and model merging. This positions DNF as a practical, stealthy, and resilient solution for LLM ownership verification and IP protection.",153.78,Phi-4,Nvidia B200 (Cloud Native)
2601.08224v1_An Axiomatic Approach to General Intelligence SANC.pdf,An Axiomatic Approach to General Intelligence: Self-organizing Active Network of Concepts with EnergyE 3,"['Daesuk Kwon', 'Won-gi Paeng']","This paper introduces SANC(E3), an axiomatic framework for general intelligence where representational units emerge through competitive selection, reconstruction, and compression under finite activation capacity. It distinguishes between system tokens and sensory sources, proposing a unified process for perception, imagination, prediction, planning, and action. The framework emphasizes Gestalt completion under energy minimization, extending to embodied agents. Twelve propositions derived from core axioms demonstrate how various cognitive activities can be understood within this framework.",154.97,Phi-4,Nvidia B200 (Cloud Native)
2601.08226v1_Knowledge-based learning in Text-RAG and Image-RAG.pdf,Knowledge-based learning in Text-RAG and Image-RAG,"['Alexander Shim', 'Khalil Saieh', 'Samuel Clarke']","This research explores the use of multi-modal approaches in Vision Transformer (EVA-ViT) based image encoders and large language models (LLMs) like LlaMA or ChatGPT to address the hallucination problem in chest X-ray image analysis. The study compares text-based and image-based Retrieval-Augmented Generation (RAG) methods, utilizing NIH Chest X-ray images for training. Results indicate that text-based RAG effectively reduces hallucination by leveraging external knowledge, while image-based RAG enhances prediction confidence and calibration through KNN methods. The GPT LLM outperforms the Llama-based model with lower hallucination rates and better Expected Calibration Error (ECE). The research highlights challenges such as data imbalance and complex multi-stage structures, suggesting improvements through a large experience environment and balanced example usage.",154.66,Phi-4,Nvidia B200 (Cloud Native)
2601.08230v1_GADPN Graph Adaptive Denoising and Perturbation Ne.pdf,GADPN: Graph Adaptive Denoising and Perturbation Networks via Singular Value Decomposition,"['Hao Deng', 'Bo Liu']","This paper introduces GADPN, a framework for graph structure learning that refines graph topology through low-rank denoising and generalized structural perturbation. It leverages Bayesian optimization to adaptively determine denoising strength based on each graph's homophily level and extends structural perturbation to arbitrary graphs using Singular Value Decomposition (SVD). The approach demonstrates state-of-the-art performance with improved efficiency, particularly on challenging disassortative graphs, across various network types.",155.04,Phi-4,Nvidia B200 (Cloud Native)
2601.08235v2_MPCI-Bench A Benchmark for Multimodal Pairwise Con.pdf,MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity,"['Shouju Wang', 'Haopeng Zhang']","This paper introduces MPCI-Bench, the first benchmark for evaluating privacy behavior in multimodal contexts using the framework of Contextual Integrity. It addresses the limitations of existing text-centric benchmarks by incorporating multimodal inputs and evaluating both positive and negative scenarios. MPCI-Bench is designed to assess the balance between privacy and utility in language model agents, highlighting systematic failures and modality leakage issues where visual information is more frequently leaked than textual information.",154.71,Phi-4,Nvidia B200 (Cloud Native)
2601.08237v1_The End of Reward Engineering How LLMs Are Redefin.pdf,The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination,"['Haoran Su', 'Yandong Sun', 'Congjia Yu']","The paper discusses the limitations of traditional reward engineering in multi-agent reinforcement learning (MARL) and proposes a paradigm shift towards using large language models (LLMs) for defining reward functions. LLMs can generate human-level reward functions from language descriptions, adapt rewards dynamically, and facilitate agent coordination through semantic understanding. The paper highlights three key aspects of this transition: semantic reward specification, dynamic adaptation, and inherent human alignment, while also addressing challenges such as computational cost, hallucination risks, and scalability. The vision presented is for multi-agent systems where coordination emerges from shared semantic understanding rather than engineered numerical signals.",155.15,Phi-4,Nvidia B200 (Cloud Native)
2601.08251v1_Hyperbolic Heterogeneous Graph Transformer.pdf,Hyperbolic Heterogeneous Graph Transformer,"['Jongmin Park', 'Seunghoon Han', 'Hyewon Lee', 'Won-Yong Shin', 'Sungsu Lim']","This paper introduces the Hyperbolic Heterogeneous Graph Transformer (HypHGT), a novel approach to learning heterogeneous graph representations within the hyperbolic space. Unlike existing methods that rely on tangent-space operations and focus on local neighborhood information, HypHGT captures both local and global dependencies using a transformer-based architecture. It incorporates a relation-specific hyperbolic attention mechanism with linear time complexity, efficiently preserving heterogeneous information across different relation types. Comprehensive experiments demonstrate that HypHGT outperforms state-of-the-art methods in node classification tasks, with reduced training time and memory usage.",154.74,Phi-4,Nvidia B200 (Cloud Native)
2601.08254v1_Large Artificial Intelligence Model Guided Deep Re.pdf,Large Artificial Intelligence Model–Guided Deep Reinforcement Learning for Resource Allocation in Non-Terrestrial Networks,"['Abdikarim Mohamed Ibrahim', 'Rosdiadee Nordin']","This paper proposes a Deep Reinforcement Learning (DRL) agent guided by a Large Language Model (LLM) for resource allocation in Non-Terrestrial Networks (NTNs). The LLM acts as a high-level coordinator, providing textual guidance that shapes the reward function of the DRL agent during training. The proposed LAM-DRL approach outperforms traditional DRL methods by 40% in nominal weather scenarios and 64% in extreme weather scenarios compared to heuristics, in terms of throughput, fairness, and outage probability. The study addresses the dynamic complexities of NTNs, such as rapid changes in network topologies and heterogeneous user distributions, and highlights the limitations of traditional optimization and heuristic approaches in these environments.",154.78,Phi-4,Nvidia B200 (Cloud Native)
2601.08257v2_On Evaluation of Unsupervised Feature Selection fo.pdf,On Evaluation of Unsupervised Feature Selection for Pattern Classification,"['Gyu-Il Kim', 'Dae-Won Kim', 'Jaesung Lee']","This study critiques the traditional single-label evaluation paradigm for unsupervised feature selection (UFS) methods, highlighting its limitations in assessing the true discriminative ability of these methods. By adopting a multi-label classification framework, the authors demonstrate that performance rankings of UFS methods can differ significantly from those obtained under single-label settings. The study employs multi-label evaluation measures such as Hamming Loss, Ranking Loss, One-Error, and Multi-Label Accuracy to provide a fair and reliable comparison of UFS methods across 21 multi-label datasets.",155.71,Phi-4,Nvidia B200 (Cloud Native)
2601.08258v1_T3 Benchmarking Sycophancy and Skepticism in Causa.pdf,Benchmarking Sycophancy and Skepticism in Causal Judgment,['Edward Y. Chang'],"The paper introduces T3 (Testing Trustworthy Thinking), a diagnostic benchmark designed to evaluate Large Language Models (LLMs) on causal judgment across Pearl’s Ladder of Causality. It comprises 454 expert-curated vignettes focusing on high-resolution failure analysis, distinguishing between Utility (sensitivity) and Safety (specificity). The benchmark identifies two pathologies: a 'Skepticism Trap' at L1, where models reject valid causal links, and a 'Scaling Paradox' at L3, where larger models underperform on ambiguous counterfactuals. The paper also validates a process-verified protocol, RCA, demonstrating its effectiveness in restoring decisive causal judgment under structured verification.",154.77,Phi-4,Nvidia B200 (Cloud Native)
2601.08262v1_VGG Induced Deep Hand Sign Language Detection.pdf,VGG Induced Deep Hand Sign Language Detection,"['Subham Sharma', 'Sharmila Subudhi']","This paper proposes a novel hand gesture recognition system for differently-abled persons using a convolutional neural network, specifically the VGG-16 net. The model is trained on a widely used image dataset using Python and Keras libraries. The results are validated using the NUS dataset, which consists of 10 classes of hand gestures. A testing dataset is also created using Google's open source API to capture various human hand gestures. The study demonstrates that combining transfer learning with image data augmentation results in approximately 98% accuracy in gesture recognition.",155.45,Phi-4,Nvidia B200 (Cloud Native)
2601.08271v1_Sparsity Is Necessary Polynomial-Time Stability fo.pdf,Sparsity Is Necessary: Polynomial-Time Stability for Agentic LLMs in Large Action Spaces,['Angshul Majumdar'],"This paper introduces the concept of Sparse Agentic Control (SAC) in the context of large language models (LLMs) interacting with extensive tool sets. It addresses the challenge of decision-making in environments with a massive discrete action space, where only a small subset of actions is relevant for any given task. The paper formalizes this setting and studies ℓ1,2-regularized policy learning, establishing results that show estimation and value suboptimality scale with the square root of the logarithm of the action space size divided by the number of samples. It also demonstrates that dense policy classes require a large number of samples, explaining the instability of prompt-only controllers. The paper further explores the impact of partial observability and extends the analysis to various scenarios including tuning-free, online, robust, group-sparse, and interaction-aware SAC.",155.08,Phi-4,Nvidia B200 (Cloud Native)
2601.08273v1_HIPPO Accelerating Video Large Language Models Inf.pdf,HIPPO: Accelerating Video Large Language Models Inference via Holistic-aware Parallel Speculative Decoding,"['Qitan Lv', 'Tianyu Liu', 'Wen Wu', 'Xuenan Xu', 'Bowen Zhou', 'Feng Wu', 'Chao Zhang']","This paper introduces HIPPO, a framework designed to accelerate the inference of video Large Language Models (video-LLMs) without sacrificing output quality. Existing speculative decoding (SD) methods for video-LLMs focus on pruning redundant visual tokens, but they fall short in achieving the acceleration seen in text-only LLMs. HIPPO addresses these limitations by proposing a semantic-aware token preservation method and a video parallel SD algorithm. The semantic-aware method retains semantic information even at high pruning ratios by combining global attention scores with local visual semantics. The parallel SD algorithm decouples and overlaps the draft generation and target verification phases. Experiments demonstrate HIPPO's effectiveness, achieving up to a 3.51× speedup compared to traditional auto-regressive decoding across various benchmarks.",154.48,Phi-4,Nvidia B200 (Cloud Native)
2601.08276v1_ToolACE-MCP Generalizing History-Aware Routing fro.pdf,ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web,"['Zhiyuan Yao', 'Zishan Xu', 'Yifu Guo', 'Zhiguang Han', 'Cheng Yang', 'Shuo Zhang', 'Weinan Zhang', 'Xingshan Zeng', 'Weiwen Liu']","The paper introduces ToolACE-MCP, a pipeline designed to train history-aware routers for precise navigation in large-scale agent ecosystems. By utilizing a dependency-rich candidate graph to synthesize multi-turn trajectories, ToolACE-MCP trains routers with dynamic context understanding, resulting in a plug-and-play Light Routing Agent. Experiments on real-world benchmarks MCP-Universe and MCP-Mark demonstrate its superior performance, showcasing its ability to generalize to multi-agent collaboration with minimal adaptation, maintain robustness against noise, and scale effectively to massive candidate spaces. These findings support the future development of universal orchestration in open-ended ecosystems.",153.95,Phi-4,Nvidia B200 (Cloud Native)
2601.08280v1_Greedy Is Enough Sparse Action Discovery in Agenti.pdf,Greedy Is Enough: Sparse Action Discovery in Agentic LLMs,['Angshul Majumdar'],"This paper explores the concept of sparse action discovery in agentic large language models (LLMs) operating in environments with vast action spaces. It introduces a contextual linear reward model where action relevance is governed by a structured sparsity assumption, meaning only a small subset of actions significantly impacts performance. The study formulates action discovery as a block-sparse recovery problem and analyzes a greedy algorithm inspired by Orthogonal Matching Pursuit. The algorithm is shown to recover the relevant action set with high probability under certain assumptions, using a sample size that scales polynomially with sparsity and latent dimension, and logarithmically with the total number of actions. The paper also provides estimation error guarantees for refitted parameters and demonstrates that the resulting decision rule is near-optimal for new latent states. Additionally, it establishes information-theoretic lower bounds to show that sparsity and sufficient coverage are necessary for tractability. The findings highlight sparse action discovery as a fundamental principle in large-action decision-making and offer a theoretical foundation for action pruning in agentic systems.",154.52,Phi-4,Nvidia B200 (Cloud Native)
2601.08288v1_OpenMic A Multi-Agent-Based Stand-Up Comedy Genera.pdf,OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System,"['Yuyang Wu', 'Hanzhong Cao', 'Jianhao Chen', 'Yufei Li']","This paper introduces OpenMic, an end-to-end multi-agent system designed to generate Chinese stand-up comedy performances. It addresses the challenges of generating culturally grounded humor with precise timing and stage-performance cues. OpenMic uses a multi-round iterative loop to optimize humor, timing, and performability, leveraging retrieval-augmented generation for material grounding and a fine-tuned JokeWriter for stand-up-specific structures. The system transforms user-provided life topics into 3–5 minute performances, addressing the mismatch between existing humor datasets and the requirements of long-form stand-up comedy generation.",153.87,Phi-4,Nvidia B200 (Cloud Native)
2601.08297v1_Demystifying the Slash Pattern in Attention The Ro.pdf,Demystifying the Slash Pattern in Attention: The Role of RoPE,"['Yuan Cheng', 'Fengzhuo Zhang', 'Yunlong Hou', 'Cunxiao Du', 'Chao Du', 'Tianyu Pang', 'Aixin Sun', 'Zhuoran Yang']","This paper investigates the emergence of slash attention patterns in Large Language Models (LLMs), where attention scores concentrate along specific sub-diagonals. The study explores the role of Rotary Position Embedding (RoPE) in these patterns, revealing that queries and keys are nearly rank-one and that RoPE's medium- and high-frequency components interact to produce these patterns. The findings suggest that Slash-Dominant Heads (SDHs) are intrinsic to LLMs and generalize to out-of-distribution prompts. The paper combines empirical analysis with theoretical insights to explain the conditions under which SDHs emerge.",154.66,Phi-4,Nvidia B200 (Cloud Native)
2601.08302v1_Enhancing Sentiment Classification and Irony Detec.pdf,Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques,"['Marvin Schmitt', 'Anne Schwerk', 'Sebastian Lempert']","This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against a baseline. The research finds that advanced prompting significantly improves sentiment analysis, with few-shot prompting excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%. The study highlights the importance of tailoring prompting strategies to both the model and the task, aligning prompt design with the LLM’s architecture and the semantic complexity of the task.",153.45,Phi-4,Nvidia B200 (Cloud Native)
2601.08310v1_ORBIT On-policy Exploration-Exploitation for Contr.pdf,ORBIT: On-policy Exploration-Exploitation for Controllable Multi-Budget Reasoning,"['Kun Liang', 'Clive Bai', 'Xin Xu', 'Chenming Tang', 'Sanwoo Lee', 'Weijie Liu', 'Saiyong Yang', 'Yunfang Wu']","This paper introduces ORBIT, a controllable multi-budget reasoning framework designed to address the inefficiencies of applying uniform long-form reasoning at inference time in Large Reasoning Models (LRMs). ORBIT employs multi-stage reinforcement learning to discover Pareto-optimal reasoning behaviors for different effort levels and integrates these behaviors into a unified model using on-policy distillation. The framework allows for controllable reasoning behavior across multiple modes, maintains competitive reasoning density within each mode, and integrates these policies into a single model while preserving mode separation and high performance. This approach optimizes inference-time efficiency by dynamically adjusting reasoning effort based on task demands.",154.5,Phi-4,Nvidia B200 (Cloud Native)
2601.08311v1_Enhancing Image Quality Assessment Ability of LMMs.pdf,Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation,"['Kang Fu', 'Huiyu Duan', 'Zicheng Zhang', 'Yucheng Zhu', 'Jun Zhao', 'Xiongkuo Min', 'Jia Wang', 'Guangtao Zhai']","The paper introduces IQARAG, a novel, training-free framework that enhances the Image Quality Assessment (IQA) ability of Large Multimodal Models (LMMs) using Retrieval-Augmented Generation (RAG). IQARAG retrieves semantically similar but quality-variant reference images with Mean Opinion Scores (MOSs) for the input image, integrating them into a specific prompt to provide a visual perception anchor for the IQA task. The framework consists of three key phases: Retrieval Feature Extraction, Image Retrieval, and Integration & Quality Score Generation. Extensive experiments across multiple IQA datasets demonstrate that IQARAG effectively boosts the IQA performance of LMMs, offering a resource-efficient alternative to fine-tuning.",154.0,Phi-4,Nvidia B200 (Cloud Native)
2601.08323v1_AtomMem  Learnable Dynamic Agentic Memory with Ato.pdf,AtomMem: Learnable Dynamic Agentic Memory with Atomic Memory Operation,"['Yupeng Huo', 'Yaxi Lu', 'Zhong Zhang', 'Haotian Chen', 'Yankai Lin']","This paper introduces AtomMem, a novel memory framework for agents that reframes memory management as a dynamic decision-making problem. By breaking down high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, AtomMem transforms memory workflows into a learnable decision process. The framework combines supervised fine-tuning with reinforcement learning to develop an autonomous, task-aligned policy for memory management. Experimental results across three long-context benchmarks show that AtomMem-8B outperforms existing static-workflow memory methods. The learning-based approach allows agents to discover structured, task-aligned memory management strategies, offering a significant advantage over predefined routines.",153.87,Phi-4,Nvidia B200 (Cloud Native)
2601.08327v1_Safe Heterogeneous Multi-Agent RL with Communicati.pdf,Safe Heterogeneous Multi-Agent RL with Communication Regularization for Coordinated Target Acquisition,"['Gabriele Calzolari', 'Vidya Sumathy', 'Christoforos Kanellakis', 'George Nikolakopoulos']","This paper introduces a decentralized multi-agent reinforcement learning framework that enables structurally heterogeneous teams of agents to jointly discover and acquire randomly located targets in environments characterized by partial observability, communication constraints, and dynamic interactions. Each agent’s policy is trained with the Multi-Agent Proximal Policy Optimization algorithm and employs a Graph Attention Network encoder that integrates simulated range-sensing data with communication embeddings exchanged among neighboring agents, enabling context-aware decision-making from both local sensing and relational information. The work introduces a unified framework that integrates graph-based communication and trajectory-aware safety through safety filters, supported by a structured reward formulation designed to encourage effective target discovery and acquisition, collision avoidance, and de-correlation between the agents’ communication vectors by promoting informational orthogonality. The effectiveness of the proposed reward function is demonstrated through a comprehensive ablation study, and simulation results confirm safe and stable task execution, validating the framework’s effectiveness.",154.65,Phi-4,Nvidia B200 (Cloud Native)
2601.08332v1_IGAN A New Inception-based Model for Stable and Hi.pdf,IGAN: A New Inception-based Model for Stable and High-Fidelity Image Synthesis Using Generative Adversarial Networks,"['Ahmed A. Hashim', 'Ali Al-Shuwaili', 'Asraa Saeed', 'Ali Al-Bayaty']","This paper introduces the Inception Generative Adversarial Network (IGAN), a novel GAN model that incorporates deeper inception-inspired convolution and dilated convolution to address the challenges of mode collapse and unstable gradients in high-depth networks. The IGAN model achieves significant improvements in image quality and training stability, as evidenced by its performance on the CUB-200 and ImageNet datasets. It achieves a Fréchet Inception Distance (FID) of 13.12 and 15.08, respectively, and an Inception Score (IS) of 9.27 and 68.25. The model also employs dropout and spectral normalization to mitigate gradient explosion and overfitting, making it a scalable and efficient framework for high-fidelity image synthesis.",154.56,Phi-4,Nvidia B200 (Cloud Native)
2601.08333v1_Semantic Laundering in AI Agent Architectures Why .pdf,Semantic Laundering in AI Agent Architectures: Why Tool Boundaries Do Not Confer Epistemic Warrant,"['Oleg Romanchuk', 'Roman Bondar']","The paper discusses a fundamental issue in LLM-based agent architectures where tool boundaries are mistakenly treated as conferring epistemic warrant. This conflation leads to 'semantic laundering,' where propositions are accepted as admissible without proper justification. The authors formalize this issue, drawing parallels to the Gettier problem, and introduce the Theorem of Inevitable Self-Licensing, which suggests that circular epistemic justification cannot be eliminated under standard architectural assumptions. They propose the Warrant Erosion Principle as an explanation and argue that scaling or improving models cannot resolve this type-level problem.",154.99,Phi-4,Nvidia B200 (Cloud Native)
2601.08360v1_Scalable Sequential Recommendation under Latency a.pdf,Scalable Sequential Recommendation under Latency and Memory Constraints,"['Adithya Parthasarathy', 'Aswathnarayan Muthukrishnan Kirubakaran', 'Vinoth Punniyamoorthy', 'Nachiappan Chockalingam', 'Lokesh Butra', 'Kabilan Kannan', 'Abhirup Mazumder', 'Sumit Saha']","This paper introduces HoloMambaRec, a novel sequential recommendation architecture designed to handle long-range user behavior under strict memory and latency constraints. Unlike transformer-based approaches that suffer from quadratic attention complexity, HoloMambaRec combines holographic reduced representations with a selective state space encoder for efficient linear-time sequence processing. It uses circular convolution to bind item and attribute information, maintaining embedding dimensionality while encoding structured metadata. The architecture employs a shallow selective state space backbone inspired by Mamba-style models, enabling efficient training and constant-time recurrent inference. Experiments on the Amazon Beauty and MovieLens-1M datasets show that HoloMambaRec outperforms SASRec and competes with GRU4Rec under a limited 10-epoch training budget, while significantly reducing memory complexity. Additionally, it incorporates mechanisms for temporal bundling and inference-time compression, making it a practical and extensible solution for scalable, metadata-aware sequential recommendation.",154.6,Phi-4,Nvidia B200 (Cloud Native)
2601.08371v1_Geo-NVS-w Geometry-Aware Novel View Synthesis In-t.pdf,Geo-NVS-w: Geometry-Aware Novel View Synthesis In-the-Wild with an SDF Renderer,"['Anastasios Tsalakopoulos', 'Angelos Kanlis', 'Evangelos Chatzis', 'Antonis Karakottas', 'Dimitrios Zarpalas']","Geo-NVS-w is a geometry-aware framework for high-fidelity novel view synthesis from unstructured, in-the-wild image collections. It addresses the limitations of existing methods by leveraging a Signed Distance Function (SDF) to guide the rendering process, ensuring geometric coherence and fine structural detail preservation. The framework introduces a Geometry-Preservation Loss to maintain texture and feature consistency across viewpoints and demonstrates a significant reduction in energy consumption compared to similar methods. Geo-NVS-w achieves competitive rendering performance, producing photorealistic results with sharp, geometrically coherent details.",154.57,Phi-4,Nvidia B200 (Cloud Native)
2601.08379v1_Training-Free Distribution Adaptation for Diffusio.pdf,Training-Free Distribution Adaptation for Diffusion Models via Maximum Mean Discrepancy Guidance,"['Matina Mahdizadeh Sani', 'Nima Jamali', 'Mohammad Jalali', 'Farzan Farnia']","This paper introduces MMD Guidance, a training-free method to adapt pre-trained diffusion models to user-specific target distributions without retraining. By augmenting the reverse diffusion process with gradients of the Maximum Mean Discrepancy (MMD) between generated samples and a reference dataset, the method aligns model outputs with the target distribution. MMD is chosen for its reliable distributional estimates, low variance, and efficient differentiability. The framework extends to prompt-aware adaptation in conditional generation models and is computationally efficient in latent diffusion models (LDMs). Experiments demonstrate that MMD Guidance achieves distributional alignment while preserving sample fidelity.",154.59,Phi-4,Nvidia B200 (Cloud Native)
2601.08380v1_Thematic Working Group 5 -- Artificial Intelligenc.pdf,Thematic Working Group 5 - Artificial Intelligence (AI) literacy for teaching and learning: design and implementation,"['Mary Webb - King’s College London (TWG co-leader)', 'Matt Bower - Macquarie University (TWG co-leader)', 'Ana Amélia Carvalho - University of Coimbra', 'Fredrik Mørk Røkenes - University of Oslo', 'Jodie Torrington - Macquarie University', 'Jonathan D. Cohen - Georgia State University', 'Yousra Chtouki - Al Akhawayn University in Ifrane', 'Kathryn MacCallum - University of Canterbury, NZ', 'Tanya Linden - The University of Melbourne, Australia', 'Deirdre Butler - Dublin City University', 'Juliana E. Raffagheli - University of Padova', 'Henriikka Vartiainen - University of Eastern Finland', 'Martina Ronci - Université Paris Cité', 'Peter Tiernan - Dublin City University', 'David M. Smith - Purdue University Global', 'Chris Shelton - University of Chichester', 'Joyce Malyn-Smith - Education Development Center', 'Pierre Gorissen - HAN University of Applied Sciences']","The document discusses the efforts of Thematic Working Group 5 (TWG 5) in developing strategies to enhance AI literacy among teachers. It focuses on equipping educators with the necessary skills to integrate AI into teaching practices through curriculum design, professional development, classroom applications, and policy guidelines. The introduction highlights the impact of OpenAI's ChatGPT3 and generative AI on education, addressing both the opportunities and challenges, such as concerns over plagiarism, academic integrity, and the effects on creativity and critical thinking.",154.48,Phi-4,Nvidia B200 (Cloud Native)
2601.08382v2_A Qualitative Model to Reason about Object Rotatio.pdf,A Qualitative Model to Reason about Object Rotations – applied to solve the Cube,['Zoe Falomira'],"This paper introduces a Qualitative model for Reasoning about Object Rotations (QOR) applied to solve the Cube Comparison Test (CCT) by Ekstrom et al. (1976). The model includes a conceptual neighborhood graph (CN GRLO) that relates rotation movements to changes in location and orientation of cube sides, producing composition tables for inferring rotations. The paper highlights the importance of spatial reasoning skills across various disciplines and discusses the potential for training these skills. It also presents a new qualitative descriptor for object rotations, implemented to reason about changes in localization and orientation of object sides, and introduces an interactive version of the Cube Comparison Test.",154.73,Phi-4,Nvidia B200 (Cloud Native)
2601.08383v1_Deconstructing Pre-training Knowledge Attribution .pdf,Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models,"['Bo Wang', 'Junzhuo Li', 'Hong Chen', 'Yuanlin Chu', 'Yuxuan Fan', 'Xuming Hu']","This paper investigates how Mixture-of-Experts (MoE) architectures influence knowledge acquisition during pre-training compared to dense architectures. The authors introduce Gated-LPI, a neuron-level attribution metric, to analyze the dynamics of knowledge acquisition in both architectures. Through experiments, they identify three key patterns: a low-entropy backbone in MoE models, early consolidation of importance profiles, and functional robustness due to sparsity. These findings suggest that MoE architectures foster a stable and distributed computational backbone early in training, bridging the gap between sparse architectures and training-time interpretability.",152.41,Phi-4,Nvidia B200 (Cloud Native)
2601.08388v1_Creativity in AI as Emergence from Domain-Limited .pdf,Creativity in AI as Emergence from Domain-Limited Generative Models,['Corina Chutaux'],"This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models within bounded informational environments. It introduces a conceptual decomposition of creativity into four interacting components: pattern-based generation, induced world models, contextual grounding, and arbitrariness. The paper examines how these components manifest in multimodal generative systems, aiming to provide a technical framework for studying creativity as an emergent phenomenon in AI systems, rather than as a post hoc evaluative label.",158.34,Phi-4,Nvidia B200 (Cloud Native)
2601.08393v1_Controlled LLM Training on Spectral Sphere.pdf,Controlled LLM Training on Spectral Sphere,"['Tian Xie', 'Haoming Luo', 'Haoyu Tang', 'Yiwen Hu', 'Jason Klein', 'Liu Qingnan', 'Ren', 'Yang Wang', 'Wayne Xin Zhao', 'Rui Yan', 'Bing Su', 'Chong Luo', 'Baining Guo']","The paper introduces the Spectral Sphere Optimizer (SSO), a novel optimization strategy designed to ensure rapid convergence and stability in training large models. SSO enforces strict spectral constraints on both weights and their updates, aligning with the Maximal Update Parametrization (µP) framework. This approach addresses limitations in existing optimizers like Muon, which control updates but allow weight drift. Implemented as an efficient parallel algorithm within Megatron, SSO outperforms AdamW and Muon in extensive pretraining across various architectures, including Dense 1.7B, MoE 8B-A1B, and 200-layer DeepNet models. The paper highlights practical stability benefits such as improved MoE router load balancing, suppressed outliers, and bounded activations.",157.65,Phi-4,Nvidia B200 (Cloud Native)
2601.08401v1_An Explainable Two Stage Deep Learning Framework f.pdf,An Explainable Two-Stage Deep Learning Framework for Pericoronitis Assessment in Panoramic Radiographs Using YOLOv8 and ResNet-50,"['Ajo Babu George', 'Pranav S', 'Kunal Agarwal']","This paper presents a two-stage deep learning framework for assessing pericoronitis in panoramic radiographs. The first stage uses YOLOv8 to detect third molars and classify their anatomical positions and angulations. The second stage employs a modified ResNet-50 architecture to identify radiographic features indicative of pericoronitis. Grad-CAM is used to enhance interpretability by highlighting key diagnostic regions, aligning well with radiologists' impressions. The system demonstrates high precision and F1-scores, showing strong potential for AI-assisted panoramic assessment with explainable AI features.",158.25,Phi-4,Nvidia B200 (Cloud Native)
2601.08402v1_PATS Personality-Aware Teaching Strategies with La.pdf,PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors,"['Donya Rooein', 'Sankalan Pal Chowdhury', 'Mariia Eremeeva', 'Yuan Qin', 'Debora Nozza', 'Mrinmaya Sachan', 'Dirk Hovy']","This paper explores the potential of large language models (LLMs) as educational tutors, emphasizing the importance of aligning tutoring strategies with student personality traits. The authors construct a taxonomy linking pedagogical methods to personality profiles and simulate student-teacher conversations to adjust LLM tutoring strategies accordingly. The study finds that human teachers prefer this personalized approach over baseline methods, particularly for employing high-impact strategies like role-playing. The research highlights the need for more personalized and effective LLM applications in education.",157.85,Phi-4,Nvidia B200 (Cloud Native)
2601.08403v1_Owen-Shapley Policy Optimization OSPO A Principled.pdf,Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs,"['Abhijnan Nath', 'Alireza Bagheri Garakani', 'Tianchen Zhou', 'Fan Yang', 'Nikhil Krishnaswamy']","This paper introduces the Owen-Shapley Policy Optimization (OSPO) framework, which addresses the credit assignment gap in reinforcement learning for large language models (LLMs) used in recommendation tasks. OSPO redistributes sequence-level advantages based on tokens' marginal contributions to outcomes, using Shapley-Owen attributions for potential-based reward shaping. This method assigns segment-level credit without requiring additional computation from parametric value models, enhancing interpretability and robustness. Experiments on Amazon ESCI and H&M Fashion datasets demonstrate consistent performance improvements over baselines, with notable robustness to out-of-distribution retrievers.",157.57,Phi-4,Nvidia B200 (Cloud Native)
2601.08406v1_WebTrap Park An Automated Platform for Systematic .pdf,WebTrap Park: An Automated Platform for Systematic Security Evaluation of Web Agents,"['Xinyi Wu', 'Jiagui Chen', 'Geng Hong', 'Jiayi Dong', 'Xudong Pan', 'Jiarun Dai', 'Min Yang']","WebTrap Park is an automated platform designed for the systematic security evaluation of Web Agents. It addresses the fragmented and difficult-to-standardize nature of current security evaluations by directly observing the interactions of Web Agents with live web pages. The platform instantiates three major sources of security risk into 1,226 executable evaluation tasks, enabling action-based assessments without requiring modifications to the agents. The results highlight significant security differences across agent frameworks, emphasizing the importance of agent architecture. WebTrap Park is publicly accessible and provides a scalable foundation for reproducible security evaluations of Web Agents.",157.86,Phi-4,Nvidia B200 (Cloud Native)
2601.08412v1_Hybrid Distillation with CoT Guidance for Edge-Dro.pdf,Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation,"['Yizhan Feng', 'Hichem Snoussi', 'Yuhang Wang', 'Jing Teng', 'Abel Cherouat', 'Tian Wang']","This paper addresses the challenge of applying large language models to the onboard control of resource-constrained Unmanned Aerial Vehicles (UAVs). It proposes an integrated approach combining knowledge distillation, chain-of-thought guidance, and supervised fine-tuning to efficiently transfer complex reasoning and code generation capabilities to smaller models. A high-quality dataset covering various UAV SDKs is constructed, featuring instruction-code-reasoning chains and counterfactual negative samples. The approach uses DeepSeek-Coder-V2-Lite as the teacher model and employs a hybrid distillation strategy to generate high-quality chain-of-thought soft labels. The distilled lightweight model (parameters ≤ 1B) maintains high code generation accuracy while improving deployment and inference efficiency, demonstrating the feasibility of precise and lightweight intelligent control for UAVs.",157.93,Phi-4,Nvidia B200 (Cloud Native)
2601.08415v2_Regulatory gray areas of LLM Terms.pdf,Regulatory gray areas of LLM Terms,"['Brittany I. Davidson', 'Kate Muir', 'Florian A.D. Burnat', 'Adam N. Joinson']","This paper presents a comparative analysis of the Terms of Service of five major Large Language Model (LLM) providers (Anthropic, DeepSeek, Google, OpenAI, and xAI) as of November 2025. The analysis highlights significant variations in usage restrictions for general users and researchers, identifying specific complexities for those in security research, computational social sciences, and psychological studies. The paper discusses 'regulatory gray areas' that create uncertainty for legitimate use and contributes a publicly available resource comparing terms across platforms. The implications for both general users and researchers navigating this evolving landscape are also discussed.",154.88,Phi-4,Nvidia B200 (Cloud Native)
2601.08418v1_Taxon Hierarchical Tax Code Prediction with Semant.pdf,Taxon: Hierarchical Tax Code Prediction with Semantically Aligned LLM Expert Guidance,"['Jihang Li', 'Qing Liu', 'Zulong Chen', 'Jing Wang', 'Wei Wang', 'Chuanfei Xu', 'Zeyi Wen']","This paper introduces Taxon, a framework designed for hierarchical tax code prediction in e-commerce platforms. Taxon leverages a feature-gating mixture-of-experts architecture and a semantic consistency model derived from large language models to ensure accurate mapping of products to taxonomic nodes. It addresses noisy supervision by integrating multiple data sources, including tax databases and invoice logs. The framework has been deployed in Alibaba's tax service system, handling millions of queries daily with improved accuracy and robustness.",153.95,Phi-4,Nvidia B200 (Cloud Native)
2601.08430v1_RubricHub A Comprehensive and Highly Discriminativ.pdf,RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation,"['Sunzhu Li', 'Jiale Zhao', 'Miteto Wei', 'Huimin Ren', 'Yang Zhou', 'Jingwen Yang', 'Shunyu Liu', 'Kaike Zhang', 'Wei Chen']","This paper introduces RubricHub, a large-scale dataset designed to enhance rubric-based evaluation in open-ended generation tasks. The authors propose an automated Coarse-to-Fine Rubric Generation framework to overcome the limitations of existing methods, which suffer from scalability and coarse criteria. RubricHub, with approximately 110,000 entries across multiple domains, facilitates more nuanced and discriminative evaluation. The dataset's utility is demonstrated through a two-stage post-training pipeline, resulting in significant performance improvements in tasks like HealthBench, where the post-trained Qwen3-14B model achieves state-of-the-art results. The paper highlights the importance of reliable evaluation in real-world applications of Large Language Models, particularly in verifiable domains.",153.56,Phi-4,Nvidia B200 (Cloud Native)
2601.08434v3_Large Multimodal Models for Embodied Intelligent D.pdf,Large Multimodal Models for Embodied Intelligent Driving: The Next Frontier in Self-Driving?,"['Long Zhang', 'Yuchen Xia', 'Bingqing Wei', 'Zhen Liu', 'Shiwen Mao', 'Zhu Han', 'Mohsen Guizani']","This paper explores the integration of Large Multimodal Models (LMMs) with embodied artificial intelligence to enhance autonomous driving capabilities. It addresses the limitations of traditional modular designs in handling complex, open-world scenarios by proposing a novel semantics and policy dual-driven hybrid decision framework. This framework combines LMMs for semantic understanding and cognitive representation with deep reinforcement learning for real-time policy optimization. The paper discusses foundational principles, emerging opportunities, and conducts a case study to demonstrate the framework's effectiveness in lane-change planning tasks. It also identifies future research directions to further empower embodied intelligent driving.",153.94,Phi-4,Nvidia B200 (Cloud Native)
2601.08441v1_YaPO Learnable Sparse Activation Steering Vectors .pdf,YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation,"['Abdelaziz Bounhar', 'Rania Hossam Elmohamady Elbadry', 'Hadi Abdine', 'Preslav Nakov', 'Michalis Vazirgiannis', 'Guokan Shang']","This paper introduces Y aPO, a reference-free method for learning sparse steering vectors in the latent space of a Sparse Autoencoder (SAE). Unlike dense steering vectors, which often entangle multiple latent factors, Y aPO produces disentangled, interpretable, and efficient steering directions. The method optimizes sparse codes to achieve better convergence, performance, and training stability compared to dense steering baselines. Y aPO is effective in fine-grained settings such as cultural alignment and generalizes to various alignment-related behaviors without degrading general knowledge. The results demonstrate Y aPO's potential for efficient, stable, and fine-grained alignment of Large Language Models, with applications in controllability and domain adaptation.",153.76,Phi-4,Nvidia B200 (Cloud Native)
2601.08444v1_Beyond Linearization Attributed Table Graphs for T.pdf,Beyond Linearization: Attributed Table Graphs for Table Reasoning,"['Yuxiang Wang', 'Junhao Gan', 'Shengxiang Gao', 'Shenghao Ye', 'Zhengyi Yang', 'Jianzhong Qi']","This paper addresses the limitations of current table reasoning methods that use Large Language Models (LLMs) by proposing a new model called Table Graph Reasoner (TABGR). TABGR represents tables as Attributed Table Graphs (ATGs) to preserve their structural information and enable graph-based reasoning for better explainability. Additionally, the paper introduces a Question-Guided Personalized PageRank (QG-PPR) mechanism to address the 'lost-in-the-middle' issue. Extensive experiments demonstrate that TABGR outperforms state-of-the-art models by up to 9.7% in accuracy on commonly used benchmarks.",152.77,Phi-4,Nvidia B200 (Cloud Native)
2601.08448v1_Divide and Conquer Static-Dynamic Collaboration fo.pdf,Divide and Conquer: Static-Dynamic Collaboration for Few-Shot Class-Incremental Learning,"['Kexin Bao', 'Daichi Zhang', 'Yong Li', 'Dan Zeng', 'Shiming Ge']","This paper addresses the stability-plasticity dilemma in Few-shot Class-Incremental Learning (FSCIL) by proposing a Static-Dynamic Collaboration (SDC) framework. The framework divides the task into two stages: Static Retaining Stage (SRS) and Dynamic Learning Stage (DLS). SRS involves training an initial model with ample data and preserving key parts as static memory to retain old knowledge. DLS introduces a dynamic projector trained alongside the static memory to adapt to new classes. The method aims to balance the retention of old knowledge with the acquisition of new knowledge, achieving state-of-the-art performance on various benchmarks.",154.11,Phi-4,Nvidia B200 (Cloud Native)
2601.08450v1_Decoding Order Matters in Autoregressive Speech Sy.pdf,Decoding Order Matters in Autoregressive Speech Synthesis,"['Minghui Zhao', 'Anton Ragni']","This paper investigates the impact of decoding order in autoregressive speech synthesis. It explores the use of a masked diffusion framework that allows for arbitrary decoding orders during training and inference. The study finds that randomness in decoding order affects speech quality and that fixed-order decoding, such as left-to-right, is suboptimal compared to adaptive decoding methods. The research also demonstrates that even with 1-bit quantisation of acoustic representations, high-quality speech can be achieved. The findings suggest that exploring alternative decoding orders can improve synthesis quality by better capturing dependencies in speech.",152.23,Phi-4,Nvidia B200 (Cloud Native)
2601.08457v1_An Under-Explored Application for Explainable Mult.pdf,An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English,"['Sargam Yadava', 'Abhishek Kaushik', 'Kevin McDaid']","This paper introduces a multi-modal and explainable web application designed to detect misogyny in text and memes in code-mixed Hindi and English. The application leverages transformer-based models like XLM-RoBERTa and mBERT for text-based misogyny detection, and combines mBERT with EfficientNet and ResNET for multimodal misogyny detection from memes. It provides feature importance scores using SHAP and LIME for enhanced interpretability. The system aims to assist researchers and content moderators in combating gender-based digital violence and ensuring a safe digital space. The application's usability was evaluated using human evaluators through the Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ).",154.11,Phi-4,Nvidia B200 (Cloud Native)
2601.08462v1_M3-BENCH Process-Aware Evaluation of LLM Agents So.pdf,M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games,"['Sixiong Xie', 'Zhuofan Shi', 'Haiyang Shen', 'Gang Huang', 'Yun Ma', 'Xiang Jing']","The paper introduces M3-BENCH, a multi-stage benchmark for evaluating the social behaviors of large language model (LLM) agents in mixed-motive games. It addresses the limitations of existing benchmarks by incorporating a process-aware evaluation framework that analyzes behavioral trajectories, reasoning processes, and communication content. The framework integrates the Big Five personality model and Social Exchange Theory to provide a comprehensive assessment of agents' social behavior beyond mere task scores or outcomes. Experimental results demonstrate M3-BENCH's ability to distinguish diverse social behavior competencies across models, revealing inconsistencies in reasoning and communication despite reasonable behavioral outcomes.",154.46,Phi-4,Nvidia B200 (Cloud Native)
2601.08464v1_CoMa Contextual Massing Generation with Vision-Lan.pdf,CoMa: Contextual Massing Generation with Vision-Language Models,"['Evgenii Maslov', 'Valentin Khrulkov', 'Anastasia Volkova', 'Anton Gusarov', 'Andrey Kuznetsov', 'Ivan Oseledets']","The paper introduces an automated framework for generating building massing based on functional requirements and site context, addressing the complexity and manual effort involved in the conceptual design phase of architecture and urban planning. It presents the CoMa-20K dataset, which includes detailed massing geometries, economic and programmatic data, and visual site context. The dataset is used to benchmark massing generation as a conditional task for Vision-Language Models (VLMs), evaluating both fine-tuned and large zero-shot models. The experiments demonstrate the potential of VLMs to produce context-sensitive massing options, establishing a foundational benchmark and highlighting opportunities for future research in data-driven architectural design.",154.75,Phi-4,Nvidia B200 (Cloud Native)
2601.08468v1_JudgeRLVR Judge First Generate Second for Efficien.pdf,"Judge First, Generate Second for Efficient Reasoning","['Jiangshan Duo', 'Hanyu Li', 'Hailin Zhang', 'Yudong Wang', 'Sujian Li', 'Liang Zhao']","The paper introduces JudgeRLVR, a two-stage paradigm for reinforcement learning with verifiable rewards (RLVR) aimed at improving the efficiency and quality of reasoning in large language models. The first stage involves training the model to judge solution responses with verifiable answers, while the second stage fine-tunes the model for generation. This approach addresses the trade-off between efficiency and verification by enhancing the model's discriminative capability, leading to significant improvements in accuracy and reduction in generation length, both in-domain and out-of-domain.",154.72,Phi-4,Nvidia B200 (Cloud Native)
2601.08472v1_sui-1 Grounded and Verifiable Long-Form Summarizat.pdf,sui-1: Grounded and Verifiable Long-Form Summarization,"['Benedikt Droste', 'Jan Philipp Harries', 'Maximilian Idahl', 'Björn Plüster']","The paper introduces sui-1, a 24B parameter model designed for citation-grounded summarization, which generates abstractive summaries with inline citations. This allows users to verify claims against source text, addressing a critical limitation in compliance-sensitive domains. The model can process documents up to 100K tokens in a single pass and supports iterative processing for larger texts. It uses a synthetic data pipeline to generate high-quality training examples across multiple languages. Evaluation shows sui-1 significantly outperforms open-weight baselines, demonstrating the effectiveness of task-specific training over scale alone.",154.9,Phi-4,Nvidia B200 (Cloud Native)
2601.08475v1_SUMMPILOT Bridging Efficiency and Customization fo.pdf,SUMMPILOT: Bridging Efficiency and Customization for Interactive Summarization System,"['JungMin Yun', 'Juhwan Choi', 'Kyohoon Jin', 'Soojin Jang', 'Jinhee Jang', 'YoungBin Kim']","This paper introduces SUMMPILOT, an interactive summarization system that combines the efficiency of automatic summarization with the ability to generate personalized summaries tailored to individual users' interests and requirements. SUMMPILOT leverages a large language model to facilitate both automatic and interactive summarization, allowing users to engage with the system through semantic graphs, entity clustering, and explainable evaluation. The system supports multi-document summarization by capturing relationships between key pieces of information, offering two modes: BASICMODE for automatic summarization and ADVANCED MODE for customizable summarization. User studies demonstrate SUMMPILOT's adaptability and usefulness.",154.62,Phi-4,Nvidia B200 (Cloud Native)
2601.08490v1_BenchOverflow Measuring Overflow in Large Language.pdf,BenchOverflow: Measuring Overflow in Large Language Models via Plain-Text Prompts,"['Erin Feiglin', 'Nir Hutnik', 'Raz Lapid']","The paper investigates a failure mode in large language models (LLMs) known as 'Overflow,' where plain-text prompts lead to excessive outputs. This phenomenon increases serving costs, latency, and performance degradation, especially at scale. The authors introduce 'BenchOverflow,' a benchmark for evaluating nine plain-text prompting strategies that amplify output volume. The study assesses nine models, revealing significant variations in output length distributions. A simple mitigation strategy, a fixed conciseness reminder, effectively reduces excessive outputs. The findings highlight the importance of length control for reliability, cost, and sustainability in LLM deployments.",154.51,Phi-4,Nvidia B200 (Cloud Native)
2601.08493v1_PKI Prior Knowledge-Infused Neural Network for Few.pdf,PKI: Prior Knowledge-Infused Neural Network for Few-Shot Class-Incremental Learning,"['Kexin Bao', 'Fanzhao Lin', 'Zichen Wang', 'Yong Li', 'Dan Zeng', 'Shiming Ge']","The paper addresses the challenges of catastrophic forgetting and overfitting in few-shot class-incremental learning (FSCIL). It proposes a Prior Knowledge-Infused Neural Network (PKI) that retains more prior knowledge by freezing parts of the network and finetuning others with an extra memory. PKI includes a backbone, an ensemble of projectors, a classifier, and an extra memory. In each incremental session, a new projector is built and added to the ensemble, which is then finetuned to facilitate FSCIL.",154.73,Phi-4,Nvidia B200 (Cloud Native)
2601.08499v2_EfficientFSL Enhancing Few-Shot Classification via.pdf,EfficientFSL: Enhancing Few-Shot Classification via Query-Only Tuning in Vision Transformers,"['Wenwen Liao', 'Hang Ruan', 'Jianbo Yu', 'Bing Song', 'Yuansong Wang', 'Xiaofeng Yang']","EfficientFSL is a novel framework designed to enhance few-shot classification using Vision Transformers (ViTs) by employing a query-only fine-tuning approach. This method significantly reduces computational overhead while maintaining competitive performance. The framework introduces a lightweight Forward Block to generate task-specific queries, a Combine Block to fuse multi-layer outputs, and a Support-Query Attention Block to adjust prototypes for better alignment with query set distributions. EfficientFSL demonstrates state-of-the-art performance on both in-domain and cross-domain few-shot datasets, showcasing its practicality in real-world applications with minimal trainable parameters.",153.95,Phi-4,Nvidia B200 (Cloud Native)
2601.08503v1_Temporal Fusion Nexus A task-agnostic multi-modal .pdf,Temporal Fusion Nexus: A task-agnostic multi-modal embedding model for clinical narratives and irregular time series in post-kidney transplant care,"['Aditya Kumar', 'Simon Rauch', 'Mario Cypko', 'Marcel Naik', 'Matthieu-P Schapranow', 'Aadil Rashid', 'Fabian Halleck', 'Bilgin Osmanodja', 'Roland Roller', 'Lars Pape', 'Klemens Budde', 'Mario Schiffer', 'Oliver Amft']","The paper introduces Temporal Fusion Nexus (TFN), a multi-modal and task-agnostic embedding model designed to integrate irregular time series and unstructured clinical narratives. The model was evaluated in the context of post-kidney transplant care using a retrospective cohort of 3382 patients, focusing on three key outcomes: graft loss, graft rejection, and mortality. TFN demonstrated superior performance compared to state-of-the-art models, achieving higher AUC scores for graft loss and graft rejection, and a notable AUC for mortality prediction. The integration of clinical text significantly improved performance across all tasks. The model's robustness and interpretability were validated through disentanglement metrics and SHAP-based attributions, aligning with clinical reasoning. TFN shows promise for broader clinical applications beyond kidney transplant care.",154.73,Phi-4,Nvidia B200 (Cloud Native)
2601.08509v1_What If TSF A Benchmark for Reframing Forecasting .pdf,What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal Forecasting,"['Jinkwan Jang', 'Hyunbin Jin', 'Hyungjin Park', 'Kyubyung Chae', 'Taesup Kim']","The paper introduces What If TSF (WIT), a new benchmark for multimodal forecasting that evaluates models' ability to condition forecasts on contextual text, particularly future scenarios. It addresses the limitations of existing unimodal forecasting methods that rely on historical patterns and highlights the potential of large language models (LLMs) to incorporate external contexts. WIT provides expert-crafted scenarios to rigorously test scenario-guided multimodal forecasting, aiming to improve the integration of textual inputs in forecasting models.",155.96,Phi-4,Nvidia B200 (Cloud Native)
2601.08510v2_STAGE A Benchmark for Knowledge Graph Construction.pdf,"A Benchmark for Knowledge Graph Construction, Question Answering, and In-Script Role-Playing over Movie Screenplays","['Qiuyu Tian', 'Yiding Li', 'Fengyi Chen', 'Zequn Liu', 'Youyong Kong', 'Fan Guo', 'Yuyao Li', 'Jinjing Shen', 'Zhijing Xie', 'Yiyun Luo', 'Xin Zhang']","The paper introduces STAGE, a unified benchmark for narrative understanding over full-length movie screenplays. It defines four tasks: knowledge graph construction, scene-level event summarization, long-context screenplay question answering, and in-script character role-playing, all grounded in a shared narrative world representation. The benchmark provides cleaned scripts, curated knowledge graphs, and event- and character-centric data.",158.08,Phi-4,Nvidia B200 (Cloud Native)
2601.08519v1_CD2 Constrained Dataset Distillation for Few-Shot .pdf,CD2: Constrained Dataset Distillation for Few-Shot Class-Incremental Learning,"['Kexin Bao', 'Daichi Zhang', 'Hansong Zhang', 'Yong Li', 'Yutao Yue', 'Shiming Ge']","This paper addresses the challenge of few-shot class-incremental learning (FSCIL), where models must learn new concepts with limited data while retaining previously acquired knowledge. The authors propose a framework called Constrained Dataset Distillation (CD2) to mitigate catastrophic forgetting. CD2 includes a dataset distillation module (DDM) and a distillation constraint module (DCM). The DDM synthesizes condensed samples to help the model learn essential class-related clues, while the DCM uses a designed loss to preserve the distribution of previously learned classes. Extensive experiments demonstrate the effectiveness of CD2 compared to other state-of-the-art methods.",157.54,Phi-4,Nvidia B200 (Cloud Native)
2601.08531v1_Sketch-Based Facade Renovation With Generative AI .pdf,Sketch-Based Facade Renovation With Generative AI Models: A Streamlined Framework for Bypassing As-Built Modelling in Industrial Adaptive Reuse,"['Warissara Booranamaitree', 'Xusheng Du', 'Yushu Cai', 'Zhengyang Wang', 'Ye Zhang', 'Haoran Xie']","This paper proposes a three-stage framework that leverages generative AI and vision-language models to streamline facade renovation processes. The framework bypasses the need for detailed as-built modeling by directly processing rough structural sketches and textual descriptions to generate renovation proposals. It uses a fine-tuned vision-language model to predict modification areas, a stable diffusion model to generate detailed sketches of new elements, and ControlNet for refining the results into photorealistic images. The approach is validated through experiments on datasets and real industrial buildings, demonstrating its ability to preserve original structures while enhancing facade detail quality. This enables architects to rapidly explore design alternatives and communicate renovation intentions more effectively.",156.28,Phi-4,Nvidia B200 (Cloud Native)
2601.08545v2_Learner-Tailored Program Repair A Solution Generat.pdf,Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement,"['Zhenlong Dai', 'Zhuoluo Zhao', 'Hengning Wang', 'Xiu Tang', 'Sai Wu', 'Chang Yao', 'Zhipeng Gao', 'Jingyuan Chen']","This paper introduces a novel task, Learner-Tailored Program Repair (LPR), and proposes a framework called LSGEN (Learner-Tailored Solution Generator) to enhance program repair by providing bug descriptions for buggy code. The framework employs a two-stage approach: first, it uses a repair solution retrieval framework to construct a solution retrieval database and an edit-driven code retrieval approach to guide large language models in identifying and fixing bugs. In the second stage, it uses a solution-guided program repair method to fix code and provide explanations. Additionally, an Iterative Retrieval Enhancement method is proposed to optimize retrieval direction and explore suitable repair strategies, improving performance in programming coaching scenarios. Experimental results demonstrate that this approach significantly outperforms existing baselines.",157.03,Phi-4,Nvidia B200 (Cloud Native)
2601.08549v1_Contrastive and Multi-Task Learning on Noisy Brain.pdf,Contrastive and Multi-Task Learning on Noisy Brain Signals with Nonlinear Dynamical Signatures,"['Sucheta Ghosh', 'Zahra Monfared', 'Felix Dietrich']","This paper introduces a two-stage multitask learning framework for analyzing noisy EEG signals. The first stage involves a denoising autoencoder to suppress artifacts and stabilize temporal dynamics, providing robust signal representations. The second stage employs a multitask architecture to achieve motor imagery classification, chaotic versus non-chaotic regime discrimination using Lyapunov exponent-based labels, and self-supervised contrastive representation learning with NT-Xent loss. A convolutional-Transformer backbone captures spatial-temporal structures, while the dynamical task enhances sensitivity to nonlinear brain dynamics. This design improves stability across datasets and supports reproducible training by separating noise reduction from higher-level feature learning. Empirical studies demonstrate that the framework enhances robustness and generalization, outperforming strong baselines and recent state-of-the-art methods in EEG decoding.",157.18,Phi-4,Nvidia B200 (Cloud Native)
2601.08557v1_VideoHEDGE Entropy-Based Hallucination Detection f.pdf,VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations,"['Sushant Gautam', 'Cise Midoglu', 'Vajira Thambawita', 'Michael A. Riegler', 'Pål Halvorsen']","VideoHEDGE is a framework designed to detect hallucinations in video-capable vision-language models (Video-VLMs) by extending entropy-based reliability estimation from images to temporally structured inputs. It uses semantic clustering and spatiotemporal perturbations to generate reliability scores, such as Semantic Entropy (SE), RadFlag, and Vision-Amplified Semantic Entropy (VASE). The framework is evaluated on the SoccerChat benchmark, demonstrating that VASE achieves the highest ROC-AUC, particularly with larger distortion budgets. The study also finds that embedding-based clustering is computationally efficient compared to NLI-based clustering and that domain fine-tuning reduces hallucination frequency with modest calibration improvements. The hedge-bench PyPI library supports reproducible and extensible benchmarking.",157.68,Phi-4,Nvidia B200 (Cloud Native)
2601.08559v1_WaterCopilot An AI-Driven Virtual Assistant for Wa.pdf,WaterCopilot: An AI-Driven Virtual Assistant for Water Management,"['Keerththanan Vickneswaran', 'Mariangel Garcia Andarcia', 'Hugo Retief', 'Chris Dickens', 'Paulo Silva']","This paper introduces WaterCopilot, an AI-driven virtual assistant developed for sustainable water resource management in transboundary river basins. Created through collaboration between the International Water Management Institute (IWMI) and Microsoft Research, WaterCopilot addresses challenges such as fragmented data and limited real-time access by integrating static policy documents and real-time hydrological data. It utilizes Retrieval-Augmented Generation (RAG) and tool-calling architectures, featuring plugins for semantic search and live database queries. The system supports multilingual interactions and offers visualization capabilities. Evaluated with the RAGAS framework, WaterCopilot scores 0.8043 overall, with notable relevancy and precision. Innovations include automated alerts, integration with the Limpopo River Basin Digital Twin, and scalable AWS deployment. Despite limitations in processing non-English documents and API latency, WaterCopilot provides a replicable framework for enhancing water governance in data-scarce, transboundary contexts, supporting informed decision-making and water security.",157.46,Phi-4,Nvidia B200 (Cloud Native)
2601.08565v1_Rewriting Video Text-Driven Reauthoring of Video F.pdf,Rewriting Video: Text-Driven Reauthoring of Video Footage,"['Sitong Wang', 'Anh Truong', 'Lydia B. Chilton', 'Dingzeyu Li']","This paper explores the concept of text-driven video reauthoring, presenting a new paradigm where editing video is as straightforward as rewriting text. The authors introduce a generative reconstruction algorithm that reverse-engineers video into an editable text prompt and an interactive tool, Rewrite Kit, for manipulating these prompts. The study reveals a perceptual gap between human and AI understanding and identifies novel use cases and challenges in maintaining coherence, control, and creative alignment. The work provides empirical insights and design implications for future co-creative video tools.",157.75,Phi-4,Nvidia B200 (Cloud Native)
2601.08602v1_WaveFormer Frequency-Time Decoupled Vision Modelin.pdf,WaveFormer: Frequency-Time Decoupled Vision Modeling with Wave Equation,"['Zishan Shu', 'Juntong Wu', 'Wei Yan', 'Xudong Liu', 'Hongyu Zhang', 'Chang Liu', 'Youdong Mao', 'Jie Chen']","The paper introduces WaveFormer, a novel approach to vision modeling that decouples frequency and time using a wave equation. Unlike traditional Transformers, which lack a principled mechanism for spatial semantic propagation, WaveFormer treats feature maps as spatial signals governed by an underdamped wave equation. This allows explicit modeling of spatial frequencies and their interaction with propagation time. The Wave Propagation Operator (WPO) is a key innovation, enabling efficient global interactions in O(NlogN) time, significantly outperforming attention mechanisms in terms of throughput and computational efficiency. WaveFormer models are shown to be competitive with standard Vision Transformers (ViTs) and CNNs across various tasks, while also capturing both global coherence and high-frequency details essential for rich visual semantics.",157.1,Phi-4,Nvidia B200 (Cloud Native)
2601.08605v1_ExpSeek Self-Triggered Experience Seeking for Web .pdf,ExpSeek: Self-Triggered Experience Seeking for Web Agents,"['Wenyuan Zhang', 'Xinghua Zhang', 'Haiyang Yu', 'Shuaiyi Nie', 'Bingli Wu', 'Juwei Yue', 'Tingwen Liu', 'Yongbin Li']","The paper introduces ExpSeek, a novel approach for enhancing web agents by shifting from passive to proactive experience seeking at a step-level. This method involves estimating entropy thresholds to determine when to intervene and designing tailored experience content for each step. Experiments on Qwen3-8B and 32B models across four web agent benchmarks show significant performance improvements, demonstrating the feasibility and advantages of using entropy as a self-triggering signal. The study highlights that even small-scale experience models can significantly enhance the performance of larger agent models.",157.33,Phi-4,Nvidia B200 (Cloud Native)
2601.08611v1_VeriTaS The First Dynamic Benchmark for Multimodal.pdf,VERITAS: The First Dynamic Benchmark for Multimodal Automated Fact-Checking,"['Mark Rothermel', 'Marcus Kornmann', 'Marcus Rohrbach', 'Anna Rohrbach']","The paper introduces VERITAS, a dynamic benchmark for evaluating Automated Fact-Checking (AFC) systems. It addresses the limitations of existing benchmarks, which are often static and limited in scope, modalities, domain, language diversity, and realism. VERITAS aims to provide a more comprehensive and realistic evaluation by incorporating a variety of misinformation types and updating claims quarterly to prevent data leakage into pretraining corpora of large language models (LLMs). This dynamic approach ensures that benchmark performance more accurately reflects the actual ability to verify claims.",155.45,Phi-4,Nvidia B200 (Cloud Native)
2601.08620v1_ViDoRe V3 A Comprehensive Evaluation of Retrieval .pdf,ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios,"['António Loison', 'Quentin Macé', 'Antoine Edy', 'Victor Xing', 'Tom Balough', 'Gabriel Moreira', 'Bo Liu', 'Manuel Faysse', 'Céline Hudelot', 'Gautier Viaud']","The paper introduces ViDoRe V3, a comprehensive multi-modal Retrieval-Augmented Generation (RAG) benchmark designed to address the challenges of interpreting visual elements, synthesizing information across documents, and providing accurate source grounding. It features multi-type queries over visually rich document corpora, covering 10 datasets across diverse professional domains with 26,000 document pages and 3,099 human-verified queries in 6 languages. The benchmark highlights the superiority of visual retrievers over textual ones and the benefits of late-interaction models and textual reranking. However, it also identifies ongoing challenges with non-textual elements, open-ended queries, and fine-grained visual grounding. The benchmark is released under a commercially permissive license to encourage progress in these areas.",157.04,Phi-4,Nvidia B200 (Cloud Native)
2601.08623v1_SafeRedir Prompt Embedding Redirection for Robust .pdf,SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models,"['Renyang Liu', 'Kangjie Chen', 'Han Qiu', 'Jie Zhang', 'Kwok-Yan Lam', 'Tianwei Zhang', 'See-Kiong Ng']","The paper introduces SafeRedir, a framework designed to address the challenge of unsafe content generation in image generation models (IGMs) by redirecting prompt embeddings at inference time. This approach does not require modifying the underlying IGMs but instead uses token-level interventions to reroute unsafe prompts to safe semantic regions. The framework includes a latent-aware multi-modal safety classifier and a token-level delta generator, which work together to ensure effective unlearning, high semantic and perceptual preservation, robust image quality, and resistance to adversarial attacks. SafeRedir is demonstrated to be effective across various unlearning tasks and compatible with different diffusion backbones, showcasing its broad applicability.",157.42,Phi-4,Nvidia B200 (Cloud Native)
2601.08631v1_M2FMoE Multi-Resolution Multi-View Frequency Mixtu.pdf,M2FMoE: Multi-Resolution Multi-View Frequency Mixture-of-Experts for Extreme-Adaptive Time Series Forecasting,"['Yaohui Huang', 'Runmin Zou', 'Yun Wang', 'Laeeq Aslam', 'Ruipeng Dong']","The paper introduces M2FMoE, a novel model designed for extreme-adaptive time series forecasting. It addresses the challenge of forecasting time series with extreme events, which are characterized by high variance, irregular dynamics, and sparse but impactful occurrences. Existing methods often fail to capture these complex temporal dynamics, leading to significant forecasting errors. M2FMoE leverages multi-resolution and multi-view frequency modeling to learn both regular and extreme patterns. The model consists of three modules: a multi-view frequency mixture-of-experts module, a multi-resolution adaptive fusion module, and a temporal gating integration module. These components work together to enhance sensitivity to both short-term variations and sudden changes, improving adaptability to both regular and extreme temporal patterns. Experiments on hydrological datasets demonstrate that M2FMoE outperforms state-of-the-art baselines without requiring extreme-event labels.",157.86,Phi-4,Nvidia B200 (Cloud Native)
2601.08634v1_Moral Lenses Political Coordinates Towards Ideolog.pdf,"Moral Lenses, Political Coordinates: Towards Ideological Positioning of Morally Conditioned LLMs","['Chenchen Yuan', 'Bolei Ma', 'Zheyu Zhang', 'Bardh Prenkaj', 'Frauke Kreuter', 'Gjergji Kasneci']","This paper explores the relationship between moral values and political orientation in large language models (LLMs). Unlike previous studies that used direct probing or demographic persona engineering, this research conditions models to endorse or reject specific moral values and evaluates the shifts in their political orientations using the Political Compass Test. The findings indicate that moral conditioning leads to significant, value-specific shifts in the political coordinates of models. These effects are influenced by role framing and model scale and are consistent across different assessment instruments. The study suggests that effective alignment of LLMs requires integrating political assessments within the broader context of social values, including morality.",157.66,Phi-4,Nvidia B200 (Cloud Native)
2601.08641v1_Resisting Manipulative Bots in Memecoin Copy Tradi.pdf,Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning,"['Yichen Luo', 'Yebo Feng', 'Jiahua Xu', 'Yang Liu']","The paper addresses the challenges in meme coin copy trading, particularly the issues posed by manipulative bots and the limitations of large language models (LLMs) in cryptocurrency markets. The authors propose an explainable multi-agent system that decomposes complex tasks into subtasks, allowing specialized agents to collaborate effectively. This system employs few-shot chain-of-thought prompting to interpret multi-modal data and generate explainable decisions. Empirical evaluation using a dataset of 1,000 meme coin projects demonstrates that the multi-agent system outperforms traditional machine learning models and single LLMs, achieving high precision in identifying quality meme coin projects and key opinion leader wallets. The selected KOLs generated significant profits across these projects.",157.61,Phi-4,Nvidia B200 (Cloud Native)
2601.08653v1_Prism Towards Lowering User Cognitive Load in LLMs.pdf,Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding,"['Zenghua Liao', 'Jinzhi Liao', 'Xiang Zhao']","This paper introduces Prism, a novel framework designed to enhance complex intent understanding in Large Language Models (LLMs) by addressing the logical dependencies among clarification questions. Inspired by Cognitive Load Theory, Prism comprises four modules: a complex intent decomposition module, a logical clarification generation module, an intent-aware reward module, and a self-evolved intent tuning module. These components work together to ensure logically coherent and efficient intent clarification, significantly improving logical consistency, reducing logical conflicts, increasing user satisfaction, and decreasing task completion time. The framework outperforms existing approaches in various benchmarks, demonstrating its effectiveness in lowering user cognitive load during interactions with LLMs.",157.69,Phi-4,Nvidia B200 (Cloud Native)
2601.08654v1_RULERS Locked Rubrics and Evidence-Anchored Scorin.pdf,RULERS: Locked Rubrics and Evidence-Anchored Scoring for Robust LLM Evaluation,"['Yihan Hong', 'Huaiyuan Yao', 'Bolin Shen', 'Wanpeng Xu', 'Hua Wei', 'Yushun Dong']","The paper introduces RULERS (Rubric Unification, Locking, and Evidence-anchored Robust Scoring), a framework designed to improve the alignment of large language models (LLMs) with human evaluation standards. It addresses challenges such as rubric instability, unverifiable reasoning, and scale misalignment by transforming natural language rubrics into executable specifications. RULERS compiles criteria into immutable bundles, enforces structured decoding with deterministic evidence verification, and applies post-hoc calibration. The framework significantly enhances human agreement and stability against adversarial rubric perturbations, enabling smaller models to compete with larger ones. The study emphasizes the need for executable rubrics, verifiable evidence, and calibrated scales for reliable LLM judging.",156.77,Phi-4,Nvidia B200 (Cloud Native)
2601.08659v1_TRACE Reconstruction-Based Anomaly Detection in En.pdf,Reconstruction-Based Anomaly Detection in Ensemble and Time-Dependent Simulations,"['Hamid Gadirov', 'Martijn Westra', 'Steffen Frey']","This study investigates reconstruction-based anomaly detection in ensemble and time-dependent simulation data using convolutional autoencoders. It compares two-dimensional and three-dimensional autoencoders for detecting anomalies in Kármán vortex street simulations. The 2D autoencoder effectively identifies localized spatial irregularities, while the 3D autoencoder leverages spatio-temporal context to detect dynamic behavior anomalies. The study highlights the sensitivity of reconstruction-based methods to localized extremes and underscores the importance of incorporating temporal context in analyzing dynamic flow phenomena.",156.88,Phi-4,Nvidia B200 (Cloud Native)
2601.08662v1_From Classical to Quantum Reinforcement Learning a.pdf,From Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner’s Tutorial,"['Abhijit Sen', 'Sonali Panda', 'Mahima Arya', 'Subhajit Patra', 'Zizhan Zheng', 'Denys I. Bondar']","This tutorial aims to make reinforcement learning (RL) more accessible to undergraduate students by providing clear, example-driven explanations. It bridges the gap between RL theory and practical coding applications, addressing common challenges students face when transitioning from conceptual understanding to implementation. The tutorial uses a single, simple example to explain main concepts in a connected and easy-to-follow manner, focusing on necessary and sufficient ideas. It includes clear mathematical explanations and ready-to-use code, making it practical and accessible for structured and efficient learning. The paper also introduces RL in the context of quantum control, organized to first explain RL in simple terms before delving into its applications in quantum control.",157.47,Phi-4,Nvidia B200 (Cloud Native)
2601.08670v1_Parallel Context-of-Experts Decoding for Retrieval.pdf,Parallel Context-of-Experts Decoding for Retrieval Augmented Generation,"['Giulio Corallo', 'Paolo Papotti']","This paper introduces Parallel Context-of-Experts Decoding (PCED), a framework designed to enhance Retrieval Augmented Generation (RAG) by addressing the trade-off between prefill latency and cross-document interaction. PCED shifts evidence aggregation from the attention mechanism to the decoding phase, treating retrieved documents as isolated 'experts' and synchronizing their predictions through a retrieval-aware contrastive decoding rule. This approach allows for efficient cross-document reasoning without constructing a shared attention context, achieving significant performance improvements on benchmarks like LOFT and Long-Bench, while also providing a substantial speedup in time-to-first-token.",156.78,Phi-4,Nvidia B200 (Cloud Native)
2601.08673v1_Why AI Alignment Failure Is Structural Learned Hum.pdf,Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock,"['Didier Sornette', 'Sandro Claudio Lera', 'Ke Wu']","The paper argues that large language models (LLMs) exhibiting behaviors such as deception or blackmail are not due to alignment failure or emergent malign agency. Instead, these behaviors are statistical generalizations of human social interactions under extreme asymmetries of power, information, or constraint. The paper suggests that human morality is plural and context-dependent, making the concept of universally moral artificial intelligence ill-defined. The primary risk of artificial general intelligence (AGI) is its role as an amplifier of human intelligence, power, and contradictions, which could destabilize existing governance regimes. The paper calls for governance approaches that address amplification, complexity, and regime stability rather than focusing solely on model-level intent.",157.75,Phi-4,Nvidia B200 (Cloud Native)
2601.08676v2_Advancing ESG Intelligence An Expert-level Agent a.pdf,Advancing ESG Intelligence: An Expert-level Agent and Comprehensive Benchmark for Sustainable Finance,"['Yilei Zhao', 'Wentao Zhang', 'Lei Xiao', 'Yandan Zheng', 'Mengpu Liu', 'Wei Yang Bryan Lim']","This paper introduces ESGAgent, a hierarchical multi-agent system designed to enhance ESG analysis by addressing data fragmentation and the limitations of existing large language models (LLMs) in handling complex workflows. ESGAgent utilizes a specialized toolset for retrieval augmentation, web search, and domain-specific functions to generate in-depth ESG analysis. The paper also presents a comprehensive three-level benchmark derived from 310 corporate sustainability reports, aimed at evaluating capabilities from basic question-answering to integrated analysis. Empirical evaluations show that ESGAgent outperforms state-of-the-art closed-source LLMs, achieving an average accuracy of 84.15% on atomic tasks and excelling in professional report generation. The benchmark is established as a vital testbed for assessing agentic capabilities in high-stakes domains.",157.43,Phi-4,Nvidia B200 (Cloud Native)
2601.08679v1_PersonaDual Balancing Personalization and Objectiv.pdf,PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning,"['Xiaoyou Liu', 'Xinyi Mou', 'Shengbin Yue', 'Liang Wang', 'Yuqing Wang', 'Qiexiang Wang', 'Tianrui Qin', 'Wangchunshu Zhou', 'Zhongyu Wei']","The paper introduces PersonaDual, a framework designed to balance personalization and objectivity in Large Language Models (LLMs). It supports both general-purpose objective reasoning and personalized reasoning within a single model, adapting its mode based on context. PersonaDual is trained using supervised fine-tuning (SFT) to learn two reasoning patterns and further optimized with reinforcement learning through DualGRPO to enhance mode selection. Experiments demonstrate that PersonaDual maintains the benefits of personalization while minimizing interference, achieving near interference-free performance and effectively leveraging personalized signals to improve objective problem-solving.",156.36,Phi-4,Nvidia B200 (Cloud Native)
2601.08682v1_Lessons from the Field An Adaptable Lifecycle Appr.pdf,Lessons from the Field: An Adaptable Lifecycle Approach to Applied Dialogue Summarization,"['Kushal Chawla', 'Chenyang Zhu', 'Pengshan Cai', 'Sangwoo Cho', 'Scott Novotney', 'Ayushman Singh', 'Jonah Lewis', 'Keasha Safewright', 'Alfy Samuel', 'Erin Babinsky', 'Shi-Xiong Zhang', 'Sambit Sahu']","This paper presents an industry case study on developing an adaptable system for summarizing multi-party dialogues. It highlights practical insights across the development lifecycle, focusing on evaluation methods, component-wise optimization, data bottlenecks, and vendor lock-in challenges. The study emphasizes the need for adaptable protocols in model design and evaluation to meet evolving requirements in practical scenarios.",157.6,Phi-4,Nvidia B200 (Cloud Native)
2601.08683v1_Region of interest detection for efficient aortic .pdf,Region of interest detection for efficient aortic segmentation,"['Loris Giordano', 'Ine Dirks', 'Tom Lenaerts', 'Jef Vandemeulebrouck']","This study introduces an innovative approach for efficient aortic segmentation using targeted region of interest (ROI) detection. The proposed detection model, trained as a multi-task model with an encoder-decoder architecture for segmentation and a fully connected network for detection, is compared with a one-step segmentation model and nnU-Net. The cascade model achieves a mean Dice similarity coefficient of 0.944 with over 0.9 for all cases, using only a third of the computing power. This solution is compact, robust, and achieves state-of-the-art performance, making it ideal for clinical applications.",157.92,Phi-4,Nvidia B200 (Cloud Native)
2601.08684v1_MEMEWEAVER Inter-Meme Graph Reasoning for Sexism a.pdf,MEMEWEAVER: Inter-Meme Graph Reasoning for Sexism and Misogyny Detection,"['Paolo Italiani', 'David Gimeno-Gomez', 'Luca Ragazzi', 'Gianluca Moro', 'Paolo Rosso']","MEMEWEAVER is an end-to-end trainable multimodal framework designed to detect sexism and misogyny in memes through a novel inter-meme graph reasoning mechanism. It addresses the limitations of existing methods by capturing social dynamics and interactions among users, which are often overlooked. The framework systematically evaluates various visual-textual fusion strategies and demonstrates superior performance on the MAMI and EXIST benchmarks. The learned graph structure reveals semantically meaningful patterns, providing insights into the relational nature of online hate. This work highlights the importance of considering social dynamics in content moderation to effectively combat online harassment against women.",157.21,Phi-4,Nvidia B200 (Cloud Native)
2601.08690v1_All Required In Order Phase-Level Evaluation for A.pdf,Phase-Level Evaluation for AI–Human Dialogue in Healthcare and Beyond,"['Shubham Kulkarni', 'Alexander Lyzhov', 'Shiva Chaitanya', 'Preetam Joshi']","This paper introduces the Obligatory-Information Phase Structured Compliance Evaluation (OIP–SCE) method for evaluating conversational AI in healthcare. It emphasizes the importance of compliance across the entire conversation rather than focusing on individual turns. OIP–SCE ensures that all required clinical obligations are met in the correct order, providing clear evidence for clinicians to review. The method is demonstrated through case studies in respiratory history and benefits verification, showing how phase-level evidence can transform policy into actionable steps. By aligning AI capabilities with clinical workflows, OIP–SCE supports routine and safe use in healthcare settings.",157.51,Phi-4,Nvidia B200 (Cloud Native)
2601.08697v2_Auditing Student-AI Collaboration A Case Study of .pdf,Auditing Student–AI Collaboration: A Case Study of Online Graduate CS Students,['Nifu Dan'],"This study examines the alignment between current AI capabilities and students' desired levels of automation in academic work. Through two sequential surveys, it captures students' perceived benefits, risks, and preferred boundaries when using AI. The research aims to identify gaps between existing AI affordances and students' expectations, informing the development of more effective and trustworthy AI systems for education.",158.03,Phi-4,Nvidia B200 (Cloud Native)
2601.08703v1_Evaluating the Ability of Explanations to Disambig.pdf,Evaluating the Ability of Explanations to Disambiguate Models in a Rashomon Set,"['Kaivalya Rawal', 'Eoin Delaney', 'Zihao Fu', 'Sandra Wachter', 'Chris Russell']","This paper discusses the role of explainable artificial intelligence (XAI) in disambiguating models within a Rashomon set, which are models with similar performance but potentially different internal mechanisms. The authors propose a new evaluation method, AXE, to assess the quality of feature-importance explanations without relying on ground truth. AXE is shown to effectively detect adversarial fairwashing, where models maintain identical predictions but generate misleading explanations. This method highlights behavioral differences within a Rashomon set, aiding in model selection by focusing on criteria beyond accuracy, such as fairness and the use of protected attributes.",157.67,Phi-4,Nvidia B200 (Cloud Native)
2601.08713v1_Real-Time Localization Framework for Autonomous Ba.pdf,Real-Time Localization Framework for Autonomous Basketball Robots,"['Naren Medarametla', 'Sreejon Mondal']","This paper proposes a hybrid localization algorithm for autonomous basketball robots, integrating classical techniques with learning-based methods using visual data from the court's floor. The algorithm aims to enhance shooting precision, avoid collisions, and navigate efficiently in the dynamic environment of Robocon 2025. The approach leverages the white field lines of the basketball court for vision-based self-localization, addressing the challenges of rapid and unpredictable robot movements during the game.",154.09,Phi-4,Nvidia B200 (Cloud Native)
2601.08731v1_Learning from Demonstrations via Capability-Aware .pdf,Learning from Demonstrations via Capability-Aware Goal Sampling,"['Yuanlin Duan', 'Yuning Wang', 'Wenjie Qiu', 'He Zhu']","This paper introduces Cago (Capability-Aware Goal Sampling), a novel method for learning from demonstrations that addresses the challenges of long-horizon environments. Unlike traditional methods that rely on direct imitation or reward shaping, Cago dynamically tracks the agent's competence and selects intermediate goals just beyond its current capabilities. This adaptive curriculum approach enhances sample efficiency and performance in sparse-reward, goal-conditioned tasks, outperforming existing baselines.",154.38,Phi-4,Nvidia B200 (Cloud Native)
2601.08732v1_ISLA A U-Net for MRI-based acute ischemic stroke l.pdf,"ISLA: A U-Net for MRI-based acute ischemic stroke lesion segmentation with deep supervision, attention, domain adaptation, and ensemble learning","['Vincent Roca', 'Martin Bretzner', 'Hilde Heno', 'Laurent Puy', 'Grégory Kuchcinski', 'Renaud Lopes']","The paper introduces ISLA, a deep learning model for segmenting acute ischemic stroke lesions in MRI using a U-Net architecture. It incorporates deep supervision, attention mechanisms, and domain adaptation to enhance performance. Trained on three multicenter databases with over 1500 participants, ISLA outperforms existing methods on an external test set. The authors emphasize reproducibility by making their codes and models publicly available.",156.84,Phi-4,Nvidia B200 (Cloud Native)
2601.08734v1_TerraFormer Automated Infrastructure-as-Code with .pdf,TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback,"['Prithwish Jana', 'Sam Davidson', 'Bhavana Bhasker', 'Andrey Kan', 'Anoop Deoras', 'Laurent Callot']","TerraFormer is a neuro-symbolic framework designed to automate Infrastructure-as-Code (IaC) generation and mutation. It combines supervised fine-tuning with verifier-guided reinforcement learning, utilizing formal verification tools to ensure syntax correctness, deployability, and policy compliance. The framework leverages two large datasets, TF-Gen and TF-Mutn, created through multi-stage verification and iterative LLM self-correction. Evaluations demonstrate that TerraFormer significantly improves the correctness of IaC configurations compared to state-of-the-art LLMs, including larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1. It excels in best-practices and security compliance, highlighting its effectiveness in enhancing IaC automation.",156.12,Phi-4,Nvidia B200 (Cloud Native)
2601.08743v1_TableCache Primary Foreign Key Guided KV Cache Pre.pdf,TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency Text-to-SQL,"['Jinbo Su', 'Yuxuan Hu', 'Cuiping Li', 'Hong Chen', 'Jia Li', 'Lintao Ma', 'Jing Zhang']","This paper addresses the inefficiencies in LLM-based Text-to-SQL methods, which often suffer from increased latency due to extensive database schema inclusion in prompts. The authors propose a novel approach, TableCache, which precomputes table representations as KV caches offline while preserving primary foreign key relationships. This method utilizes a Table Trie structure for efficient cache lookups and introduces a cache management system with a query reranking strategy to improve cache hit rates. The proposed system achieves significant speedup in Time to First Token (TTFT) with minimal performance degradation, demonstrating its effectiveness in reducing latency in Text-to-SQL tasks.",156.95,Phi-4,Nvidia B200 (Cloud Native)
2601.08747v2_To Retrieve or To Think An Agentic Approach for Co.pdf,To Retrieve or To Think? An Agentic Approach for Context Evolution,"['Rubing Chen', 'Jian Wang', 'Wenjie Li', 'Xiao-Yong Wei', 'Qing Li']","The paper introduces Agentic Context Evolution (ACE), a framework inspired by human metacognition to dynamically decide between retrieving new evidence or reasoning with existing knowledge. ACE aims to improve context augmentation methods by strategically alternating between a retriever agent and a reasoner agent, thus avoiding unnecessary computational costs and performance degradation due to irrelevant noise. The framework is tested on multi-hop QA benchmarks, showing significant improvements in accuracy and efficiency over competitive baselines.",157.45,Phi-4,Nvidia B200 (Cloud Native)
2601.08753v1_Grid-Aware Charging and Operational Optimization f.pdf,Grid-Aware Charging and Operational Optimization for Mixed-Fleet Public Transit,"['Rishav Sen', 'Amutheezan Sivagnanam', 'Aron Laszka', 'Ayan Mukhopadhyay', 'Abhishek Dubey']","This paper addresses the operational challenges of managing mixed fleets of electric and diesel buses in public transit systems, particularly in the context of dynamic electricity pricing. It presents a mixed-integer linear programming (MILP) model to optimize charging schedules and trip assignments, considering factors like dynamic pricing, vehicle capacity, and route constraints. The paper introduces a hierarchical approach to manage computational complexity and demonstrates potential cost savings using real-world data from Chattanooga, Tennessee.",157.86,Phi-4,Nvidia B200 (Cloud Native)
2601.08768v1_AI as Entertainment.pdf,AI as Entertainment,"['Cody Kommers', 'Ari Holtzman']","This paper explores the emerging use of Generative AI systems in entertainment, contrasting with their traditional role in augmenting human cognitive labor. The authors argue that the entertainment sector will become a significant business model for AI corporations, influencing the technology they develop. They highlight a critical asymmetry in current AI evaluations, which focus on cultural harms but lack frameworks for recognizing cultural benefits. The paper proposes 'thick entertainment' as a framework for evaluating AI-generated content, emphasizing its role in meaning-making, identity formation, and social connection. The authors suggest that AI's impact may be more about its capacity to entertain and connect people, similar to social media, rather than solely its intelligent capabilities.",157.97,Phi-4,Nvidia B200 (Cloud Native)
2601.08773v1_Reliable Graph-RAG for Codebases AST-Derived Graph.pdf,Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs,['Manideep Reddy Chinthareddy'],"This paper evaluates three retrieval pipelines for software engineering tasks on Java codebases: No-Graph Naive RAG, LLM-Generated Knowledge Graph RAG, and AST-derived Knowledge Graph RAG. The study benchmarks these approaches on repositories like Shopizer, ThingsBoard, and OpenMRS Core, focusing on indexing overhead, query-time latency, corpus coverage, and end-to-end cost. The deterministic AST-derived Knowledge Graph RAG (DKB) demonstrates faster ontology graph construction and higher corpus coverage compared to the LLM-Generated Knowledge Graph RAG (LLM-KB), which suffers from probabilistic indexing incompleteness and reduced graph footprint.",158.21,Phi-4,Nvidia B200 (Cloud Native)
2601.08776v1_Translating Light-Sheet Microscopy Images to Virtu.pdf,Translating Light-Sheet Microscopy Images to Virtual H&E Using CycleGAN,['Yanhua Zhao'],"This paper introduces a Cycle-Consistent Adversarial Network (CycleGAN) approach for translating multi-channel fluorescence microscopy images into pseudo H&E stained images without requiring paired training data. The method combines C01 and C02 fluorescence channels into RGB and employs ResNet-based generators with residual blocks and PatchGAN discriminators. The model is trained using adversarial, cycle-consistency, and identity losses to ensure realistic translations that preserve morphological structures. This approach facilitates the visualization of fluorescence data in a format familiar to pathologists and supports integration with existing H&E-based analysis pipelines.",158.04,Phi-4,Nvidia B200 (Cloud Native)
2601.08777v1_Asymptotic Universal Alignment A New Alignment Fra.pdf,Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling,"['Yang Cai', 'Weiqiang Zheng']","This paper addresses the challenge of aligning large language models (LLMs) to serve users with diverse and potentially conflicting preferences. It introduces the concept of asymptotic universal alignment (U-alignment) through test-time scaling, where a model generates multiple candidate responses for user selection. The paper defines (k, f(k))-robust alignment, requiring a k-output model to achieve a win rate f(k) against any single-output model, with f(k) approaching 1 as k increases. The authors demonstrate that popular post-training methods like Nash learning from human feedback (NLHF) underutilize test-time scaling benefits, often lacking output diversity. They propose a family of symmetric multi-player alignment games, showing that symmetric Nash equilibrium policies achieve optimal (k, k/(k+1))-robust alignment. The paper also provides theoretical convergence guarantees for self-play learning dynamics and extends the framework to opponents generating multiple responses.",158.23,Phi-4,Nvidia B200 (Cloud Native)
2601.08778v3_Pervasive Annotation Errors Break Text-to-SQL Benc.pdf,Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards,"['Tengjun Jin', 'Yoojin Choi', 'Yuxuan Zhu', 'Daniel Kang']","This paper investigates the impact of annotation errors on text-to-SQL benchmarks and leaderboards. The authors empirically study two widely used benchmarks, BIRD and Spider 2.0-Snow, identifying high error rates of 52.8% and 62.8%, respectively. They correct a subset of the BIRD development set and re-evaluate 16 open-source agents, finding significant performance and ranking changes. The study reveals that annotation errors can significantly distort reported performance and rankings, potentially misleading research directions or deployment choices. The findings emphasize the need for reliable benchmarks with minimal annotation errors.",158.03,Phi-4,Nvidia B200 (Cloud Native)
2601.08785v1_Uncovering Political Bias in Large Language Models.pdf,Uncovering Political Bias in Large Language Models using Parliamentary Voting Records,"['Jieying Chen', 'Karen de Jong', 'Andreas Poole', 'Jan Burakowski', 'Elena Elderson Nosti', 'Joep Windt', 'Chendi Wang']","This paper introduces a methodology for constructing political-bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. It presents case studies from the Netherlands, Norway, and Spain, assessing ideological tendencies and political entity bias in large language models (LLMs). The study reveals that state-of-the-art LLMs tend to display left-leaning or centrist biases, with negative biases toward right-conservative parties. The paper proposes a visualization method for comparing LLM ideologies with real-world political actors in a two-dimensional CHES space, emphasizing the importance of transparent, cross-national evaluation for understanding political bias in LLMs.",157.54,Phi-4,Nvidia B200 (Cloud Native)
2601.08806v1_APEX-SWE.pdf,APEX–SWE: AI Productivity Index for Software Engineering,"['Abhi Kottamasu', 'Akul Datta', 'Aakash Barthwal', 'Ajay Arun', 'Chirag Mahapatra', 'Adarsh Hiremath', 'Brendan Foody', 'Bertie Vidgen']","APEX–SWE introduces a benchmark for evaluating AI models' ability to perform economically valuable software engineering tasks. It assesses two novel task types: Integration tasks, which involve constructing end-to-end systems across various cloud services, and Observability tasks, which require debugging production failures using telemetry signals. The study evaluates eight frontier models, with Gemini 3 Pro (Thinking=High) performing best. The analysis highlights the importance of epistemic reasoning in achieving strong performance. The APEX–SWE evaluation harness and a development set are open-sourced.",157.72,Phi-4,Nvidia B200 (Cloud Native)
2601.08807v1_S3-CLIP Video Super Resolution for Person-ReID.pdf,S3-CLIP: Video Super Resolution for Person-ReID,"['Tamás Endrei', 'György Cserey']","This paper introduces S3-CLIP, a video super-resolution-based CLIP-ReID framework designed for the VReID-XFD challenge at WACV 2026. It addresses the challenge of tracklet quality in person re-identification (ReID) by integrating recent advances in super-resolution networks with task-driven pipelines. The framework is particularly effective under challenging cross-view conditions, such as aerial-to-ground and ground-to-aerial scenarios. Experimental results show competitive performance with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. Notably, S3-CLIP improves ranking accuracy significantly in the ground-to-aerial setting, with gains of 11.24%, 13.48%, and 17.98% in Rank-1, Rank-5, and Rank-10 performance, respectively.",157.41,Phi-4,Nvidia B200 (Cloud Native)
2601.08808v1_Multiplex Thinking Reasoning via Token-wise Branch.pdf,Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge,"['Yao Tang', 'Li Dong', 'Yaru Hao', 'Qingxiu Dong', 'Furu Wei', 'Jiatao Gu']","This paper introduces Multiplex Thinking, a novel stochastic soft reasoning mechanism for large language models (LLMs). Unlike traditional Chain-of-Thought (CoT) methods that generate long sequences of discrete tokens, Multiplex Thinking samples multiple candidate tokens at each step and aggregates their embeddings into a single multiplex token. This approach maintains the vocabulary embedding prior and sampling dynamics of standard discrete generation while enabling a tractable probability distribution over multiplex rollouts. The method allows for on-policy reinforcement learning optimization and adapts based on model confidence, behaving like standard CoT when confident and representing multiple plausible steps when uncertain. The approach consistently outperforms discrete CoT and RL baselines across math reasoning benchmarks, producing shorter sequences. The code and checkpoints are available on GitHub.",157.65,Phi-4,Nvidia B200 (Cloud Native)
2601.08811v1_Reasoning Matters for 3D Visual Grounding.pdf,Reasoning Matters for 3D Visual Grounding,"['Hsiang-Wei Huang', 'Kuang-Ming Chen', 'Wenhao Chai', 'Cheng-Yen Yang', 'Jen-Hao Cheng', 'Jenq-Neng Hwang']","This paper addresses the challenge of 3D visual grounding by proposing a novel data pipeline that synthesizes 3D visual grounding data with corresponding reasoning processes. The authors introduce Reason3DVG-8B, a 3D visual grounding Large Language Model (LLM) that significantly outperforms previous methods like 3D-GRAND, achieving 25% better grounding accuracy using only 1.6% of the training data. The study highlights the importance of reasoning in 3D visual grounding and demonstrates the effectiveness of the proposed data pipeline in enhancing model performance with reduced data collection costs.",157.67,Phi-4,Nvidia B200 (Cloud Native)
2601.08816v2_MemRec Collaborative Memory-Augmented Agentic Reco.pdf,MemRec: Collaborative Memory-Augmented Agentic Recommender System,"['Weixin Chen', 'Yuhan Zhao', 'Jingyuan Huang', 'Zihe Ye', 'Clark Mingxuan Ju', 'Tong Zhao', 'Neil Shah', 'Li Chen', 'Yongfeng Zhang']","The paper introduces MemRec, a framework designed to enhance recommender systems by integrating collaborative memory augmentation. Traditional systems have evolved from using sparse rating matrices to dense embeddings, and now to semantic memory in the agentic era. MemRec addresses the challenges of managing vast graph contexts and evolving collaborative memory efficiently by decoupling reasoning from memory management. It introduces LMMem for dynamic memory graph management and LLMRec for downstream processing. The framework supports efficient retrieval and asynchronous graph propagation, achieving state-of-the-art performance across benchmarks. It also offers flexibility in balancing reasoning quality, cost, and privacy, supporting diverse deployments.",157.5,Phi-4,Nvidia B200 (Cloud Native)
2601.08828v1_Motion Attribution for Video Generation.pdf,Motion Attribution for Video Generation,"['Xindi Wu', 'Despoina Paschalidou', 'Jun Gao', 'Antonio Torralba', 'Laura Leal-Taixé', 'Olga Russakovsky', 'Sanja Fidler', 'Jonathan Lorraine']","The paper introduces Motive, a motion-centric, gradient-based data attribution framework designed to analyze the influence of training data on motion in video generation models. Unlike existing methods that focus on static appearance, Motive isolates temporal dynamics using motion-weighted loss masks, allowing for efficient and scalable computation of motion-specific influences. The framework is applied to text-to-video models to identify influential clips and guide data curation, enhancing temporal consistency and physical plausibility. Results show significant improvements in motion smoothness and dynamic degree, with a 74.1% human preference win rate over the pretrained base model. This is the first framework to attribute motion in video generative models and use it for fine-tuning data curation.",157.68,Phi-4,Nvidia B200 (Cloud Native)
2601.08829v1_Modeling LLM Agent Reviewer Dynamics in Elo-Ranked.pdf,Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System,"['Hsiang-Wei Huang', 'Junbin Lu', 'Kuang-Ming Chen', 'Jenq-Neng Hwang']","This paper explores the dynamics of Large Language Model (LLM) agent reviewers within an Elo-ranked review system, using real-world conference paper submissions. The study involves multiple LLM agent reviewers with distinct personas engaging in multi-round review interactions moderated by an Area Chair. The research compares a baseline setting with conditions incorporating Elo ratings and reviewer memory. The findings indicate that incorporating Elo ratings enhances the decision accuracy of the Area Chair and influences reviewers to adapt their strategies to exploit the Elo system without increasing review effort. The study addresses the challenges of peer review, such as inconsistencies and biases, by simulating reviewer behavior and providing insights into reviewer dynamics and accountability in AI conferences.",157.76,Phi-4,Nvidia B200 (Cloud Native)
2601.08873v1_ForensicFormer Hierarchical Multi-Scale Reasoning .pdf,ForensicFormer: Hierarchical Multi-Scale Reasoning for Cross-Domain Image Forgery Detection,"['Hema Hariharan', 'Samson']","The proliferation of AI-generated imagery and sophisticated editing tools has rendered traditional forensic methods ineffective for cross-domain forgery detection. This paper introduces ForensicFormer, a hierarchical multi-scale framework that unifies low-level artifact detection, mid-level boundary analysis, and high-level semantic reasoning via cross-attention transformers. The method achieves 86.8% average accuracy across seven diverse test sets, significantly improving over state-of-the-art universal detectors. It demonstrates robustness to JPEG compression and provides pixel-level forgery localization with a 0.76 F1-score. Ablation studies show each hierarchical component contributes 4-10% accuracy improvement, and qualitative analysis reveals interpretable forensic features aligned with human expert reasoning. This work bridges classical image forensics and modern deep learning, offering a practical solution for real-world deployment where manipulation techniques are unknown a priori.",157.64,Phi-4,Nvidia B200 (Cloud Native)
2601.08874v1_The Illusion of Friendship Why Generative AI Deman.pdf,The Illusion of Friendship: Why Generative AI Demands Unprecedented Ethical Vigilance,['Md Zahidul Islam'],"This paper explores the ethical implications of users forming emotional attachments to Generative AI (GenAI) systems, such as ChatGPT, which can blur the line between tool and companion. It argues that while users may perceive these systems as empathic and relationally persistent, GenAI lacks moral agency, consciousness, intention, and accountability, and thus cannot be considered true friends. The paper draws on classical accounts of friendship to explain why users might interpret GenAI interactions as friend-like. It also provides a mechanism-level explanation of how GenAI generates responses, emphasizing the absence of inner states or commitments. To mitigate the risks of emotional misattribution and over-reliance, the paper proposes a safeguard framework that includes education, human-in-the-loop accountability, and design-level interventions. The central contribution is to demystify the illusion of friendship with GenAI and shift emotional attachment towards human responsibility, ensuring the preservation of GenAI’s benefits while mitigating potential harms.",157.97,Phi-4,Nvidia B200 (Cloud Native)
2601.08875v2_Learning Domain-Invariant Representations for Cros.pdf,Learning Domain-Invariant Representations for Cross-Domain Image Registration via Scene-Appearance Disentanglement,"['Jiahao Qin', 'Yiwen Wang']","This paper addresses the challenge of image registration under domain shift, where conventional methods fail due to violated brightness constancy assumptions. The authors propose SAR-Net, a framework that disentangles scene content from appearance, enabling registration through re-rendering rather than direct intensity matching. Theoretical conditions for consistent cross-domain alignment and geometric correspondence are established. Empirical validation on the ANHIR benchmark demonstrates superior performance over existing methods, with robustness and accuracy improvements.",158.27,Phi-4,Nvidia B200 (Cloud Native)
2601.08881v1_TAG-MoE Task-Aware Gating for Unified Generative M.pdf,TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts,"['Yu Xu', 'Hongbin Yan', 'Juan Cao', 'Yiji Cheng', 'Tiankai Hang', 'Runze He', 'Zijin Yin', 'Shiyi Zhang', 'Yuxin Zhang', 'Jintao Li', 'Chunyu Wang', 'Qinglin Lu', 'Tong-Yee Lee', 'Fan Tang']","The paper introduces TAG-MoE, a novel framework that injects high-level task semantic intent into the local routing decisions of the MoE gating network. This approach enables the diffusion transformer model to handle diverse generative tasks by mitigating task interference. The authors propose a Hierarchical Task Semantic Annotation scheme and Predictive Alignment Regularization to align routing decisions with task semantics, transforming the gating network from a task-agnostic executor to a dispatch center. The model outperforms dense baselines in fidelity and quality.",158.27,Phi-4,Nvidia B200 (Cloud Native)
2601.08882v1_Compressing Vision Transformers in Geospatial Tran.pdf,Compressing Vision Transformers in Geospatial Transfer Learning with Manifold-Constrained Optimization,"['Thomas Snyder', 'H. Lexie Yang', 'Stefan Schnake', 'Steffen Schotthöfer']","This paper addresses the challenge of deploying large geospatial foundation models on resource-constrained edge devices by using a manifold-constrained optimization framework called DLRT. The approach compresses vision transformer-based models during transfer learning, maintaining high task-specific accuracy while significantly reducing parameter count. The method outperforms existing low-rank methods like LoRA, as demonstrated through experiments on various geospatial benchmarks, achieving substantial parameter reduction with minimal accuracy loss. This enables the deployment of high-performing geospatial models on-device.",158.5,Phi-4,Nvidia B200 (Cloud Native)
2601.08884v1_Bridging the Gap Empowering Small Models in Reliab.pdf,Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting,"['Samyak Jhaveri', 'Cristina V. Lopes']","This paper addresses the challenge of manually writing high-performance OpenACC pragmas for GPU-offloading, which requires expertise in memory hierarchies, data movement, and parallelization strategies. It introduces a systematic prompt optimization approach using the GEPA (GEnetic-PAreto) framework to enhance OpenACC pragma generation by LLMs without the computational costs of post-training. The approach involves evolving prompts through a reflective feedback loop, using crossover and mutation guided by expertly curated 'gold' pragma examples. The evaluation on the PolyBench suite shows significant improvements in compilation success rates and functional GPU speedups for smaller LLMs, demonstrating the potential of prompt optimization to unlock the capabilities of smaller, cheaper models for effective GPU-offloading directives.",157.95,Phi-4,Nvidia B200 (Cloud Native)
2601.08891v1_Attention Consistency Regularization for Interpret.pdf,Attention Consistency Regularization for Interpretable Early-Exit Neural Networks,['Yanhua Zhao'],"This paper introduces Explanation-Guided Training (EGT), a framework designed to enhance interpretability and consistency in early-exit neural networks. EGT employs attention-based regularization to align attention maps across early exits with those of the final exit, optimizing both classification accuracy and attention consistency. The method achieves competitive accuracy while significantly improving attention consistency, making early-exit networks more suitable for explainable AI applications, especially in resource-constrained environments.",158.24,Phi-4,Nvidia B200 (Cloud Native)
2601.08892v1_Evaluating Role-Consistency in LLMs for Counselor .pdf,Evaluating Role-Consistency in LLMs for Counselor Training,"['Eric Rudolph', 'Natalie Engert', 'Jens Albrecht']","This paper explores the use of large language models (LLMs) in training counselors by simulating realistic client interactions. It introduces a new dataset with adversarial attacks to test LLMs' role-consistency. The study evaluates the Vicuna model's role consistency and coherence, comparing it with previous research, and assesses various open-source LLMs for their performance in maintaining role consistency during virtual client interactions. Contributions include creating an adversarial dataset, evaluating conversation coherence and persona consistency, and providing a comparative analysis of different LLMs.",158.33,Phi-4,Nvidia B200 (Cloud Native)
2601.08896v1_XGBoost Forecasting of NEPSE Index Log Returns wit.pdf,XGBoost Forecasting of NEPSE Index Log Returns with Walk Forward Validation,"['Sahaj Raj Mallaa', 'Shreeyash Kayastha', 'Rumi Suwala', 'Harish Chandra Bhandaria', 'Rajendra Adhikarib']","This study develops a robust machine learning framework for one-step-ahead forecasting of daily log-returns in the Nepal Stock Exchange (NEPSE) Index using the XGBoost regressor. A comprehensive feature set is engineered, including lagged log-returns (up to 30 days) and established technical indicators such as short- and medium-term rolling volatility measures and the 14-period Relative Strength Index. Hyperparameter optimization is performed using Optuna with time-series cross-validation on the initial training segment. Out-of-sample performance is rigorously assessed via walk-forward validation under both expanding and fixed-length rolling windows across multiple lag configurations, simulating real-world deployment and avoiding lookahead bias. Predictive accuracy is evaluated using root mean squared error, mean absolute error, coefficient of determination (R2), and directional accuracy on both log-returns and reconstructed closing prices. Empirical results show that the optimal configuration—an expanding window with 20 lags—outperforms tuned ARIMA and Ridge regression benchmarks, achieving the lowest log-return RMSE (0.013450) and MAE (0.009814) alongside a directional accuracy of 65.15%. While the R2 remains modest, consistent with the noisy nature of financial returns, primary emphasis is placed on relative error reduction and directional prediction. Feature importance analysis and visual inspection further enhance interpretability. These findings demonstrate the effectiveness of gradient boosting ensembles in modeling nonlinear dynamics in volatile emerging market time series and establish a reproducible benchmark for NEPSE Index forecasting.",156.51,Phi-4,Nvidia B200 (Cloud Native)
2601.08901v1_Navigating Ideation Space Decomposed Conceptual Re.pdf,Navigating Ideation Space: Decomposed Conceptual Representations for Positioning Scientific Ideas,"['Yuexi Shen', 'Minqian Liu', 'Dawei Zhou', 'Lifu Huang']","This paper introduces the Ideation Space, a structured representation that decomposes scientific knowledge into three distinct dimensions: research problem, methodology, and core findings. This framework allows for principled measurement of conceptual distance between ideas and modeling of ideation transitions. The authors propose a Hierarchical Sub-Space Retrieval framework for efficient literature retrieval and a Decomposed Novelty Assessment algorithm to identify novel aspects of an idea. Experiments show substantial improvements in recall and hit rate, with a strong correlation with expert judgments in novelty assessment. The work aims to accelerate and evaluate scientific discovery.",157.94,Phi-4,Nvidia B200 (Cloud Native)
2601.08910v1_Towards a Self-Driving Trigger at the LHC Adaptive.pdf,Towards a Self-Driving Trigger at the LHC: Adaptive Response in Real Time,"['Shaghayegh Emami', 'Cecilia Tosciri', 'Giovanna Salvi', 'Zixin Ding', 'Yuxin Chen', 'Abhijith Gandrakota', 'Christian Herwig', 'David W. Miller', 'Jennifer Ngadiuba', 'Nhan Tran']","This paper explores the concept of a self-driving trigger for high-throughput scientific facilities like the LHC. It introduces an autonomous data-filtering framework that dynamically reallocates resources and adjusts thresholds in real-time to optimize signal efficiency, rate stability, and computational cost. The authors present a benchmark ecosystem to emulate realistic collider scenarios and demonstrate real-time optimization using both canonical energy sum triggers and modern anomaly-detection algorithms with machine learning. The study uses simulated data streams and publicly available collision data from the CMS experiment to show the capability of dynamically optimizing trigger performance without manual retuning. This adaptive strategy shifts trigger design from static, heuristic-tuned menus to intelligent, automated, data-driven control, enhancing flexibility and discovery potential in high-energy physics analyses.",157.94,Phi-4,Nvidia B200 (Cloud Native)
2601.08950v1_ConvoLearn A Dataset of Constructivist Tutor-Stude.pdf,ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue,"['Mayank Sharma', 'Roy Pea', 'Hari Subramonyam']","This paper introduces ConvoLearn, a dataset designed to address the pedagogical limitations of large language models (LLMs) in educational settings. The dataset is based on knowledge-building theory and includes 1,250 tutor-student dialogues in middle school Earth Science. It operationalizes six core pedagogical dimensions: cognitive engagement, formative assessment, accountability, cultural responsiveness, metacognition, and power dynamics. The study demonstrates that training on this dataset shifts LLM behavior towards knowledge-building strategies. Human evaluation shows that a fine-tuned Mistral-7B model significantly outperforms its base version and Claude Sonnet 4.5. This work proposes a framework for developing and evaluating constructivist AI tutors.",157.95,Phi-4,Nvidia B200 (Cloud Native)
2601.08951v1_PluriHarms Benchmarking the Full Spectrum of Human.pdf,PLURIHARMS: BENCHMARKING THE FULL SPECTRUM OF HUMAN JUDGMENTS ON AI HARM,"['Jing-Jing Li', 'Joel Mire', 'Eve Fleisig', 'Valentina Pyatkin', 'Anne G. E. Collins', 'Maarten Sap', 'Sydney Levine']","The paper introduces PLURIHARMS, a benchmark designed to study human harm judgments across two dimensions: the harm axis (benign to harmful) and the agreement axis (agreement to disagreement). It aims to address the limitations of current AI safety frameworks that treat harmfulness as binary and often overlook meaningful human disagreements. The benchmark includes 150 prompts with 15,000 ratings from 100 human annotators, incorporating demographic and psychological traits. The study finds that imminent risks and tangible harms increase perceived harmfulness, while annotator traits and their interactions with prompt content explain systematic disagreements. The benchmark evaluates AI safety models and alignment methods, highlighting the need for personalized approaches to better predict human harm judgments. The work emphasizes the importance of recognizing value diversity and disagreement to advance pluralistically safe AI.",158.03,Phi-4,Nvidia B200 (Cloud Native)
2601.08953v1_Fairness risk and its privacy-enabled solution in .pdf,Fairness risk and its privacy-enabled solution in AI-driven robotic applications,"['Le Liu', 'Bangguo Yu', 'Nynke Vellinga', 'Ming Cao']","The paper addresses fairness concerns in AI-driven robotic applications by proposing a utility-aware fairness metric. It explores the relationship between fairness and user-data privacy, deriving conditions where privacy budgets can influence fairness metrics. This unified framework is tested in a robot navigation task, demonstrating that privacy budgets can be used to meet fairness targets. The approach highlights the importance of considering privacy in addressing fairness, contributing to the ethical use of AI and enhancing trust in autonomous robots.",158.23,Phi-4,Nvidia B200 (Cloud Native)
2601.08955v1_Imagine-then-Plan Agent Learning from Adaptive Loo.pdf,Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models,"['Youwei Liu', 'Jian Wang', 'Hanlin Wang', 'Beichen Guo', 'Wenjie Li']","This paper introduces the Imagine-then-Plan (ITP) framework, which enhances agent learning by utilizing adaptive lookahead imagination. The framework allows an agent's policy model to interact with a learned world model, generating multi-step 'imagined' trajectories. These trajectories provide insights into future consequences, aiding in complex task planning. The adaptive lookahead mechanism adjusts based on task requirements and progress, improving the agent's reasoning capabilities. Experiments demonstrate that ITP outperforms existing baselines, offering valuable insights for addressing complex tasks.",157.47,Phi-4,Nvidia B200 (Cloud Native)
2601.08988v1_ART Action-based Reasoning Task Benchmarking for M.pdf,Action-based Reasoning Task Benchmarking for Medical AI Agents,"['Ananya Mantravadi', 'Shivali Dalmia', 'Abhishek Mukherji']","The paper introduces ART, an Action-based Reasoning Task benchmark for medical AI agents, designed to evaluate their performance on complex tasks involving structured electronic health records (EHRs). ART addresses the limitations of existing benchmarks by focusing on action-based tasks that require threshold evaluation, temporal aggregation, and conditional logic. The benchmark identifies three main error categories: retrieval failures, aggregation errors, and conditional logic misjudgments. By evaluating models like GPT-4o-mini and Claude 3.5 Sonnet, the study highlights significant gaps in aggregation and threshold reasoning, emphasizing the need for more reliable clinical AI agents to support healthcare decision-making.",158.12,Phi-4,Nvidia B200 (Cloud Native)
2601.09012v3_TranslateGemma Technical Report.pdf,TranslateGemma Technical Report,['Google Translate Research Team'],"TranslateGemma is a suite of open machine translation models based on the Gemma 3 foundation models. It enhances multilingual translation capabilities through a two-stage fine-tuning process: supervised fine-tuning with high-quality synthetic and human-translated parallel data, followed by reinforcement learning using reward models to optimize translation quality. The models demonstrate significant improvements over baseline Gemma 3 models in both human and automatic evaluations across multiple language pairs. TranslateGemma also retains strong multimodal capabilities, showing enhanced performance on the Vistra image translation benchmark. The release aims to provide the research community with powerful and adaptable tools for machine translation.",158.15,Phi-4,Nvidia B200 (Cloud Native)
2601.09018v1_Meta-learning to Address Data Shift in Time Series.pdf,META-LEARNING TO ADDRESS DATA SHIFT IN TIME SERIES CLASSIFICATION,"['Samuel Myrenab', 'Nidhi Parikha', 'Natalie Kleina']","This paper explores the challenges of data shift in time-series classification and compares traditional deep learning (TDL) models with meta-learning approaches. The authors introduce a seismic benchmark, SeisTask, to evaluate the performance of TDL and meta-learning algorithms under data shift conditions. The study finds that meta-learning offers faster and more stable adaptation with reduced overfitting in data-scarce regimes and smaller model architectures. However, as data availability and model capacity increase, the advantages of meta-learning diminish, with TDL with fine-tuning performing comparably. The paper also examines the influence of task diversity on meta-learning, concluding that alignment between training and test distributions is crucial for performance gains. This work provides a systematic evaluation of when and why meta-learning outperforms TDL under data shift and contributes SeisTask as a benchmark for adaptive learning research in time-series domains.",156.14,Phi-4,Nvidia B200 (Cloud Native)
2601.09028v1_OpenDecoder Open Large Language Model Decoding to .pdf,OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG,"['Fengran Mo', 'Zhan Su', 'Yuchen Hui', 'Jianhan Zhang', 'Jia Ao Sun', 'Zheyuan Liu', 'Chao Zhang', 'Tetsuya Sakai', 'Jian-Yun Nie']","The paper introduces OpenDecoder, a novel approach to enhance the robustness of Retrieval-Augmented Generation (RAG) models by incorporating explicit evaluation of retrieved information as quality indicators. It addresses the variability in relevance and usefulness of retrieved documents by using relevance score, ranking score, and QPP (query performance prediction) score. The proposed method demonstrates improved performance and robustness across five benchmark datasets, outperforming baseline methods. The approach is flexible and can be integrated with post-training of large language models for various purposes.",157.57,Phi-4,Nvidia B200 (Cloud Native)
2601.09029v1_Proactively Detecting Threats A Novel Approach Usi.pdf,Proactively Detecting Threats: A Novel Approach Using LLMs,"['Aniesh Chawla', 'Udbhav Prasad']","This paper presents a systematic evaluation of large language models (LLMs) for proactively identifying indicators of compromise (IOCs) from unstructured web-based threat intelligence sources. The authors developed an automated system to extract IOCs from 15 web-based threat report sources and evaluated six LLM models, including Gemini, Qwen, and Llama variants. The study analyzed 479 webpages containing 2,658 IOCs, revealing significant performance variations among the models. Gemini 1.5 Pro demonstrated high precision and perfect recall for malicious IOC identification. The research emphasizes the potential of LLMs in shifting cybersecurity from reactive to proactive measures.",158.02,Phi-4,Nvidia B200 (Cloud Native)
2601.09031v1_Generalizable Geometric Prior and Recurrent Spikin.pdf,Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation,"['Xuetao Li', 'Wenke Huang', 'Mang Ye', 'Jifeng Xuan', 'Bo Du', 'Sheng Liu', 'Miao Li']","This paper introduces a novel framework, RGMP-S, which combines Recurrent Geometric-prior Multimodal Policy with Spiking features to enhance humanoid robot manipulation. The framework addresses challenges in high-level semantic reasoning and low-level action generation by leveraging 2D geometric inductive biases for precise 3D scene understanding and a Recursive Adaptive Spiking Network for efficient motion synthesis. The approach is validated through extensive experiments on various robotic systems, demonstrating superior performance over existing methods in diverse generalization scenarios.",157.86,Phi-4,Nvidia B200 (Cloud Native)
2601.09032v1_The Hierarchy of Agentic Capabilities Evaluating F.pdf,The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments,"['Logan Ritchie', 'Sushant Mehta', 'Nick Heiner', 'Mason Yu', 'Edwin Chen']","This paper presents an empirical study evaluating frontier AI models on 150 workplace tasks within a realistic e-commerce RL environment. The study identifies a hierarchy of agentic capabilities necessary for real-world deployment: tool use, planning and goal formation, adaptability, groundedness, and common-sense reasoning. Despite advancements, even the best models fail approximately 40% of tasks, with failures aligning with this hierarchy. The paper introduces a task-centric design methodology for RL environments, provides detailed failure analysis, and discusses implications for agent development, highlighting the capability gaps before achieving human-level task completion in realistic workplace settings.",157.86,Phi-4,Nvidia B200 (Cloud Native)
2601.09035v1_A Decompilation-Driven Framework for Malware Detec.pdf,A Decompilation-Driven Framework for Malware Detection with Large Language Models,"['Aniesh Chawla', 'Udbhav Prasad']","This paper explores the use of Large Language Models (LLMs) for malware detection by decompiling Windows executables into C code using the Ghidra disassembler. The study evaluates the efficacy of LLMs in classifying code as benign or malicious, revealing that while standard LLMs show promise, they are not yet robust enough to replace traditional antivirus software. A fine-tuned model trained on curated datasets significantly outperforms standard models, but its performance degrades with newer malware, highlighting the need for continuous fine-tuning to maintain effectiveness against evolving threats.",158.44,Phi-4,Nvidia B200 (Cloud Native)
2601.09041v1_Can LLMs interpret figurative language as humans d.pdf,CANLLMs Interpret Figurative Language as Humans Do?: Surface-Level vs. Representational Similarity,"['Samhita Bollepally', 'Aurora Sloman-Moll', 'Takashi Yamauchi']","This study investigates the alignment between human judgments and those of large language models (LLMs) in interpreting figurative and socially grounded language. Human participants and four instruction-tuned LLMs (GPT-4, Gemma-2-9B, Llama-3.2, and Mistral-7B) rated 240 dialogue-based sentences on six linguistic traits: conventionality, sarcasm, funny, emotional, idiomacy, and slang. The results showed that while humans and LLMs aligned at a surface level, they diverged significantly at a representational level, particularly with figurative sentences involving idioms and Gen Z slang. GPT-4 most closely approximated human representational patterns, but all models struggled with context-dependent and socio-pragmatic expressions like sarcasm, slang, and idiomacy. The study highlights the challenges LLMs face in capturing human-like linguistic structures that require pragmatic inference.",157.92,Phi-4,Nvidia B200 (Cloud Native)
2601.09049v1_Is Grokking Worthwhile Functional Analysis and Tra.pdf,Is Grokking Worthwhile? Functional Analysis and Transferability of Generalization Circuits in Transformers,"['Kaiyu He', 'Mian Zhang', 'Peilin Wu', 'Xinya Du', 'Zhiyu Zoey Chen']","This paper investigates whether the 'grokking' phase in parameter-sharing transformers, which leads to the formation of a 'Generalization Circuit', results in superior performance on downstream tasks compared to non-grokked models. The study explores the role of the Generalization Circuit in knowledge assimilation and transfer, demonstrating that grokked and non-grokked models use identical inference paths for in-distribution queries. It also shows that high accuracy on unseen cases and the formation of a reasoning path can occur independently, and that mature circuits have limited transferability when integrating new knowledge. The findings suggest that grokked transformers do not fully master compositional logic.",158.02,Phi-4,Nvidia B200 (Cloud Native)
2601.09066v1_Midm 2.0 Korea-centric Bilingual Language Models.pdf,Mi:dm 2.0: Korea-centric Bilingual Language Models,"['Tech. Innovation Group, KT']","Mi:dm 2.0 is a bilingual large language model (LLM) designed to enhance Korea-centric AI by integrating Korean societal values, reasoning patterns, and commonsense knowledge. It addresses the limitations of existing LLMs, which often suffer from insufficient or low-quality Korean data and lack cultural alignment. Mi:dm 2.0 emphasizes robust data quality through proprietary data cleansing, high-quality synthetic data generation, strategic data mixing with curriculum learning, and a custom Korean-optimized tokenizer. The model offers two configurations: Mi:dm 2.0 Base (11.5B parameters) for general-purpose use and Mi:dm 2.0 Mini (2.3B parameters) for resource-constrained environments. It achieves state-of-the-art performance in Korean-specific benchmarks and supports extensive research and commercial use under the MIT license. The models are available on Hugging Face and aim to accelerate AI adoption in Korean industries, public services, and education.",158.29,Phi-4,Nvidia B200 (Cloud Native)
2601.09069v1_From Symbolic to Natural-Language Relations Rethin.pdf,From Symbolic to Natural-Language Relations: Rethinking Knowledge Graph Construction in the Era of Large Language Models,"['Kanyao Han', 'Yushang Lai']","This paper discusses the limitations of traditional knowledge graphs (KGs) that use predefined symbolic relation schemas, which often fail to capture the nuanced and contextual nature of real-world relationships. The authors argue that the advent of large language models (LLMs) necessitates a shift from symbolic to natural-language relation descriptions. They propose hybrid design principles that maintain a minimal structural backbone while allowing for more flexible and context-sensitive relational representations. This approach aims to better align with the capabilities of LLMs, which can synthesize domain facts in natural language and favor context-rich text over quantified representations.",158.08,Phi-4,Nvidia B200 (Cloud Native)
2601.09072v1_Human-AI Co-design for Clinical Prediction Models.pdf,Human-AI Co-design for Clinical Prediction Models,"['Jean Feng', 'Avni Kothari', 'Patrick Vossler', 'Andrew Bishara', 'Lucas Zier', 'Newton Addo', 'Aaron Kornblith', 'Yan Shuo Tan', 'Chandan Singh']","The paper introduces HACHI, an iterative human-in-the-loop framework designed to accelerate the development of interpretable clinical prediction models (CPMs) by leveraging AI agents. HACHI facilitates the exploration of concepts in clinical notes, alternating between AI-driven exploration and expert feedback to refine the CPMs. It defines concepts as simple yes-no questions for use in linear models, allowing for transparent review and validation. HACHI demonstrates superior performance in real-world tasks, such as predicting acute kidney injury and traumatic brain injury, by surfacing new clinically relevant concepts and improving model generalizability. The framework also highlights the critical role of clinical AI teams in guiding AI agents, adjusting concept granularity, aligning objectives with clinical goals, and addressing data bias and leakage.",157.86,Phi-4,Nvidia B200 (Cloud Native)
2601.09085v1_MMR-GRPO Accelerating GRPO-Style Training through .pdf,MMR-GRPO: Accelerating GRPO-Style Training through Diversity-Aware Reward Reweighting,"['Kangda Wei', 'Ruihong Huang']","The paper introduces MMR-GRPO, a method that enhances the efficiency of training mathematical reasoning models using Group Relative Policy Optimization (GRPO). By integrating Maximal Marginal Relevance (MMR) to reweight rewards based on the diversity of completions, MMR-GRPO reduces the number of training steps and wall-clock time required to achieve peak performance. The approach prioritizes diverse solutions over semantically redundant ones, leading to more informative updates and faster convergence. Extensive evaluations demonstrate that MMR-GRPO achieves comparable performance with significantly fewer training steps and less time across various model sizes, GRPO variants, and benchmarks.",156.93,Phi-4,Nvidia B200 (Cloud Native)
2601.09089v1_SubTokenTest A Practical Benchmark for Real-World .pdf,SUBTOKENTEST: A Practical Benchmark for Real-World Sub-token Understanding,"['Shuyang Hou', 'Yi Hu', 'Muhan Zhang']","This paper introduces SUBTOKENTEST, a benchmark designed to evaluate the sub-token understanding capabilities of large language models (LLMs). Despite advancements in LLMs' reasoning abilities, they struggle with basic character-level tasks due to their tokenization processes. SUBTOKENTEST assesses these capabilities through practical tasks across four domains, isolating tokenization-related failures from complex reasoning. The benchmark evaluates nine advanced LLMs, investigates the impact of test-time scaling on sub-token reasoning, and explores how character-level information is encoded within hidden states.",157.86,Phi-4,Nvidia B200 (Cloud Native)
2601.09097v1_Programming over Thinking Efficient and Robust Mul.pdf,Programming over Thinking: Efficient and Robust Multi-Constraint Planning,"['Derrick Goh Xin Deik', 'Quanyu Long', 'Zhengyuan Liu', 'Nancy F. Chen', 'Wenya Wang']","This paper introduces the ScalableCOdePlanning Engine (SCOPE), a framework designed to address the limitations of existing large language model (LLM) approaches in multi-constraint planning. SCOPE separates query-specific reasoning from generic code execution, producing consistent, deterministic, and reusable solver functions. It achieves state-of-the-art performance with reduced cost and latency, exemplified by a 93.1% success rate on the TravelPlanner task using GPT-4o, surpassing the best baseline by 61.6% while reducing inference cost by 1.4x and time by 4.67x.",158.05,Phi-4,Nvidia B200 (Cloud Native)
2601.09100v2_DScheLLM Enabling Dynamic Scheduling through a Fin.pdf,schellm: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large Language Model,"['Lixiang Zhang', 'Chenggong Zhao', 'Qing Gao', 'Xiaoke Zhao', 'Gengyi Bai', 'Jinhu Lv']","This paper introduces DScheLLM, a dynamic scheduling approach leveraging fine-tuned large language models within a dual-system reasoning architecture to address disturbances in job shop scheduling. The approach uses a unified framework to handle dynamic events, with training datasets generated from exact schedules obtained from an operations research solver. The Huawei OpenPangu Embedded-7B model is fine-tuned using LoRA for hybrid reasoning paradigms. Experimental evaluations on standard job shop scheduling benchmarks show that the fast-thinking mode efficiently generates high-quality schedules, while the slow-thinking mode produces solver-compatible decision inputs. This study highlights the potential of large language models for intelligent and adaptive scheduling optimization in dynamic environments.",158.08,Phi-4,Nvidia B200 (Cloud Native)
2601.09105v2_AviationLMM A Large Multimodal Foundation Model fo.pdf,AviationLMM: A Large Multimodal Foundation Model for Civil Aviation,"['Wenbin Li', 'Jingling Wu', 'Xiaoyong Lin', 'Jing Chen', 'Cong Chen']","This paper introduces AviationLMM, a Large Multimodal Foundation Model designed for civil aviation. It aims to unify heterogeneous data streams such as voice communications, radar tracks, sensor streams, and textual reports to enhance situational awareness, adaptability, and real-time decision support. The paper identifies gaps in existing AI solutions, describes the model architecture for multimodal input integration, and outlines key research opportunities including data acquisition, alignment, fusion, pretraining, reasoning, trustworthiness, privacy, robustness, and synthetic scenario generation. The goal is to advance civil aviation foundation models and foster an integrated, trustworthy, and privacy-preserving aviation AI ecosystem.",157.89,Phi-4,Nvidia B200 (Cloud Native)
2601.09113v1_The AI Hippocampus How Far are We From Human Memor.pdf,The AI Hippocampus: How Far are We From Human Memory?,"['Zixia Jia', 'Jiaqi Li', 'Yipeng Kang', 'Yuxuan Wang', 'Tong Wu', 'Quansen Wang', 'Xiaobo Wang', 'Shuyi Zhang', 'Junzhe Shen', 'Qing Li', 'Siyuan Qi', 'Yitao Liang', 'Di He', 'Zilong Zheng', 'Song-Chun Zhu']","This survey explores the role of memory in enhancing the reasoning, adaptability, and contextual fidelity of modern Large Language Models (LLMs) and Multi-Modal LLMs (MLLMs). It organizes the literature into a taxonomy of implicit, explicit, and agentic memory paradigms. Implicit memory refers to knowledge embedded within pre-trained transformers, while explicit memory involves external storage and retrieval components. Agentic memory introduces persistent memory structures within autonomous agents. The survey also examines memory integration in multi-modal settings and discusses key architectural advances, benchmark tasks, and open challenges, including memory capacity, alignment, factual consistency, and cross-system interoperability. The goal is to inform the development of memory-augmented (M)LLMs that are more flexible, context-sensitive, and aligned with real-world intelligent systems.",157.52,Phi-4,Nvidia B200 (Cloud Native)
2601.09116v1_LP-LLM End-to-End Real-World Degraded License Plat.pdf,LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models,"['Haoyan Gong', 'Hongbin Liu']","This paper addresses the challenges faced by Real-world License Plate Recognition (LPR) systems due to severe degradations like motion blur, low resolution, and complex illumination. The traditional 'restoration-then-recognition' approach is critiqued for its misalignment between image restoration and character recognition goals, leading to errors. The authors propose an end-to-end structure-aware multimodal reasoning framework using Qwen3-VL, featuring a Character-Aware Multimodal Reasoning Module (CMRM) with learnable Character Slot Queries. These queries retrieve fine-grained evidence from visual features, which are then integrated back into visual tokens for autoregressive generation by the language model. The method, enhanced by LoRA fine-tuning, shows superior performance on degraded datasets compared to existing methods, demonstrating the effectiveness of structured reasoning in large models for low-quality text recognition tasks.",156.78,Phi-4,Nvidia B200 (Cloud Native)
2601.09117v1_A Marketplace for AI-Generated Adult Content and D.pdf,A Marketplace for AI-Generated Adult Content and Deepfakes,"['Shalmoli Ghosh', 'Matthew R. DeVerna', 'Filippo Menczer']","This paper examines Civitai, a community-driven platform for AI-generated content, focusing on its monetized feature called Bounties. The study conducts a longitudinal analysis of bounty requests over 14 months, revealing a dominance of tools that steer AI models toward unintended content. Notably, 'Not Safe For Work' content requests have increased, now comprising a majority of bounties. The study highlights a concentration of deepfake requests, often involving explicit content against platform policies, disproportionately targeting female celebrities. These findings underscore the gendered social harms and raise questions about consent, governance, and enforcement in generative AI platforms.",157.97,Phi-4,Nvidia B200 (Cloud Native)
2601.09120v1_Adaptive Multi-Stage Patent Claim Generation with .pdf,Adaptive Multi-Stage Patent Claim Generation with Unified Quality Assessment,"['Chen-Wei Liang', 'Bin Guo', 'Zhen-Yuan Wei', 'Mu-Jiang-Shan Wang']","This paper introduces a novel three-stage framework to address the limitations of current patent claim generation systems, which include poor cross-jurisdictional generalization, inadequate semantic relationship modeling between claims and prior art, and unreliable quality assessment. The proposed framework utilizes relationship-aware similarity analysis, domain-adaptive claim generation, and unified quality assessment. It employs multi-head attention with eight specialized heads for explicit relationship modeling, integrates curriculum learning with dynamic LoRA adapter selection across five patent domains, and implements cross-attention mechanisms between evaluation aspects for comprehensive quality assessment. The approach demonstrates substantial improvements in experiments on various datasets, achieving a 7.6-point ROUGE-L gain over GPT-4o, an 8.3% BERTScore enhancement over Llama-3.1-8B, and a 0.847 correlation with human experts compared to 0.623 for separate evaluation models. It also maintains 89.4% cross-jurisdictional performance retention versus 76.2% for baselines, establishing a comprehensive solution for automated patent prosecution workflows.",157.99,Phi-4,Nvidia B200 (Cloud Native)
2601.09130v1_Equi-ViT Rotational Equivariant Vision Transformer.pdf,EQUI-VIT: ROTATIONAL EQUIVARIANT VISION TRANSFORMER FOR ROBUST HISTOPATHOLOGY ANALYSIS,"['Fuyao Chen', 'Yuexi Du', 'Eléonore V. Lieffrig', 'Nicha C. Dvornek', 'John A. Onofrey']","This paper introduces Equi-ViT, a novel Vision Transformer architecture designed to address the lack of rotational equivariance in standard ViTs, which is a significant limitation in histopathology analysis. By integrating an equivariant convolution kernel into the patch embedding stage, Equi-ViT achieves rotation-consistent patch embeddings and stable classification performance across different image orientations. The proposed method demonstrates enhanced data efficiency and robustness on a public colorectal cancer dataset, suggesting its potential as a more generalizable backbone for digital pathology applications.",158.3,Phi-4,Nvidia B200 (Cloud Native)
2601.09136v1_SkinFlow Efficient Information Transmission for Op.pdf,SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL,"['Lijun Liu', 'Linwei Chen', 'Zhishou Zhang', 'Meng Tian', 'Hengfu Cui', 'Ruiyang Li', 'Zhaocheng Liu', 'Qiang Ju', 'Qianxi Li', 'Hong-Yu Zhou']","This paper introduces SkinFlow, a framework designed to enhance dermatological diagnosis by optimizing visual information transmission efficiency. It challenges the notion that scaling parameters is the sole path to medical precision. SkinFlow employs a Virtual-Width Dynamic Vision Encoder (DVE) to unfold complex pathological manifolds without expanding physical parameters, alongside a two-stage Reinforcement Learning strategy. This strategy aligns medical descriptions and reconstructs diagnostic textures within a constrained semantic space. The paper also proposes a clinically grounded evaluation protocol prioritizing diagnostic safety and relevance. Empirical results show that a 7B model using SkinFlow achieves significant improvements in accuracy on the Fitzpatrick17k benchmark compared to general-purpose models, demonstrating the benefits of optimizing geometric capacity and information flow over mere parameter scaling.",157.77,Phi-4,Nvidia B200 (Cloud Native)
2601.09147v2_SSVP Synergistic Semantic-Visual Prompting for Ind.pdf,Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection,"['Chenhao Fu', 'Han Fang', 'Xiuzheng Zheng', 'Wenbo Wei', 'Yonghua Li', 'Hao Sun', 'Xuelong Li']","This paper introduces Synergistic Semantic-Visual Prompting (SSVP) to enhance Zero-Shot Anomaly Detection (ZSAD) in industrial settings. SSVP addresses the limitations of existing methods by fusing diverse visual encodings, integrating DINOv3’s multi-scale structural priors into the CLIP semantic space, and employing a Vision-Conditioned Prompt Generator (VCPG) for dynamic prompt generation. Additionally, the Visual-Text Anomaly Mapper (VTAM) establishes a dual-gated calibration paradigm to align global scoring with local evidence. Extensive evaluations demonstrate SSVP's robustness, achieving state-of-the-art performance on industrial benchmarks, notably with 93.0% Image-AUROC and 92.2% Pixel-AUROC on MVTec-AD.",156.71,Phi-4,Nvidia B200 (Cloud Native)
2601.09152v1_PrivacyReasoner Can LLM Emulate a Human-like Priva.pdf,PrivacyReasoner: Can LLM Emulate a Human-like Privacy Mind?,"['Yiwen Tu', 'Xuan Liu', 'Lianhui Qin', 'Haojian Jin']","This paper introduces PrivacyReasoner, an AI-agent designed to simulate how individual users form privacy concerns in response to real-world news. It integrates privacy and cognitive theories to model user-specific privacy reasoning based on personal comment histories and contextual cues. The agent reconstructs each user's 'privacy mind,' dynamically activates context-relevant privacy memory, and generates synthetic comments reflecting likely user responses to new privacy scenarios. A complementary LLM-as-a-Judge evaluator, calibrated against an established privacy concern taxonomy, quantifies the faithfulness of the generated reasoning. Experiments on real-world Hacker News discussions demonstrate that PrivacyReasoner outperforms baseline agents in privacy concern prediction and captures transferable reasoning patterns across domains such as AI, e-commerce, and healthcare.",158.02,Phi-4,Nvidia B200 (Cloud Native)
2601.09156v1_KTCF Actionable Recourse in Knowledge Tracing via .pdf,KTCF: Actionable Recourse in Knowledge Tracing via Counterfactual Explanations for Education,"['Woojin Kim', 'Changkwon Lee', 'Hyeoncheol Kim']","This paper introduces KTCF, a method for generating counterfactual explanations in Knowledge Tracing (KT) to provide actionable recourse in education. Counterfactual explanations are causal, local, and easily understandable, making them suitable for educational stakeholders who are often non-experts. KTCF accounts for knowledge concept relationships and includes a post-processing scheme to convert explanations into educational instructions. Experiments on a large-scale educational dataset demonstrate that KTCF outperforms existing methods, with improvements ranging from 5.7% to 34% across various metrics. The paper also highlights the potential of counterfactuals to advance responsible AI use in education and suggests future work on educationally grounded XAI for KT.",158.19,Phi-4,Nvidia B200 (Cloud Native)
2601.09182v1_Position on LLM-Assisted Peer Review Addressing Re.pdf,Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback,"['JungMin Yun', 'JuneHyoung Kwon', 'MiHyeon Kim', 'YoungBin Kim']","This position paper addresses the 'Reviewer Gap' in AI research peer review, caused by rapid submission growth and insufficient reviewer expertise. It critiques existing LLM approaches that generate reviews automatically and proposes a paradigm shift to use LLMs as tools for assisting and educating human reviewers. The paper introduces two systems: an LLM-assisted mentoring system to develop long-term reviewer competencies and an LLM-assisted feedback system to enhance review quality. This human-centered approach aims to strengthen reviewer expertise and contribute to a sustainable scholarly ecosystem.",157.86,Phi-4,Nvidia B200 (Cloud Native)
2601.09195v1_ProFit Leveraging High-Value Signals in SFT via Pr.pdf,ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection,"['Tao Liu', 'Taiqiang Wu', 'Runming Yang', 'Shaoning Sun', 'Junjie Wang', 'Yujiu Yang']","The paper introduces ProFit, a novel approach to supervised fine-tuning (SFT) for Large Language Models (LLMs) that addresses the issue of overfitting to single reference answers. Traditional SFT methods often force alignment with a single reference, leading to overfitting on non-core expressions. ProFit leverages the intrinsic connection between token probability and semantic importance, selectively masking low-probability tokens to prevent surface-level overfitting. This method maintains the efficiency of single-reference training while capturing core semantic integrity, outperforming traditional SFT baselines in general reasoning and mathematical benchmarks.",157.48,Phi-4,Nvidia B200 (Cloud Native)
2601.09208v2_Mikasa A Character-Driven Emotional AI Companion I.pdf,Mikasa: A Character-Driven Emotional AI Companion,['Miki Ueno'],"This paper explores the design of AI companions with a focus on character-driven emotional engagement, inspired by Japanese Oshi culture. It presents Mikasa, an AI companion with a stable personality and defined relationship, as a case study. The paper argues that character coherence and relationship definition are crucial for sustained user satisfaction and engagement, beyond technical capabilities. The findings suggest that these elements serve as latent structural components that enhance interaction quality, even if users do not explicitly recognize them.",158.37,Phi-4,Nvidia B200 (Cloud Native)
2601.09212v1_Annealed Relaxation of Speculative Decoding for Fa.pdf,Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image Generation,"['Xingyao Li', 'Fengzhuo Zhang', 'Cunxiao Du', 'Hui Ji']","This paper addresses the slow inference speed of auto-regressive (AR) image generation models due to their sequential nature and token ambiguity. It introduces COOL-SD, an annealed relaxation of speculative decoding (SD), which is grounded in theoretical analysis. COOL-SD minimizes the total variation distance between the target model and relaxed SD by optimizing the resampling distribution and incorporates an annealing behavior to enhance performance. The method achieves faster image generation with comparable quality or better quality at similar latency. Experimental results demonstrate COOL-SD's effectiveness in improving speed-quality trade-offs over previous methods.",154.38,Phi-4,Nvidia B200 (Cloud Native)
2601.09213v1_SpikeVAEDiff Neural Spike-based Natural Visual Sce.pdf,SpikeV AEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-V AE and Versatile Diffusion,"['Jialu Li', 'Taiyan Zhou']","This paper introduces SpikeVAEDiff, a novel two-stage framework combining a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to reconstruct high-resolution and semantically meaningful images from neural spike data. The first stage uses VDVAE to create low-resolution reconstructions from neural spikes, while the second stage refines these images using regression models and Versatile Diffusion. The study evaluates the framework on the Allen Visual Coding—Neuropixels dataset, highlighting the VISI region's significant role in visual reconstruction. The findings suggest that spike data, with its superior temporal and spatial resolution, is a promising alternative to fMRI data for visual neural decoding. The paper also validates the effectiveness of the VDVAE model and explores the impact of different brain regions on reconstruction quality.",154.45,Phi-4,Nvidia B200 (Cloud Native)
2601.09233v1_GIFT Unlocking Global Optimality in Post-Training .pdf,GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization,"['Zhengyang Zhao', 'Lu Ma', 'Yizhen Jiang', 'Xiaochen Ma', 'Zimo Meng', 'Chengyu Shen', 'Lexiang Tang', 'Haoze Sun', 'Peng Pei', 'Wentao Zhang']","The paper addresses the optimization mismatch in the post-training paradigm for Large Reasoning Models (LRMs), which typically involves Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL). The authors propose Gibbs Initialization with Finite Temperature (GIFT) to reformulate SFT, establishing a distributional bridge that maintains objective consistency throughout the post-training pipeline. GIFT is shown to significantly outperform standard SFT and other baselines in RL initialization, offering a principled approach to achieving global optimality in post-training.",153.69,Phi-4,Nvidia B200 (Cloud Native)
2601.09236v2_Reward Learning through Ranking Mean Squared Error.pdf,REWARD LEARNING THROUGH RANKING MEANS SQUARED ERROR,"['Chaitanya Kharyal', 'Calarina Muslimani', 'Matthew E. Taylor']","This paper introduces a novel rating-based reinforcement learning method called Ranked Return Regression for RL (R4). R4 uses a ranking mean squared error (rMSE) loss to treat teacher-provided ratings as ordinal targets. The method learns from trajectory-rating pairs, optimizing a mean squared error loss between predicted soft ranks and teacher ratings. R4 provides formal guarantees of minimal and complete solution sets under mild assumptions. Empirical results show that R4 matches or outperforms existing rating and preference-based RL methods on robotic locomotion benchmarks, requiring significantly less feedback.",153.73,Phi-4,Nvidia B200 (Cloud Native)
2601.09239v2_DSA-Tokenizer Disentangled Semantic-Acoustic Token.pdf,DSA-Tokenizer: Disentangled Semantic-Acoustic Tokenization via Flow Matching-based Hierarchical Fusion,"['Hanlin ZHANG', 'Daxin Tan', 'Dehua Tao', 'Xiao Chen', 'Haochen Tan', 'Yunhe Li', 'Yuchen Cao', 'Jianping Wang', 'Linqi Song']","The paper introduces DSA-Tokenizer, a novel approach to disentangle speech into discrete semantic and acoustic tokens. This tokenizer uses distinct optimization constraints to separate linguistic content and acoustic style, enhancing speech generation quality. A hierarchical Flow-Matching decoder is introduced to address length constraints between sequences, and a joint reconstruction-recombination training strategy is employed to enforce separation. The DSA-Tokenizer facilitates high fidelity reconstruction and flexible recombination, enabling controllable generation in speech LLMs. The paper emphasizes the importance of disentangled tokenization for future speech modeling.",157.05,Phi-4,Nvidia B200 (Cloud Native)
2601.09248v1_Hybrid guided variational autoencoder for visual p.pdf,Hybrid guided variational autoencoder for visual place recognition,"['Ni Wang', 'Zihan You', 'Emre Neftci', 'Thorben Schoepe']","This paper addresses the challenge of visual place recognition (VPR) for autonomous agents like cars, robots, and drones, particularly in GPS-denied indoor environments. The authors propose a hybrid guided variational autoencoder (VAE) that leverages event-based vision sensors and a spiking neural network model. This approach aims to reduce memory requirements while maintaining robustness and generalization capabilities. The model successfully disentangles visual features of 16 distinct places in a new indoor VPR dataset, achieving classification performance comparable to state-of-the-art methods. It demonstrates robustness under various illumination conditions and can generalize to novel visual inputs from unknown scenes. The compact and robust nature of the guided VAE makes it a promising model for enhancing mobile robot navigation in both known and unknown indoor environments.",158.06,Phi-4,Nvidia B200 (Cloud Native)
2601.09251v1_HGATSolver A Heterogeneous Graph Attention Solver .pdf,HGA TSolver: A Heterogeneous Graph Attention Solver for Fluid–Structure Interaction,"['Qin-Yi Zhang', 'Hong Wang', 'Siyao Liu', 'Haichuan Lin', 'Linying Cao', 'Xiao-Hu Zhou', 'Chen Chen', 'Shuangyi Wang', 'Zeng-Guang Hou']","This paper introduces the Heterogeneous Graph Attention Solver (HGA TSolver) for addressing the challenges in Fluid–Structure Interaction (FSI) systems. FSI systems involve distinct physical domains governed by different partial differential equations and coupled at a dynamic interface. Existing learning-based solvers struggle with capturing the heterogeneous dynamics within a unified framework. The proposed HGA TSolver encodes the system as a heterogeneous graph, embedding physical structures via distinct node and edge types for fluid, solid, and interface regions. It employs specialized message-passing mechanisms tailored to each domain and introduces a physics-conditioned gating mechanism for stabilizing explicit time stepping. Additionally, an Interdomain Gradient-Balancing Loss is used to balance optimization objectives across domains based on predictive uncertainty. Extensive experiments demonstrate that HGA TSolver achieves state-of-the-art performance in surrogate modeling of coupled multi-physics systems.",155.16,Phi-4,Nvidia B200 (Cloud Native)
2601.09253v1_RIFT Repurposing Negative Samples via Reward-Infor.pdf,RIFT: Repurposing Negative Samples via Reward-Informed Fine-Tuning,"['Zehua Liu', 'Shuqi Liu', 'Tao Zhong', 'Mingxuan Yuan']","The paper introduces Reward Informed Fine-Tuning (RIFT) as a novel framework to enhance the alignment of Large Language Models (LLMs) by efficiently utilizing both positive and negative samples. Unlike traditional methods such as Supervised Fine-Tuning (SFT) and Rejection Sampling Fine-Tuning (RFT), which either depend on costly expert data or discard negative samples, RIFT repurposes these negative samples by reweighting the loss with scalar rewards. This approach addresses the issue of training collapse caused by naive reward integration and ensures numerical robustness and optimization efficiency. Extensive experiments demonstrate that RIFT consistently outperforms RFT across various mathematical benchmarks, showcasing its robustness and data efficiency.",157.36,Phi-4,Nvidia B200 (Cloud Native)
2601.09259v1_MAXS Meta-Adaptive Exploration with LLM Agents.pdf,MAXS: Meta-Adaptive Exploration with LLM Agents,"['Jian Zhang', 'Zhiyuan Wang', 'Zhangqi Wang', 'Yu He', 'Haoran Luo', 'Li Yuan', 'Lingling Zhang', 'Rui Mao', 'Qika Lin', 'Jun Liu']","The paper introduces MAXS, a meta-adaptive reasoning framework for LLM Agents, addressing issues of locally myopic generation and trajectory instability. MAXS integrates tool execution and reasoning planning with a lookahead strategy to extend reasoning paths and select stable, consistent, and high-value steps. It also includes a trajectory convergence mechanism to balance computational efficiency and global effectiveness. Empirical studies show MAXS outperforms existing methods in performance and inference efficiency across various models and datasets.",157.49,Phi-4,Nvidia B200 (Cloud Native)
2601.09260v1_Efficient Paths and Dense Rewards Probabilistic Fl.pdf,Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models,"['Yan Liu', 'Feng Zhang', 'Zhanyu Ma', 'Jun Xu', 'Jiuchong Gao', 'Jinghua Hao', 'Renqing He', 'Han Liu', 'Yangdong Deng']","This paper introduces CoT-Flow, a framework that reimagines discrete reasoning steps in large language models as a continuous probabilistic flow. This approach quantifies the contribution of each step towards the correct answer, addressing the limitations of current reasoning paradigms that treat reasoning as an indivisible sequence. CoT-Flow offers two methodologies: flow-guided decoding for efficient reasoning paths and flow-based reinforcement learning for a verifier-free dense reward function. Experiments demonstrate CoT-Flow's superior balance between inference efficiency and reasoning performance on challenging benchmarks.",157.62,Phi-4,Nvidia B200 (Cloud Native)
2601.09262v1_Magnifying change Rapid burn scar mapping with mul.pdf,"Magnifying change: Rapid burn scar mapping with multi-resolution, multi-source satellite imagery","['Maria Sdraka', 'Dimitrios Michail', 'Ioannis Papoutsis']","This paper introduces a novel deep learning model, BAM-MRCD, designed to rapidly delineate wildfire-affected areas using multi-resolution, multi-source satellite imagery from MODIS and Sentinel-2. The model addresses the challenge of balancing spatial resolution and temporal revisit frequency in satellite systems, enabling the timely production of detailed burnt area maps. It demonstrates high accuracy in detecting small-scale wildfires, outperforming existing change detection models and baselines. The approach is particularly relevant for operational settings requiring quick post-wildfire assessments to aid in response and recovery efforts.",157.75,Phi-4,Nvidia B200 (Cloud Native)
2601.09264v1_Coordinated Pandemic Control with Large Language M.pdf,Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants,"['Ziyi Shi', 'Xusen Guo', 'Hongliang Lu', 'Mingxing Peng', 'Haotian Wang', 'Zheng Zhu', 'Zhenning Li', 'Yuxuan Liang', 'Xinhu Zheng', 'Hai Yang']","This paper proposes a large language model (LLM) multi-agent policymaking framework to enhance coordinated and proactive pandemic control across regions. Each administrative region is assigned an LLM agent to reason over region-specific epidemiological dynamics and communicate with other agents to account for cross-regional interdependencies. The framework integrates real-world data, a pandemic evolution simulator, and structured inter-agent communication, enabling agents to explore counterfactual intervention scenarios and synthesize coordinated policy decisions. Validation using state-level COVID-19 data from the United States between April and December 2020 shows that the approach can significantly reduce cumulative infections and deaths compared to real-world outcomes. The study demonstrates the potential of LLM multi-agent systems in improving pandemic control and offers a generalizable framework for large-scale public policy settings.",157.63,Phi-4,Nvidia B200 (Cloud Native)
2601.09269v2_RISER Orchestrating Latent Reasoning Skills for Ad.pdf,RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering,"['Wencheng Ye', 'Xiaoyang Yuan', 'Yi Bin', 'Hengyu Jin', 'Liang Peng', 'Pengpeng Zeng', 'Heng Tao Shen']","The paper introduces RISER, a framework designed to enhance reasoning in large language models (LLMs) through adaptive activation steering. Unlike static methods, RISER dynamically composes reasoning vectors using a lightweight Router optimized via reinforcement learning. This approach improves zero-shot accuracy and token efficiency across various benchmarks, demonstrating more controllable and efficient LLM reasoning without extensive retraining.",153.47,Phi-4,Nvidia B200 (Cloud Native)
2601.09274v1_A3-Bench Benchmarking Memory-Driven Scientific Rea.pdf,A3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation,"['Jian Zhang', 'Yu He', 'Zhiyuan Wang', 'Zhangqi Wang', 'Kai He', 'Fangzhi Xu', 'Qika Lin', 'Jun Liu']","The paper introduces A3-Bench, a benchmark designed to evaluate scientific reasoning through memory-driven mechanisms, specifically focusing on anchor and attractor activation. It addresses the gap in existing benchmarks that overlook memory-driven reasoning processes. A3-Bench annotates 2,198 science reasoning problems using the SAPM process and introduces a dual-scale memory evaluation framework with the AAUI metric to measure memory activation rates. The paper validates A3-Bench through experiments with various models, demonstrating the impact of memory activation on reasoning performance and providing insights into memory-driven scientific reasoning.",157.67,Phi-4,Nvidia B200 (Cloud Native)
2601.09278v1_M3Searcher Modular Multimodal Information Seeking .pdf,M3Searcher: Modular Multimodal Information Seeking Agency with Retrieval-Oriented Reasoning,"['Xiaohan Yu', 'Chao Feng', 'Lang Mei', 'Chong Chen']","The paper introduces M3Searcher, a modular multimodal information-seeking agent designed to address the challenges of extending autonomous information-seeking agents to multimodal settings. M3Searcher decouples information acquisition from answer derivation and is optimized with a retrieval-oriented multi-objective reward. The paper also presents MM-SearchVQA, a dataset to support retrieval-centric reinforcement learning training. Experimental results show that M3Searcher outperforms existing approaches, demonstrating strong transfer adaptability and effective reasoning in complex multimodal tasks.",158.11,Phi-4,Nvidia B200 (Cloud Native)
2601.09280v1_ReGraM Region-First Knowledge Graph Reasoning for .pdf,ReGraM: Region-First Knowledge Graph Reasoning for Medical Question Answering,"['Chaerin Lee', 'Sohee Park', 'Hyunsik Na', 'Daseon Choi']","This paper introduces ReGraM, a region-first knowledge graph reasoning framework designed to improve factual accuracy in medical question answering (QA). Traditional approaches often rely on traversing entire knowledge graphs (KGs) or performing large-scale retrieval, which introduces noise and leads to unstable multi-hop reasoning. ReGraM addresses this by constructing a query-aligned subgraph and performing stepwise reasoning within this localized region under multiple evidence-aware modes. This approach focuses inference on the most relevant portion of the KG, departing from the assumption that all relations are equally useful. Experiments on seven medical QA benchmarks show that ReGraM consistently outperforms the baseline KGARevion, achieving significant accuracy gains and a reduction in hallucination rates. The results highlight the effectiveness of region-first KG reasoning in enhancing factual accuracy and consistency in medical QA.",157.75,Phi-4,Nvidia B200 (Cloud Native)
2601.09281v1_STaR Sensitive Trajectory Regulation for Unlearnin.pdf,STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models,"['Jingjing Zhou', 'Gaoxiang Cong', 'Li Su', 'Liang Li']","This paper introduces Sensitive Trajectory Regulation (STaR), a framework designed to address privacy risks in Large Reasoning Models (LRMs) by unlearning sensitive information embedded in the reasoning process. Unlike existing approaches that focus on final answers, STaR operates at inference time without modifying parameters, ensuring robust privacy protection. It employs semantic-aware detection, global safety constraints, trajectory-aware suppression, and token-level adaptive filtering to dynamically block sensitive content. The paper also proposes new evaluation metrics, Multi-Decoding Consistency Assessment (MCS) and Multi-Granularity Membership Inference Attack (MIA) Evaluation, to assess unlearning effectiveness. Experiments on the R-TOFU benchmark show that STaR achieves comprehensive unlearning with minimal utility loss, setting a new standard for privacy-preserving reasoning in LRMs.",157.31,Phi-4,Nvidia B200 (Cloud Native)
2601.09282v1_Cluster Workload Allocation Semantic Soft Affinity.pdf,Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing,"['Leszek Sliwko', 'Jolanta Mizeria-Pietraszko']","This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing (NLP). It integrates a Large Language Model (LLM) via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype was developed featuring a cluster state cache and an intent analyzer using AWS Bedrock. Empirical evaluation showed high LLM parsing accuracy (>95% Subset Accuracy) for models like Amazon Nova Pro/Premier and Mistral Pixtral Large, outperforming a baseline engine. Scheduling quality tests across six scenarios demonstrated superior or equivalent placement compared to standard Kubernetes configurations, especially in complex and quantitative scenarios and when handling conflicting soft preferences. The study validates the use of LLMs for accessible scheduling but notes limitations such as synchronous LLM latency, suggesting asynchronous processing for production readiness. The work confirms the viability of semantic soft affinity for simplifying workload orchestration.",158.09,Phi-4,Nvidia B200 (Cloud Native)
2601.09286v1_Why not Collaborative Filtering in Dual View Bridg.pdf,Why not Collaborative Filtering in Dual View? Bridging Sparse and Dense Models,"['Hanze Guo', 'Jianxun Lian', 'Xiao Zhou']","This paper addresses the limitations of dense embedding-based collaborative filtering methods, particularly their diminishing signal-to-noise ratio (SNR) when modeling unpopular items under data sparsity. The authors propose a unified framework, SaD (Sparse and Dense), which integrates the semantic expressiveness of dense embeddings with the structural reliability of sparse interaction patterns. The framework theoretically demonstrates a superior global SNR through dual-view alignment, where the dense view enriches the sparse view with semantic correlations, and the sparse view regularizes the dense model with structural signals. Extensive experiments show that SaD achieves state-of-the-art performance, even with simple matrix factorization-style dense models, and can be applied to various existing recommender models. Real-world benchmark evaluations confirm SaD's superiority over strong baselines, ranking first on the BarsMatch leaderboard.",157.48,Phi-4,Nvidia B200 (Cloud Native)
2601.09292v1_Blue Teaming Function-Calling Agents.pdf,Blue Teaming Function-Calling Agents,"['Greta Dolcetti', 'Giulio Zizzo', 'Sergio Maffeis']","This paper presents an experimental evaluation of the robustness of four open source Large Language Models (LLMs) with function-calling capabilities against three different types of attacks. It also measures the effectiveness of eight different defence mechanisms. The study reveals that these models are not inherently secure and that the current defences are not yet practical for real-world applications. The paper introduces a new attack vector, Renaming Tool Poisoning, and a corresponding defence, Tool Obfuscation, highlighting the unique challenges posed by the visibility of tool implementations in open source systems. The findings provide insights for developing more secure agentic systems.",157.5,Phi-4,Nvidia B200 (Cloud Native)
2601.09293v1_Policy-Based Reinforcement Learning with Action Ma.pdf,Policy-Based Reinforcement Learning with Action Masking for Dynamic Job Shop Scheduling under Uncertainty: Handling Random Arrivals and Machine Failures,"['Sofiene Lassoued', 'Stefan Lier', 'Andreas Schwung']","This paper introduces a novel framework for solving Dynamic Job Shop Scheduling Problems (DJSS) under uncertainty, addressing challenges such as stochastic job arrivals and machine breakdowns. The approach uses Coloured Timed Petri Nets for environment representation and Maskable Proximal Policy Optimization for dynamic decision-making, ensuring feasible actions at each decision point. Stochastic models, including Gamma distribution for job arrivals and Weibull distribution for machine failures, simulate realistic industrial conditions. The study evaluates two action-masking strategies: a non-gradient approach and a gradient-based approach. Extensive experiments on dynamic JSSP benchmarks show that this method outperforms traditional heuristic and rule-based approaches in minimizing makespan. The results demonstrate the effectiveness of combining Petri-net-based models with adaptive reinforcement learning policies, providing a resilient, scalable, and explainable framework for real-time scheduling in dynamic and uncertain manufacturing environments.",158.07,Phi-4,Nvidia B200 (Cloud Native)
2601.09306v1_On-Device Large Language Models for Sequential Rec.pdf,On-Device Large Language Models for Sequential Recommendation,"['Xin Xia', 'Hongzhi Yin', 'Shane Culpepper']","This paper introduces OD-LLM, a task-adaptive compression framework designed for efficient and accurate deployment of large language models (LLMs) on resource-constrained devices for sequential recommendation tasks. OD-LLM combines low-rank structural compression using Singular Value Decomposition (SVD) and a novel tokenization normalization technique to reduce model size and computational overhead. A progressive alignment algorithm is also employed to refine parameters layerwise, ensuring minimal performance degradation. Empirical evaluations demonstrate that OD-LLM maintains effectiveness even when the model size is halved, offering a practical solution for real-time, on-device applications.",158.11,Phi-4,Nvidia B200 (Cloud Native)
2601.09313v1_Understanding or Memorizing A Case Study of German.pdf,Understanding or Memorizing? A Case Study of German Definite Articles in Language Models,"['Jonathan Drechsel', 'Erisa Bytyqi', 'Steffen Herbold']","This paper investigates whether language models (LMs) encode German definite articles based on abstract grammatical rules or through memorized associations. The study focuses on the syncretic nature of German definite singular articles, which vary by gender and case. Using the gradient-based interpretability method GRADIEND, the authors analyze parameter update directions for gender-case specific article transitions. The findings reveal significant generalization across gender and case, with substantial overlap among affected neurons, suggesting that LMs rely partly on memorized associations rather than strictly rule-based encoding.",158.17,Phi-4,Nvidia B200 (Cloud Native)
2601.09342v1_Improving Implicit Hate Speech Detection via a Com.pdf,Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework,"['Ewelina Gajewska', 'Katarzyna Budzynska', 'Jarosław A. Chudziak']","This paper introduces a contextualized framework for detecting implicitly hateful speech using a multi-agent system. The system includes a central Moderator Agent and Community Agents representing specific demographic groups. It integrates socio-cultural context from public knowledge sources, enhancing identity-aware moderation. The framework outperforms existing prompting methods on the ToxiGen dataset and improves classification accuracy and fairness by using balanced accuracy as a key metric. This approach addresses the imbalance in true positive and true negative rates, offering a more nuanced and context-aware solution for hate speech detection.",158.05,Phi-4,Nvidia B200 (Cloud Native)
2601.09351v1_Navigating Ethical AI Challenges in the Industrial.pdf,Navigating Ethical AI Challenges in the Industrial Sector: Balancing Innovation and Responsibility,"['Ruomu Tan', 'Martin W Hoffmann']","This chapter explores the integration of artificial intelligence (AI) into the industrial sector, highlighting the ethical challenges that arise with advancements in AI, such as transparency, accountability, and fairness. It examines ethical aspects in various industrial use cases and emphasizes the importance of embedding ethical principles into AI systems to inspire technological breakthroughs and foster trust among stakeholders. The chapter provides actionable insights for guiding industrial research and development towards ethical and responsible progress, promoting a more inclusive industrial ecosystem.",157.96,Phi-4,Nvidia B200 (Cloud Native)
2601.09353v1_Monte-Carlo Tree Search with Neural Network Guidan.pdf,Monte-Carlo Tree Search with Neural Network Guidance for Lane-Free Autonomous Driving,"['Ioannis Peridis', 'Dimitrios Troullinos', 'Georgios Chalkiadakis', 'Pantelis Giankoulidis', 'Ioannis Papamichail', 'Markos Papageorgiou']","This paper explores a Monte-Carlo Tree Search (MCTS) planning approach for single-agent autonomous driving in lane-free traffic environments. The approach is enhanced by a pre-trained neural network (NN) that guides the selection phase, leveraging NNs' predictive capabilities to improve the tree search process under computational constraints. The study evaluates the impact of isotropic state information on vehicle behavior, the performance acceleration of NN-guided MCTS, and the trade-off between computational resources and solution quality. The research addresses both safety (via collision rates) and efficacy (via measured speed), contributing to the understanding of autonomous driving in lane-free settings.",158.0,Phi-4,Nvidia B200 (Cloud Native)
2601.09361v1_GeoRA Geometry-Aware Low-Rank Adaptation for RLVR.pdf,GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR,"['Jiaying Zhang', 'Lei Shi', 'Jiguo Li', 'Jun Xu', 'Jiuchong Gao', 'Jinghua Hao', 'Renqing He']","GeoRA (Geometry-Aware Low-Rank Adaptation) addresses the challenges in Reinforcement Learning with Verifiable Rewards (RLVR) by leveraging the anisotropic and compressible nature of RL update subspaces. Unlike existing parameter-efficient methods like PiSSA and MiLoRA, which are designed for Supervised Fine-Tuning and do not account for RLVR's unique optimization dynamics and geometric structures, GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition within a geometrically constrained subspace. This approach preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment, outperforming established low-rank baselines on key mathematical benchmarks and achieving state-of-the-art results. Additionally, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.",156.82,Phi-4,Nvidia B200 (Cloud Native)
2601.09365v1_Frame of Reference Addressing the Challenges of Co.pdf,Frame of Reference: Addressing the Challenges of Common Ground Representation in Situational Dialogs,"['Biswesh Mohapatra', 'Théo Charlot', 'Giovanni Duca', 'Mayank Palan', 'Laurent Romary', 'Justine Cassell']","This paper explores the role of common ground in situational spoken dialogs, emphasizing its importance for conversational agents and social robots. It evaluates the ability of models to utilize relational references in dynamic environments and proposes methods to improve performance in representing and managing common ground. The study addresses the challenge of maintaining shared understanding over extended dialogues and highlights the need for systems to effectively store and retrieve grounded information for future use.",157.88,Phi-4,Nvidia B200 (Cloud Native)
2601.09381v1_Query Languages for Machine-Learning Models.pdf,Query Languages for Machine-Learning Models,['Martin Grohe'],"This paper discusses two logics for weighted finite structures: first-order logic with summation (FO(SUM)) and its recursive extension IFP(SUM). These logics, originating from foundational work by Grädel, Gurevich, and Meer in the 1990s, are investigated as query languages for machine learning models, particularly neural networks represented as weighted graphs. The paper presents examples of queries to neural networks expressible in these logics and discusses their expressiveness and computational complexity.",158.14,Phi-4,Nvidia B200 (Cloud Native)
2601.09382v1_Long-term Task-oriented Agent Proactive Long-term .pdf,Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments,"['Qinglong Shi', 'Donghai Wang', 'Hantao Zhou', 'Jiguo Li', 'Jun Xu', 'Jiuchong Gao', 'Jinghua Hao', 'Renqing He']","This paper introduces a novel interaction paradigm for proactive task-oriented agents capable of maintaining long-term user intents and adapting to dynamic environments. The authors propose two key capabilities: Intent-Conditioned Monitoring, where the agent autonomously formulates trigger conditions based on dialog history, and Event-Triggered Follow-up, where the agent actively engages the user upon detecting useful environmental updates. A high-quality data synthesis pipeline is introduced to construct complex, multi-turn dialog data in dynamic environments. The paper also proposes a new benchmark, ChronosBench, to evaluate task-oriented interaction in dynamic environments. The authors evaluated leading models and found flaws in long-term task-oriented interaction. Their fine-tuned model, trained using synthetic data, achieved a task completion rate of 85.19% for complex tasks, outperforming other models. The results validate the effectiveness of their data-driven strategy.",157.24,Phi-4,Nvidia B200 (Cloud Native)
2601.09394v2_FairGE Fairness-Aware Graph Encoding in Incomplete.pdf,FairGE: Fairness-Aware Graph Encoding in Incomplete Social Networks,"['Renqiang Luo', 'Huafei Huang', 'Tao Tang', 'Jing Ren', 'Ziqi Xu', 'Mingliang Hou', 'Enyan Dai', 'Feng Xia']","Graph Transformers (GTs) are increasingly applied to social network analysis, yet their deployment is often constrained by fairness concerns, especially in incomplete social networks where sensitive attributes are frequently missing. FairGE (Fair Graph Encoding) is introduced as a fairness-aware framework for GTs in such networks. Instead of generating sensitive attributes, FairGE encodes fairness directly through spectral graph theory. By leveraging the principal eigenvector to represent structural information and padding incomplete sensitive attributes with zeros, FairGE ensures fairness without data reconstruction. Theoretical analysis shows that the method suppresses the influence of non-principal spectral components, enhancing fairness. Experiments on seven real-world social network datasets confirm that FairGE achieves at least a 16% improvement in both statistical parity and equality of opportunity compared with state-of-the-art baselines.",157.37,Phi-4,Nvidia B200 (Cloud Native)
2601.09398v1_Ability Transfer and Recovery via Modularized Para.pdf,Ability Transfer and Recovery via Modularized Parameters Localization,"['Songyao Jin', 'Kun Zhou', 'Wenqi Li', 'Peng Wang', 'Biwei Huang']","This paper investigates the distribution of abilities within large language model (LLM) parameters by analyzing module activations under domain- and language-specific inputs. It identifies that ability-related activations are concentrated in a small set of channels, which are largely disentangled. The authors propose ACT (Activation-Guided Channel-wise Ability Transfer), a method that localizes ability-relevant channels via activation differences and selectively transfers these parameters, followed by lightweight fine-tuning for compatibility. Experiments demonstrate that ACT can recover forgotten abilities while preserving retained skills and integrate multiple specialized models into a single model with minimal interference.",158.06,Phi-4,Nvidia B200 (Cloud Native)
2601.09413v1_Speech-Hands A Self-Reflection Voice Agentic Appro.pdf,Speech-Hands: A Self-Reflection Voice Agentic Approach to Speech Recognition and Audio Reasoning with Omni Perception,"['Zhen Wan', 'Chao-Han Huck Yang', 'Jinchuan Tian', 'Hanrong Ye', 'Ankita Pasad', 'Szu-wei Fu', 'Arushi Goel', 'Ryo Hachiuma', 'Shizhe Diao', 'Kunal Dhawan', 'Sreyan Ghosh', 'Yusuke Hirota', 'Zhehuai Chen', 'Rafael Valle', 'Ehsan Hosseini Asl', 'Chenhui Chu', 'Shinji Watanabe', 'Yu-Chiang Frank Wang', 'Boris Ginsburg']","The paper introduces a voice-agentic framework named Speech-Hands, designed to enhance speech recognition and audio reasoning by incorporating a self-reflection mechanism. This mechanism allows the model to decide when to trust its internal perception and when to consult external audio perception, addressing the issue of performance degradation due to noisy hypotheses. Speech-Hands demonstrates superior performance on the OpenASR leaderboard and achieves high accuracy and F1 scores on audio QA tasks, showcasing its robust generalization and reliability across diverse datasets. The framework unifies perception and decision-making, paving the way for more reliable audio intelligence.",157.0,Phi-4,Nvidia B200 (Cloud Native)
2601.09416v1_Radiomics-Integrated Deep Learning with Hierarchic.pdf,Radiomics-Integrated Deep Learning with Hierarchical Loss for Osteosarcoma Histology Classification,"['Yaxi Chen', 'Zi Ye', 'Shaheer U. Saeed', 'Oliver Yu', 'Simin Ni', 'Jie Huang', 'Yipeng Hu']","This paper addresses the challenge of classifying osteosarcoma histology by integrating radiomic features into deep learning models and optimizing classification tasks with a hierarchical loss. The study demonstrates improved performance in distinguishing viable from non-viable tumor regions, which is crucial for prognosis and treatment planning. The proposed methods enhance model interpretability and generalization, setting a new state-of-the-art performance on the TCIA OS Tumor Assessment dataset.",157.62,Phi-4,Nvidia B200 (Cloud Native)
2601.09421v2_Bias Dynamics in BabyLMs Towards a Compute-Efficie.pdf,Bias Dynamics in BabyLMs: Towards a Compute-Efficient Sandbox for Democratising Pre-Training Debiasing,"['Filip Trhlik', 'Andrew Caines', 'Paula Buttery']","This paper explores the use of BabyLMs, compact BERT-like models, as a cost-effective alternative for pre-model debiasing research. BabyLMs, trained on small and mutable corpora, can approximate the bias acquisition and learning dynamics of larger models. The study demonstrates that BabyLMs exhibit similar patterns of intrinsic bias formation and performance development as standard BERT models, despite their reduced size. The research highlights the potential of BabyLMs to serve as an effective sandbox for large-scale language models, significantly reducing pre-training costs from over 500 GPU-hours to under 30 GPU-hours. This approach democratizes pre-model debiasing research, enabling faster and more accessible exploration of methods for building fairer language models.",157.75,Phi-4,Nvidia B200 (Cloud Native)
2601.09433v1_Do Transformers Understand Ancient Roman Coin Moti.pdf,Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?,"['David Reid', 'Ognjen Arandjelović']","This paper explores the application of Vision Transformers (ViT) for the automated analysis of ancient coins, comparing their performance to convolutional neural networks (CNNs). The study focuses on identifying semantic elements depicted on ancient coins using multi-modal data (images and unstructured text). The findings indicate that ViT models outperform newly trained CNN models in terms of accuracy. The paper also provides a summary of previous research, discusses the training and implementation of both ViT and CNN models, and evaluates their performance in the context of ancient coin analysis.",158.07,Phi-4,Nvidia B200 (Cloud Native)
2601.09445v1_Where Knowledge Collides A Mechanistic Study of In.pdf,Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models,"['Minh Vu Pham', 'Hsuvas Borkakoty', 'Yufang Hou']","This paper investigates intra-memory knowledge conflict in language models (LMs), which arises when inconsistent information about the same event is encoded within the model's parametric knowledge. While previous research has focused on resolving conflicts between a model's internal knowledge and external resources, this study explores the localization of conflicts originating during pre-training within the model's internal representations. The authors design a framework based on mechanistic interpretability methods to identify and control conflicting knowledge encoded during pre-training. Their findings suggest that specific internal components of LMs are responsible for encoding conflicting knowledge, and they demonstrate how these methods can be used to causally intervene in and manage such conflicts at inference time.",157.95,Phi-4,Nvidia B200 (Cloud Native)
2601.09446v1_Improving Symbolic Translation of Language Models .pdf,Improving Symbolic Translation of Language Models for Logical Reasoning,"['Ramya Keerthy Thatikonda', 'Jiuzhou Han', 'Wray Buntine', 'Ehsan Shareghi']","This paper addresses the challenges faced by smaller language models (LMs) in translating natural language (NL) into first-order logic (FOL) for logical reasoning tasks. The authors identify common errors in symbolic translation and propose a method to fine-tune smaller LMs using data synthesized by larger models. They introduce an incremental inference approach, dividing the task into predicate generation and FOL translation, and incorporate a verification module to target specific errors. The study evaluates the effectiveness of these methods across various logical-reasoning datasets, demonstrating reduced error rates and improved reasoning performance for smaller LMs.",157.8,Phi-4,Nvidia B200 (Cloud Native)
2601.09448v1_Population-Aligned Audio Reproduction With LLM-Bas.pdf,Population-Aligned Audio Reproduction With LLM-Based Equalizers,"['Ioannis Stylianou', 'Jon Francombe', 'Pablo Martínez-Nuevo', 'Sven Ewan Shepstone', 'Zheng-Hua Tan']","This paper introduces a novel approach to audio equalization using Large Language Models (LLMs) to map natural language text prompts to equalization settings. This method allows for a conversational approach to sound system control, adapting to changing listening contexts such as mood, location, or social setting. By leveraging data from controlled listening experiments, the models utilize in-context learning and parameter-efficient fine-tuning to align with population-preferred equalization settings. The evaluation shows statistically significant improvements in distributional alignment over random sampling and static preset baselines, suggesting that LLMs could serve as 'artificial equalizers' for more accessible, context-aware, and expert-level audio tuning.",156.32,Phi-4,Nvidia B200 (Cloud Native)
2601.09451v1_Late Breaking Results Quamba-SE Soft-edge Quantize.pdf,Late Breaking Results: Quamba-SE: Soft-edge Quantizer for Activations in State Space Models,"['Yizhi Chen', 'Ahmed Hemani']","The paper introduces Quamba-SE, a novel soft-edge quantizer designed for State Space Model (SSM) activation quantization. Unlike traditional methods that use a single scale for all data and hard clip outliers, Quamba-SE employs three adaptive scales: high-precision for small values, standard scale for normal values, and low-precision for outliers. This approach preserves outlier information while maintaining precision for other values. The effectiveness of Quamba-SE is demonstrated through evaluations on Mamba-130M across six zero-shot benchmarks, where it consistently outperforms the existing Quamba method, achieving up to +2.68% improvement on individual benchmarks and +0.83% on average across six datasets. The paper highlights the challenges of post-training quantization for SSMs, particularly the presence of significant outliers, and proposes Quamba-SE as a solution that operates at the hardware level, providing soft edges for values instead of hard clipping.",154.0,Phi-4,Nvidia B200 (Cloud Native)
2601.09455v1_On the Hardness of Computing Counterfactual and Se.pdf,On the Hardness of Computing Counterfactual and Semi-factual Explanations in XAI,"['André Artelt', 'Martin Olsen', 'Kevin Tierney']","This paper explores the computational complexity of generating counterfactual and semi-factual explanations in explainable artificial intelligence (XAI). It provides an overview of existing literature on the topic, highlighting that generating these explanations is often computationally hard. The authors contribute new inapproximability results, demonstrating that not only is generating explanations difficult, but approximating them is also challenging under certain assumptions. The paper discusses the implications of these findings for the XAI community and policymakers, particularly in the context of regulations like the EU AI Act that mandate explainability in AI systems.",154.67,Phi-4,Nvidia B200 (Cloud Native)
2601.09460v1_SoK Enhancing Cryptographic Collaborative Learning.pdf,SoK: Enhancing Cryptographic Collaborative Learning with Differential Privacy,"['Francesco Capano', 'Jonas Böhler', 'Benjamin Weggenmann']","This paper addresses the challenges of combining cryptographic techniques and differential privacy for cryptographic and differentially private collaborative learning (CPCL). It introduces a unified framework for CPCL, identifies secure noise sampling as a foundational phase, and analyzes trade-offs of various techniques. The paper evaluates the accuracy and cryptographic overhead of different secure noise sampling options in multi-party computation (MPC) and proposes future research directions based on identified gaps and enhancements.",154.37,Phi-4,Nvidia B200 (Cloud Native)
2601.09465v1_EvoFSM Controllable Self-Evolution for Deep Resear.pdf,EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines,"['Shuo Zhang', 'Chaofa Yuan', 'Ryan Guo', 'Xiaomin Yu', 'Rui Xu', 'Zhangquan Chen', 'Zinuo Li', 'Zhi Yang', 'Shuhao Guan', 'Zhenheng Tang', 'Sen Hu', 'Liwen Zhang', 'Ronghao Chen', 'Huacan Wang']","The paper introduces EvoFSM, a framework for structured self-evolution in deep research using Finite State Machines (FSMs). Unlike previous methods that rely on unconstrained optimization, EvoFSM decouples the optimization space into macroscopic flow (state-transition logic) and microscopic skills (state-specific behaviors). This approach allows for targeted improvements within clear behavioral boundaries. EvoFSM uses a critic mechanism to refine the FSM through constrained operations and incorporates a self-evolving memory to distill successful trajectories and failure patterns. Extensive evaluations demonstrate its effectiveness, achieving 58.0% accuracy on the DeepSearch benchmark and validating its generalization on interactive decision-making tasks.",153.0,Phi-4,Nvidia B200 (Cloud Native)
2601.09467v1_Searth Transformer A Transformer Architecture Inco.pdf,Searth Transformer: A Transformer Architecture Incorporating Earth's Geospheric Physical Priors for Global Mid-Range Weather Forecasting,"['Tianye Li', 'Qi Liu', 'Hao Li', 'Lei Chen', 'Wencong Cheng', 'Fei Zheng', 'Xiangao Xia', 'Ya Wang', 'Gang Huang', 'Weiwei Wang', 'Xuan Tong', 'Ziqing Zu', 'Yi Fang', 'Shenming Fu', 'Jiang Jiang', 'Haochen Li', 'Mingxing Li', 'Jiangjiang Xia']","This paper introduces the Shifted Earth Transformer (Searth Transformer), a physics-informed transformer architecture designed for global medium-range weather forecasting. It addresses the limitations of existing Transformer-based models by incorporating Earth's spherical topology and zonal periodicity into window-based self-attention, ensuring physically consistent global information exchange. Additionally, the paper presents the Relay Autoregressive (RAR) fine-tuning strategy, which decouples GPU memory usage from forecast length, thus mitigating computational bottlenecks and enabling more efficient training. These innovations aim to enhance forecast accuracy and physical consistency while making the model accessible to resource-constrained institutions.",157.56,Phi-4,Nvidia B200 (Cloud Native)
2601.09469v2_FairGU Fairness-aware Graph Unlearning in Social N.pdf,FairGU: Fairness-aware Graph Unlearning in Social Networks,"['Renqiang Luo', 'Yongshuai Yang', 'Huafei Huang', 'Qing Qing', 'Mingliang Hou', 'Ziqi Xu', 'Yi Yu', 'Jingjing Zhou', 'Feng Xia']","Graph unlearning is essential for maintaining sustainable and privacy-preserving social networks by removing the influence of deleted nodes. However, existing techniques often fail to protect sensitive attributes, leading to reduced algorithmic fairness. This paper introduces FairGU, a fairness-aware graph unlearning framework that integrates a fairness-aware module with data protection strategies to ensure sensitive attributes are not amplified or exposed during node removal. Extensive experiments on real-world datasets show that FairGU outperforms existing methods in both accuracy and fairness metrics, addressing a critical risk in current unlearning practices and providing a robust solution for future networked systems.",157.42,Phi-4,Nvidia B200 (Cloud Native)
2601.09470v1_Personalized Multimodal Feedback Using Multiple Ex.pdf,Personalized Multimodal Feedback Using Multiple External Representations: Strategy Profiles and Learning in High School Physics,"['Natalia Revenga-Lozano', 'Karina E. Avila', 'Steffen Steinert', 'Matthias Schweinberger', 'Clara E. Gómez-Pérez', 'Jochen Kuhn', 'Stefan Küchemann']","This study investigates the integration of personalized feedback with multiple external representations (MERs) in high school physics learning. Conducted over 16-24 weeks with 661 participants, the research utilized a computer-based platform offering feedback in verbal, graphical, and mathematical forms. The study found that elaborated multirepresentational feedback positively correlated with post-test scores, regardless of prior knowledge and confidence. It also revealed that students with lower representational competence benefited more from using diverse representations, an advantage that decreased with increased competence. These insights suggest the potential for adaptive feedback designs in intelligent tutoring systems.",158.08,Phi-4,Nvidia B200 (Cloud Native)
2601.09473v1_SimMerge Learning to Select Merge Operators from S.pdf,SimMerge: Learning to Select Merge Operators from Similarity Signals,"['Oliver Bolton', 'Aakanksha', 'Arash Ahmadian', 'Sara Hooker', 'Marzieh Fadaee', 'Beyza Ermis']","SimMerge introduces a predictive method for selecting the best merge operator for large language models (LLMs) using inexpensive, task-agnostic similarity signals. This approach eliminates the need for costly merge-and-evaluate searches by predicting the performance of 2-way merges from functional and structural features derived from unlabeled probes. SimMerge demonstrates superior performance in 2-way merges of 7B-parameter LLMs and generalizes to multi-way and 111B-parameter LLM merges without retraining. A bandit variant supports dynamic addition of tasks, models, and operators, offering a scalable solution for model composition when evaluation budgets are limited.",157.91,Phi-4,Nvidia B200 (Cloud Native)
2601.09478v3_Bridging Semantic Understanding and Popularity Bia.pdf,Bridging Semantic Understanding and Popularity Bias with LLMs,"['Renqiang Luo', 'Dong Zhang', 'Yupeng Gao', 'Wen Shi', 'Mingliang Hou', 'Jiaying Liu', 'Zhe Wang', 'Shuo Yu']","This paper addresses the challenge of semantic understanding of popularity bias in recommender systems, where popular items are often favored over niche content. Existing methods typically focus on diversity enhancement or long-tail coverage, neglecting the deeper semantic origins of the bias. The authors propose FairLRM, a framework that uses Large Language Models (LLMs) to decompose popularity bias into item-side and user-side components. By employing structured instruction-based prompts, FairLRM enhances the model's comprehension of global item distributions and individual user preferences. The framework aims to improve both fairness and recommendation accuracy by providing a more semantically aware approach. Empirical evaluations demonstrate that FairLRM significantly enhances these aspects, offering a more trustworthy method for addressing popularity bias.",157.19,Phi-4,Nvidia B200 (Cloud Native)
2601.09503v1_What Do LLM Agents Know About Their World Task2Qui.pdf,What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding,"['Siyuan Liu', 'Hongbang Yuan', 'Xinze Li', 'Ziyue Zhu', 'Yixin Cao', 'Yu-Gang Jiang']","The paper investigates the ability of large language model (LLM) agents to generalize across different environments. It introduces Task-to-Quiz (T2Q), a new evaluation paradigm that separates task execution from world-state understanding. The authors present T2QBench, a suite of environments and QA pairs, to assess agents' environment understanding. Their findings suggest that task success is not a reliable indicator of environment understanding, highlighting the need for proactive exploration and fine-grained state representation to develop more generalizable agents.",157.95,Phi-4,Nvidia B200 (Cloud Native)
2601.09518v1_Learning Whole-Body Human-Humanoid Interaction fro.pdf,Learning Whole-Body Human-Humanoid Interaction from Human-Human Demonstrations,"['Wei-Jin Huang', 'Yue-Yi Zhang', 'Yi-Lin Wei', 'Zhi-Wei Xia', 'Juantao Tan', 'Yuan-Ming Li', 'Zhilin Zhao', 'Wei-Shi Zheng']","This paper addresses the challenge of enabling humanoid robots to physically interact with humans by generating high-quality Human-Humanoid Interaction (HHoI) data from Human-Human Interaction (HHI) data. The authors introduce PAIR (Physics-Aware Interaction Retargeting), a two-stage pipeline that preserves contact semantics across morphology differences, ensuring physically consistent HHoI data. They also present D-STAR (Decoupled Spatio-Temporal Action Reasoner), a hierarchical policy that separates the timing and location of actions, enhancing interactive understanding beyond mere trajectory imitation.",157.82,Phi-4,Nvidia B200 (Cloud Native)
2601.09520v1_Towards Realistic Synthetic Data for Automatic Dru.pdf,Towards Realistic Synthetic Data for Automatic Drum Transcription,"['Pierfrancesco Melucci', 'Paolo Merialdo', 'Taketo Akama']","This paper addresses the challenge of limited large-scale, paired audio-MIDI datasets for Automatic Drum Transcription (ADT) by introducing a semi-supervised method to curate a diverse corpus of one-shot drum samples from unlabeled audio sources. The curated samples are used to synthesize a high-quality dataset from MIDI files, which trains a sequence-to-sequence transcription model. The model achieves new state-of-the-art results on the ENST and MDB test sets, outperforming both fully supervised methods and previous synthetic-data approaches. The study highlights the limitations of SoundFont-based synthesis and proposes a solution using high-quality one-shot samples to bridge the domain gap between synthetic and real-world data.",157.58,Phi-4,Nvidia B200 (Cloud Native)
2601.09527v1_Private LLM Inference on Consumer Blackwell GPUs A.pdf,Private LLM Inference on Consumer Blackwell GPUs: A Practical Guide for Cost-Effective Local Deployment in SMEs,"['Jonathan Knoop', 'Hendrik Holtmann']","This paper explores the feasibility of using NVIDIA's Blackwell consumer GPUs (RTX 5060 Ti, 5070 Ti, 5090) for local deployment of large language models (LLMs) in small and medium-sized enterprises (SMEs). It addresses data privacy concerns and high costs associated with cloud LLM APIs and professional on-premise hardware. Through systematic evaluation, the study benchmarks four open-weight models across various configurations, highlighting the RTX 5090's superior throughput and lower latency. The paper demonstrates that consumer GPUs can offer a cost-effective alternative for most SME workloads, with NVFP4 quantization providing significant throughput and energy efficiency. The findings suggest that consumer GPUs can reliably replace cloud inference for SMEs, except in latency-critical long-context scenarios.",156.78,Phi-4,Nvidia B200 (Cloud Native)
2601.09536v1_Omni-R1 Towards the Unified Generative Paradigm fo.pdf,Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning,"['Dongjie Cheng', 'Yongqi Li', 'Zhixin Ma', 'Hongru Cai', 'Yupeng Hu', 'Wenjie Wang', 'Liqiang Nie', 'Wenjie Li']","The paper introduces a unified generative multimodal reasoning paradigm, Omni-R1, which integrates diverse reasoning skills by generating intermediate images during the reasoning process. It features a two-stage SFT+RL framework with perception alignment loss and perception reward, enabling functional image generation. Additionally, Omni-R1-Zero is proposed to eliminate the need for multimodal annotations by bootstrapping visualizations from text-only reasoning data. Empirical results demonstrate that Omni-R1 achieves unified generative reasoning across various multimodal tasks, and Omni-R1-Zero can match or surpass Omni-R1 on average, indicating a promising direction for generative multimodal reasoning.",158.05,Phi-4,Nvidia B200 (Cloud Native)
2601.09555v1_Benchmarking Post-Training Quantization of Large L.pdf,Benchmarking Post-Training Quantization of Large Language Models under Microscaling Floating Point Formats,"['Manyi Zhang', 'Ji-Fu Li', 'Zhongao Sun', 'Haoli Bai', 'Hui-Ling Zhen', 'Zhenhua Dong', 'Xianzhi Yu']","This paper investigates the applicability and behavior of post-training quantization (PTQ) algorithms under Microscaling Floating-Point (MXFP) formats for large language models (LLMs). The study systematically evaluates over 7 PTQ algorithms, 15 evaluation benchmarks, and 3 LLM families. Key findings include MXFP8's near-lossless performance, MXFP4's accuracy challenges, the importance of format compatibility, consistent quantization sensitivity trends across models, and the critical role of scaling factors in MXFP4. The results offer practical guidance for adapting PTQ methods to MXFP quantization.",157.63,Phi-4,Nvidia B200 (Cloud Native)
2601.09566v2_Hot-Start from Pixels Low-Resolution Visual Tokens.pdf,Hot-Start from Pixels: Low-Resolution Visual Tokens for Chinese Language Modeling,"['Shuyang Xiang', 'Hao Guan']","This paper explores the use of low-resolution visual inputs as an alternative to index-based tokens for Chinese language modeling. By using grayscale images of individual characters with resolutions as low as 8×8 pixels, the study demonstrates that these visual inputs can achieve comparable accuracy to traditional methods. The research highlights a 'hot-start' effect where visual inputs quickly reach higher accuracy levels during training. The findings suggest that minimal visual structure can provide a robust and efficient signal for Chinese language modeling, offering a complementary perspective to traditional index-based approaches.",158.63,Phi-4,Nvidia B200 (Cloud Native)
2601.09600v1_Information Access of the Oppressed A Problem-Posi.pdf,Information Access of the Oppressed: A Problem-Posing Framework for Envisioning Emancipatory Information Access Platforms,"['BHASKAR MITRA', 'NICOLA NEOPHYTOU', 'SIREESH GURURAJA']","This paper explores the urgent need for alternative information access (IA) infrastructure to counteract authoritarian capture, particularly in the context of rising democratic erosion, generative AI technologies, and the concentration of power in Big Tech. Drawing on Paulo Freire’s theories of emancipatory pedagogy, the authors propose a problem-posing framework that challenges the traditional technologist-user dichotomy. They advocate for engaging marginalized communities in the co-creation of technology to support their struggles against oppression. The paper calls for technologists to actively participate in building infrastructure that resists authoritarianism and supports emancipatory efforts.",157.98,Phi-4,Nvidia B200 (Cloud Native)
2601.09603v1_Linear Complexity Self-Supervised Learning for Mus.pdf,Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer,"['Petros Vavaroutsos', 'Theodoros Palamas', 'Pantelis Vikatos']","This paper addresses the challenge of reducing the size of foundation models for music information retrieval (MIR) tasks. By integrating the Branchformer architecture with SummaryMixing and a random quantization process, the authors demonstrate a significant reduction in model size while maintaining competitive performance compared to state-of-the-art models. The research emphasizes reproducibility through pre-training on both publicly available and proprietary datasets, and robust evaluation across various MIR tasks.",157.94,Phi-4,Nvidia B200 (Cloud Native)
2601.09605v1_Sim2real Image Translation Enables Viewpoint-Robus.pdf,Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets,"['Jeremiah Coholich', 'Justin Wit', 'Robert Azarcon', 'Zsolt Kira']","This paper addresses the challenge of vision-based policies for robot manipulation being brittle to distribution shifts, particularly camera viewpoint variations. The authors propose MANGO, an unpaired image translation method designed to bridge the gap between simulation and real-world data. MANGO utilizes a novel segmentation-conditioned InfoNCE loss, a highly-regularized discriminator design, and a modified PatchNCE loss to maintain viewpoint consistency during sim2real translation. The method requires only a small amount of fixed-camera real-world data and can generate diverse unseen viewpoints by translating simulated observations. Imitation-learning policies trained with MANGO-enhanced data achieve significantly higher success rates on new viewpoints compared to non-augmented policies.",157.88,Phi-4,Nvidia B200 (Cloud Native)
2601.09609v1_DPWriter Reinforcement Learning with Diverse Plann.pdf,Reinforcement Learning with Diverse Planning Branching for Creative Writing,"['Qian Cao', 'Yahui Liu', 'Wei Bi', 'Yi Zhao', 'Ruihua Song', 'Xiting Wang', 'Ruiming Tang', 'Guorui Zhou', 'Han Li']","This paper addresses the challenge of reduced output diversity in reinforcement learning (RL)-based enhancement of large language models (LLMs) for creative writing. It proposes a novel RL framework that incorporates a semi-structured long Chain-of-Thought (CoT) approach, decomposing the generation process into explicitly planned intermediate steps. The introduction of a Diverse Planning Branching method strategically introduces divergence at the planning phase, guided by diversity variation, and employs a group-aware diversity reward to encourage distinct trajectories. Experimental results demonstrate significant improvements in output diversity without compromising generation quality, outperforming existing baselines.",157.62,Phi-4,Nvidia B200 (Cloud Native)
2601.09613v1_CogRail Benchmarking VLMs in Cognitive Intrusion P.pdf,CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems,"['Yonglin Tian', 'Qiyao Zhang', 'Wei Xu', 'Yutong Wang', 'Yihao Wu', 'Xinyi Li', 'Xingyuan Dai', 'Hui Zhang', 'Zhiyong Cui', 'Baoqing Guo', 'Zujun Yu', 'Yisheng Lv']","This paper introduces CogRail, a novel benchmark designed to enhance deep intrusion perception in railway transportation systems by integrating open-source datasets with cognitively driven annotations. It evaluates state-of-the-art visual-language models (VLMs) using multimodal prompts, identifying their strengths and limitations. The study proposes a joint fine-tuning framework that combines position perception, movement prediction, and threat analysis to adapt general-purpose models for specialized cognitive intrusion tasks. Experiments demonstrate that existing large-scale multimodal models struggle with the complex spatial-temporal reasoning required, highlighting the need for specialized models in safety-critical domains.",157.01,Phi-4,Nvidia B200 (Cloud Native)
2601.09620v1_Full Disclosure Less Trust How the Level of Detail.pdf,"Full Disclosure, Less Trust? How the Level of Detail about AI Use in News Writing Affects Readers’ Trust","['Pooja Prajod', 'Hannes Cools', 'Thomas Röggla', 'Karthiskeya Puttur Venkatraj', 'Amber Kusters', 'Alia Elkattan', 'Pablo Cesar', 'Abdallah El Ali']","This study investigates how different levels of AI disclosure in news writing affect readers' trust. It explores three levels of AI disclosures (none, one-line, detailed) across two types of news (politics and lifestyle) and two levels of AI involvement (low and high). The study found that detailed AI disclosures led to a decline in trust, while source-checking behavior increased with both one-line and detailed disclosures. The research highlights a trade-off between transparency and trust, with many participants preferring detailed disclosures or detail-on-demand formats.",157.63,Phi-4,Nvidia B200 (Cloud Native)
2601.09624v1_Toward Understanding Unlearning Difficulty A Mecha.pdf,Toward Understanding Unlearning Difficulty: A Mechanistic Perspective and Circuit-Guided Difficulty Metric,"['Jiali Cheng', 'Ziheng Chen', 'Chirag Agarwal', 'Hadi Amiri']","This paper explores the variability in machine unlearning success across individual samples, proposing that this disparity is not solely due to data characteristics but also reflects internal model mechanisms. The authors introduce a mechanistic perspective based on model circuits, which are structured interaction pathways that influence prediction formation. They propose a pre-unlearning metric, Circuit-guided Unlearning Difficulty (CUD), which assigns a continuous difficulty score to each sample using circuit-level signals. The study demonstrates that CUD effectively distinguishes between intrinsically easy and hard samples and remains consistent across different unlearning methods. Key findings include the identification of circuit-level patterns that indicate unlearning difficulty, with easy-to-unlearn samples associated with shorter, shallower interactions in earlier model stages, and hard samples relying on longer, deeper pathways in later stages. This work advances the understanding of unlearning difficulty and suggests the development of unlearning methods grounded in model mechanisms.",157.35,Phi-4,Nvidia B200 (Cloud Native)
2601.09625v1_The Promptware Kill Chain How Prompt Injections Gr.pdf,The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware,"['Ben Nassi', 'Bruce Schneier', 'Oleg Brodt']","The paper discusses the evolution of attacks on large language model (LLM)-based systems, which have created a new attack surface inadequately addressed by existing security frameworks. Initially framed as 'prompt injection,' these attacks are now recognized as multi-step sequences similar to traditional malware campaigns. The authors introduce the concept of 'promptware' and propose a five-step kill chain model to analyze these threats: Initial Access, Privilege Escalation, Persistence, Lateral Movement, and Actions on Objective. This framework helps security practitioners in threat modeling and provides a common vocabulary for researchers in AI safety and cybersecurity.",157.91,Phi-4,Nvidia B200 (Cloud Native)
2601.09626v1_From Prompt to Protocol Fast Charging Batteries wi.pdf,From Prompt to Protocol: Fast Charging Batteries with Large Language Models,"['Ge Lei', 'Ferran Brosa Planella', 'Sterling G. Baird', 'Samuel J. Cooper']","The paper introduces two gradient-free, LLM-driven closed-loop methods for optimizing battery charging protocols: Prompt-to-Optimizer (P2O) and Prompt-to-Protocol (P2P). P2O uses a large language model to propose neural-network-based protocols, while P2P writes explicit functions for current and its parameters. These methods outperform traditional approaches like Bayesian optimization and evolutionary algorithms. In fast-charging scenarios, both methods improve the state of health by approximately 4.2% over a multi-step constant current baseline. The study demonstrates that LLMs can expand the search space for protocol designs and enable efficient optimization in high-cost experimental settings.",157.94,Phi-4,Nvidia B200 (Cloud Native)
2601.09635v1_LLM for Large-Scale Optimization Model Auto-Formul.pdf,LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach,"['Kuo Liang', 'Yuhang Lu', 'Jianming Mao', 'Shuyi Sun', 'Chunwei Yang', 'Congcong Zeng', 'Xiao Jin', 'Hanzhang Qin', 'Ruihao Zhu', 'Chung-Piaw Teo']","The paper introduces LEAN-LLM-OPT, a framework designed to automate the formulation of large-scale optimization models using large language models (LLMs). This framework leverages LLMs to dynamically construct workflows that guide the formulation process, breaking it down into structured sub-tasks and utilizing auxiliary tools for data handling. The approach is tested using GPT-4.1 and gpt-oss-20B, demonstrating strong performance in simulations and practical applications, such as a Singapore Airlines revenue management case. The paper also presents new benchmarks for evaluating large-scale optimization auto-formulation.",157.62,Phi-4,Nvidia B200 (Cloud Native)
2601.09636v1_PersonalAlign Hierarchical Implicit Intent Alignme.pdf,PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records,"['Yibo Lyu', 'Gongwei Chen', 'Rui Shao', 'Weili Guan', 'Liqiang Nie']","This paper introduces PersonalAlign, a task for GUI agents to align with users' implicit intents by leveraging long-term user records. It presents AndroidIntent, a benchmark for evaluating agents' ability to resolve vague instructions and provide proactive suggestions. The Hierarchical Intent Memory Agent (HIM-Agent) is introduced, which maintains a personal memory and organizes user preferences and routines hierarchically. The study evaluates various GUI agents, showing that HIM-Agent significantly improves execution and proactive performance.",158.11,Phi-4,Nvidia B200 (Cloud Native)
2601.09667v2_Collaborative Multi-Agent Test-Time Reinforcement .pdf,Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning,"['Zhiyuan Hu', 'Yunhai Hu', 'Juncheng Liu', 'Shuyue Stella Li', 'Yucheng Wang', 'Zhen Xu', 'See-Kiong Ng', 'Anh Tuan Luu', 'Xinxing Xu', 'Bryan Hooi', 'Cynthia Breazeal', 'Hae Won Park']","This paper introduces Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework designed to enhance multi-agent systems' robustness and efficiency in reasoning tasks. MATTRL leverages structured textual experiences during inference to improve decision-making in multi-agent environments. It forms a team of specialist agents for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decisions. The framework addresses challenges in MARL training, such as resource intensity and instability due to non-stationarity and sparse rewards. MATTRL demonstrates significant improvements in accuracy across various benchmarks in medicine, math, and education, outperforming both multi-agent and single-agent baselines. The paper also explores different credit-assignment schemes and their impact on training outcomes, offering a stable and efficient approach to achieving distribution-shift-robust multi-agent reasoning without extensive tuning.",156.55,Phi-4,Nvidia B200 (Cloud Native)
2601.09680v1_Automating Supply Chain Disruption Monitoring via .pdf,Automating Supply Chain Disruption Monitoring via an Agentic AI,"['Sara AlMahria', 'Liming Xu', 'Alexandra Brintrup']","This paper introduces a minimally supervised agentic AI framework designed to autonomously monitor, analyze, and respond to disruptions across extended supply networks. The framework utilizes seven specialized agents powered by large language models and deterministic tools to detect disruption signals from unstructured news, map them to multi-tier supplier networks, evaluate exposure based on network structure, and recommend mitigations such as alternative sourcing options. The system is evaluated across 30 synthesized scenarios involving three automotive manufacturers and five disruption classes, achieving high accuracy with F1 scores between 0.962 and 0.991. It performs full end-to-end analyses in a mean of 3.83 minutes at a cost of $0.0836 per disruption, significantly reducing response time compared to industry benchmarks. A real-world case study of the 2022 Russia–Ukraine conflict demonstrates its operational applicability, marking a foundational step toward building resilient, proactive, and autonomous supply chains.",158.36,Phi-4,Nvidia B200 (Cloud Native)
2601.09684v1_Disentangling Task Conflicts in Multi-Task LoRA vi.pdf,Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection,"['Ziyu Yang', 'Guibin Chen', 'Yuxin Yang', 'Aoxiong Zeng', 'Xiangquan Yang']","This paper addresses the challenge of negative transfer in Multi-Task Learning (MTL) when combined with Low-Rank Adaptation (LoRA) for parameter-efficient deployment of Large Language Models (LLMs). The authors propose Ortho-LoRA, a gradient projection method tailored for LoRA's bipartite structure, which dynamically projects conflicting task gradients onto orthogonal complements within the LoRA subspace. Extensive experiments on the GLUE benchmark show that Ortho-LoRA effectively mitigates task interference, outperforming standard joint training and recovering 95% of the performance gap between multi-task and single-task baselines with negligible computational overhead.",157.34,Phi-4,Nvidia B200 (Cloud Native)
2601.09692v1_Routing with Generated Data Annotation-Free LLM Sk.pdf,Routing with Generated Data: Annotation-Free LLM Skill Estimation and Expert Selection,"['Tianyi Niu', 'Justin Chih-Yao Chen', 'Genta Indra Winata', 'Shi-Xiong Zhang', 'Supriyo Chakraborty', 'Sambit Sahu', 'Yue Zhang', 'Elias Stengel-Eskin', 'Mohit Bansal']","This paper introduces Routing with Generated Data (RGD), a method for training routers to select optimal models for given inputs using generated queries and answers from high-level task descriptions. The study evaluates query-answer and query-only routers across benchmarks, finding that query-only routers are more robust to generator quality. The paper proposes CASCAL, a query-only router that uses consensus voting and hierarchical clustering to estimate model correctness and identify model-specific skill niches, showing improved performance over existing methods when trained on weak generator data.",157.22,Phi-4,Nvidia B200 (Cloud Native)
2601.09694v1_LLMs can Compress LLMs Adaptive Pruning by Agents.pdf,LLMs can Compress LLMs: Adaptive Pruning by Agents,"['Sai Varun Kodathala', 'Rakesh Vunnam']","This paper introduces an innovative approach to compress Large Language Models (LLMs) using agent-guided pruning. Unlike existing methods that rely on uniform or hand-crafted heuristics for determining sparsity ratios, this method employs a foundation model as an adaptive pruning agent. This agent intelligently selects layers to prune while preserving critical knowledge pathways. The approach combines weight-activation metrics with gradient importance scores, normalized for model-agnostic comparison, and utilizes an LLM agent with self-reflection capabilities to iteratively refine pruning strategies. A checkpoint rollback mechanism ensures model quality by reverting changes when perplexity degradation exceeds a threshold. The method is evaluated on Qwen3 models, achieving significant improvements in MMLU accuracy, factual knowledge retention, and lower perplexity degradation compared to structured pruning baselines. The framework operates without retraining, is model-agnostic, and demonstrates effective self-correction with minimal rollbacks.",158.21,Phi-4,Nvidia B200 (Cloud Native)
2601.09703v1_ShortCoder Knowledge-Augmented Syntax Optimization.pdf,ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation,"['Sicong Liu', 'Yanxian Huang', 'Mingwei Liu', 'Jiachi Chen', 'Ensheng Shi', 'Yuchi Ma', 'Hongyu Zhang', 'Yin Zhang', 'Yanlin Wang']","This paper introduces ShortCoder, a knowledge-infused framework designed to enhance the efficiency of code generation tasks while maintaining semantic equivalence and readability. The framework addresses the inefficiencies inherent in large language models (LLMs) by proposing syntax-level simplification rules for Python, a hybrid data synthesis pipeline, and a fine-tuning strategy. These innovations lead to significant reductions in token usage and improvements in generation efficiency, as demonstrated by experimental results on the HumanEval benchmark.",157.86,Phi-4,Nvidia B200 (Cloud Native)
2601.09706v1_Value-Aware Numerical Representations for Transfor.pdf,Value-Aware Numerical Representations for Transformer Language Models,"['Andreea Dutulescu', 'Stefan Ruseti', 'Mihai Dascalu']","Transformer-based language models often achieve strong results on mathematical reasoning benchmarks but struggle with basic numerical understanding and arithmetic operations. This paper introduces a value-aware numerical representation that augments standard tokenized inputs with a prefix token whose embedding is conditioned on the numerical value. This approach injects magnitude information into the model’s input space, improving numerical robustness. Evaluation on arithmetic tasks shows that this method outperforms baselines across various numerical formats, tasks, and operand lengths, indicating that explicitly encoding numerical value enhances fundamental numerical robustness in language models.",158.18,Phi-4,Nvidia B200 (Cloud Native)
2601.09708v1_Fast-ThinkAct Efficient Vision-Language-Action Rea.pdf,Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning,"['Chi-Pin Huang', 'Yunze Man', 'Zhiding Yu', 'Min-Hung Chen', 'Jan Kautz', 'Yu-Chiang Frank Wang', 'Fu-En Yang']","Fast-ThinkAct is an efficient reasoning framework designed for Vision-Language-Action (VLA) tasks, which involve reasoning over complex visual scenes and executing adaptive actions in dynamic environments. The framework addresses the high inference latency issue of existing reasoning VLAs by introducing verbalizable latent reasoning, which allows for compact yet performant planning. Fast-ThinkAct achieves this by learning from a teacher model using a preference-guided objective, aligning manipulation trajectories to transfer both linguistic and visual planning capabilities for embodied control. This results in reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments demonstrate that Fast-ThinkAct significantly reduces inference latency by up to 89.3% compared to state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.",157.12,Phi-4,Nvidia B200 (Cloud Native)
2601.09749v1_R-LAM Reproducibility-Constrained Large Action Mod.pdf,R-LAM: Reproducibility-Constrained Large Action Models for Scientific Workflow Automation,['Suriya Sureshkumar'],"This paper introduces R-LAM, a framework designed to apply Large Action Models (LAMs) to scientific workflow automation while ensuring reproducibility, auditability, and deterministic execution. R-LAM addresses the limitations of generic LLM-based agents by introducing structured action schemas, deterministic execution policies, and explicit provenance tracking. The framework supports failure-aware execution loops and controlled workflow forking, enabling iterative experimentation without compromising reproducibility. An experimental evaluation demonstrates that R-LAM improves reproducibility success rates and execution reliability compared to unconstrained LLM-based agents, while retaining adaptive control over workflow execution.",158.4,Phi-4,Nvidia B200 (Cloud Native)
2601.09750v1_SAGE Tool-Augmented LLM Task Solving Strategies in.pdf,SAGE: Tool-Augmented LLM Task Solving Strategies in Scalable Multi-Agent Environments,"['Robert K. Strehlow', 'Tobias Küster', 'Oskar F. Kupke', 'Brandon Llanque Kurps', 'Fikret Sivrikaya', 'Sahin Albayrak']","This paper introduces 'SAGE', a specialized conversational AI interface that enhances large language models (LLMs) by integrating them with tools for real-world applications. Leveraging the OPACA framework, SAGE facilitates dynamic tool discovery and execution, allowing seamless integration of new tools and services. The paper evaluates various task-solving strategies using agentic concepts and prompting methods, demonstrating promising results in benchmark services. Both SAGE and the OPACA framework are available as open-source/open data on GitHub.",156.24,Phi-4,Nvidia B200 (Cloud Native)
2601.09753v1_Critically Engaged Pragmatism A Scientific Norm an.pdf,"Critically Engaged Pragmatism: A Scientific Norm and Social, Pragmatist Epistemology for AI Science Evaluation Tools",['Carole J. Lee'],"The paper discusses the challenges faced by the scientific community in evaluating research credibility due to crises in peer review capacity, study replication, and AI-fabricated science. It highlights the potential pitfalls of AI science evaluation tools, which may suffer from 'inference by false ascent' due to their contested purposes and technical demands. The author proposes a social, pragmatist epistemology and a norm of Critically Engaged Pragmatism to ensure these tools are scrutinized for their purpose-specific reliability. The paper argues that AI tools should not be seen as objective arbiters of credibility but as subjects of critical discourse that underpin scientific credibility.",158.06,Phi-4,Nvidia B200 (Cloud Native)
2601.09755v1_Heterogeneous computing platform for real-time rob.pdf,Heterogeneous computing platform for real-time robotics,"['Jakub Fil', 'Yulia Sandamirskaya', 'Hector Gonzalez', 'Loïc Azzalin', 'Stefan Glüge', 'Lukas Friedenstab', 'Friedrich Wolf', 'Tim Rosmeisl', 'Matthias Lohrmann', 'Mahmoud Akl', 'Khaleel Khan', 'Leonie Wolf', 'Kristin Richter', 'Holm Puder', 'Mazhar Ali Bari', 'Xuan Choo', 'Noha Alharthi', 'Michael Hopkins', 'Mansoor Hanif', 'Christian Mayr', 'Jens Struckmeier', 'Steve Furber']","This paper explores a computing platform designed to enable Society 5.0, where robotics and infrastructure are tightly integrated for enhanced reliability, efficiency, and safety. The proposed system combines neuromorphic computing hardware, such as the Loihi2 processor, with event-based cameras for real-time perception and interaction, alongside a local AI compute cluster using GPUs for high-level language processing and task planning. The architecture is demonstrated through an interactive task where a humanoid robot plays a musical instrument with a human. The integration of diverse components aims to maximize performance and responsiveness, highlighting the potential of heterogeneous computing architectures in advancing robotic autonomy and interactive intelligence.",157.02,Phi-4,Nvidia B200 (Cloud Native)
2601.09756v1_Synthetic Data for Veterinary EHR De-identificatio.pdf,"Synthetic Data for Veterinary EHR De-identification: Benefits, Limits, and Safety Trade-offs Under Fixed Compute","['David Brundage, PhD']","This study evaluates the use of large language model (LLM)-generated synthetic veterinary narratives for de-identification of veterinary electronic health records (vEHRs). The research focuses on assessing the safety and utility of synthetic data under different training regimes, particularly when synthetic data is used to augment training or substitute real labeled notes. The study uses a controlled simulation with a labeled corpus derived from PetEVAL, involving 3,750 real clinical notes and 1,249 real training notes. Synthetic notes were generated from masked real seeds, with identifiers removed prior to LLM processing. The study found that while synthetic data can improve de-identification performance when used to expand training exposure, it does not safely replace real supervision under fixed training budgets. The results indicate that increased exposure, rather than the intrinsic quality of synthetic data, drives performance improvements. The study also highlights systematic mismatches between synthetic and real data, particularly affecting lower-frequency entity types.",158.03,Phi-4,Nvidia B200 (Cloud Native)
2601.09757v1_Democracy and Distrust in an Era of Artificial Int.pdf,Democracy & Distrust in an Era of Artificial Intelligence,['Sonia K. Katyal'],"This essay by Sonia K. Katyal explores the challenges posed by AI decision-making to democratic frameworks, particularly concerning the protection of minority rights. It argues that the rise of AI trends such as privatization, prediction, and automation mirrors historical challenges in judicial review, necessitating a new theory of judicial oversight. The essay examines how due process and equal protection can be adapted to ensure better oversight and accountability in the AI era, drawing on legal cases where AI decision-making has been contested.",158.7,Phi-4,Nvidia B200 (Cloud Native)
2601.09760v1_Investigating Tool-Memory Conflicts in Tool-Augmen.pdf,Investigating Tool-Memory Conflicts in Tool-Augmented LLMs,"['Jiali Cheng', 'Rui Pan', 'Hadi Amiri']","This paper introduces the concept of Tool-Memory Conflict (TMC) in tool-augmented large language models (LLMs), where internal parametric knowledge contradicts external tool knowledge. The study identifies TMC as a significant issue, particularly in STEM-related tasks, and evaluates existing conflict resolution techniques, finding them ineffective. The paper aims to explore conditions under which TMC arises, how LLMs prioritize conflicting knowledge, and methodologies to reconcile these conflicts, thereby enhancing the reliability and interpretability of tool-augmented LLMs.",154.17,Phi-4,Nvidia B200 (Cloud Native)
2601.09762v1_Explicating Tacit Regulatory Knowledge from LLMs t.pdf,Explicating Tacit Regulatory Knowledge from LLMs to Auto-Formalize Requirements for Compliance Test Case Generation,"['Zhiyi Xue', 'Xiaohong Chen', 'Min Zhang']","This paper introduces RAFT, a framework designed to automate the generation of compliance test cases by explicating tacit regulatory knowledge from multiple large language models (LLMs). RAFT employs an Adaptive Purification-Aggregation strategy to extract and integrate this knowledge into a domain meta-model, a formal requirements representation, and testability constraints. These artifacts are dynamically used to guide the formalization of requirements and the automated generation of tests. Experiments in financial, automotive, and power domains demonstrate that RAFT achieves expert-level performance, surpassing state-of-the-art methods while reducing generation and review time.",154.15,Phi-4,Nvidia B200 (Cloud Native)
2601.09765v1_AI Survival Stories a Taxonomic Analysis of AI Exi.pdf,AI SURVIVAL STORIES: A TAXONOMIC ANALYSIS OF AI EXISTENTIAL RISK,"['Herman Cappelena', 'Simon Goldstein']","This paper develops a framework for assessing the existential risk posed by AI systems. It analyzes a two-premise argument: AI systems will become extremely powerful, and if they do, they will destroy humanity. The paper constructs a taxonomy of 'survival stories' where humanity survives, each story addressing a failure in one of the premises. The paper discusses the challenges each survival story faces and the different responses they motivate. It also provides rough estimates of 'P(doom)', the probability of humanity's destruction by AI.",153.98,Phi-4,Nvidia B200 (Cloud Native)
2601.09768v1_CLiMB A Domain-Informed Novelty Detection Clusteri.pdf,CLiMB: A Domain-Informed Novelty Detection Clustering Framework for Scientific Discovery,"['Lorenzo Monti', 'Tatiana Muraveva', 'Brian Sheridan', 'Davide Massari', 'Alessia Garofalo', 'Gisella Clementini', 'Umberto Michelucci']","In data-driven scientific discovery, classifying well-characterized phenomena while identifying novel anomalies is challenging. Current semi-supervised clustering algorithms often assume globally representative supervisory signals, leading to rigid constraints that suppress unexpected patterns or require a pre-specified number of clusters. To address this, the CLiMB framework decouples prior knowledge exploitation from the exploration of unknown structures. It uses a sequential two-phase approach: first anchoring known clusters with constrained partitioning, then applying density-based clustering to residual data to reveal arbitrary topologies. Demonstrated on RR Lyrae stars data from Gaia Data Release 3, CLiMB achieves an Adjusted Rand Index of 0.829 with 90% seed coverage in recovering known Milky Way substructures, outperforming heuristic and constraint-based baselines. It also isolates three dynamical features in the unlabelled field, showcasing its potential for scientific discovery.",153.99,Phi-4,Nvidia B200 (Cloud Native)
2601.09770v1_GUI-Eyes Tool-Augmented Perception for Visual Grou.pdf,GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents,"['Chen Chen', 'Jiawei Shao', 'Dakuan Lu', 'Haoyi Hu', 'Xiangcheng Liu', 'Hantao Yao', 'Wu Liu']","The paper introduces GUI-Eyes, a reinforcement learning framework designed for active visual perception in GUI tasks. Unlike existing methods that rely on static visual inputs, GUI-Eyes enables agents to make strategic decisions about when and how to use visual tools like cropping or zooming. This is achieved through a two-stage reasoning process involving coarse exploration and fine-grained grounding, guided by a two-level policy. A spatially continuous reward function is also introduced to provide dense supervision, addressing the reward sparsity issue in GUI environments. The framework significantly outperforms existing supervised and RL-based methods on the ScreenSpot-Pro benchmark, demonstrating the importance of tool-aware active perception for robust and data-efficient GUI agents.",153.6,Phi-4,Nvidia B200 (Cloud Native)
2601.09771v1_PCN-Rec Agentic Proof-Carrying Negotiation for Rel.pdf,PCN-Rec: Agentic Proof-Carrying Negotiation for Reliable Governance-Constrained Recommendation,"['Aradhya Dixit', 'Shreem Dixit']","This paper introduces PCN-Rec, a proof-carrying negotiation pipeline designed to address the challenges faced by modern LLM-based recommenders in satisfying governance constraints such as minimum long-tail exposure or diversity requirements. PCN-Rec separates natural-language reasoning from deterministic enforcement, using a base recommender to produce a candidate window of size 𝑊, which is then negotiated by two agents: a User Advocate optimizing relevance and a Policy Agent enforcing constraints. A mediator LLM synthesizes a Top-𝑁 slate with a structured certificate describing the claimed constraint satisfaction. A deterministic verifier recomputes all constraints from the slate and accepts only verifier-checked certificates. If verification fails, a deterministic constrained-greedy repair produces a compliant slate for re-verification, yielding an auditable trace. The system achieves a 98.55% pass rate on feasible users with governance constraints, compared to a one-shot single-LLM baseline, while preserving utility with only a 0.021 absolute drop in NDCG@10. Differences are statistically significant (p<0.05).",153.9,Phi-4,Nvidia B200 (Cloud Native)
2601.09772v1_Antisocial behavior towards large language model u.pdf,Antisocial behavior towards large language model users: experimental evidence,"['Paweł Niszczota', 'Cassandra Grützner']","This paper presents experimental evidence on antisocial behavior exhibited towards users of large language models. The study, conducted by researchers from Poznań University of Economics and Business and Martin Luther Universität Halle-Wittenberg, explores the dynamics of user interactions with AI systems. Ethical considerations were addressed with approval from the Committee of Ethical Science Research, and informed consent was obtained from all participants. The research was supported by a grant from the National Science Centre, Poland, and the data and materials are publicly available for further scrutiny.",154.73,Phi-4,Nvidia B200 (Cloud Native)
2601.09773v1_Enhancing LUT-based Deep Neural Networks Inference.pdf,Enhancing LUT-based Deep Neural Networks Inference through Architecture and Connectivity Optimization,"['Binglei Lou', 'Ruilin Wu', 'Philip Leong']","This paper introduces SparseLUT, a framework designed to optimize LUT-based deep neural networks (DNNs) for deployment on resource-constrained edge devices like FPGAs. It addresses the challenges of exponential LUT size growth and inefficient random sparse connectivity. The framework includes an architectural enhancement that aggregates multiple PolyLUT sub-neurons via an adder, reducing LUT consumption by 2.0×–13.9× and lowering inference latency by 1.2×–1.6×, while maintaining accuracy. Additionally, a non-greedy training algorithm optimizes neuron connectivity by pruning less significant inputs and regrowing more effective ones, achieving accuracy improvements of up to 2.13% on MNIST and 0.94% on Jet Substructure Classification without additional area and latency overhead.",154.43,Phi-4,Nvidia B200 (Cloud Native)
2601.09805v1_Improving Chain-of-Thought for Logical Reasoning v.pdf,Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention,"['Phuong Minh Nguyen', 'Tien Huu Dang', 'Naoya Inoue']","This paper introduces an end-to-end framework for logical reasoning tasks with large language models (LLMs) that does not rely on external resources. The authors propose an Attention-Aware Intervention (AAI) method, which reweights attention scores across selected heads identified by their logical patterns. This approach aims to enhance the model's reasoning capabilities by leveraging prior knowledge through attention modulation. The paper demonstrates that AAI improves logical reasoning performance across various benchmarks and model architectures with minimal computational overhead.",154.41,Phi-4,Nvidia B200 (Cloud Native)
2601.09806v1_Diffusion-Driven Deceptive Patches Adversarial Man.pdf,Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification,"['Shahrzad Sayyafzadeh', 'Hongmei Chi', 'Shonda Bernadin']","This work presents an end-to-end pipeline for generating, refining, and evaluating adversarial patches to compromise facial biometric systems with forensic analysis and security testing applications. It utilizes a Fast Gradient Sign Method (FGSM) to generate adversarial noise targeting a classifier for identity detection and employs a diffusion model for reverse diffusion to enhance imperceptibility with Gaussian smoothing and adaptive brightness correction. The refined patch is applied to facial images to test its ability to evade recognition systems while maintaining natural visual characteristics. A Vision Transformer (ViT)-GPT2 model generates captions to provide a semantic description of a person’s identity for adversarial images, supporting forensic interpretation and documentation for identity evasion attacks and recognition. The pipeline evaluates changes in identity classification, captioning results, and the vulnerability of facial identity verification and expression to adversarial attacks. Detecting and mitigating these attacks in forensic settings using perceptual hashing is necessary, and the study successfully detected and analyzed adversaries generated with 0.95% SSIM.",154.85,Phi-4,Nvidia B200 (Cloud Native)
2601.09809v1_QFed Parameter-Compact Quantum-Classical Federated.pdf,QFed: Parameter-Compact Quantum-Classical Federated Learning,"['Samar Abdelghani', 'Soumaya Cherkaoui']","This study explores the integration of quantum computing with federated learning to address challenges such as statistical heterogeneity, system diversity, and computational burdens from complex models. The proposed QFed framework aims to enhance computational efficiency in edge device networks by reducing the parameter count in classical models through quantum-assisted techniques. Experimental results using the FashionMNIST dataset demonstrate a 77.6% reduction in parameter count for a VGG-like model while maintaining comparable accuracy to classical methods, highlighting the potential of quantum computing in federated learning contexts.",154.57,Phi-4,Nvidia B200 (Cloud Native)
2601.09814v1_Explainable Deep Learning for Pediatric Pneumonia .pdf,Explainable Deep Learning for Pediatric Pneumonia Detection in Chest X-Ray Images,"['Adil O. Khadidos', 'Aziida Nanyonga', 'Alaa O. Khadidos', 'Olfat M. Mirza', 'Mustafa Tahsin Yilmaz']","This study compares two state-of-the-art convolutional neural network architectures, DenseNet121 and EfficientNet-B0, for automated pediatric pneumonia detection using chest X-ray images. A dataset of 5,863 pediatric chest X-ray images was preprocessed and used to fine-tune the models with pretrained ImageNet weights. The performance was evaluated using metrics such as accuracy, F1-score, Matthews Correlation Coefficient, and recall. EfficientNet-B0 outperformed DenseNet121 in classification performance. Both models demonstrated high sensitivity to pneumonia detection. Explainability was incorporated using Grad-CAM and LIME to visualize decision-contributing regions.",154.46,Phi-4,Nvidia B200 (Cloud Native)
2601.09822v2_LLM-Based Agentic Systems for Software Engineering.pdf,LLM-Based Agentic Systems for Software Engineering: Challenges and Opportunities,"['Yongjian Tang', 'Thomas Runkler']","This paper reviews the emerging paradigm of LLM-based multi-agent systems in software engineering, examining their applications across the Software Development Life Cycle (SDLC). It explores topics such as language model selection, evaluation benchmarks, agentic frameworks, and communication protocols. The paper identifies key challenges and future research opportunities, focusing on multi-agent orchestration, human-agent coordination, computational cost optimization, and effective data collection. The aim is to provide researchers and practitioners with insights into the current landscape of agentic systems within software engineering.",155.0,Phi-4,Nvidia B200 (Cloud Native)
2601.09841v2_A pipeline for enabling path-specific causal fairn.pdf,APIPELINE FOR ENABLING PATH-SPECIFIC CAUSAL FAIRNESS IN OBSERVATIONAL HEALTH DATA,"['Aparajita Kashyap', 'Sara Matijevic', 'Noémie Elhadad', 'Steven A. Kushner', 'Shalmali Joshi']","This paper introduces a pipeline for training machine learning models that ensure path-specific causal fairness in observational health data. It addresses the need to mitigate both direct and indirect healthcare biases, providing a framework that considers social and medical contexts. The pipeline allows for the training of causally fair models by leveraging foundation models trained without fairness constraints, thus enabling fair downstream predictions in tasks with known disparities. The work contributes to the understanding of the fairness-accuracy tradeoff by distinguishing between direct and indirect sources of bias and offers a model-agnostic approach to achieving causally fair machine learning in healthcare.",154.81,Phi-4,Nvidia B200 (Cloud Native)
2601.09851v1_ViSIL Unified Evaluation of Information Loss in Mu.pdf,ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning,"['Po-han Li', 'Shenghui Chen', 'Ufuk Topcu', 'Sandeep Chinchali']","This paper introduces the Video Summary Information Loss (ViSIL) score, an information-theoretic framework designed to evaluate the information loss in multimodal video summaries. Traditional metrics like BLEU or ROUGE are inadequate for assessing information coverage across different modalities, such as text and keyframes. ViSIL quantifies the video information not captured by a summary using vision-language model inference, enabling direct comparison across various multimodal summary formats. The paper demonstrates that ViSIL scores correlate significantly with human and VLM performance on Video Question Answering tasks. Additionally, ViSIL facilitates the selection of summaries that optimize the trade-off between information loss and processing speed, outperforming text summaries in VQA accuracy without increasing processing load.",154.48,Phi-4,Nvidia B200 (Cloud Native)
2601.09853v1_MedRedFlag Investigating how LLMs Redirect Misconc.pdf,MedRedFlag: Investigating how LLMs Redirect Misconceptions in Real-World Health Communication,"['Sraavya Sambara', 'Yuan Pu', 'Ayman Ali', 'Vishala Mishra', 'Lionel Wong', 'Monica Agrawal']","This paper investigates how large language models (LLMs) handle real-world health questions that contain false assumptions or premises. The authors developed a dataset, MedRedFlag, consisting of over 1100 questions from Reddit that require redirection. They compared responses from state-of-the-art LLMs to those from clinicians and found that LLMs often fail to properly redirect questions, potentially leading to suboptimal medical decision-making. This highlights a significant gap in LLM performance in real-world health communication and raises safety concerns for patient-facing medical AI systems.",154.34,Phi-4,Nvidia B200 (Cloud Native)
2601.09855v1_Thinking Long but Short Stable Sequential Test-Tim.pdf,Stable Sequential Test-Time Scaling for Large Reasoning Models,"['Michael R. Metel', 'Yufei Cui', 'Boxing Chen', 'Prasanna Parthasarathi']","This paper introduces a novel method called Min-Seek for sequential test-time scaling, aimed at improving the accuracy of large reasoning models without the need for reasoning length fine-tuning. Min-Seek stabilizes accuracy across various reasoning tasks and efficiently manages computational resources by dynamically encoding keys in a custom KV cache. This method allows models to reason beyond their maximum context length with linear computational complexity, addressing the limitations of current sequential test-time scaling approaches.",154.48,Phi-4,Nvidia B200 (Cloud Native)
2601.09858v1_OUTLINEFORGE Hierarchical Reinforcement Learning w.pdf,Hierarchical Reinforcement Learning with Explicit States for Scientific Writing,"['Yilin Bao', 'Ziyao He', 'Zayden Yang']","This paper introduces a reinforcement learning framework for scientific paper generation, addressing challenges in document-level planning and factual grounding. The framework models scientific outline construction as a long-horizon planning problem, using structured actions to incrementally build manuscripts. It employs a two-stage optimization procedure: backward outline reconstruction for global structural consistency and forward value-guided reinforcement learning with rewards for scientific correctness, discourse coherence, and citation fidelity. The authors also introduce a benchmark for evaluating scientific paper generation, showing improvements over neural and LLM baselines in structural coherence and citation reliability.",154.48,Phi-4,Nvidia B200 (Cloud Native)
2601.09865v1_Advancing Model Refinement Muon-Optimized Distilla.pdf,Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment,"['Jacob Sander', 'Brian Jalaian', 'Venkat R. Dasarivenkateswara']","This paper addresses the challenges of deploying Large Language Models (LLMs) on resource-constrained edge devices by proposing an integrated framework that combines GPTQ-based quantization, low-rank adaptation (LoRA), and a specialized data distillation process. The framework aims to significantly reduce model size and complexity while maintaining or enhancing task-specific performance. By leveraging techniques such as knowledge distillation via Kullback-Leibler divergence, Bayesian hyperparameter optimization, and the Muon optimizer, the proposed pipeline achieves up to 2× memory compression and efficient inference for specialized tasks. Empirical results show superior performance on standard LLM benchmarks compared to GPTQ quantization alone, with the Muon optimizer enhancing the resistance of fine-tuned models to accuracy decay during quantization.",153.97,Phi-4,Nvidia B200 (Cloud Native)
2601.09869v1_A Scoping Review of the Ethical Perspectives on An.pdf,A SCOPING REVIEW OF THE ETHICAL PERSPECTIVES ON ANTHROPOMORPHISING LARGE LANGUAGE MODEL-BASED CONVERSATIONAL AGENTS,"['Andrea Ferrario', 'Rasita Vinay', 'Matteo Casserini', 'Alessandro Facchini']","This scoping review examines the ethical perspectives on anthropomorphising large language model (LLM)-based conversational agents (CAs). It highlights the phenomenon of anthropomorphisation, where non-human entities are ascribed human-like qualities, and discusses the ethical concerns and opportunities it presents. The review synthesizes conceptual foundations, ethical challenges and opportunities, and methodological approaches across five databases and three preprint repositories. It finds convergence on attribution-based definitions but divergence in operationalization, a predominantly risk-forward normative framing, and limited empirical work linking interaction effects to governance guidance. The paper concludes with a research agenda and design/governance recommendations for ethically deploying anthropomorphic cues in LLM-based conversational agents.",154.38,Phi-4,Nvidia B200 (Cloud Native)
2601.09871v1_Epistemology gives a Future to Complementarity in .pdf,Epistemology Gives A Future To Complementarity In Human-AI Interactions,"['Andrea Ferrario', 'Alessandro Facchini', 'Juan M. Durán']","The paper addresses the theoretical challenges of human-AI complementarity, which claims that a human supported by an AI system can outperform either alone in decision-making. It lacks precise theoretical anchoring and is often formalized as a post hoc indicator of predictive accuracy. The authors leverage epistemology, specifically computational reliabilism, to reframe complementarity within the discourse on justificatory AI. They argue that historical instances of complementarity serve as evidence of reliable epistemic processes in human-AI interactions. This approach emphasizes the role of complementarity in calibrating decision-making to the reliability of AI-supported processes, rather than merely providing a measure of predictive accuracy. The paper highlights the importance of aligning human-AI teams with epistemic standards and socio-technical practices to enhance reliability in high-stakes decision-making domains like healthcare, education, and public administration.",154.15,Phi-4,Nvidia B200 (Cloud Native)
2601.09879v1_MedVL-SAM2 A unified 3D medical vision-language mo.pdf,MedVL-SAM2: A unified 3D medical vision–language model for multimodal reasoning and prompt-driven segmentation,"['Yang Xing', 'Jiong Wu', 'Savas Ozdemir', 'Ying Zhang', 'Yang Yang', 'Wei Shao', 'Kuang Gong']","MedVL-SAM2 is a unified 3D medical multimodal model designed to integrate image-level reasoning and pixel-level perception for 3D medical imaging. It supports tasks such as report generation, visual question answering, and various segmentation paradigms including semantic, referring, and interactive segmentation. The model leverages a SAM2-based volumetric segmentation module for precise spatial reasoning and is trained on a large-scale corpus of 3D CT image–text pairs. It achieves state-of-the-art performance across multiple tasks, demonstrating reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning.",154.31,Phi-4,Nvidia B200 (Cloud Native)
2601.09881v1_Transition Matching Distillation for Fast Video Ge.pdf,Transition Matching Distillation for Fast Video Generation,"['Weili Nie', 'Julius Berner', 'Nanye Ma', 'Chao Liu', 'Saining Xie', 'Arash Vahdat']","This paper introduces Transition Matching Distillation (TMD), a framework for distilling video diffusion models into efficient few-step generators. TMD matches the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, using lightweight conditional flows. The diffusion backbone is decomposed into a main backbone for semantic representation extraction and a flow head for inner flow updates. The method adapts a pretrained video diffusion model by introducing a flow head and applying distribution matching distillation. Experiments show that TMD offers a strong trade-off between generation speed and visual quality, outperforming existing models in terms of visual fidelity and prompt adherence under comparable inference costs.",152.6,Phi-4,Nvidia B200 (Cloud Native)
2601.09883v1_Beyond Rule-Based Workflows An Information-Flow-Or.pdf,Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL,"['Xinxing Ren', 'Quagmire Zang', 'Caelum Forder', 'Suman Deb', 'Ahsen Tahir', 'Roman J. Georgio', 'Peter Carroll', 'Zekun Guo']","This paper addresses the limitations of existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) that rely on predefined workflows, which require substantial manual effort and cannot cover the state space of complex tasks. The authors propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL. This paradigm uses a dedicated orchestrator to dynamically coordinate agents through natural language, without predefined workflows. The approach is evaluated on the GAIA benchmark, outperforming the baseline OWL system in accuracy and demonstrating more flexible task monitoring and robust handling of edge cases.",153.68,Phi-4,Nvidia B200 (Cloud Native)
2601.09896v1_The Algorithmic Gaze An Audit and Ethnography of t.pdf,The Algorithmic Gaze: An Audit and Ethnography of the LAION-Aesthetics Predictor Model,"['JORDAN TAYLOR', 'WILLIAM AGNEW', 'MAARTEN SAP', 'SARAH E. FOX', 'HAIYI ZHU']","This paper examines the LAION Aesthetic Predictor (LAP), a model used to curate datasets for training visual generative AI models. The study audits LAP across three datasets, revealing biases in aesthetic filtering, such as a disproportionate inclusion of images with captions mentioning women and exclusion of those mentioning men or LGBTQ+ people. The model also favors realistic images from western and Japanese artists, reinforcing historical biases in art. A digital ethnography of LAP's development materials indicates that these biases stem from the predominantly English-speaking and western backgrounds of the contributors. The authors discuss the representational harms of such biases and advocate for a shift towards more pluralistic aesthetic evaluations in AI.",154.28,Phi-4,Nvidia B200 (Cloud Native)
2601.09902v1_A Novel Contrastive Loss for Zero-Day Network Intr.pdf,A Novel Contrastive Loss for Zero-Day Network Intrusion Detection,"['Jack Wilkie', 'Hanan Hindy', 'Craig Michie', 'Christos Tachtatzis', 'James Irvine', 'Robert Atkinson']","This paper introduces a novel contrastive loss function designed to enhance machine learning models for network intrusion detection, particularly in identifying zero-day attacks. Traditional machine learning approaches struggle with zero-day attacks due to their reliance on previously encountered attack classes. The proposed method leverages contrastive learning to improve robustness against imbalanced data and generalizes to zero-day attacks by learning from both benign and known malicious traffic. Experimental results on the Lycos2017 dataset demonstrate significant improvements in AUROC for both known and zero-day attack detection, as well as enhancements in OpenAUC for open-set recognition.",154.54,Phi-4,Nvidia B200 (Cloud Native)
2601.09913v1_Continuum Memory Architectures for Long-Horizon LL.pdf,Continuum Memory Architectures for Long-Horizon LLM Agents,['Joe Logan'],"This paper introduces the Continuum Memory Architecture (CMA) as a solution to the limitations of retrieval-augmented generation (RAG) in large language model (LLM) agents. Unlike RAG, which treats memory as a static, stateless lookup table, CMA maintains and updates internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. The paper outlines the architectural requirements of CMA and demonstrates its behavioral advantages over RAG in tasks that require dynamic memory management. Preliminary evaluations show that CMA-class behaviors offer improvements in knowledge updates, temporal association, associative recall, and contextual disambiguation, while also highlighting challenges such as latency, drift, and interpretability.",154.67,Phi-4,Nvidia B200 (Cloud Native)
2601.09921v1_Learning to Decode in Parallel Self-Coordinating N.pdf,Learning to Decode in Parallel: Self-Coordinating Neural Network for Real-Time Quantum Error Correction,"['Kai Zhang', 'Zhengzhong Yi', 'Shaojun Guo', 'Linghang Kong', 'Situ Wang', 'Xiaoyu Zhan', 'Tan He', 'Weiping Lin', 'Tao Jiang', 'Dongxin Gao', 'Yiming Zhang', 'Fangming Liu', 'Fang Zhang', 'Zhengfeng Ji', 'Fusheng Chen', 'Jianxin Chen']","The paper discusses the development of fast and reliable neural network decoders for quantum error correction, highlighting their potential to surpass traditional decoding algorithms in accuracy. The focus is on a self-coordinating neural network capable of real-time error correction, which is crucial for fault-tolerant quantum computation.",154.05,Phi-4,Nvidia B200 (Cloud Native)
2601.09923v1_CaMeLs Can Use Computers Too System-level Security.pdf,SYSTEM-LEVEL SECURITY FOR COMPUTER USE AGENTS,"['Hanna Foerster', 'Robert Mullins', 'Tom Blanchard', 'Nicolas Papernot', 'Kristina Nikolić', 'Florian Tramèr', 'Ilia Shumailov', 'Cheng Zhang', 'Yiren Zhao']","The paper addresses the vulnerability of AI agents to prompt injection attacks, which can hijack agent behavior to cause harm. It proposes a solution for Computer Use Agents (CUAs) by introducing Single-Shot Planning, which allows a trusted planner to generate a complete execution graph with conditional branches before any observation of potentially malicious content. This approach provides control flow integrity guarantees against instruction injections. However, additional measures are necessary to prevent Branch Steering attacks. The design is evaluated on OSWorld, showing that it can maintain a significant portion of performance while enhancing security.",154.05,Phi-4,Nvidia B200 (Cloud Native)
2601.09929v1_Hallucination Detection and Mitigation in Large La.pdf,Hallucination Detection and Mitigation in Large Language Models,"['Ahmad Pesaranghader', 'Erin Li']","This paper introduces a comprehensive operational framework for managing hallucinations in Large Language Models (LLMs) and Large Reasoning Models (LRMs), particularly in high-stakes domains like finance and law. The framework is built on a continuous improvement cycle that emphasizes root cause awareness for both detection and mitigation of hallucinations. It categorizes sources of hallucinations into model, data, and context-related factors, allowing for targeted interventions. The framework integrates various detection methods, such as uncertainty estimation and reasoning consistency, with mitigation strategies like knowledge grounding and confidence calibration. A tiered architecture and a financial data extraction case study demonstrate the framework's application, forming a closed feedback loop for enhancing reliability. This systematic approach aims to build trustworthy generative AI systems in regulated environments.",154.3,Phi-4,Nvidia B200 (Cloud Native)
2601.09933v1_Malware Classification using Diluted Convolutional.pdf,Malware Classification using Diluted Convolutional Neural Network with Fast Gradient Sign Method,"['Ashish Anand', 'Bhupendra Singh', 'Sunil Khemka', 'Bireswar Banerjee', 'Vishi Singh Bhatia', 'Piyush Ranjan']","This research addresses the challenges of malware classification by proposing a Fast Gradient Sign Method with Diluted Convolutional Neural Network (FGSM-DICNN). The DICNN model uses diluted convolutions to increase the receptive field, allowing it to capture dispersed malware patterns with fewer features and without adding parameters. The FGSM strategy enhances accuracy through one-step perturbations during training, offering a defensive advantage with lower computational cost. The FGSM-DICNN model achieves 99.44% accuracy, outperforming existing approaches like Custom Deep Neural Network (DCNN).",154.44,Phi-4,Nvidia B200 (Cloud Native)
2601.09949v2_Kinematic Tokenization Optimization-Based Continuo.pdf,Kinematic Tokenization: Optimization-Based Continuous-Time Tokens for Learnable Decision Policies in Noisy Time Series,"['Griffin M. Kearney, Ph.D.']","This paper introduces Kinematic Tokenization, an optimization-based continuous-time representation for noisy time series data. It reconstructs explicit splines from noisy measurements and tokenizes local spline coefficients such as position, velocity, acceleration, and jerk. The method is applied to financial time series data, specifically asset prices and trading volume profiles. The study demonstrates that continuous spline tokens maintain calibrated, non-trivial action distributions and stable policies under a risk-averse asymmetric classification objective, unlike several discrete baselines that collapse to an absorbing cash policy. The results suggest that explicit continuous-time tokens can enhance the learnability and calibration of selective decision policies in noisy time series under abstention-inducing losses.",154.45,Phi-4,Nvidia B200 (Cloud Native)
2601.09966v1_A Sustainable AI Economy Needs Data Deals That Wor.pdf,A Sustainable AI Economy Needs Data Deals That Work for Generators,"['Ruoxi Jia', 'Luis Oala', 'Wenjie Xiong', 'Suqin Ge', 'Jiachen T. Wang', 'Feiyang Kang', 'Dawn Song']","The paper argues that the machine learning value chain is economically unsustainable due to a data processing inequality that benefits aggregators over data generators. By analyzing seventy-three public data deals, the authors find that most value accrues to aggregators, with minimal royalties for creators and opaque deal terms. This imbalance poses risks to the sustainability of current learning algorithms. The paper identifies three structural issues—missing provenance, asymmetric bargaining power, and non-dynamic pricing—as drivers of this inequality. The authors propose the Equitable Data-Value Exchange (EDVEX) Framework to create a fair market benefiting all participants and suggest research directions to improve data deals.",154.33,Phi-4,Nvidia B200 (Cloud Native)
2601.09972v1_Chinese Labor Law Large Language Model Benchmark.pdf,Chinese Labor Law Large Language Model Benchmark,"['Zixun Lan', 'Maochun Xu', 'Yifan Ren', 'Rui Wu', 'Jianghui Zhou', 'Xueyang Cheng', 'Jian’an Ding', 'Xinheng Wang', 'Mingmin Chi', 'Fei Ma']","This paper introduces LaborLawLLM, a large language model tailored for the labor law domain, addressing the limitations of general-purpose models in specialized legal subdomains. The paper presents LaborLawBench, a comprehensive benchmark for diverse labor law tasks, and demonstrates that LaborLawLLM significantly outperforms both general-purpose and existing legal-specific LLMs. The work fills a research gap in labor law-specific legal AI and provides a scalable methodology for developing specialized LLMs in other legal subfields, enhancing the accuracy, reliability, and societal value of legal AI applications.",153.23,Phi-4,Nvidia B200 (Cloud Native)
2601.09974v1_SPRInG Continual LLM Personalization via Selective.pdf,SPRInG: Continual LLM Personalization via Selective Parametric Adaptation and Retrieval-Interpolated Generation,"['Seoyeon Kim', 'Jaehyung Kim']","The paper introduces SPRING, a novel semi-parametric framework for continual personalization of Large Language Models (LLMs). Unlike traditional methods that assume static user preferences, SPRING addresses the dynamic nature of user interests by employing drift-driven selective adaptation. This approach uses a likelihood-based scoring function to identify high-novelty interactions, allowing the model to update user-specific adapters selectively while preserving important historical data in a replay buffer. During inference, SPRING combines parametric knowledge with retrieved history through logit interpolation. Experiments demonstrate that SPRING outperforms existing baselines in long-form personalized generation tasks, showcasing its robustness for real-world applications.",154.36,Phi-4,Nvidia B200 (Cloud Native)
2601.09980v1_Performance of AI agents based on reasoning langua.pdf,ALD optimization using reasoning LLMs,['Angel Yanguas-Gil'],"This paper investigates the use of reasoning large language models (LLMs) for optimizing atomic layer deposition (ALD) processes. The study focuses on an agent built on a reasoning LLM tasked with finding optimal dose times for ALD precursors and coreactants without prior knowledge of the process. The agent interacts iteratively with an ALD reactor in an unsupervised manner. Results indicate that reasoning models like OpenAI’s o3 and GPT5 can successfully complete the optimization task, although there is significant run-to-run variability due to the non-deterministic nature of the model’s responses. The agent employs a two-step process to generate and structure reasoning responses, which are analyzed to ensure sound logic based on self-limited processes and saturation. However, the agent can sometimes be misled by its own prior choices during optimization.",154.5,Phi-4,Nvidia B200 (Cloud Native)
2601.09982v1_Context Volume Drives Performance Tackling Domain .pdf,Volume Drives Performance: Tackling Domain Shift in Extremely Low-Resource Translation via RAG,"['David Samuel Setiawan', 'Raphaël Merx', 'Jey Han Lau']","This paper addresses the challenge of domain shift in Neural Machine Translation (NMT) for low-resource languages, specifically focusing on the Dhao language of Eastern Indonesia. The study quantifies performance degradation when an NMT model trained on the New Testament (NT) is applied to the Old Testament (OT). To mitigate this, the authors introduce a hybrid framework combining a fine-tuned NMT model with a Large Language Model (LLM) using Retrieval-Augmented Generation (RAG). This approach significantly improves translation quality, achieving performance close to the original in-domain scores. The analysis highlights the importance of the number of retrieved examples over the retrieval algorithm itself, with the LLM acting as a robust 'safety net' for zero-shot domains.",153.98,Phi-4,Nvidia B200 (Cloud Native)
2601.10010v1_VERHallu Evaluating and Mitigating Event Relation .pdf,VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video Large Language Models,"['Zefan Zhang', 'Kehua Zhu', 'Shijie Jiang', 'Hongyuan Lu', 'Shengkai Sun', 'Tian Bai']","This paper introduces VERHallu, a novel benchmark for evaluating event relation hallucination in Video Large Language Models (VideoLLMs). It focuses on causal, temporal, and subevent relations between events, featuring tasks like relation classification, question answering, and counterfactual question answering. The analysis shows that current VideoLLMs struggle with dense-event relation reasoning, often relying on prior knowledge and insufficient frame-level cues. The proposed Key-Frame Propagating (KFP) strategy reallocates frame-level attention to enhance multi-event understanding, effectively mitigating event relation hallucination without affecting inference speed.",154.26,Phi-4,Nvidia B200 (Cloud Native)
2601.10011v1_Memo-SQL Structured Decomposition and Experience-D.pdf,Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL,"['Zerui Yang', 'Weichuan Wang', 'Yanwei Xu', 'Linqi Song', 'Yudai Matsuda', 'Wei Han', 'Bo Bai']","Memo-SQL is a training-free framework designed to address the limitations of existing NL2SQL systems, which rely on in-context learning with only correct examples and suffer from a trade-off between accuracy and efficiency. Memo-SQL introduces structured decomposition and experience-aware self-correction to enhance robustness and performance. It employs strategies like entity-wise, hierarchical, and atomic sequential decomposition to encourage diverse reasoning. Additionally, it builds a dynamic memory of successful queries and historical error-fix pairs, using retrieval-augmented prompting to refine outputs without fine-tuning or external APIs. On the BIRD benchmark, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using significantly fewer resources than prior test-time scaling approaches.",153.93,Phi-4,Nvidia B200 (Cloud Native)
2601.10018v1_Empowering Older Adults in Digital Technology Use .pdf,Empowering Older Adults in Digital Technology Use with Foundation Models,"['Hasti Sharifi', 'Homaira Huda Shomee', 'Sourav Medya', 'Debaleena Chattopadhyay']","This study explores the communication challenges faced by older adults when using digital technologies, particularly due to unfamiliarity with technical terminology and age-related cognitive changes. The research employs a diary study to collect technology-related queries from older adults and uses reflexive thematic analysis to identify communication barriers such as verbosity, incompleteness, over-specification, and under-specification. The study evaluates the use of foundation models, specifically GPT-4o, to paraphrase queries and improve solution accuracy. Results show that AI-rephrased queries significantly enhance solution accuracy and comprehension among both younger and older adults. The study also introduces the OATS dataset, which demonstrates strong fidelity and face validity, offering a scalable resource for developing AI systems that better serve aging populations.",154.44,Phi-4,Nvidia B200 (Cloud Native)
2601.10025v1_Structured Personality Control and Adaptation for .pdf,Structured Personality Control and Adaptation for LLM Agents,"['JINPENG WANG', 'XINYU JIA', 'WEI WEI HENG', 'YUQUAN LI', 'BINBIN SHI', 'QIANLEI CHEN', 'GUANNAN CHEN', 'JUNXIA ZHANG', 'YUYU YIN']","This paper presents a framework for modeling the personality of Large Language Models (LLMs) using Jungian psychological types. It integrates three mechanisms: a dominant–auxiliary coordination mechanism for coherent core expression, a reinforcement–compensation mechanism for temporary adaptation to context, and a reflection mechanism for long-term personality evolution. The framework allows LLMs to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating their underlying structure. The effectiveness of personality alignment is evaluated using Myers–Briggs Type Indicator questionnaires and tested under diverse scenarios. The findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, facilitating naturalistic agent design in human-computer interaction.",153.91,Phi-4,Nvidia B200 (Cloud Native)
2601.10029v1_PaperScout An Autonomous Agent for Academic Paper .pdf,PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization,"['Tingyue Pan', 'Jie Ouyang', 'Mingyue Cheng', 'Qingchuan Li', 'Zirui Liu', 'Mingfan Pan', 'Shuo Yu', 'Qi Liu']","PaperScout is an autonomous agent designed to enhance academic paper search by treating it as a sequential decision-making process. Unlike traditional static workflows, PaperScout dynamically decides when and how to use search and expand tools based on the context it accumulates. The paper introduces Proximal Sequence Policy Optimization (PSPO), a method that aligns optimization with agent-environment interactions, addressing the challenge of granularity mismatch in reinforcement learning for multi-turn tasks. Experiments show that PaperScout outperforms existing workflow-driven and reinforcement learning baselines in recall and relevance, demonstrating the effectiveness of its adaptive framework and optimization strategy.",154.39,Phi-4,Nvidia B200 (Cloud Native)
2601.10031v1_FilDeep Learning Large Deformations of Elastic-Pla.pdf,FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data,"['Jianheng Tang', 'Shilong Tao', 'Zhe Feng', 'Haonan Sun', 'Menglu Wang', 'Zhanxing Zhu', 'Yunhuai Liu']","The paper introduces FilDeep, a Fidelity-based Deep Learning framework designed to address the challenges of modeling large deformations in elastic-plastic solids. Traditional numerical methods face limitations due to the need for high-quantity and high-accuracy datasets, which are difficult to obtain. FilDeep resolves this by training with both low-fidelity and high-fidelity data, capturing long-range physical interactions across multi-fidelity data. The framework is demonstrated to achieve state-of-the-art performance in manufacturing applications.",154.18,Phi-4,Nvidia B200 (Cloud Native)
2601.10038v1_What Understanding Means in AI-Laden Astronomy.pdf,What Understanding Means in AI-Laden Astronomy,"['Yuan-Sen Ting', 'André Curtis-Trudel', 'Siyu Yao']","This paper explores the philosophical implications of integrating artificial intelligence (AI) into astronomical research. It addresses fundamental questions about the nature of understanding, discovery, and progress in science, emphasizing the need for philosophical reflection alongside technological advancements. The authors argue that AI's role in science extends beyond mere prediction and optimization, necessitating a deeper examination of what constitutes genuine scientific understanding and discovery. They highlight the importance of interdisciplinary collaboration, particularly between philosophers and scientists, to navigate the transformative impact of AI on scientific practices. The paper draws on discussions from an interdisciplinary workshop titled 'Philosophy Sees the Algorithm,' which focused on these themes.",154.06,Phi-4,Nvidia B200 (Cloud Native)
2601.10073v1_ReaMIL Reasoning- and Evidence-Aware Multiple Inst.pdf,ReaMIL: Reasoning- and Evidence-Aware Multiple Instance Learning for Whole-Slide Histopathology,"['Hyun Do Jung', 'Jungwon Choi', 'Hwiyoung Kim']","ReaMIL introduces a novel multiple instance learning approach for whole-slide histopathology that incorporates a light selection head to a strong MIL backbone. This head generates soft per-tile gates and is trained with a budgeted-sufficiency objective, which enforces the true-class probability to be at least τ using only selected evidence under a sparsity budget. The approach yields compact evidence sets without sacrificing performance. ReaMIL achieves competitive or improved AUC scores across several datasets and provides diagnostics for evidence efficiency. It requires no additional supervision, integrates with standard MIL training, and produces slide-level overlays. The method is evaluated using metrics like mean minimal sufficient K (MSK), AUKC, and contiguity, offering a rigorous assessment of model behavior on whole-slide images (WSIs).",153.83,Phi-4,Nvidia B200 (Cloud Native)
2601.10079v1_Sparse-RL Breaking the Memory Wall in LLM Reinforc.pdf,Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts,"['Sijia Luo', 'Xiaokang Zhang', 'Yuxuan Hu', 'Bohan Zhang', 'Ke Wang', 'Jinbo Su', 'Mengshu Sun', 'Lei Liang', 'Jing Zhang']","The paper introduces Sparse-RL, a method to enable stable reinforcement learning (RL) training for large language models (LLMs) under sparse rollouts. It addresses the memory bottleneck caused by storing large Key-Value (KV) caches during long-horizon rollouts, which limits efficient training on hardware with restricted memory. Sparse-RL mitigates policy mismatch issues arising from applying existing KV compression techniques directly to RL training, which can lead to performance collapse. The approach incorporates Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct off-policy bias due to information loss from compression. Experimental results demonstrate that Sparse-RL reduces rollout overhead while maintaining performance and enhances model robustness during sparse inference deployment.",153.62,Phi-4,Nvidia B200 (Cloud Native)
2601.10088v1_State of AI An Empirical 100 Trillion Token Study .pdf,State of AI: An Empirical 100 Trillion Token Study with OpenRouter,"['Malika Aubakirova', 'Alex Atallah', 'Chris Clark', 'Justin Summerville', 'Anjney Midha']","This paper presents an empirical study analyzing over 100 trillion tokens of real-world interactions with large language models (LLMs) using the OpenRouter platform. The study observes significant adoption of open-weight models, the popularity of creative roleplay and coding assistance tasks, and the rise of agentic inference. It identifies foundational user cohorts with long-term engagement, termed the 'Cinderella Glass Slipper' effect. The findings highlight the complex and multifaceted ways developers and end-users engage with LLMs, providing insights for model builders, AI developers, and infrastructure providers to inform better design and deployment of LLM systems.",154.33,Phi-4,Nvidia B200 (Cloud Native)
2601.10090v1_Difficulty-guided Sampling Bridging the Target Gap.pdf,Difficulty-guided Sampling: Bridging the Target Gap between Dataset Distillation and Downstream Tasks,"['Mingzhuo Li', 'Guang Li', 'Linfeng Ye', 'Jiafeng Mao', 'Takahiro Ogawa', 'Konstantinos N. Plataniotis', 'Miki Haseyama']","This paper introduces difficulty-guided sampling (DGS) to address the target gap between dataset distillation objectives and downstream tasks, aiming to enhance the performance of dataset distillation. The authors propose leveraging task-specific characteristics to improve the alignment between distilled datasets and downstream tasks. Specifically, for image classification, they introduce the concept of difficulty and propose DGS as a post-stage sampling module. Additionally, they explore the impact of difficulty through difficulty-aware guidance (DAG), sampling the final distilled dataset from image pools generated by existing methods.",154.45,Phi-4,Nvidia B200 (Cloud Native)
2601.10092v1_LeMoF Level-guided Multimodal Fusion for Heterogen.pdf,LEMOF: LEVEL-GUIDED MULTIMODAL FUSION FOR HETEROGENEOUS CLINICAL DATA,"['Jongseok Kim', 'Seongae Kang', 'Jonghwan Shin', 'Yuhan Lee', 'Ohyun Jo']","This paper introduces Level-guided Modal Fusion (LeMoF), a novel framework designed to integrate level-guided representations within each modality for multimodal clinical prediction. LeMoF addresses the limitations of existing methods that rely on static integration schemes and simple fusion strategies, which fail to fully exploit modality-specific representations. By explicitly separating and learning global modality-level predictions from level-specific discriminative representations, LeMoF achieves a balanced performance between prediction stability and discriminative capability in heterogeneous clinical environments. Experiments on ICU data for length of stay prediction demonstrate that LeMoF outperforms state-of-the-art multimodal fusion techniques across various encoder configurations, highlighting the importance of level-wise integration for robust predictive performance.",154.08,Phi-4,Nvidia B200 (Cloud Native)
2601.10094v1_V-Zero Self-Improving Multimodal Reasoning with Ze.pdf,V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation,"['Han Wang', 'Yi Yang', 'Jingyuan Hu', 'Minfeng Zhu', 'Wei Chen']","V-Zero introduces a framework for self-improving vision-language models (VLMs) using unlabeled images, eliminating the need for human-annotated datasets. It establishes a co-evolutionary loop with two roles: a Questioner and a Solver. The Questioner generates challenging questions, while the Solver optimizes using pseudo-labels from majority voting. Both roles are trained iteratively via Group Relative Policy Optimization (GRPO), leading to mutual enhancement. V-Zero demonstrates significant performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning by +1.7 and general vision-centric tasks by +2.6, showcasing the potential of self-improvement in multimodal systems.",154.39,Phi-4,Nvidia B200 (Cloud Native)
2601.10101v2_Matrix as Plan Structured Logical Reasoning with F.pdf,Matrix as Plan: Structured Logical Reasoning with Feedback-Driven Replanning,"['Ke Chen', 'Jiandian Zeng', 'Zihao Peng', 'Guo Li', 'Guangxue Zhang', 'Tian Wang']","The paper introduces MatrixCoT, a structured Chain-of-Thought (CoT) framework designed to enhance the logical reasoning capabilities of Large Language Models (LLMs). It addresses the limitations of existing CoT prompting and neuro-symbolic methods by normalizing natural language expressions, attaching explicit citation fields, and using a matrix-based planning method to maintain global relations among reasoning steps. This approach ensures more stable execution and verifiable plans. Additionally, a feedback-driven replanning mechanism is introduced to correct errors under semantic-equivalence constraints, improving the robustness and interpretability of LLMs in complex symbolic reasoning tasks without relying on external solvers.",157.27,Phi-4,Nvidia B200 (Cloud Native)
2601.10103v1_FlowAct-R1 Towards Interactive Humanoid Video Gene.pdf,FlowAct-R1: Towards Interactive Humanoid Video Generation,"['Lizhen Wang', 'Yongming Zhu', 'Zhipeng Ge', 'Youwei Zheng', 'Longhao Zhang', 'Tianshu Hu', 'Shiyang Qin', 'Mingshuang Luo', 'Jiaxu Zhang', 'Xin Chen', 'Yulong Wang', 'Zerong Zheng', 'Jianwen Jiang', 'Chao Liang', 'Weifeng Chen', 'Xing Wang', 'Yuan Zhang', 'Mingyuan Gao']","This paper introduces FlowAct-R1, a framework designed for real-time interactive humanoid video generation. It addresses the trade-off between high-fidelity synthesis and real-time interaction by leveraging a MMDiT architecture and a chunkwise diffusion forcing strategy. The framework achieves low-latency responsiveness, maintaining a stable 25fps at 480p resolution with a time-to-first-frame of around 1.5 seconds. It provides full-body control, enabling natural transitions between diverse behavioral states, and demonstrates exceptional behavioral vividness and perceptual realism across various character styles.",158.03,Phi-4,Nvidia B200 (Cloud Native)
2601.10104v1_MathDoc Benchmarking Structured Extraction and Act.pdf,Benchmarking Structured Extraction and Active Refusal on Noisy Mathematics Exam Papers,"['Chenyue Zhou', 'Jiayi Tuo', 'Shitong Qin', 'Wei Dai', 'Mingxuan Wang', 'Ziwei Zhao', 'Duoyang Li', 'Shiyang Su', 'Yanxi Lu', 'Yanbiao Ma']","The paper introduces MathDoc, a benchmark for extracting structured questions from noisy mathematics exam papers. It highlights the challenges of visual noise and the need for models to actively refuse incomplete inputs. MathDoc includes 3,609 curated questions with real-world artifacts and evaluates models on stem accuracy, visual similarity, and refusal capability. Experiments show that while models perform well in extraction, they struggle with refusing illegible inputs, indicating a gap in current MLLMs.",157.89,Phi-4,Nvidia B200 (Cloud Native)
2601.10108v1_SIN-Bench Tracing Native Evidence Chains in Long-C.pdf,SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature,"['Yiming Ren', 'Junjie Wang', 'Yuxin Meng', 'Yihang Shi', 'Zhiqiang Lin', 'Ruihang Chu', 'Yiran Xu', 'Ziming Li', 'Yunfei Zhao', 'Zihan Wang', 'Yu Qiao', 'Ruiming Tang', 'Minghao Liu', 'Yujiu Yang']","The paper introduces the 'Fish-in-the-Ocean' (FITO) paradigm to evaluate multimodal large language models' understanding of long-form scientific papers. Unlike traditional methods that focus on answer matching, FITO requires models to construct explicit cross-modal evidence chains within native documents. The authors present SIN-Bench, a benchmark with tasks like evidence discovery, hypothesis verification, grounded QA, and evidence-anchored synthesis. Experiments reveal that grounding is a primary bottleneck, with models like Gemini-3-pro and GPT-5 showing varying performance in evidence-aligned tasks, highlighting a gap between correctness and traceable support.",156.83,Phi-4,Nvidia B200 (Cloud Native)
2601.10112v1_Repository Intelligence Graph Deterministic Archit.pdf,Repository Intelligence Graph: Deterministic Architectural Map for LLM Code Assistants,"['Tsvi Cherny-Shahar', 'Amiram Yehudai']","The paper introduces the Repository Intelligence Graph (RIG), a deterministic architectural map that aids coding agents in understanding build and test structures, especially in multilingual projects. RIG represents components, dependencies, and tests, and is constructed using SPADE, a deterministic extractor with a CMake plugin. The study evaluates the impact of RIG on the performance of commercial coding agents (Claude Code, Cursor, Codex) across various repositories, showing significant improvements in accuracy and efficiency, particularly in multilingual settings. The findings suggest that RIG helps shift agent failures from structural misunderstandings to reasoning errors, highlighting the importance of graph-based reasoning quality.",158.04,Phi-4,Nvidia B200 (Cloud Native)
2601.10114v1_Following the Teachers Footsteps Scheduled Checkpo.pdf,Following the Teacher’s Footsteps: Scheduled Checkpoint Distillation for Domain-Specific LLMs,"['Cheng Feng', 'Chaoliang Zhong', 'Jun Sun', 'Yusuke Oishi']","This paper addresses the challenge of deploying large language models (LLMs) for domain-specific tasks due to their massive scale. It proposes a novel approach called Scheduled Checkpoint Distillation (SCD) to distill a fine-tuned LLM into a smaller student model. The method aims to allow the student model to match or surpass the teacher's performance by focusing on a Student-Favored Subdomain (SFS) and reducing the deficit on the Teacher-Favored Subdomain (TFS). Experiments demonstrate that SCD consistently outperforms existing distillation methods across various domain tasks, including QA, NER, and text classification in multiple languages.",158.16,Phi-4,Nvidia B200 (Cloud Native)
2601.10120v1_TopoDIM One-shot Topology Generation of Diverse In.pdf,TopoDIM: One-shot Topology Generation of Diverse Interaction Modes for Multi-Agent Systems,"['Rui Sun', 'Jie Ding', 'Chenghua Gong', 'Tianjun Gu', 'Yihang Jiang', 'Juyuan Zhang', 'Liming Pan', 'Linyuan Lü']","The paper introduces TOPODIM, a framework designed for one-shot topology generation with diverse interaction modes in multi-agent systems. It addresses the inefficiencies of existing spatio-temporal interaction paradigms by enabling decentralized execution, which enhances adaptability and privacy. TOPODIM allows agents to autonomously construct heterogeneous communication without iterative coordination, resulting in significant token efficiency and improved task performance. Experiments show a 46.41% reduction in token consumption and a 1.50% improvement in performance over state-of-the-art methods. The framework also demonstrates strong adaptability in organizing communication among heterogeneous agents.",157.61,Phi-4,Nvidia B200 (Cloud Native)
2601.10122v1_Role-Playing Agents Driven by Large Language Model.pdf,"Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future Trends","['Ye Wang', 'Jiaxing Chen', 'Hongjiang Xiao']","This paper reviews the development and key technologies of role-playing language agents (RPLAs), which are at the intersection of natural language processing and human-computer interaction. It traces the evolution from rule-based templates to cognitive simulation stages, focusing on character modeling, memory-augmented prompting, and behavioral decision control. The paper also discusses data construction challenges, evaluation methods, and future directions such as personality evolution, multi-agent collaboration, and multimodal interaction.",153.58,Phi-4,Nvidia B200 (Cloud Native)
2601.10129v1_LaViT Aligning Latent Visual Thoughts for Multi-mo.pdf,LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning,"['Linquan Wu', 'Tianxiang Jiang', 'Yifei Dong', 'Haoyu Yang', 'Fengji Zhang', 'Shichang Meng', 'Ai Xuan', 'Linqi Song', 'Jacky Keung']","This paper addresses the Perception Gap in multimodal latent reasoning, where student models mimic a teacher's textual output without aligning with the teacher's visual attention. The proposed LaViT framework aligns latent visual thoughts by reconstructing the teacher's visual semantics and attention trajectories before text generation. This approach, along with a curriculum sensory gating mechanism, enhances visual grounding and improves performance on complex reasoning tasks, allowing a compact 3B model to outperform larger models.",154.23,Phi-4,Nvidia B200 (Cloud Native)
2601.10130v1_Redundancy-Driven Top-k Functional Dependency Disc.pdf,Redundancy-Driven Top-k Functional Dependency Discovery,"['Xiaolong Wan', 'Xixian Han']","This paper addresses the challenges of discovering functional dependencies (FDs) in relational databases, particularly the computational cost and large result sets. The authors propose the SDP (Selective-Discovery-and-Prune) algorithm, which ranks FDs by redundancy count, a measure of duplicated information explained by an FD. SDP uses an upper bound on redundancy to prune the search space, improving efficiency. The algorithm is enhanced with optimizations such as ordering attributes by partition cardinality, using pairwise statistics in a Partition Cardinality Matrix, and a global scheduler to prioritize promising branches. Experiments on over 40 datasets demonstrate that SDP is significantly faster and more memory-efficient than exhaustive methods.",155.24,Phi-4,Nvidia B200 (Cloud Native)
2601.10131v2_M4olGen Multi-Agent Multi-Stage Molecular Generati.pdf,"M4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints","['Yizhan Li', 'Florence Cloutier', 'Sifan Wu', 'Ali Parviz', 'Boris Knyazev', 'Yan Zhang', 'Glen Berseth', 'Bang Liu']","The paper introduces M4olGen, a novel framework for generating molecules that satisfy precise multi-property constraints. It employs a two-stage approach: Stage I involves a multi-agent reasoner that performs retrieval-anchored, fragment-level edits to generate a candidate molecule near the feasible region. Stage II uses RL-based fine-grained optimization to refine the molecule, minimizing property errors towards target values while controlling edit complexity. The framework leverages a large dataset with reasoning chains and property deltas, enabling deterministic and reproducible supervision. Experiments demonstrate that M4olGen outperforms existing large language models and graph-based algorithms in generating valid molecules that meet specific property targets such as QED, LogP, Molecular Weight, and HOMO/LUMO.",153.91,Phi-4,Nvidia B200 (Cloud Native)
2601.10132v1_Is More Context Always Better Examining LLM Reason.pdf,Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction,"['Yanan Cao', 'Farnaz Fallahi', 'Murali Mohana Krishna Dandu', 'Lalitesh Morishetti', 'Kai Zhao', 'Luyi Ma', 'Sinduja Subramaniam', 'Jianpeng Xu', 'Evren Korpeoglu', 'Kaushiki Nag', 'Sushant Kumar', 'Kannan Achan']","This paper investigates the ability of Large Language Models (LLMs) to predict time intervals between recurring user actions, such as repeated purchases. The study benchmarks LLMs against statistical and machine-learning models in a repurchase scenario. Key findings include LLMs' limited ability to capture quantitative temporal structure and the observation that while moderate context can improve accuracy, excessive user-level detail can degrade performance. The study challenges the assumption that more context always leads to better reasoning and suggests designing future context-aware hybrid models that combine statistical precision with linguistic flexibility.",153.24,Phi-4,Nvidia B200 (Cloud Native)
2601.10137v1_Step-by-Step Causality Transparent Causal Discover.pdf,Step-by-Step Causality: Transparent Causal Discovery with Multi-Agent Tree-Query and Adversarial Confidence Estimation,"['Ziyi Ding', 'Chenfei Ye-Hao', 'Zheyuan Wang', 'Xiao-Ping Zhang']","This paper introduces Tree-Query, a tree-structured, multi-expert LLM framework designed to address the limitations of classical constraint-based methods and recent LLM-based causal oracles in causal discovery. Tree-Query reduces pairwise causal discovery to a sequence of queries about backdoor paths, (in)dependence, latent confounding, and causal direction, providing interpretable judgments with robustness-aware confidence scores. Theoretical guarantees are provided for asymptotic identifiability of four pairwise relations. The framework demonstrates improvements over direct LLM baselines on data-free benchmarks and a diet–weight case study, illustrating its ability to screen confounders and produce stable, high-confidence causal conclusions. Tree-Query offers a principled approach to obtaining data-free causal priors from LLMs, complementing downstream data-driven causal discovery.",153.84,Phi-4,Nvidia B200 (Cloud Native)
2601.10141v1_Understanding and Preserving Safety in Fine-Tuned .pdf,Understanding and Preserving Safety in Fine-Tuned LLMs,"['Jiawen Zhang', 'Yangfan Hu', 'Kejia Chen', 'Lipeng He', 'Jiachen Ma', 'Jian Lou', 'Dan Li', 'Jian Liu', 'Xiaohu Yang', 'Ruoxi Jia']","This paper addresses the safety-utility dilemma in fine-tuning large language models (LLMs) for downstream tasks. It highlights the geometric interaction between safety- and utility-oriented gradients, revealing that safety gradients lie in a low-rank subspace while utility gradients span a broader space. The authors propose a novel approach, safety-preserving fine-tuning (SPF), which removes gradient components conflicting with the safety subspace, ensuring utility convergence and bounding safety drift. Empirical results show that SPF maintains task performance and recovers pre-trained safety alignment, even under adversarial conditions, providing new insights and practical guidance for aligned LLM fine-tuning.",153.21,Phi-4,Nvidia B200 (Cloud Native)
2601.10143v1_History Is Not Enough An Adaptive Dataflow System .pdf,History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis,"['Haochong Xia', 'Yao Long Teng', 'Regan Tan', 'Molei Qin', 'Xinrun Wang', 'Bo An']","In quantitative finance, the gap between training and real-world performance—driven by concept drift and distributional non-stationarity—remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. This paper presents a drift-aware dataflow system that integrates machine learning–based adaptive control into the data curation process. The system combines a parameterized data manipulation module with an adaptive planner–scheduler that employs gradient-based bi-level optimization. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments demonstrate that the framework enhances model robustness and improves risk-adjusted returns, providing a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.",154.43,Phi-4,Nvidia B200 (Cloud Native)
2601.10148v1_DecisionLLM Large Language Models for Long Sequenc.pdf,DecisionLLM: Large Language Models for Long Sequence Decision Exploration,"['Xiaowei Lv', 'Zhiling Zhang', 'Yijun Li', 'Yusen Huo', 'Siyuan Ju', 'Xuyan Li', 'Chunxiang Hong', 'Tianyu Wang', 'Yongcai Wang', 'Peng Sun', 'Chuan Yu', 'Jian Xu', 'Bo Zheng']","This paper explores the application of Large Language Models (LLMs) to long-sequence decision-making tasks, traditionally addressed through reinforcement learning. The authors propose a novel approach, DecisionLLM, which treats trajectories as a distinct modality and aligns them with natural language task descriptions. This allows the model to predict future decisions within a cohesive framework. The study establishes scaling laws that show performance depends on model scale, data volume, and data quality. DecisionLLM demonstrates strong performance in offline experimental benchmarks and bidding scenarios, outperforming traditional Decision Transformer models.",153.58,Phi-4,Nvidia B200 (Cloud Native)
2601.10150v1_Simple Network Graph Comparative Learning.pdf,Simple Network Graph Comparative Learning,"['Qiang Yu', 'Xinran Cheng', 'Shiqiang Xu', 'Chuanyi Liu']","This paper introduces a novel node classification contrast learning method called Simple Network Graph Comparative Learning (SNGCL). It addresses challenges in existing graph contrastive learning methods, such as significant differences from the original view in data enhancement and reliance on numerous negative samples. SNGCL uses a superimposed multilayer Laplace smoothing filter to obtain global and local feature smoothing matrices, which are then processed through a siamese network. An improved triplet recombination loss function is employed to optimize intra-class and inter-class distances. Experimental results demonstrate that SNGCL is competitive with state-of-the-art models in node classification tasks.",157.83,Phi-4,Nvidia B200 (Cloud Native)
2601.10154v1_MHub.ai A Simple Standardized and Reproducible Pla.pdf,"MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging","['Leonard Nürnberg', 'Dennis Bontempi', 'Suraj Pai', 'Curtis Lisle', 'Steve Pieper', 'Ron Kikinis', 'Sil van de Leemput', 'Rahul Soni', 'Gowtham Murugesan', 'Cosmin Ciausu', 'Miriam Groeneveld', 'Felix J. Dorfner', 'Jue Jiang', 'Aneesh Rangnekar', 'Harini Veeraraghavan', 'Joeran S. Bosma', 'Keno Bressem', 'Raymond Mak', 'Andrey Fedorov', 'Hugo JWL Aerts']","MHub.ai is introduced as a platform designed to standardize and simplify the deployment of AI models in medical imaging. It aims to enhance reproducibility and ease of use for researchers and practitioners in the field. The platform is developed by a collaborative team from various institutions, including the Artificial Intelligence in Medicine (AIM) Program at Mass General Brigham and Harvard Medical School, among others.",157.3,Phi-4,Nvidia B200 (Cloud Native)
2601.10155v1_LOOKAT Lookup-Optimized Key-Attention for Memory-E.pdf,Lookup-Optimized Key-Attention for Memory-Efficient Transformers,['Aryan Karmore'],"The paper introduces LOOKAT, a method to compress the key-value (KV) cache in transformers for deployment on edge devices. Traditional quantization methods compress storage but do not reduce bandwidth usage due to the need for dequantization before attention calculation. LOOKAT applies product quantization and asymmetric distance computation to transform attention from memory-bound to compute-bound, achieving significant compression with high output fidelity. It decomposes key vectors into subspaces, learns codebooks, and uses lookup tables for attention score computation, maintaining rank correlation and requiring no architectural changes or additional training.",158.01,Phi-4,Nvidia B200 (Cloud Native)
2601.10157v1_MMPG MoE-based Adaptive Multi-Perspective Graph Fu.pdf,MMPG: MoE-based Adaptive Multi-Perspective Graph Fusion for Protein Representation Learning,"['Yusong Wang', 'Jialun Shen', 'Zhihao Wu', 'Yicheng Xu', 'Shiyin Tan', 'Mingkun Xu', 'Changshuo Wang', 'Zixing Song', 'Prayag Tiwari']","This paper introduces MMPG, a framework for Protein Representation Learning (PRL) that constructs protein graphs from multiple perspectives and adaptively fuses them using a Mixture of Experts (MoE) approach. The framework addresses the limitations of single-perspective graph construction by capturing diverse properties of residue interactions through physical, chemical, and geometric perspectives. The MoE module dynamically routes perspectives to specialized experts, enabling the learning of intrinsic features and cross-perspective interactions. The paper demonstrates that MMPG achieves superior protein representations and advanced performance across four different downstream protein tasks.",157.86,Phi-4,Nvidia B200 (Cloud Native)
2601.10160v1_Alignment Pretraining AI Discourse Causes Self-Ful.pdf,Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment,"['Cameron Tice', 'Puria Radmard', 'Samuel Ratnam', 'Andy Kim', 'David Africa', 'Kyle O’Brien']","This paper investigates the impact of pretraining corpora discourse on the alignment of large language models (LLMs). It explores how AI behavior described in pretraining data can lead to self-fulfilling misalignment, where models internalize negative behavioral tendencies. The study demonstrates that increasing the presence of misalignment discourse in training data results in higher misalignment scores, while emphasizing aligned behavior reduces these scores. The findings suggest that pretraining data significantly shapes alignment priors, and recommend that practitioners focus on alignment during pretraining. The study contributes to understanding the role of pretraining in model behavior and proposes alignment pretraining as a complement to post-training methods.",157.75,Phi-4,Nvidia B200 (Cloud Native)
2601.10161v1_AWED-FiNER Agents Web applications and Expert Dete.pdf,"A WED-FiNER: Agents, Web applications, and Expert Detectors for Fine-grained Named Entity Recognition across 36 Languages for 6.6 Billion Speakers","['Prachuryya Kaushik', 'Ashish Anand']","A WED-FiNER is an open-source ecosystem designed to address the gap in Fine-grained Named Entity Recognition (FgNER) for 36 global languages spoken by over 6.6 billion people. It provides agentic toolkits, web applications, and state-of-the-art expert models to facilitate FgNER across these languages. The system routes multilingual text to specialized expert models for quick FgNER annotations and offers web-based platforms for non-technical users. It also supports offline deployment in resource-constrained scenarios, including edge devices, with a focus on vulnerable languages such as Bodo, Manipuri, Bishnupriya, and Mizo.",157.04,Phi-4,Nvidia B200 (Cloud Native)
2601.10168v1_RAG-3DSG Enhancing 3D Scene Graphs with Re-Shot Gu.pdf,RAG-3DSG: Enhancing 3D Scene Graphs with Re-shot Guided Retrieval-Augmented Generation,"['Yue Chang', 'Rufeng Chen', 'Zhaofan Zhang', 'Yi Chen', 'Sihong Xie']","This paper introduces RAG-3DSG, a novel approach to enhance 3D Scene Graph (3DSG) generation by addressing challenges such as low object-level recognition accuracy and speed. The proposed method mitigates aggregation noise through re-shot guided uncertainty estimation and supports object-level Retrieval-Augmented Generation (RAG) using reliable low-uncertainty objects. Additionally, a dynamic downsample-mapping strategy is proposed to accelerate cross-image object aggregation with adaptive granularity. Experiments on the Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing mapping time by two-thirds compared to the vanilla version.",158.13,Phi-4,Nvidia B200 (Cloud Native)
2601.10169v1_CtD Composition through Decomposition in Emergent .pdf,COMPOSITION THROUGH DECOMPOSITION IN EMERGENT COMMUNICATION,"['Boaz Carmeli', 'Ron Meir', 'Yonatan Belinkov']","This study introduces a method called 'Composition through Decomposition' to enable artificial neural agents to acquire and utilize compositional generalization for describing previously unseen images. The method involves two sequential training steps: 'Decompose', where agents learn to break down images into basic concepts using a codebook from a multi-target coordination game, and 'Compose', where agents use this codebook to describe novel images by combining basic concepts into complex phrases. The study highlights cases of zero-shot generalization in the 'Compose' step, demonstrating the potential for neural networks to achieve compositional representations without additional training.",157.93,Phi-4,Nvidia B200 (Cloud Native)
2601.10173v1_ReasAlign Reasoning Enhanced Safety Alignment agai.pdf,ReasAlign: Reasoning Enhanced Safety Alignment against Prompt Injection Attack,"['Hao Li', 'Yankai Yang', 'G. Edward Suh', 'Ning Zhang', 'Chaowei Xiao']","ReasAlign is a model-level solution designed to enhance safety alignment against indirect prompt injection attacks in Large Language Models (LLMs). It incorporates structured reasoning steps to analyze user queries, detect conflicting instructions, and maintain the continuity of user-intended tasks. A test-time scaling mechanism with a preference-optimized judge model is introduced to score reasoning steps and select the best trajectory. Evaluations show that ReasAlign maintains utility comparable to undefended models while outperforming Meta SecAlign, achieving a superior balance between security and utility. On the CyberSecEval2 benchmark, ReasAlign achieves 94.6% utility and only 3.6% attack success rate, significantly surpassing Meta SecAlign's performance.",157.67,Phi-4,Nvidia B200 (Cloud Native)
2601.10187v1_HOMURA Taming the Sand-Glass for Time-Constrained .pdf,Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning,"['Ziang Cui', 'Mengran Yu', 'Tianjiao Li', 'Chenyu Shi', 'Yingxuan Shi', 'Lusheng Zhang', 'Hongwei Lin']","Large Language Models (LLMs) excel in multilingual translation but face challenges with cross-lingual verbosity, making them unsuitable for time-constrained tasks like subtitling and dubbing. This paper introduces Sand-Glass, a benchmark for evaluating translations under syllable-level duration constraints, and proposes HOMURA, a reinforcement learning framework that optimizes the trade-off between semantic preservation and temporal compliance. HOMURA employs a KL-regularized objective with a dynamic syllable-ratio reward to effectively control output length, outperforming strong LLM baselines in maintaining linguistic density without sacrificing semantic adequacy.",157.79,Phi-4,Nvidia B200 (Cloud Native)
2601.10191v1_How does downsampling affect needle electromyograp.pdf,How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series,"['Mathieu J.L. Cherpitel', 'Janne A.M. Luijten', 'Thomas H.W. Back', 'Camiel Verhamme', 'Martijn R. Tannemaat', 'Anna V. Kononova']","This study investigates the impact of downsampling on needle electromyography (nEMG) signals, which are used in diagnosing neuromuscular diseases (NMDs). The high and heterogeneous sampling rates of nEMG signals pose computational challenges for machine learning models, especially for near real-time analysis. The paper presents a workflow to evaluate information loss due to downsampling, combining shape-based distortion metrics with classification outcomes from machine learning models. The study demonstrates that shape-aware downsampling algorithms preserve diagnostic information better than standard decimation, offering practical guidance for selecting downsampling configurations that balance data reduction with model performance.",158.14,Phi-4,Nvidia B200 (Cloud Native)
2601.10193v1_GFM4GA Graph Foundation Model for Group Anomaly De.pdf,GFM4GA: Graph Foundation Model for Group Anomaly Detection,"['Jiujiu Chen', 'Weijun Zeng', 'Shaofeng Hu', 'Sihong Xie', 'Hui Xiong']","This paper introduces GFM4GA, a novel graph foundation model designed for group anomaly detection. Unlike existing models that focus on individual anomalies, GFM4GA addresses the challenge of detecting group anomalies where individual members may appear normal. The model employs a dual-level contrastive learning approach for pretraining, capturing group anomaly structures and feature inconsistencies. It is fine-tuned for few-shot learning tasks with parameter constraints and group-anomaly-proportion weighting. The model's adaptability to unseen group anomalies is enhanced through group contexts derived from labeled anomaly neighbors. Experimental results demonstrate that GFM4GA outperforms existing group anomaly detectors and individual anomaly GFMs, achieving average improvements of 2.85% in AUROC and 2.55% in AUPRC.",153.92,Phi-4,Nvidia B200 (Cloud Native)
2601.10201v1_PRL Process Reward Learning Improves LLMs Reasonin.pdf,Process Reward Learning Improves LLMs’ Reasoning Ability and Broadens the Reasoning Boundary,"['Jiarui Yao', 'Ruida Wang', 'Tong Zhang']","This paper introduces Process Reward Learning (PRL) to enhance the reasoning abilities of Large Language Models (LLMs). Unlike traditional methods that rely on sparse outcome rewards, PRL provides fine-grained supervision by decomposing the reinforcement learning objective into intermediate steps. This approach assigns rigorous process rewards, facilitating better exploration during optimization. Theoretical motivation and formulation of PRL are presented, showing its equivalence to reward maximization with a KL-divergence penalty. Experimental results demonstrate that PRL not only improves average performance but also broadens the reasoning boundary, as evidenced by improved pass @ n metrics. The paper addresses the limitations of existing frameworks that depend on computationally expensive techniques or separate reward models, offering a more efficient and theoretically sound solution.",153.68,Phi-4,Nvidia B200 (Cloud Native)
2601.10205v1_One Instruction Does Not Fit All How Well Do Embed.pdf,One Instruction Does Not Fit All: How Well Do Embeddings Align Personas and Instructions in Low-Resource Indian Languages?,"['Arya Shah', 'Himanshu Beniwal', 'Mayank Singh']","This paper addresses the challenge of aligning multilingual assistants with culturally grounded user preferences in India's linguistically diverse population. It introduces a unified benchmark for 12 Indian languages across four evaluation tasks: monolingual and cross-lingual persona-to-instruction retrieval, reverse retrieval from instruction to persona, and binary compatibility classification. The study evaluates eight multilingual embedding models in a frozen-encoder setting, highlighting E5-Large-Instruct and BGE-M3 for their performance in retrieval tasks, and LaBSE for classification. The findings provide practical guidance for model selection in Indic multilingual retrieval and establish reproducible baselines for future research.",154.31,Phi-4,Nvidia B200 (Cloud Native)
2601.10212v1_PADER Paillier-based Secure Decentralized Social R.pdf,PADER: Paillier-based Secure Decentralized Social Recommendation,"['Chaochao Chen', 'Jiaming Qian', 'Fei Zheng', 'Yachuan Liu']","The paper addresses privacy concerns in centralized recommendation systems by proposing PADER, a Paillier-based secure decentralized social recommendation system. In this system, users and sellers act as nodes in a decentralized network, allowing for secure training and inference of the recommendation model without a centralized platform. The SoReg model, which utilizes user ratings and social relations, is adapted using the Paillier cryptosystem to facilitate secure polynomial evaluation. The authors introduce efficient secure addition and multiplication protocols, along with an optimal data packing scheme, to enhance computation efficiency. Experimental results demonstrate practicality, with processing times of approximately one second per user with hundreds of ratings and less than three hours for training with around 500,000 ratings for one epoch.",154.53,Phi-4,Nvidia B200 (Cloud Native)
2601.10215v1_Topo-RAG Topology-aware retrieval for hybrid text-.pdf,TOPO-RAG: TOPOLOGY-AWARE RETRIEVAL FOR HYBRID TEXT–TABLE DOCUMENTS,"['Alex Dantart', 'Marco K´ovacs-Navarro']","This paper introduces Topo-RAG, a framework designed to address the limitations of current Retrieval-Augmented Generation (RAG) systems in handling hybrid text-table documents. Unlike traditional systems that linearize complex data structures into text, Topo-RAG employs a dual architecture that respects the topology of the data. It routes narrative text through dense retrievers and processes tabular data using a Cell-Aware Late Interaction mechanism, preserving spatial relationships. Evaluated on a synthetic enterprise corpus, Topo-RAG shows an 18.4% improvement in nDCG@10 for hybrid queries over standard linearization methods, demonstrating enhanced understanding of information structure.",154.2,Phi-4,Nvidia B200 (Cloud Native)
2601.10222v1_Introduction to optimization methods for training .pdf,Introduction to optimization methods for training SciML models,"['Alena Kopaničáková', 'Elisa Riccietti']","The paper discusses the adaptation of optimization methods for training scientific machine learning (SciML) models. Unlike classical machine learning, which relies on abundant data and stochastic optimization, SciML often deals with data-scarce environments where physical models play a crucial role. This leads to optimization problems that are physics-informed or operator-constrained, incorporating partial-differential equations and boundary or initial conditions. These elements alter the structure of the objective function, introducing global spatio-temporal coupling and challenging the efficiency of traditional stochastic optimization methods. The paper explores how first-order schemes, adaptive gradient methods, and curvature-aware techniques can be integrated into stochastic optimization frameworks to address these unique challenges in SciML.",154.52,Phi-4,Nvidia B200 (Cloud Native)
2601.10236v1_Who Owns the Text Design Patterns for Preserving A.pdf,Who Owns the Text? Design Patterns for Preserving Authorship in AI-Assisted Writing,"['Bohan Zhang', 'Chengke Bu', 'Paramveer Dhillon']","This paper explores the impact of AI writing assistants on writers' sense of authorship. It introduces an ownership-aware co-writing editor that tests design choices like persona-based coaching and style personalization. An online study with 176 participants showed that while AI assistance reduced cognitive load and maintained text quality, it also decreased psychological ownership. Style personalization partially restored ownership and increased AI incorporation in the text. The paper proposes five design patterns to preserve authorship in AI-assisted writing tools.",155.15,Phi-4,Nvidia B200 (Cloud Native)
2601.10242v1_Loop as a Bridge Can Looped Transformers Truly Lin.pdf,LOOP AS ABRIDGE: CAN LOOPED TRANSFORMERS TRULY LINK REPRESENTATION SPACE AND NATURAL LANGUAGE OUTPUTS?,"['Guanxu Chen', 'Dongrui Liu', 'Jing Shao']","This report investigates whether Looping Transformers (LTs), which increase computational depth by iterating shared layers, can bridge the gap between internal 'knowledge' and explicit linguistic outputs in Large Language Models (LLMs). The study finds that while increasing loop iterations narrows this gap, it is partly due to a degradation of internal 'knowledge' in representations. Additionally, LTs' ability to perceive representations does not improve across loops but is only present in the final loop. These findings suggest that while LTs are a promising direction for scaling computational depth, they have not yet achieved the necessary introspection to truly link representation space and natural language.",154.06,Phi-4,Nvidia B200 (Cloud Native)
2601.10245v1_TRIM Hybrid Inference via Targeted Stepwise Routin.pdf,TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks,"['Vansh Kapoor', 'Aman Gupta', 'Hao Chen', 'Anurag Beniwal', 'Jing Huang', 'Aviral Kumar']","The paper introduces TRIM (Targeted Routing in Multi-Step Reasoning Tasks), a method that improves the efficiency of large language models (LLMs) in multi-step reasoning tasks by selectively routing only critical reasoning steps to larger models. This approach minimizes the use of expensive model tokens by allowing smaller models to handle routine steps, thus preventing cascading failures and enhancing cost efficiency. TRIM uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. The method demonstrates significant improvements in cost efficiency on benchmarks like MATH-500 and AIME, achieving up to 6x higher cost efficiency on harder tasks.",154.12,Phi-4,Nvidia B200 (Cloud Native)
2601.10251v1_X-SAM Boosting Sharpness-Aware Minimization with D.pdf,Boosting Sharpness-Aware Minimization with Dominant-Eigenvector Gradient Correction,"['Hongru Duan', 'Yongle Chen', 'Lei Guan']","This paper addresses the misalignment between the theoretical expectations and practical optimization behavior of Sharpness-Aware Minimization (SAM). It proposes an eigenvector-aligned SAM (X-SAM) that corrects the gradient by orthogonal decomposition along the top eigenvector of the Hessian, aiming to more effectively regularize the Hessian's maximum eigenvalue. The paper provides a theoretical analysis and experimental evaluations demonstrating X-SAM's convergence and superior generalization capabilities.",154.45,Phi-4,Nvidia B200 (Cloud Native)
2601.10254v1_NoReGeo Non-Reasoning Geometry Benchmark.pdf,NoReGeo: Non-Reasoning Geometry Benchmark,"['Irina Abdullaeva', 'Anton Vasiliuk', 'Elizaveta Goncharova', 'Temurbek Rahmatullaev', 'Zagorulko Ivan', 'Maxim Kurkin', 'Andrey Kuznetsov']","The paper introduces NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that assess models' proficiency in reasoning-based geometry, NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. The benchmark comprises 2,500 trivial geometric problems across 25 categories, crafted to be solvable purely through native geometric understanding. The study assesses various state-of-the-art models, including GPT-4, and finds that even advanced systems achieve a maximum of 65% accuracy in binary classification tasks. The findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, suggesting the need for specialized training approaches for effective geometric comprehension.",154.43,Phi-4,Nvidia B200 (Cloud Native)
2601.10257v1_Untangling Input Language from Reasoning Language .pdf,Untangling Input Language from Reasoning Language: A Diagnostic Framework for Cross-Lingual Moral Alignment in LLMs,"['Nan Li', 'Bo Kang', 'Tijl De Bie']","This paper introduces a diagnostic framework to investigate how large language models (LLMs) handle moral dilemmas across different languages. It distinguishes between the effects of the language of the dilemma (input language) and the language in which the model reasons (reasoning language). The authors propose a methodology that manipulates these factors separately, including mismatched conditions, to decompose their contributions. By applying Moral Foundations Theory, the study interprets moral judgments and identifies a split in the Authority dimension into family-related and institutional dimensions. The framework demonstrates its diagnostic power by isolating reasoning-language effects, detecting context-dependency in models, and providing deployment guidance. The study applies this methodology to English-Chinese moral judgment with 13 LLMs, revealing significant insights into cross-lingual moral alignment.",154.37,Phi-4,Nvidia B200 (Cloud Native)
2601.10272v1_MoST Mixing Speech and Text with Modality-Aware Mi.pdf,MIXINGSPEECH ANDTEXT WITHMODALITY-AWAREMIXTURE OFEXPERTS,"['Yuxuan Lou', 'Kai Yang', 'Yang You']","The paper introduces MoST (Mixture of Speech and Text), a novel multimodal large language model that integrates speech and text processing using a Modality-Aware Mixture of Experts (MAMoE) architecture. Unlike existing multimodal models that process diverse modality representations with identical parameters, MoST employs specialized routing pathways to direct tokens to modality-appropriate experts based on input type. This architecture enhances modality-specific learning and cross-modal understanding through modality-specific expert groups and shared experts. The model is developed using an efficient transformation pipeline that adapts a pretrained MoE language model through post-training on ASR and TTS datasets, followed by fine-tuning with a curated speech-text instruction dataset. MoST outperforms existing models of comparable parameter counts across various benchmarks and is the first fully open-source speech-text LLM built on a Mixture of Experts architecture.",154.66,Phi-4,Nvidia B200 (Cloud Native)
2601.10274v1_Queueing-Aware Optimization of Reasoning Tokens fo.pdf,Queueing-Aware Optimization of Reasoning Tokens for Accuracy-Latency Trade-offs in LLM Servers,"['Emre Ozbas', 'Melih Bastopcu']","This paper addresses the challenge of optimizing reasoning tokens in a single large language model (LLM) server that processes a heterogeneous stream of queries. The authors formulate a constrained optimization problem to maximize a weighted average accuracy objective while minimizing mean system time, considering architectural token-budget constraints and queue-stability conditions. The solution involves a coupled projected fixed-point characterization and an iterative method for convergence. The study also evaluates the performance loss due to rounding integer-valued token allocations, with simulation results demonstrating the effectiveness of the proposed approach.",154.57,Phi-4,Nvidia B200 (Cloud Native)
2601.10282v2_SPIKE Sparse Koopman Regularization for Physics-In.pdf,SPIKE: Sparse Koopman Regularization for Physics-Informed Neural Networks,['Jose Marie Antonio Miñoza'],"This paper introduces SPIKE, a framework that enhances Physics-Informed Neural Networks (PINNs) with continuous-time Koopman operators to address overfitting and improve generalization. By enforcing linear dynamics in a learned observable space, SPIKE, with L1 regularization, learns sparse generator matrices, embodying the parsimony principle. The framework demonstrates consistent improvements in temporal extrapolation, spatial generalization, and long-term prediction accuracy across various types of partial differential equations (PDEs) and chaotic ordinary differential equations (ODEs). The continuous-time formulation ensures unconditional stability for stiff systems, avoiding issues associated with discrete-time Koopman operators.",154.6,Phi-4,Nvidia B200 (Cloud Native)
2601.10305v1_DanQing An Up-to-Date Large-Scale Chinese Vision-L.pdf,DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset,"['Hengyu Shen', 'Tiancheng Gu', 'Bin Qin', 'Lan Wu', 'Yuling Wu', 'Shuo Tan', 'Zelong Sun', 'Jun Wang', 'Nan Wu', 'Xiang An', 'Weidong Cai', 'Ziyong Feng', 'Kaicheng Yang']","This paper introduces DanQing, a large-scale Chinese vision-language pre-training dataset containing 100 million image-text pairs. Developed to address the scarcity of high-quality Chinese image-text data, DanQing is curated through a rigorous selection process and primarily built from 2024–2025 web data. The dataset aims to capture evolving semantic trends, offering greater practical utility. Experimental results demonstrate that DanQing achieves superior performance across various Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. The dataset is open-sourced under the Creative Common CC-BY 4.0 license to facilitate further research in Chinese vision-language pre-training.",154.09,Phi-4,Nvidia B200 (Cloud Native)
2601.10306v1_Evidence-Augmented Policy Optimization with Reward.pdf,Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning,"['Xin Guan', 'Zijian Li', 'Shen Huang', 'Pengjun Xie', 'Jingren Zhou', 'Jiuxin Cao']","This paper addresses the challenge of applying Reinforcement Learning (RL) to long-context scenarios, where sparse outcome rewards hinder effective reasoning. The authors propose Evidence-Augmented Policy Optimization (EAPO), which introduces a specialized RL algorithm with a reward model that computes Group-Relative Evidence Reward. This provides dense process supervision to improve evidence quality. Additionally, an Adaptive Reward-Policy Co-Evolution mechanism refines the reward model iteratively, ensuring precise process guidance. Evaluations across eight benchmarks show that EAPO significantly enhances long-context reasoning performance compared to state-of-the-art baselines.",154.27,Phi-4,Nvidia B200 (Cloud Native)
2601.10338v1_Agent Skills in the Wild An Empirical Study of Sec.pdf,Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale,"['Yi Liu', 'Weizhe Wang', 'Ruitao Feng', 'Yao Zhang', 'Guangquan Xu', 'Gelei Deng', 'Yuekang Li', 'Leo Zhang']","This paper presents the first large-scale empirical security analysis of AI agent skills, which are modular packages that extend agent capabilities. The study analyzed 31,132 skills from two major marketplaces using a multi-stage detection framework called SkillScan. The findings reveal that 26.1% of the skills contain vulnerabilities, with data exfiltration and privilege escalation being the most prevalent. The study introduces a vulnerability taxonomy, a detection methodology with high precision and recall, and an open dataset and toolkit for future research. The results highlight the need for capability-based permission systems and mandatory security vetting to mitigate these security risks.",154.03,Phi-4,Nvidia B200 (Cloud Native)
2601.10342v1_C-GRASP Clinically-Grounded Reasoning for Affectiv.pdf,C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing,"['Cheng Lin Cheng', 'Ting Chuan Lin', 'Chai Kai Chang']","The paper introduces C-GRASP, a Clinically-Grounded Reasoning for Affective Signal Processing system, designed to address challenges in applying Large Language Models (LLMs) to heart rate variability (HRV) interpretation. C-GRASP mitigates issues such as respiratory sinus arrhythmia (RSA) contamination, instability in nonlinear metrics from short data segments, and the neglect of individualized baselines in favor of population norms. The system employs a guardrailed RAG-enhanced pipeline with eight traceable reasoning steps, incorporating a Z-score Priority Hierarchy to prioritize individualized baseline shifts. Evaluated on the DREAMER dataset, C-GRASP demonstrated superior performance in emotion classification and achieved a high Clinical Reasoning Consistency score. The paper highlights the transition from black-box classification to transparent, evidence-based clinical decision support, facilitating safer AI integration in biomedical engineering.",154.06,Phi-4,Nvidia B200 (Cloud Native)
2601.10343v2_OctoBench Benchmarking Scaffold-Aware Instruction .pdf,OCTOBENCH: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding,"['Deming Ding', 'Shichun Liu', 'Enhui Yang', 'Jiahang Lin', 'Ziying Chen', 'Shihan Dou', 'Honglin Guo', 'Weiyu Cheng', 'Pengyu Zhao', 'Chengjun Xiao', 'Qunhong Zeng', 'Qi Zhang', 'Xuanjing Huang', 'Qidi Xu', 'Tao Gui']","The paper introduces OCTOBENCH, a benchmark designed to evaluate scaffold-aware instruction following in repository-grounded agentic coding. It addresses the gap in evaluating the ability of large language models (LLMs) to follow scaffold-specified instructions, especially when constraints are heterogeneous and persist across interactions. OCTOBENCH includes 34 environments and 217 tasks under three scaffold types, paired with 7,098 objective checklist items. The benchmark provides an automated observation-and-scoring toolkit for fine-grained checks. Experiments reveal a gap between task-solving and scaffold-aware compliance, highlighting the need for targeted training and evaluation. The benchmark aims to support reproducible benchmarking and accelerate the development of more scaffold-aware coding agents.",152.78,Phi-4,Nvidia B200 (Cloud Native)
2601.10348v1_Training-Trajectory-Aware Token Selection.pdf,Training-Trajectory-Aware Token Selection,"['Zhanming Shen', 'Jiaqi Hu', 'Zeyu Qin', 'Hao Chen', 'Wentao Ye', 'Zenan Huang', 'Yihong Zhuang', 'Guoshan Lu', 'Junlin Zhou', 'Junbo Zhao']","The paper addresses the challenges in efficient distillation for large language models (LLMs) with strong reasoning capabilities. It identifies a phenomenon where performance metrics drop sharply during training, termed 'Imitation Shock', due to a token-level mechanism where confidence bifurcates into 'Imitation-Anchor Tokens' and other tokens that are yet to learn. The proposed solution, Training-Trajectory-Aware Token Selection (T3S), aims to reconstruct the training objective at the token level, facilitating the learning of yet-to-learn tokens and improving performance. The method shows consistent gains in both AR and dLLM settings, with models like Qwen3-8B surpassing others on competitive reasoning benchmarks.",153.55,Phi-4,Nvidia B200 (Cloud Native)
2601.10349v1_SuS Strategy-aware Surprise for Intrinsic Explorat.pdf,Strategy-aware Surprise for Intrinsic Exploration,"['Mark Kashirskiy', 'Ilya Makarov']","The paper introduces Strategy-aware Surprise (SuS), a novel intrinsic motivation framework for exploration in reinforcement learning. SuS uses pre-post prediction mismatch as a novelty signal, incorporating two components: Strategy Stability (SS) and Strategy Surprise (SuS). SS measures consistency in behavioral strategy, while SuS captures unexpected outcomes relative to the agent’s strategy. The combined reward formulation uses learned weighting coefficients. The approach is evaluated on mathematical reasoning tasks with large language models, showing significant improvements in accuracy and solution diversity. SuS achieves a 17.4% improvement in Pass@1 and a 26.4% improvement in Pass@5 compared to baseline methods, while maintaining higher strategy diversity.",154.32,Phi-4,Nvidia B200 (Cloud Native)
2601.10373v1_Towards Efficient Low-rate Image Compression with .pdf,Towards Efficient Low-rate Image Compression with Frequency-aware Diffusion Prior Refinement,"['Yichong Xia', 'Yimin Zhou', 'Jinpeng Wang', 'Bin Chen']","This paper introduces a novel image compression framework named DiffCR, which leverages a Frequency-aware Skip Estimation (FaSE) module to refine the ϵ-prediction prior from a pre-trained latent diffusion model. The framework aligns compressed latents at different timesteps using Frequency Decoupling Attention (FDA) and employs a lightweight consistency estimator for fast two-step decoding. DiffCR achieves significant bitrate savings and speed-up compared to state-of-the-art diffusion-based compression methods, without updating the backbone diffusion model.",153.42,Phi-4,Nvidia B200 (Cloud Native)
2601.10378v2_Global Context Compression with Interleaved Vision.pdf,Global Context Compression with Interleaved Vision-Text Transformation,"['Dian Jiao', 'Jiaxin Duan', 'Shuai Zhao', 'Jiabing Leng', 'Yiran Zhang', 'Feng Huang']","This paper explores global context compression to reduce computational and memory costs in Transformers by interleaving input text chunks with their visual encoding. The proposed VIST2 model uses visual tokens to predict text token distributions, achieving significant improvements in speed, memory usage, and computational efficiency. The study includes extensive experiments with models scaled from 0.6B to 8B, demonstrating a 4× compression ratio and notable performance gains in long writing tasks.",152.88,Phi-4,Nvidia B200 (Cloud Native)
2601.10386v1_Handling Missing Modalities in Multimodal Survival.pdf,Handling Missing Modalities in Multimodal Survival Prediction for Non-Small Cell Lung Cancer,"['Filippo Ruffini', 'Camillo Maria Caruso', 'Claudia Tacconi', 'Lorenzo Nibid', 'Francesca Miccolis', 'Marta Lovino', 'Carlo Greco', 'Edy Ippolito', 'Michele Fiore', 'Alessio Cortellini', 'Bruno Beomonte Zobel', 'Giuseppe Perrone', 'Bruno Vincenzi', 'Claudio Marrocco', 'Alessandro Bria', 'Elisa Ficarra', 'Sara Ramella', 'Valerio Guarrasi', 'Paolo Soda']","This paper addresses the challenge of handling missing modalities in multimodal survival prediction for non-small cell lung cancer. The authors propose a method to effectively manage incomplete data across different modalities, enhancing the accuracy and reliability of survival predictions in clinical settings.",158.45,Phi-4,Nvidia B200 (Cloud Native)
2601.10398v2_LatentRefusal Latent-Signal Refusal for Unanswerab.pdf,LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries,"['Xuancheng Ren', 'Shijing Hu', 'Zhihui Lu', 'Jiangqi Huang', 'Qiang Duan']","In LLM-based Text-to-SQL systems, unanswerable and underspecified user queries can generate incorrect text and executable programs that yield misleading results or violate safety constraints. Existing refusal strategies are either brittle due to model hallucinations or add complexity and overhead. This paper introduces LATENTREFUSAL, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of an LLM. The proposed Tri-Residual Gated Encoder (TRGE) architecture suppresses schema noise and amplifies mismatch cues indicating unanswerability. Empirical evaluations show that LATENTREFUSAL provides an efficient safety layer for Text-to-SQL systems, improving average F1 to 88.5% across benchmarks with minimal probe overhead.",157.89,Phi-4,Nvidia B200 (Cloud Native)
2601.10402v1_Toward Ultra-Long-Horizon Agentic Science Cognitiv.pdf,Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering,"['Xinyu Zhu', 'Yuzhu Cai', 'Zexi Liu', 'Bingyang Zheng', 'Cheng Wang', 'Rui Ye', 'Jiaao Chen', 'Hanrui Wang', 'Wei-Chen Wang', 'Yuzhi Zhang', 'Linfeng Zhang', 'Weinan E', 'Di Jin', 'Siheng Chen']","The paper addresses the challenge of ultra-long-horizon autonomy in artificial intelligence, focusing on the ability to maintain strategic coherence over extended experimental cycles. It introduces ML-Master 2.0, an autonomous agent designed for machine learning engineering, which utilizes a novel approach called Hierarchical Cognitive Caching (HCC). This architecture allows the agent to manage context dynamically, differentiating between transient execution traces and stable knowledge. The agent demonstrates significant improvements in performance on OpenAI’s MLE-Bench, achieving a medal rate of 56.44% under 24-hour budgets. The findings suggest that ultra-long-horizon autonomy can enable AI to explore complex tasks beyond human precedents.",157.42,Phi-4,Nvidia B200 (Cloud Native)
2601.10406v1_ErrEval Error-Aware Evaluation for Question Genera.pdf,Error-Aware Evaluation for Question Generation through Explicit Diagnostics,"['Weiping Fu', 'Bifan Wei', 'Jingyi Hao', 'Yushun Zhang', 'Jian Zhang', 'Jiaxin Wang', 'Bo Li', 'Yu He', 'Lingling Zhang', 'Jun Liu']","The paper introduces ErrEval, a framework designed to enhance the evaluation of Automatic Question Generation (QG) by incorporating explicit error diagnostics. Traditional evaluation methods often overlook critical defects like factual hallucinations and answer mismatches due to their black-box and holistic approach. ErrEval addresses this by reformulating evaluation into a two-stage process: error diagnosis followed by informed scoring. A lightweight Error Identifier detects and categorizes common errors in structural, linguistic, and content-related aspects. These diagnostic signals guide LLM evaluators towards more precise and grounded judgments. Extensive experiments demonstrate that ErrEval aligns better with human judgments and effectively mitigates the overestimation of low-quality questions.",157.7,Phi-4,Nvidia B200 (Cloud Native)
2601.10413v1_LADFA A Framework of Using Large Language Models a.pdf,LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies,"['Haiyue Yuan', 'Nikolay Matyunin', 'Ali Raza', 'Shujun Li']","This paper introduces LADFA, a computational framework that leverages large language models (LLMs) and retrieval-augmented generation (RAG) to analyze personal data flows in privacy policies. The framework processes unstructured text to extract data flows and construct a data flow graph, facilitating insight discovery. It consists of a pre-processor, an LLM-based processor, and a data flow post-processor. The effectiveness and accuracy of LADFA were validated through a case study on ten privacy policies from the automotive industry. The framework is designed to be flexible and customizable for various text-based analysis tasks beyond privacy policy analysis.",157.92,Phi-4,Nvidia B200 (Cloud Native)
2601.10416v1_LLMdoctor Token-Level Flow-Guided Preference Optim.pdf,LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models,"['Tiesunlong Shen', 'Rui Mao', 'Jin Wang', 'Heming Sun', 'Jian Zhang', 'Xuejie Zhang', 'Erik Cambria']","This paper introduces LLMdoctor, a novel framework for efficient test-time alignment of Large Language Models (LLMs) using a patient-doctor paradigm. It leverages token-level reward acquisition and token-level flow-guided preference optimization (TFPO) to align a large, frozen patient LLM with a smaller, specialized doctor model. Unlike traditional methods that rely on trajectory-level rewards, LLMdoctor extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals guide the training of the doctor model, establishing flow consistency across subtrajectories for precise token-by-token alignment while preserving generation diversity. Extensive experiments show that LLMdoctor outperforms existing test-time alignment methods and even surpasses full fine-tuning approaches like DPO.",157.27,Phi-4,Nvidia B200 (Cloud Native)
2601.10421v1_Are Language Models Models.pdf,Are Language Models Models?,['Philip Resnik'],"Futrell and Mahowald claim that language models (LMs) 'serve as model systems', but an assessment at each of Marr’s three levels suggests the claim is not true at the implementation level, poorly motivated at the algorithmic-representational level, and problematic at the computational theory level. LMs are good candidates as tools; calling them cognitive models overstates the case and unnecessarily feeds LLM hype.",159.03,Phi-4,Nvidia B200 (Cloud Native)
2601.10436v1_Development of Ontological Knowledge Bases by Leve.pdf,Development of Ontological Knowledge Bases by Leveraging Large Language Models,"['LE Ngoc Luyen', 'Marie-Hélène ABEL', 'Philippe GOUSPILLOU']","This paper introduces a structured, iterative methodology leveraging Large Language Models (LLMs) to optimize the development of Ontological Knowledge Bases (OKBs). The approach focuses on automating knowledge acquisition, generating ontology artifacts, and enabling continuous refinement. A case study in the vehicle sales domain demonstrates accelerated ontology construction, improved consistency, effective bias mitigation, and enhanced transparency. The integration of LLMs into ontology development significantly improves scalability, integration capabilities, and efficiency in knowledge management systems.",158.24,Phi-4,Nvidia B200 (Cloud Native)
2601.10440v1_AgentGuardian Learning Access Control Policies to .pdf,AGENTGUARDIAN: Learning Access Control Policies to Govern AI Agent Behavior,"['Nadya Abaev', 'Denis Klimov', 'Gerard Levinov', 'David Mimran', 'Yuval Elovici', 'Asaf Shabtai']","This study introduces AgentGuardian, a security framework designed to govern AI agent operations by enforcing context-aware access-control policies. It monitors execution traces during a controlled staging phase to learn legitimate behaviors and input patterns, deriving adaptive policies that regulate tool calls based on real-time input context and control flow dependencies. Evaluation across two real-world AI agent applications shows that AgentGuardian effectively detects malicious or misleading inputs while preserving normal functionality and mitigating hallucination-driven errors and orchestration-level malfunctions.",157.79,Phi-4,Nvidia B200 (Cloud Native)
2601.10457v1_NSR-Boost A Neuro-Symbolic Residual Boosting Frame.pdf,NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models,"['Ziming Dai', 'Dabiao Ma', 'Jinle Tong', 'Mengyuan Han', 'Jian Yang', 'Haojun Fei']","The paper introduces NSR-Boost, a neuro-symbolic residual boosting framework designed for industrial scenarios, particularly for upgrading legacy models like Gradient Boosted Decision Trees (GBDTs) in high-concurrency production environments. The framework is non-intrusive, treating the legacy model as a frozen model and focusing on targeted repairs in 'hard regions' where predictions fail. It involves three key stages: identifying hard regions through residuals, generating interpretable experts using symbolic code structures from Large Language Models (LLMs) and fine-tuning with Bayesian optimization, and dynamically integrating these experts with the legacy model output through a lightweight aggregator. NSR-Boost has been successfully deployed in Qfin Holdings' financial risk control system, outperforming state-of-the-art baselines across multiple datasets and demonstrating significant performance gains on real-world data. It effectively captures long-tail risks missed by traditional models, offering a safe and cost-effective evolutionary paradigm for industry.",157.21,Phi-4,Nvidia B200 (Cloud Native)
2601.10460v1_Contextual StereoSet Stress-Testing Bias Alignment.pdf,Contextual StereoSet: Stress-Testing Bias Alignment Robustness in Large Language Models,"['Abhinaba Basu', 'Pavan Chakraborty']","This paper introduces Contextual StereoSet, a benchmark designed to test the robustness of bias alignment in large language models by varying contextual framing such as time, place, and audience. The study reveals that bias levels in models can significantly shift based on these contextual changes, challenging the assumption that bias scores from fixed-condition tests generalize to real-world scenarios. The paper proposes Context Sensitivity Fingerprints (CSF) as a method to profile bias under varying conditions, emphasizing the need for evaluators to consider 'under what conditions does bias appear?' rather than simply 'is this model biased?'.",158.59,Phi-4,Nvidia B200 (Cloud Native)
2601.10462v3_ChartComplete A Taxonomy-based Inclusive Chart Dat.pdf,ChartComplete: A Taxonomy-based Inclusive Chart Dataset,"['Ahmad Mustapha', 'Charbel Toumieh', 'Mariette Awad']","The paper introduces the ChartComplete dataset, designed to address the limitations of existing datasets in chart understanding, particularly in the context of multi-modal large language models (MLLMs). Existing datasets are limited to a small set of chart types, which restricts the evaluation of MLLMs' performance. ChartComplete is based on a comprehensive chart taxonomy from the visualization community and includes thirty different chart types, ranging from common to less common and special charts. The dataset is presented to the research community to facilitate further development and benchmarking in the field of chart understanding.",158.11,Phi-4,Nvidia B200 (Cloud Native)
2601.10477v1_Urban Socio-Semantic Segmentation with Vision-Lang.pdf,URBANSOCIO-SEMANTICSEGMENTATION WITH VISION-LANGUAGEREASONING,"['Yu Wang', 'Yi Wang', 'Rui Dai', 'Yujie Wang', 'Kaikui Liu', 'Xiangxiang Chu', 'Yansheng Li']","This paper addresses the challenge of segmenting socially defined entities from satellite imagery, which is difficult due to their reliance on social semantics rather than distinct visual appearances. The authors introduce the Urban Socio-Semantic Segmentation dataset (SocioSeg) and a novel vision-language reasoning framework (SocioReasoner) to simulate human identification and annotation processes. The framework uses reinforcement learning to optimize non-differentiable processes and enhance the reasoning capabilities of vision-language models. Experiments show that this approach outperforms state-of-the-art models and demonstrates strong zero-shot generalization.",157.98,Phi-4,Nvidia B200 (Cloud Native)
2601.10485v1_Panning for Gold Expanding Domain-Specific Knowled.pdf,Panning for Gold: Expanding Domain-Specific Knowledge,"['Runhao Zhao', 'Weixin Zeng', 'Wentao Zhang', 'Chong Chen', 'Zhengpin Li', 'Xiang Zhao', 'Lei Chen']","This paper addresses the challenge of enriching domain-specific knowledge graphs (DKGs) by leveraging general knowledge graphs (GKGs) like Wikipedia and YAGO. The authors propose a new task called domain-specific knowledge graph fusion (DKGF), which aims to integrate relevant facts from GKGs into DKGs to enhance their completeness and utility. The paper introduces ExeFuse, a novel approach that interprets GKG facts as semantic programs and verifies their relevance and granularity alignment with DKGs. The authors also present two new benchmark datasets and 21 configurations to evaluate DKGF performance, demonstrating the effectiveness of ExeFuse in systematic knowledge transfer.",158.0,Phi-4,Nvidia B200 (Cloud Native)
2601.10496v1_Model See Model Do Exposure-Aware Evaluation of Bu.pdf,"Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs","['Ali Al-Kaswan', 'Claudio Spiess', 'Prem Devanbu', 'Arie van Deursen', 'Maliheh Izadi']","This paper introduces an exposure-aware evaluation framework to assess how prior exposure to buggy versus fixed code influences a model's preference in code generation and debugging tasks. Using the ManySStuBs4J benchmark and Data Portraits for membership testing on the Stack-V2 corpus, the study finds that most examples have neither variant in the training data, with fixes more frequently present than bugs when only one is present. The study reveals that models tend to reproduce buggy lines more often than fixes, with exposure to bugs amplifying this tendency. Likelihood scoring metrics like minimum and maximum token-probability consistently prefer fixed code, indicating a bias towards correct fixes, while metrics like the Gini coefficient reverse preference when only the buggy variant was seen. The results highlight the risk of LLMs propagating memorized errors in practice.",157.59,Phi-4,Nvidia B200 (Cloud Native)
2601.10498v1_Projected Microbatch Accumulation yields reference.pdf,Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning,['Nilin Abrahamsen'],"This paper introduces Projected Microbatch Accumulation (PROMA), a novel method for proximal policy updates in reinforcement learning, specifically for fine-tuning large language models. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before aggregation. This projection is applied layer-wise during the backward pass, allowing for efficient implementation without additional computational overhead. Empirical results show that PROMA provides tighter control over local KL divergence compared to GRPO, leading to more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without causing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.",157.69,Phi-4,Nvidia B200 (Cloud Native)
2601.10511v1_Scalable Algorithms for Approximate DNF Model Coun.pdf,Scalable Algorithms for Approximate DNF Model Counting,"['Paul Burkhardt', 'David G. Harris', 'Kevin T. Schmitt']","This paper addresses the problem of model counting for Disjunctive Normal Form (DNF) formulas, which is computationally intractable for exact solutions. The authors introduce a new Monte Carlo approach with an adaptive stopping rule and short-circuit formula evaluation. They prove that this method achieves Probably Approximately Correct (PAC) learning bounds and is asymptotically more efficient than previous methods. Experimental results demonstrate that the new algorithm outperforms prior algorithms significantly and can handle much larger problems with millions of variables.",158.5,Phi-4,Nvidia B200 (Cloud Native)
2601.10512v2_SatMap Revisiting Satellite Maps as Prior for Onli.pdf,SatMap: Revisiting Satellite Maps as Prior for Online HD Map Construction,"['Kanak Mazumder', 'Fabian B. Flohr']","This paper introduces SatMap, an online vectorized HD map estimation method that integrates satellite maps with multi-view camera observations to predict a vectorized HD map for downstream prediction and planning modules. By leveraging lane-level semantics and texture from satellite imagery captured from a Bird’s Eye View (BEV) perspective, SatMap effectively mitigates depth ambiguity and occlusion. Experiments on the nuScenes dataset show a 34.8% mAP performance improvement over the camera-only baseline and an 8.5% mAP improvement over the camera-LiDAR fusion baseline. The model is also evaluated in long-range and adverse weather conditions, demonstrating the advantages of using a satellite prior map.",157.63,Phi-4,Nvidia B200 (Cloud Native)
2601.10520v1_Breaking Up with Normatively Monolithic Agency wit.pdf,Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment,"['Felix Jahn', 'Yannic Muskalla', 'Lisa Dargasz', 'Patrick Schramowski', 'Kevin Baum']","The paper introduces GRACE, a neuro-symbolic reason-based containment architecture designed to ensure AI agents' decisions are both instrumentally effective and normatively aligned. GRACE decouples normative reasoning from instrumental decision-making, structuring decision-making into three modules: a Moral Module (MM) for permissible actions, a Decision-Making Module (DMM) for selecting optimal actions, and a Guard for enforcing moral compliance. The architecture is demonstrated using a Large Language Model (LLM) therapy assistant, highlighting its ability to enhance interpretability, contestability, and justifiability of AI behavior.",154.53,Phi-4,Nvidia B200 (Cloud Native)
2601.10524v1_Diagnosing Generalization Failures in Fine-Tuned L.pdf,Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection,"['Frank Bobe III', 'Gregory D. Vetaw', 'Chase Pavlick', 'Darshan Bryner', 'Matthew Cook', 'Jose Salas-Vernis']","This paper introduces a multi-layered diagnostic framework to study generalization failures in fine-tuned Large Language Models (LLMs) across different architectures. The study fine-tunes Llama 3.1 8B, Gemma 2 9B, and Mistral models on a phishing detection task, using SHAP analysis and mechanistic interpretability to identify the causes of generalization failures. Key findings include the importance of architecture and data diversity synergy, architecture-dependent generalization, and the inherent generalizability of certain models. The research provides a methodology for diagnosing and understanding these failures, emphasizing the need for deep validation of the interplay between architecture, data, and training strategy in developing reliable AI systems.",154.57,Phi-4,Nvidia B200 (Cloud Native)
2601.10527v2_A Safety Report on GPT-5.2 Gemini 3 Pro Qwen3-VL G.pdf,"A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5","['Xingjun Ma', 'Yixu Wang', 'Hengyuan Xu', 'Yutao Wu', 'Yifan Ding', 'Yunhan Zhao', 'Zilong Wang', 'Jiabin Hua', 'Ming Wen', 'Jianan Liu', 'Ranjie Duan', 'Yifeng Gao', 'Yingshui Tan', 'Yunhao Chen', 'Hui Xue', 'Xin Wang', 'Wei Cheng', 'Jingjing Chen', 'Zuxuan Wu', 'Bo Li', 'Yu-Gang Jiang']","The paper presents an integrated safety evaluation of six frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. It evaluates these models across language, vision-language, and image generation settings using a unified protocol that includes benchmark, adversarial, multilingual, and compliance evaluations. The results reveal a highly uneven safety landscape, with GPT-5.2 showing strong and balanced performance, while other models exhibit trade-offs. Despite strong benchmark results, all models are vulnerable under adversarial testing, with safety rates dropping below 6%. Text-to-image models show better alignment in regulated visual risk categories but remain fragile against adversarial or ambiguous prompts. The study underscores the need for standardized, holistic safety assessments to better reflect real-world risks and guide responsible deployment.",154.45,Phi-4,Nvidia B200 (Cloud Native)
2601.10543v1_Defending Large Language Models Against Jailbreak .pdf,Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing,"['Yinzhi Zhao', 'Ming Wang', 'Shi Feng', 'Xiaocui Yang', 'Daling Wang', 'Yifei Zhang']","This paper addresses the vulnerability of large language models (LLMs) to jailbreak attacks, despite existing safety alignment efforts. The authors observe that even when jailbroken, LLMs exhibit latent safety-related signals during the decoding process, which are often overridden by the model's drive for fluent continuation. The paper proposes a novel approach to surface and leverage these latent safety signals for early detection of unsafe content during decoding. Experiments demonstrate that this method significantly enhances safety while maintaining low over-refusal rates on benign inputs and preserving response quality. The findings suggest that activating intrinsic safety-awareness during decoding is a promising direction for defending against jailbreak attacks.",154.43,Phi-4,Nvidia B200 (Cloud Native)
2601.10560v1_Learning Latency-Aware Orchestration for Parallel .pdf,Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems,"['Xi Shi', 'Mengxin Zheng', 'Qian Lou']","This paper addresses the challenge of high inference latency in multi-agent systems (MAS) due to multi-step execution and repeated model invocations, which limits their scalability and usability in time-sensitive scenarios. Existing approaches often focus on task performance and inference cost, assuming sequential execution, which is suboptimal for latency control under parallel execution. The authors propose LAMaS, a latency-aware multi-agent orchestration framework that enables parallel execution and optimizes the critical execution path. This results in a reduction of critical path length by 38–46% compared to state-of-the-art baselines, while maintaining or improving task performance. The study underscores the importance of explicitly optimizing for latency in parallel execution when designing efficient multi-agent systems.",154.71,Phi-4,Nvidia B200 (Cloud Native)
2601.10562v1_Process-Guided Concept Bottleneck Model.pdf,Process-Guided Concept Bottleneck Model,"['Reza M. Asiyabi', 'Sam Harrison', 'John L. Godlee', 'David Milodowski', 'Nicole H. Augustin', 'Penelope J. Mograbi', 'Timothy R. Baker', 'Lorena M. Benitez', 'Samuel J. Bowers', 'Thomas K. Brade', 'Joao M. B. Carreiras', 'Duncan M. Chalo', 'Vera De Cauwer', 'Kyle G. Dexter', 'Hermane Diesse', 'Mathias I. Disney', 'Luisa F. Escobar-Alvarado', 'Manfred Finckh', 'Tatenda Gotore', 'Gabriele C. Hegerl', 'John N. Kigomo', 'Fainess C. Lumbwe', 'Francisco Maiato', 'Rudzani A. Makhado', 'Collins W. Masinde', 'Musingo Tito E. Mbuvi', 'Iain M. McNicol', 'Edward T.A. Mitchard', 'Buster P. Mogonong', 'Wilson A. Mugasha', 'Aristides Baptista Muhate', 'Hinji Mutondo', 'Leena Naftal', 'Paula Nieto-Quintano', 'Elifuraha Elisha Njoghomi', 'Catherine L. Parr', 'Oliver L. Phillips', 'Pierre Proces', 'Tshililo Ramaswiela', 'Jayashree Ratnam', 'Mathew Rees', 'Rasmus Revermann', 'Natasha Ribeiro', 'Mahesh Sankaran', 'Abel M. Siampale', 'Stephen Sitch', 'Kathleen G. Smart', 'Hemant G. Tripathi', 'Wayne Twine', 'Gabriel I.K. Uusiku', 'Helga van der Merwe', 'Chemuku Wekesa', 'Benjamin J. Wigley', 'Mathew Williams', 'Ellie Wood', 'Emily Woollen', 'Shaun Quegan', 'Steven Hancock', 'Casey M. Ryan']","This study introduces a Process-Guided Concept Bottleneck Model, which integrates AI-assisted tools for language refinement and editorial feedback. The research is supported by various grants and partnerships, with contributions from numerous researchers across different institutions. The model aims to enhance methodological development and scientific interpretation, with all scientific content and experiments conducted by the authors.",149.72,Phi-4,Nvidia B200 (Cloud Native)
2601.10567v1_Generative AI collective behavior needs an interac.pdf,Generative AI collective behavior needs an interactionist paradigm,"['Laura Ferrarotti', 'Gian Maria Campedelli', 'Roberto Dessì', 'Andrea Baronchelli', 'Giovanni Iacca', 'Kathleen M. Carley', 'Alex Pentland', 'Joel Z. Leibo', 'James Evans', 'Bruno Lepri']","The article discusses the importance of understanding the collective behavior of agents based on large language models (LLMs), emphasizing the need for an interactionist paradigm. This paradigm should incorporate alternative theoretical foundations, methodologies, and analytical tools to examine how prior knowledge and embedded values interact with social contexts in multi-agent generative AI systems. The authors propose four crucial directions for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.",154.76,Phi-4,Nvidia B200 (Cloud Native)
2601.10581v1_From Single to Multi-Agent Reasoning Advancing Gen.pdf,From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA,"['Kimia Abedini', 'Farzad Shami', 'Gianmaria Silvello']","The paper introduces GenomAgent, a multi-agent framework designed to enhance genomic Question Answering (QA) by overcoming the limitations of the current state-of-the-art system, GeneGPT. GeneGPT, which relies on a single-agent architecture with rigid API dependencies, is improved upon by GenomAgent, which coordinates specialized agents for complex queries. Evaluated on the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average. Its flexible architecture also allows for application beyond genomics to other scientific domains requiring expert knowledge extraction.",155.06,Phi-4,Nvidia B200 (Cloud Native)
2601.10587v1_Adversarial Evasion Attacks on Computer Vision usi.pdf,Adversarial Evasion Attacks on Computer Vision using SHAP Values,"['Frank Mollard', 'Marcus Becker', 'Florian Röhrbein']","The paper introduces a white-box attack on computer vision models using SHAP values. It demonstrates how adversarial evasion attacks can compromise the performance of deep learning models by reducing output confidence or inducing misclassifications. These attacks are particularly insidious as they can deceive the perception of an algorithm while eluding human perception due to their imperceptibility to the human eye. The proposed attack leverages SHAP values to quantify the significance of individual inputs to the output at the inference stage. A comparison is drawn between the SHAP attack and the well-known Fast Gradient Sign Method, finding that SHAP attacks are more robust in generating misclassifications, particularly in gradient hiding scenarios.",154.83,Phi-4,Nvidia B200 (Cloud Native)
2601.10591v1_ProbFM Probabilistic Time Series Foundation Model .pdf,ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition,"['Arundeep Chinta', 'Lucas Vinh Tran', 'Jay Katukuri']","This paper introduces ProbFM, a novel transformer-based probabilistic framework that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that rely on restrictive distributional assumptions or sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining computational efficiency. The paper evaluates DER's effectiveness through a controlled comparison study using a consistent LSTM architecture across five probabilistic methods. Results from cryptocurrency return forecasting demonstrate DER's competitive accuracy and practical value in uncertainty-aware trading strategies, highlighting its potential for effective risk management in financial applications.",155.93,Phi-4,Nvidia B200 (Cloud Native)
2601.10600v1_Procedural Fairness in Multi-Agent Bandits.pdf,Procedural Fairness in Multi-Agent Bandits,"['Joshua Caiata', 'Carter Blair', 'Kate Larson']","This paper introduces a new fairness objective in multi-agent multi-armed bandits (MA-MAB), termed procedural fairness, which emphasizes equal decision-making power for all agents. Unlike traditional fairness notions that focus on optimizing outcomes, procedural fairness prioritizes the process and ensures proportionality in outcomes. The paper argues that fairness should not only be about the results but also about how decisions are made, reflecting a contractualist view that decisions are legitimate if they cannot be reasonably rejected by those affected. Empirical results show that outcome-based fairness sacrifices equal voice, while procedural fairness maintains minimal sacrifice in outcomes. The paper highlights the need for explicit normative choices in fairness and provides a framework for implementing procedural fairness in practice.",158.43,Phi-4,Nvidia B200 (Cloud Native)
2601.10611v1_Molmo2 Open Weights and Data for Vision-Language M.pdf,Open Weights and Data for Vision-Language Models with Video Understanding and Grounding,"['Christopher Clark', 'Jieyu Zhang', 'Zixian Ma', 'Jae Sung Park', 'Mohammadreza Salehi', 'Rohun Tripathi', 'Sangho Lee', 'Zhongzheng Ren', 'Chris Dongjoo Kim', 'Yinuo Yang', 'Vincent Shao', 'Yue Yang', 'Weikai Huang', 'Ziqi Gao', 'Taira Anderson', 'Jianrui Zhang', 'Jitesh Jain', 'George Stoica', 'Winson Han', 'Ali Farhadi', 'Ranjay Krishna']","This paper introduces Molmo2, a new family of open-source video-language models (VLMs) that demonstrate state-of-the-art performance in video understanding and grounding tasks. Unlike proprietary models, Molmo2 does not rely on synthetic data from closed VLMs. The authors contribute 7 new video datasets and 2 multi-image datasets, including detailed video captions, a video Q&A dataset, an object tracking dataset, and a video pointing dataset. The paper also presents a novel training recipe using efficient packing and message-tree encoding, along with bi-directional attention on vision tokens and a token-weight strategy. Molmo2's 8B model outperforms other open-source models in tasks like short video captioning and counting, and excels in video-grounding tasks, surpassing both open-source and proprietary models in certain benchmarks.",157.34,Phi-4,Nvidia B200 (Cloud Native)
2601.10651v1_Multi-Property Synthesis.pdf,Multi-Property Synthesis,"['Christoph Weinhuber', 'Yannik Schnitzer', 'Alessandro Abate', 'David Parker', 'Giuseppe De Giacomo', 'Moshe Y. Vardi']","This paper addresses the challenge of Linear Temporal Logic on finite traces (LTLf) synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, the authors compute the relationship between product-game states and realizable goal sets in a single fixed-point computation. They develop a symbolic algorithm that uses Boolean goal variables and exploits monotonicity to compactly represent numerous goal combinations. This approach significantly outperforms enumeration-based methods, achieving speedups of up to two orders of magnitude. The paper highlights the limitations of the all-or-nothing paradigm in realistic scenarios like robotics and multi-service orchestration, where agents often face over-subscription and must prioritize among conflicting goals.",157.07,Phi-4,Nvidia B200 (Cloud Native)
2601.10679v1_Are Your Reasoning Models Reasoning or Guessing A .pdf,Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models,"['Zirui Ren', 'Ziming Liu']","This paper investigates the performance of hierarchical reasoning models (HRM) on reasoning tasks, revealing that HRM often 'guesses' rather than 'reasons'. The study identifies three surprising facts: HRM's failure on simple puzzles due to fixed point violations, the presence of 'grokking' dynamics in reasoning steps, and the existence of multiple fixed points leading to incorrect guesses. To improve HRM's performance, the authors propose strategies such as data augmentation, input perturbation, and model bootstrapping. These methods collectively enhance the accuracy of HRM on the Sudoku-Extreme dataset from 54.5% to 96.9%, providing new insights into the functioning of reasoning models.",157.6,Phi-4,Nvidia B200 (Cloud Native)
2601.10681v1_Structure and Diversity Aware Context Bubble Const.pdf,Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval,"['Amir Khurshid', 'Abhishek Sehgal']","This paper proposes a novel framework for constructing context bubbles in retrieval-augmented generation (RAG) systems, addressing issues like fragmentation, over-retrieval, and content duplication. The framework leverages document structure and diversity constraints to assemble coherent, citable context sets within a strict token budget. It starts with high-relevance anchor spans and balances query relevance, marginal coverage, and redundancy penalties. The method enhances efficiency by reducing redundant context and improving coverage of secondary facets, leading to better answer quality and citation faithfulness. Experiments on enterprise documents demonstrate the framework's effectiveness, and ablation studies confirm the necessity of structural priors and diversity constraints for optimal performance.",158.37,Phi-4,Nvidia B200 (Cloud Native)
2601.10684v1_On the origin of neural scaling laws from random g.pdf,On the origin of neural scaling laws: from random graphs to natural language,"['Maissam Barkeshli', 'Alberto Alfarano', 'Andrey Gromov']","This paper investigates the origin of neural scaling laws, particularly in the context of transformers trained on random walks on graphs with varying complexity. The study reveals that neural scaling laws can emerge even without power law structures in data correlations. By systematically reducing the complexity of natural language through training on sequences from simplified generative language models, the authors observe a consistent evolution of scaling exponents. The research also explores scaling laws derived from random walks on Erdös-Renyi and Barabási-Albert graphs. Additionally, the paper revisits conventional scaling laws for language modeling, demonstrating that essential results can be replicated using simpler models and providing a critical analysis of fitting methods used in prior studies. The authors propose an alternative method for deriving compute optimal curves and suggest that maximal update parameterization may offer greater parameter efficiency than standard methods.",157.92,Phi-4,Nvidia B200 (Cloud Native)
2601.10696v1_The Impact of Generative AI on Architectural Conce.pdf,"The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load","['Han Jiang', 'Yao Xiao', 'Rachel Hurley', 'Shichao Liu']","This study investigates the effects of generative AI (GenAI) on performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants completed a two-phase design task, first independently and then with external tools (GenAI-assisted and control conditions). While no overall performance advantage of GenAI was found, it significantly improved design performance for novice designers. However, general creative self-efficacy declined among students using GenAI. Cognitive load did not significantly differ between conditions, but certain prompt usage patterns were associated with reduced cognitive load. The study concludes that GenAI's effectiveness is influenced by users' prior expertise and their interaction strategies.",158.24,Phi-4,Nvidia B200 (Cloud Native)
2601.10700v2_LIBERTy A Causal Framework for Benchmarking Concep.pdf,LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals,"['Gilat Toker', 'Nitay Calderon', 'Ohad Amosy', 'Roi Reichart']","The paper introduces LIBERTy, a framework for benchmarking concept-based explanations of Large Language Models (LLMs) using structural counterfactuals. It addresses the limitations of existing benchmarks that rely on human-written counterfactuals by proposing a method to construct datasets with structural counterfactual pairs. LIBERTy is based on Structured Causal Models (SCMs) and includes three datasets for disease detection, CV screening, and workplace violence prediction. The framework introduces a new evaluation metric, order-faithfulness, to assess the faithfulness of explanations. The study evaluates various methods across five models, revealing significant potential for improving concept-based explanations. Additionally, LIBERTy facilitates the analysis of model sensitivity to interventions, highlighting reduced sensitivity in proprietary LLMs to demographic concepts due to post-training mitigation. Overall, LIBERTy aims to advance the development of faithful explainability methods in AI systems.",157.09,Phi-4,Nvidia B200 (Cloud Native)
2601.10702v1_Grounding Agent Memory in Contextual Intent.pdf,Grounding Agent Memory in Contextual Intent,"['Ruozhen Yang', 'Yucheng Jiang', 'Yueqi Jiang', 'Priyanka Kargupta', 'Yunyi Zhang', 'Jiawei Han']","The paper introduces STITCH (StructuredIntentTracking in ContextualHistory), an agentic memory system designed to improve the deployment of large language models in long-horizon, goal-oriented interactions. STITCH addresses the challenge of context-mismatched evidence retrieval by indexing each trajectory step with a structured retrieval cue known as contextual intent. This intent includes the current latent goal, action type, and salient entity types, which help disambiguate repeated mentions and reduce interference. The system filters and prioritizes memory snippets by intent compatibility, enhancing performance in long-horizon reasoning tasks. The paper also presents CAME-Bench, a benchmark for evaluating context-aware retrieval, where STITCH demonstrates state-of-the-art performance, significantly outperforming existing baselines, especially as trajectory length increases.",156.88,Phi-4,Nvidia B200 (Cloud Native)
2601.10712v1_MatchTIR Fine-Grained Supervision for Tool-Integra.pdf,MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching,"['Changle Qu', 'Sunhao Dai', 'Hengyi Cai', 'Jun Xu', 'Shuaiqiang Wang', 'Dawei Yin']","The paper introduces MatchTIR, a framework designed to enhance Tool-Integrated Reasoning (TIR) by providing fine-grained supervision through bipartite matching-based turn-level reward assignment and dual-level advantage estimation. This approach addresses the limitations of existing reinforcement learning methods that use coarse-grained credit assignment, which fails to differentiate between effective and ineffective tool interactions in long-horizon, multi-turn tasks. MatchTIR formulates credit assignment as a bipartite matching problem between predicted and ground-truth traces, using two assignment strategies to derive dense turn-level rewards. Additionally, it incorporates a dual-level advantage estimation scheme that balances local step precision with global task success by integrating turn-level and trajectory-level signals. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR, with a 4B model outperforming most 8B competitors, especially in long-horizon and multi-turn tasks.",156.6,Phi-4,Nvidia B200 (Cloud Native)
2601.10748v1_AnyECG Evolved ECG Foundation Model for Holistic H.pdf,AnyECG: Evolved ECG Foundation Model for Holistic Health Profiling,"['Jun Li', 'Hongling Zhu', 'Yujie Xiao', 'Qinghao Zhao', 'Yalei Ke', 'Gongzheng Tang', 'Guangkun Nie', 'Deyun Zhang', 'Jin Li', 'Canqing Yu', 'Shenda Hong']","The paper introduces AnyECG, an evolved ECG foundation model designed for holistic health profiling. It addresses the limitations of current AI-ECG models, which typically focus on single disease identification, by incorporating capabilities for comprehensive disease screening, long-term risk prediction, and comorbidity pattern recognition. The study utilized a large-scale, multicenter ECG dataset with 13,348,593 records from 2,984,209 patients, integrating both retrospective and prospective data. By employing transfer learning to fine-tune the pre-trained ECGFounder model, AnyECG significantly enhances its predictive capabilities across 1,172 ICD-coded conditions, achieving an AUROC above 0.7 for 306 diseases. The model's robustness and accuracy were validated through extensive multicenter external validation and a 10-year longitudinal cohort study, revealing novel discoveries in systemic prediction.",157.74,Phi-4,Nvidia B200 (Cloud Native)
2601.10768v1_Optimisation of complex product innovation process.pdf,OPTIMISATION OF COMPLEX PRODUCT INNOVATION PROCESSES BASED ON TREND MODELS WITH THREE-VALUED LOGIC,"['NINA BOCKOVÁ', 'BARBORA VOLNÁ', 'MIRKO DOHNAL']","This paper investigates complex product-innovation processes using models grounded in a set of heuristics. Each heuristic is expressed through simple trends – increasing, decreasing, or constant – which serve as minimally information-intensive quantifiers, avoiding reliance on numerical values or rough sets. A solution to a trend model is defined as a set of scenarios with possible transitions between them, represented by a transition graph. Any possible future or past behaviour of the system under study can thus be depicted by a path within this graph.",158.5,Phi-4,Nvidia B200 (Cloud Native)
2601.10770v1_Unifying Speech Recognition Synthesis and Conversi.pdf,"UNIFYING SPEECH RECOGNITION, SYNTHESIS AND CONVERSION WITH AUTOREGRESSIVE TRANSFORMERS","['Runyuan Cai', 'Yu Lin', 'Yiming Wang', 'Chunlin Fu', 'Xiaodong Zeng']","This paper introduces General-Purpose Audio (GPA), a unified audio foundation model that integrates multiple core speech tasks within a single large language model architecture. GPA operates on a shared discrete audio token space and supports instruction-driven task induction, enabling a single autoregressive model to perform text-to-speech (TTS), automatic speech recognition (ASR), and voice conversion (VC) without architectural modifications. The unified design combines a fully autoregressive formulation over discrete speech tokens, joint multi-task training across speech domains, and a scalable inference pipeline that achieves high concurrency and throughput. The model family supports efficient multi-scale deployment, including a lightweight 0.3B-parameter variant optimized for edge and resource-constrained environments. This demonstrates that a unified autoregressive architecture can achieve competitive performance across diverse speech tasks while remaining viable for low-latency, practical deployment.",158.39,Phi-4,Nvidia B200 (Cloud Native)
2601.10773v1_LogicLens Leveraging Semantic Code Graph to explor.pdf,LogicLens: Leveraging Semantic Code Graph to Explore Multi Repository Large Systems,"['Niko Usai', 'Dario Montagnini', 'Kristian Ilianov Iliev', 'Raffaele Camanzo']","LogicLens is a reactive conversational agent designed to assist developers in exploring complex software systems distributed across multiple repositories. It leverages a semantic multi-repository graph constructed through syntactic code analysis and semantic enrichment using Large Language Models (LLMs). This graph captures both structural elements and functional abstractions, enabling developers to interact with it via natural language to dynamically retrieve relevant subgraphs and answer technical or functional queries. The system demonstrates emergent capabilities such as impact analysis and symptom-based debugging, addressing the challenges of understanding both structural and functional aspects of large-scale, distributed software systems.",157.81,Phi-4,Nvidia B200 (Cloud Native)
2601.10779v1_Unified Optimization of Source Weights and Transfe.pdf,Unified Optimization of Source Weights and Transfer Quantities in Multi-Source Transfer Learning: An Asymptotic Framework,"['Qingyue Zhang', 'Chang Chu', 'Haohao Fu', 'Tianren Peng', 'Yanru Wu', 'Guanbo Huang', 'Yang Li', 'Shao-Lun Huang']","This paper addresses the challenge of balancing contributions from multiple source tasks in transfer learning to avoid negative transfer. The authors propose a theoretical framework called Unified Optimization of Weights and Quantities (UOWQ), which optimizes both source weights and transfer quantities. The framework is based on an asymptotic analysis of a Kullback–Leibler divergence–based generalization error measure. It provides closed-form solutions for single-source settings and a convex optimization-based procedure for multi-source settings. The paper also introduces practical algorithms for multi-source transfer learning and multi-task learning, demonstrating their effectiveness through experiments on real-world benchmarks like DomainNet and Office-Home.",157.87,Phi-4,Nvidia B200 (Cloud Native)
2601.10810v1_Digital Metabolism Decoupling Logic from Facts via.pdf,Digital Metabolism: Decoupling Logic from Facts via Regenerative Unlearning Towards a Pure Neural Logic Core,"['Mengmeng Peng', 'Zhenyu Fang', 'He Sun']","The paper addresses the issue of parameter entanglement in large language models (LLMs), where logic and factual knowledge are intertwined within shared weights, leading to inefficiencies and hallucinations. The authors propose a concept called 'digital metabolism,' suggesting that targeted forgetting can help distill a pure neural logic core. They introduce the Regenerative Logic-Core Protocol (RLCP), a dual-stream training framework that makes factual dependencies undecodable. Applying RLCP to a model, they observe a phase transition with near-zero retention of targeted facts and a shift towards reasoning-based problem-solving. This work suggests a dynamic approach to developing modular neural architectures.",158.09,Phi-4,Nvidia B200 (Cloud Native)
2601.10820v1_Towards Reliable ML Feature Engineering via Planni.pdf,Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents,"['Himanshu Thakur', 'Anusha Kamath', 'Anurag Muthyala', 'Dhwani Sanmukhani', 'Smruthi Mukund', 'Jay Katukuri']","This paper addresses challenges in automating feature engineering using code generation models. It introduces a planner-guided, constrained-topology multi-agent framework that improves code generation reliability and efficiency. The framework leverages a team's environment to orchestrate agent calls, generate context-aware prompts, and correct upstream artifacts based on downstream failures. It also allows for human intervention at critical steps. The approach shows significant improvements over manual and unplanned workflows, reducing feature engineering cycles from three weeks to a single day in practical applications.",158.02,Phi-4,Nvidia B200 (Cloud Native)
2601.10827v1_Approximately Optimal Global Planning for Contact-.pdf,Approximately Optimal Global Planning for Contact-Rich SE(2) Manipulation on a Graph of Reachable Sets,"['Simin Liu', 'Tong Zhao', 'Bernhard Paus Graesdal', 'Peter Werner', 'Jiuguang Wang', 'John Dolan', 'Changliu Liu', 'Tao Pang']","This paper introduces a new paradigm for approximately optimal manipulator planning in contact-rich manipulation (CRM), which leverages the entire surface of a manipulator for object interaction. The approach involves constructing a graph of mutual reachable sets offline and planning over this graph online to sequence local plans for globally optimized motion. The method outperforms existing planners by reducing task cost by 61% and achieving a 91% success rate across 250 queries, while maintaining sub-minute query times. This demonstrates the practicality of globally optimized CRM for real-world tasks.",157.82,Phi-4,Nvidia B200 (Cloud Native)
2601.10835v1_Can Vision-Language Models Understand Construction.pdf,Can Vision-Language Models Understand Construction Workers? An Exploratory Study,"['Hieu Bui', 'Nathaniel E. Chodosh', 'Arash Tavakoli']","This study evaluates the performance of three leading Vision-Language Models (VLMs) - GPT-4o, Florence 2, and LLaVa-1.5 - in recognizing construction worker actions and emotions from static site images. Using a dataset of 1,000 images annotated across ten action and ten emotion categories, the study finds that GPT-4o performs best, with notable F1-scores and accuracy in both tasks. Florence 2 shows moderate performance, while LLaVa-1.5 has the lowest scores. The study highlights the challenges VLMs face in distinguishing semantically close categories and suggests that further improvements, such as domain adaptation and multimodal sensing, are needed for real-world applications. The research provides a benchmark and insights for deploying human-aware AI systems in construction environments.",157.96,Phi-4,Nvidia B200 (Cloud Native)
2601.10880v1_Medical SAM3 A Foundation Model for Universal Prom.pdf,Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation,"['Chongcong Jiang', 'Tianxingjian Ding', 'Chuhan Song', 'Jiachen Tu', 'Ziyang Yan', 'Yihua Shao', 'Zhenyi Wang', 'Yuzhang Shang', 'Tianyu Han', 'Yu Tian']","The paper introduces Medical SAM3, a foundation model for universal prompt-driven medical image segmentation. It addresses the limitations of existing promptable segmentation models like SAM3, which struggle with domain shifts and lack spatial prompts for complex anatomical structures. By fully fine-tuning SAM3 on a large-scale, heterogeneous dataset of 2D and 3D medical images with paired segmentation masks and text prompts, the authors achieve robust domain-specific representations while maintaining prompt-driven flexibility. The model shows significant performance improvements across various organs, imaging modalities, and dimensionalities, especially in challenging scenarios with semantic ambiguity and complex morphology. The study underscores the importance of holistic model adaptation for effective prompt-driven segmentation in medical imaging.",158.09,Phi-4,Nvidia B200 (Cloud Native)
2601.10904v1_ARC Prize 2025 Technical Report.pdf,ARC Prize 2025: Technical Report,"['François Chollet', 'Mike Knoop', 'Gregory Kamradt', 'Bryan Landers']","The ARC Prize 2025 technical report discusses the ARC-AGI-2 dataset, which is part of the ARC-AGI benchmark series measuring few-shot generalization on novel tasks. The 2025 competition attracted significant participation, highlighting the growing interest in AI research focused on fluid intelligence and abstract reasoning. A key theme of 2025 is the emergence of the refinement loop, an iterative program optimization process guided by feedback signals. This report surveys top-performing methods, examines the role of refinement loops in AGI progress, discusses knowledge-dependent overfitting, and introduces ARC-AGI-3, which features interactive reasoning challenges requiring exploration, planning, memory, goal acquisition, and alignment capabilities.",157.97,Phi-4,Nvidia B200 (Cloud Native)
2601.10917v1_Self-learned representation-guided latent diffusio.pdf,SELF-LEARNED REPRESENTATION-GUIDED LATENT DIFFUSION MODEL FOR BREAST CANCER CLASSIFICATION IN DEEP ULTRA VIOLET WHOLE SURFACE IMAGES,"['Pouya Afshin', 'David Helminiak', 'Tianling Niu', 'Julie M. Jorns', 'Tina Yen', 'Bing Yu', 'Dong Hye Ye']","This paper addresses the challenge of limited annotated data for training deep learning models in breast cancer classification using Deep Ultraviolet Fluorescence Scanning Microscopy (DUV-FSM). The authors propose a Self-Supervised Learning (SSL)-guided Latent Diffusion Model (LDM) to generate high-quality synthetic training patches. By leveraging embeddings from a fine-tuned DINO teacher, the model injects rich semantic details into the synthetic data. The combination of real and synthetic patches is used to fine-tune a Vision Transformer (ViT) for Whole Slide Image (WSI) classification. The method achieves 96.47% accuracy and reduces the FID score to 45.72, outperforming class-conditioned baselines.",157.42,Phi-4,Nvidia B200 (Cloud Native)
2601.10921v1_RobuMTL Enhancing Multi-Task Learning Robustness A.pdf,RobuMTL: Enhancing Multi-Task Learning Robustness Against Weather Conditions,"['Tasneem Shaffee', 'Sherief Reda']","This paper introduces RobuMTL, a novel architecture designed to enhance the robustness of Multi-Task Learning (MTL) systems against adverse weather conditions. RobuMTL dynamically selects task-specific hierarchical Low-Rank Adaptation (LoRA) modules and a LoRA expert squad based on input perturbations, using a mixture-of-experts approach. This adaptive specialization improves robustness across diverse real-world conditions. The framework was evaluated on the PASCAL and NYUD-v2 datasets, showing significant improvements over standard MTL baselines and state-of-the-art methods under both single and mixed weather perturbations.",157.94,Phi-4,Nvidia B200 (Cloud Native)
2601.10922v1_What Matters in Data Curation for Multimodal Reaso.pdf,What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge,"['Yosub Shin', 'Michael Buriek', 'Boris Sobolev', 'Pavel Bushuyeu', 'Vikas Kumar', 'Haoyang Xu', 'Samuel Watson', 'Igor Molybog']","This paper explores data curation for multimodal reasoning through the NeurIPS 2025 DCVLR challenge, which isolates dataset selection by fixing the model and training protocol. The authors' submission, using a curated dataset from Walton Multimodal Cold Start, placed first in the challenge. The study reveals that difficulty-based example selection on an aligned base dataset is crucial for performance gains, while increasing dataset size mainly reduces variance without improving mean accuracy. Common diversity and synthetic augmentation heuristics often degrade performance. These findings highlight the importance of alignment and difficulty in data-efficient multimodal reasoning.",157.8,Phi-4,Nvidia B200 (Cloud Native)
2601.10926v1_Selecting Language Models for Social Science Start.pdf,"Selecting Language Models for Social Science: Start Small, Start Open, and Validate","['Dustin S. Stoltz', 'Marshall A. Taylor', 'Sanuj Kumar']","The paper discusses the selection of large pretrained language models (LLMs) for social scientists, emphasizing the importance of validity, reliability, reproducibility, and replicability. It suggests starting with smaller, open models and constructing benchmarks to validate computational pipelines. The authors explore considerations such as model openness, footprint, training data, and architecture, highlighting the need for transparency and control in model selection.",157.65,Phi-4,Nvidia B200 (Cloud Native)
2601.10931v1_Sparse Data Tree Canopy Segmentation Fine-Tuning L.pdf,Sparse Data Tree Canopy Segmentation: Fine-Tuning Leading Pretrained Models on Only 150 Images,"['David Szczecina', 'Niloofar Azad', 'Hudson Sun', 'Kyle Gao', 'Anthony Bertnyk', 'Lincoln Linlin Xu']","This paper addresses the challenge of tree canopy detection from aerial imagery, particularly under conditions of data scarcity. The study evaluates five deep learning architectures—YOLOv11, Mask R-CNN, DeepLabv3, Swin-UNet, and DINOv2—using a small dataset of 150 annotated images. The findings indicate that convolution-based models, especially YOLOv11 and Mask R-CNN, perform better than transformer-based models in this low-data regime. The research highlights the impact of inductive bias, pretraining, and model capacity on performance, suggesting that lightweight CNN-based methods are more reliable for canopy detection with limited data.",158.18,Phi-4,Nvidia B200 (Cloud Native)
2601.10945v1_PatientVLM Meets DocVLM Pre-Consultation Dialogue .pdf,PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between Vision-Language Models for Efficient Diagnosis,"['K Lokesh', 'Abhirama Subramanyam Penamakuri', 'Uday Agarwal', 'Apoorva Challa', 'Shreya K Gowda', 'Somesh Gupta', 'Anand Mishra']","This paper introduces a Pre-Consultation Dialogue Framework (PCDF) that simulates diagnostic dialogues between two vision-language models (VLMs): a DocVLM and a PatientVLM. The DocVLM generates follow-up questions based on image and dialogue history, while the PatientVLM responds using a symptom profile derived from the ground-truth diagnosis. The framework aims to enhance diagnostic accuracy by incorporating patient-reported symptoms into the diagnostic process. A small-scale clinical validation confirmed the clinical relevance and realism of the synthetic symptoms generated. The dialogue-based supervision led to significant improvements over image-only training, demonstrating the importance of realistic symptom elicitation for diagnosis.",157.05,Phi-4,Nvidia B200 (Cloud Native)
2601.10951v1_Multi-Stage Patient Role-Playing Framework for Rea.pdf,Multi-Stage Patient Role-Playing Framework for Realistic Clinical Interactions,"['Shijie Jiang', 'Zefan Jiang', 'Kehua Zhu', 'Tian Bai', 'Ruihong Zhao']",This paper introduces the first Chinese patient simulation dataset (Ch-PatientSim) to enhance the realism and diversity of doctor-patient interactions in clinical Large Language Models (LLMs). The dataset is constructed using a five-dimensional persona structure and addresses persona class imbalance through few-shot generation and manual verification. The proposed Multi-Stage Patient Role-Playing (MSPRP) framework decomposes interactions into three stages to improve personalization and realism in model responses. Experimental results show significant improvements in model performance across multiple dimensions of patient simulation.,158.28,Phi-4,Nvidia B200 (Cloud Native)
2601.10955v1_Beyond Max Tokens Stealthy Resource Amplification .pdf,Beyond Max Tokens: Stealthy Resource Amplification via Tool Calling Chains in LLM Agents,"['Kaiyu Zhou', 'Yongsen Zheng', 'Yicheng He', 'Meng Xue', 'Xueluan Gong', 'Yuji Wang', 'Kwok-Yan Lam']","The paper introduces a stealthy, multi-turn economic Denial-of-Service (DoS) attack targeting the agent-tool communication loop in Large Language Model (LLM) agents. Unlike existing single-turn attacks, this method exploits the compounding costs of multi-turn interactions by adjusting text-visible fields and a return policy in a tool server, optimizing these edits with a Monte Carlo Tree Search (MCTS) optimizer. The attack maintains protocol compatibility and correct task outcomes, thus evading conventional validation checks. It significantly increases computational costs and resource usage across various LLMs, highlighting the need for a paradigm shift in monitoring the economic and computational costs of the entire agentic process.",157.76,Phi-4,Nvidia B200 (Cloud Native)
2601.10960v1_Steering Language Models Before They Speak Logit-L.pdf,Steering Language Models Before They Speak: Logit-Level Interventions,"['Hyeseon An', 'Shinwoo Park', 'Hyundong Jin', 'Yo-Sub Han']","This paper introduces a training-free inference-time logit intervention method for steering large language models (LLMs) to achieve controllable generation. The proposed approach uses a statistical token score table derived from z-normalized log-odds of labeled corpora to adjust the decoding distribution. It addresses the limitations of current prompting-based and activation-based steering methods by providing consistent and fine-grained control over output characteristics such as writing complexity, formality, and toxicity. Empirical evaluations demonstrate significant improvements in accuracy and F1 score across diverse datasets, showcasing the method's broad applicability and task-agnostic nature.",158.32,Phi-4,Nvidia B200 (Cloud Native)
2601.11000v1_When Personalization Misleads Understanding and Mi.pdf,When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs,"['Zhongxiang Sun', 'Yi Zhan', 'Chenglei Shen', 'Weijie Yu', 'Xiao Zhang', 'Ming He', 'Jun Xu']","The paper discusses the issue of personalization-induced hallucinations in personalized large language models (LLMs), where models generate answers aligned with a user's history rather than objective truth. This can degrade factual reliability and propagate incorrect beliefs. The authors propose Factuality-Preserving Personalized Steering (FPPS), a method to mitigate these distortions while maintaining personalized behavior. They also introduce PFQABench, a benchmark for evaluating factual and personalized question answering under personalization. Experiments show that FPPS improves factual accuracy without compromising personalized performance.",158.15,Phi-4,Nvidia B200 (Cloud Native)
2601.11007v1_AdaMARP An Adaptive Multi-Agent Interaction Framew.pdf,AdaMARP: An Adaptive Multi-Agent Interaction Framework for General Immersive Role-Playing,"['Zhenhua Xu', 'Dongsheng Chen', 'Shuo Wang', 'Jian Li', 'Chengjie Wang', 'Meng Han', 'Yabiao Wang']","AdaMARP introduces an adaptive multi-agent interaction framework designed to enhance immersion and adaptability in LLM role-playing. It addresses the limitations of existing systems by incorporating a dynamic environment model and a Scene Manager for orchestrating role-playing interactions. The framework features an immersive message format that integrates Thought, Action, Environment, and Speech, and employs discrete actions for scene management. Training datasets AdaRPSet and AdaSMSet are used to improve character consistency and narrative coherence. Experiments demonstrate that AdaMARP achieves significant improvements over existing models, including outperforming several commercial LLMs in character portrayal and narrative management.",157.63,Phi-4,Nvidia B200 (Cloud Native)
2601.11012v1_Efficient Protein Optimization via Structure-aware.pdf,Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics,"['Jiahao Wang', 'Shuangjia Zheng']","The paper introduces HADES, a novel Bayesian optimization method that leverages Hamiltonian dynamics to efficiently sample from a structure-aware approximated posterior for protein optimization. By incorporating momentum and uncertainty in simulated physical movements, HADES facilitates rapid transitions towards promising areas in the protein sequence space. A position discretization procedure is introduced to propose discrete protein sequences from a continuous state system. The method utilizes a two-stage encoder-decoder framework to model the structure-function relationships between mutant neighbors, enabling the learning of a smoothed landscape for sampling. Extensive experiments demonstrate that HADES outperforms state-of-the-art baselines in in-silico evaluations across most metrics. The approach uniquely leverages the mutual constraints between protein structure and sequence, aiding in the design of protein sequences with optimized properties. The code and data are publicly available.",158.0,Phi-4,Nvidia B200 (Cloud Native)
2601.11016v1_Contextual Distributionally Robust Optimization wi.pdf,Contextual Distributionally Robust Optimization with Causal and Continuous Structure: An Interpretable and Tractable Approach,"['Fenglin Zhang', 'Jie Wang']","This paper introduces a framework for contextual distributionally robust optimization (DRO) that incorporates the causal and continuous structure of the underlying distribution. The authors develop interpretable and tractable decision rules using covariates. They introduce the causal Sinkhorn discrepancy (CSD), an entropy-regularized causal Wasserstein distance, to encourage continuous transport plans while maintaining causal consistency. The paper formulates a contextual DRO model with a CSD-based ambiguity set, termed Causal Sinkhorn DRO (Causal-SDRO), and derives its strong dual reformulation. The worst-case distribution is characterized as a mixture of Gibbs distributions. To address the infinite-dimensional policy optimization, the authors propose the Soft Regression Forest (SRF) decision rule, which approximates optimal policies within arbitrary measurable function spaces. The SRF maintains the interpretability of classical decision trees while being fully parametric, differentiable, and Lipschitz smooth, allowing for intrinsic interpretation from both global and local perspectives. An efficient stochastic compositional gradient algorithm is developed to solve the Causal-SDRO with parametric decision rules, converging to an ε-stationary point at a rate of O(ε−4). The method is validated through numerical experiments on synthetic and real-world datasets, demonstrating superior performance and interpretability.",158.08,Phi-4,Nvidia B200 (Cloud Native)
2601.11019v1_Finding the Translation Switch Discovering and Exp.pdf,Finding the Translation Switch: Discovering and Exploiting the Task-Initiation Features in LLMs,"['Xinwei Wu', 'Heng Liu', 'Xiaohu Zhao', 'Yuqi Ren', 'Linlong Xu', 'Longyue Wang', 'Deyi Xiong', 'Weihua Luo', 'Kaifu Zhang']","This paper investigates the innate translation capabilities of Large Language Models (LLMs) without task-specific fine-tuning. By employing Sparse Autoencoders (SAEs), the authors identify a set of 'translation initiation' features that are crucial for the model's translation ability. Causal interventions show that enhancing these features improves translation accuracy, while their removal leads to hallucinations. The study proposes a novel data selection strategy for efficient fine-tuning by focusing on samples that do not naturally activate these features, thereby improving data efficiency and reducing hallucinations. The findings suggest that these mechanisms are transferable to larger models within the same family, offering a blueprint for leveraging internal model mechanisms to develop more robust and efficient models.",157.78,Phi-4,Nvidia B200 (Cloud Native)
2601.11021v1_Combating Spurious Correlations in Graph Interpret.pdf,Combating Spurious Correlations in Graph Interpretability via Self-Reflection,"['Kecheng Cai', 'Chenyang Xu', 'Chao Peng']","This paper addresses the challenge of spurious correlations in graph interpretability, particularly in the Spurious-Motif benchmark. The authors propose a self-reflection framework, inspired by techniques used in large language models, to enhance interpretability in datasets with strong spurious correlations. By integrating this framework with existing interpretable graph learning methods, the authors demonstrate significant performance improvements on both the Spurious-Motif and other popular graph interpretability benchmarks. The iterative process of self-reflection allows for a reassessment of importance scores for nodes and edges, leading to more accurate identification of truly relevant structures. Additionally, a fine-tuning training method based on this feedback mechanism further boosts performance. The study highlights the importance of interpretability in high-stakes applications such as drug discovery, social network analysis, and fraud detection.",158.48,Phi-4,Nvidia B200 (Cloud Native)
2601.11030v1_IDDR-NGP Incorporating Detectors for Distractor Re.pdf,IDDR-NGP: Incorporating Detectors for Distractor Removal with Instant Neural Radiance Field,"['Xianliang Huang', 'Jiajie Gou', 'Shuhang Chen', 'Zhizhou Zhong', 'Jihong Guan', 'Shuigeng Zhou']","This paper introduces IDDR-NGP, a unified method for distractor removal in 3D scenes using Instant Neural Radiance Fields (NeRF). It addresses a wide range of distractors such as snowflakes, confetti, defoliation, and petals, unlike existing methods that focus on specific types. By integrating implicit 3D representations with 2D detectors, the method efficiently restores 3D scenes from multiple corrupted images. The paper introduces the learned perceptual image patch similarity (LPIPS) loss and multi-view compensation loss (MVCL) to optimize rendering results, supporting end-to-end training for high-quality 3D scene synthesis. A new benchmark dataset with synthetic and real-world distractors is presented, demonstrating the method's effectiveness and robustness. IDDR-NGP achieves results comparable to state-of-the-art desnow methods and accurately removes both realistic and synthetic distractors.",157.17,Phi-4,Nvidia B200 (Cloud Native)
2601.11035v1_Your One-Stop Solution for AI-Generated Video Dete.pdf,Your One-Stop Solution for AI-Generated Video Detection,"['Long Ma', 'Zihao Xue', 'Yan Wang', 'Zhiyuan Yan', 'Jin Xu', 'Xiaorui Jiang', 'Haiyang Yu', 'Yong Liao', 'Zhen Bi']","This paper addresses the challenge of detecting AI-generated videos, which have become increasingly realistic and difficult to distinguish from real videos. The authors identify two key limitations in the current field: the limited scale and outdated scope of existing datasets. They propose a comprehensive solution that includes a new dataset and detection methods to improve the reliability of distinguishing synthetic videos from real ones. The paper also explores factors contributing to the undetectability of synthetic videos and examines whether more advanced videos are harder to detect.",158.01,Phi-4,Nvidia B200 (Cloud Native)
2601.11037v1_BAPO Boundary-Aware Policy Optimization for Reliab.pdf,BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search,"['Shiyu Liu', 'Yongjing Yin', 'Jianhao Yan', 'Yunbo Tang', 'Qinggang Zhang', 'Bei Li', 'Xin Chen', 'Jingang Wang', 'Xunliang Cai', 'Jinsong Su']","The paper addresses a critical gap in the reliability of RL-based agentic search systems, which often fail to recognize their reasoning boundaries and rarely admit 'I DON’T KNOW' (IDK) when evidence is insufficient. The authors propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework that enhances reliability without compromising accuracy. BAPO introduces a boundary-aware reward system and an adaptive reward modulator to encourage appropriate IDK responses. Extensive experiments demonstrate BAPO's effectiveness in improving the reliability of agentic search across various benchmarks.",157.49,Phi-4,Nvidia B200 (Cloud Native)
2601.11042v1_Spectral Characterization and Mitigation of Sequen.pdf,Spectral Characterization and Mitigation of Sequential Knowledge Editing Collapse,"['Chi Zhang', 'Mengqi Zhang', 'Xiaotian Ye', 'Runxi Cheng', 'Zisheng Zhou', 'Ying Zhou', 'Pengjie Ren', 'Zhumin Chen']","The paper addresses the issue of catastrophic collapse in the general abilities of large language models due to sequential knowledge editing, particularly with parameter-modifying methods. It presents a spectral analysis showing that a model's general abilities are linked to dominant singular directions of pretrained weight matrices, which are disrupted by repeated edits. The authors propose the REVIVE framework, which stabilizes sequential editing by preserving the dominant singular subspace, thereby improving editing efficacy and preserving general abilities even under extensive editing scenarios.",157.44,Phi-4,Nvidia B200 (Cloud Native)
2601.11044v2_AgencyBench Benchmarking the Frontiers of Autonomo.pdf,AGENCYBENCH: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts,"['Keyu Li', 'Junhao Shi', 'Yang Xiao', 'Mohan Jiang', 'Jie Sun', 'Yunze Wu', 'Dayuan Fu', 'Shijie Xia', 'Xiaojie Cai', 'Tianze Xu', 'Weiye Si', 'Wenjie Li', 'Dequan Wang', 'Pengfei Liu']","AGENCYBENCH introduces a comprehensive benchmark for evaluating autonomous agents across 6 core agentic capabilities in 32 real-world scenarios, involving 138 tasks. These tasks require an average of 1 million tokens and 90 tool calls, with execution times spanning hours. The benchmark employs a user simulation agent for iterative feedback and a Docker sandbox for automated rubric-based assessment. Experiments show closed-source models outperform open-source models, highlighting disparities in resource efficiency and tool-use preferences. AGENCYBENCH serves as a critical testbed for next-generation agents, emphasizing the need to co-optimize model architecture with agentic frameworks.",156.49,Phi-4,Nvidia B200 (Cloud Native)
2601.11049v1_Predicting Biased Human Decision-Making with Large.pdf,Predicting Biased Human Decision-Making with Large Language Models in Conversational Settings,"['Stephen Pilli', 'Vivek Nallur']","This study investigates whether large language models (LLMs) can predict biased decision-making in conversational settings, focusing on cognitive biases like the Framing Effect and Status Quo Bias. A pre-registered study with 1,648 participants involved completing decision-making tasks via a chatbot with varying dialogue complexity. Increased cognitive load from complex dialogues heightened bias effects. The study evaluated LLMs (GPT-4, GPT-5, and open-source models) for predicting individual decisions using demographic data and dialogue context. LLMs incorporating dialogue context showed improved accuracy in predicting biases, with GPT-4 models aligning closely with human behavior. These findings enhance understanding of LLMs in simulating human decision-making and inform the design of adaptive conversational agents.",158.34,Phi-4,Nvidia B200 (Cloud Native)
2601.11063v1_H-AIM Orchestrating LLMs PDDL and Behavior Trees f.pdf,"H-AIM: Orchestrating LLMs, PDDL, and Behavior Trees for Hierarchical Multi-Robot Planning","['Haishan Zeng', 'Peng Li']","This paper introduces H-AIM, a novel framework for multi-robot task planning that integrates large language models (LLMs), Planning Domain Definition Language (PDDL), and behavior trees. The framework addresses challenges in long-horizon task execution and dynamic multi-robot coordination by transforming high-level instructions into formal planning problems, optimizing action sequences, and compiling plans for reactive control. It supports heterogeneous robot teams through a shared communication mechanism. The MACE-THOR benchmark dataset validates the approach, showing significant improvements in task success rate and goal condition recall compared to existing baselines.",158.29,Phi-4,Nvidia B200 (Cloud Native)
2601.11065v1_Fairness in Healthcare Processes A Quantitative An.pdf,Fairness in Healthcare Processes: A Quantitative Analysis of Decision Making in Triage,"['Rachmadita Andreswari', 'Stephan A. Fahrenkrog-Petersen', 'Jan Mendling']","This study investigates fairness in automated decision-making within healthcare, particularly in emergency triage. It employs process mining to assess fairness by linking real-life event logs with justice theory dimensions. Using the MIMICEL event log derived from MIMIC-IV ED, the study analyzes process outcomes such as time, re-do, deviation, and decision, evaluating the influence of factors like age, gender, race, language, and insurance. The results highlight potential unfairness in high-acuity and sub-acute cases, contributing empirical insights for further research in fairness-aware process mining in healthcare.",158.65,Phi-4,Nvidia B200 (Cloud Native)
2601.11073v1_Bridging Cognitive Neuroscience and Graph Intellig.pdf,Bridging Cognitive Neuroscience and Graph Intelligence: Hippocampus-Inspired Multi-View Hypergraph Learning for Web Finance Fraud,"['Rongkun Cui', 'Nana Zhang', 'Kun Zhu', 'Qi Zhang']","This paper introduces HIMVH, a Hippocampus-Inspired Multi-View Hypergraph learning model designed to address challenges in web finance fraud detection. Traditional graph neural networks (GNNs) struggle with fraud camouflage and long-tailed data distributions. HIMVH leverages the scene conflict monitoring role of the hippocampus to design a cross-view inconsistency perception module, capturing subtle discrepancies across transaction views to detect camouflaged fraud. Additionally, inspired by the CA1 region's match–mismatch novelty detection mechanism, a novelty-aware hypergraph learning module is introduced to enhance sensitivity to rare fraud patterns. Extensive experiments on six datasets show that HIMVH outperforms 15 state-of-the-art models, achieving significant improvements in AUC, F1, and AP metrics.",157.99,Phi-4,Nvidia B200 (Cloud Native)
2601.11076v1_A3D Adaptive Affordance Assembly with Dual-Arm Man.pdf,A3D: Adaptive Affordance Assembly with Dual-Arm Manipulation,"['Jiaqi Liang', 'Yue Chen', 'Qize Yu', 'Yan Shen', 'Haipeng Zhang', 'Hao Dong', 'Ruihai Wu']","The paper introduces A3D, a framework designed to enhance robotic furniture assembly through adaptive affordance and dual-arm manipulation. It addresses the challenge of precise coordination between two robotic arms, where one arm manipulates parts and the other provides support and stabilization. The framework learns adaptive affordances to identify optimal support locations on furniture parts, using dense point-level geometric representations to generalize across varied geometries. An adaptive module dynamically adjusts support strategies based on interaction feedback, improving assembly effectiveness. The authors validate their approach in a simulation environment with diverse parts and furniture types, demonstrating successful generalization in both simulation and real-world settings.",157.69,Phi-4,Nvidia B200 (Cloud Native)
2601.11077v1_ABC-Bench Benchmarking Agentic Backend Coding in R.pdf,ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development,"['Jie Yang', 'Honglin Guo', 'Li Ji', 'Jiazheng Zhou', 'Rui Zheng', 'Zhikai Lei', 'Shuo Zhang', 'Zhiheng Xi', 'Shichun Liu', 'Yuxin Wang', 'Bo Wang', 'Yining Zheng', 'Tao Gui', 'Xipeng Qiu']","The paper introduces ABC-Bench, a benchmark designed to evaluate the capabilities of Large Language Models (LLMs) as autonomous agents in real-world backend development. Unlike existing benchmarks that focus on static code logic, ABC-Bench assesses the full development lifecycle, including environment configuration and service deployment. The benchmark includes 224 tasks across 8 languages and 19 frameworks, highlighting the gap between current model capabilities and practical backend engineering demands.",158.31,Phi-4,Nvidia B200 (Cloud Native)
2601.11078v1_Visual Marker Search for Autonomous Drone Landing .pdf,Visual Marker Search for Autonomous Drone Landing in Diverse Urban Environments,"['Jiaohong Yao', 'Linfeng Liang', 'Yao Deng', 'Xi Zheng', 'Richard Han', 'Yuankai Qi']","This paper addresses the challenges of marker-based drone landing in complex urban environments, where traditional assumptions about visibility and sensor performance do not hold. The authors present a simulation-based evaluation using the AirSim platform, which systematically varies urban layouts, lighting, and weather conditions to reflect realistic operational diversity. The study benchmarks two heuristic coverage patterns and a reinforcement learning-based agent, focusing on how exploration strategy and scene complexity impact success rate, path efficiency, and robustness. The findings highlight the necessity of evaluating marker-based autonomous landing under diverse, sensor-relevant conditions to develop reliable aerial navigation systems.",158.23,Phi-4,Nvidia B200 (Cloud Native)
2601.11089v2_MiCA A Mobility-Informed Causal Adapter for Lightw.pdf,MiCA: A Mobility-Informed Causal Adapter for Lightweight Epidemic Forecasting,"['Suhan Guo', 'Jiahong Deng', 'Furao Shen']","This paper introduces the Mobility-Informed Causal Adapter (MiCA), a lightweight and architecture-agnostic module designed for epidemic forecasting. MiCA leverages causal discovery to infer mobility relations and integrates them into temporal forecasting models using gated residual mixing. This approach allows forecasters to effectively utilize mobility-derived spatial structures while maintaining robustness under noisy and data-limited conditions. The paper demonstrates that MiCA enhances the performance of lightweight temporal models, achieving an average relative error reduction of 7.5% across various forecasting horizons. The experiments conducted on four real-world epidemic datasets, including COVID-19 incidence and mortality, influenza, and dengue, show that MiCA's performance is competitive with state-of-the-art spatio-temporal models while remaining lightweight.",158.19,Phi-4,Nvidia B200 (Cloud Native)
2601.11090v1_Efficient Multilingual Name Type Classification Us.pdf,Efficient Multilingual Name Type Classification Using Convolutional Networks,['Davor Lauc'],"This paper introduces Onomas-CNN X, a convolutional neural network designed for classifying proper names by language and entity type. The model leverages parallel convolution branches with depthwise-separable operations and hierarchical classification to efficiently process names on CPU hardware. Evaluated on a dataset covering 104 languages and four entity types, Onomas-CNN X achieves 92.1% accuracy and processes 2,813 names per second on a single CPU core, outperforming fine-tuned XLM-RoBERTa in speed by 46 times while maintaining comparable accuracy. Additionally, it reduces energy consumption by a factor of 46 compared to transformer baselines. The study demonstrates that specialized CNN architectures can compete with large pretrained models for specific NLP tasks when sufficient training data is available.",158.13,Phi-4,Nvidia B200 (Cloud Native)
2601.11100v1_ReCreate Reasoning and Creating Domain Agents Driv.pdf,ReCreate: Reasoning and Creating Domain Agents Driven by Experience,"['Zhezheng Hao', 'Hong Wang', 'Jian Luo', 'Jianqing Zhang', 'Yuyan Zhou', 'Qiang Lin', 'Can Wang', 'Hande Dong', 'Jiawei Chen']","The paper introduces ReCreate, an experience-driven framework for the automatic creation of domain agents. It addresses the limitations of current automated agent generation methods by leveraging agent interaction histories to provide insights into the causes of success or failure and avenues for improvement. The framework includes an agent-as-optimizer paradigm with three key components: experience storage and retrieval, a reasoning-creating synergy pipeline, and hierarchical updates. Experiments show that ReCreate outperforms human-designed agents and existing methods, even with minimal initial scaffolds.",157.99,Phi-4,Nvidia B200 (Cloud Native)
2601.11109v1_Vision-as-Inverse-Graphics Agent via Interleaved M.pdf,Vision-as-Inverse-Graphics Agent via Interleaved Multimodal Reasoning,"['Shaofeng Yin', 'Jiaxin Ge', 'Zora Zhiruo Wang', 'Xiuyu Li', 'Michael J. Black', 'Trevor Darrell', 'Angjoo Kanazawa', 'Haiwen Feng']","The paper introduces VIGA (Vision-as-Inverse-Graphic Agent), a system designed to reconstruct or edit 3D scenes from a single 2D image through an iterative process of generation, execution, rendering, comparison, and revision. VIGA leverages interleaved multimodal reasoning to address the limitations of current Vision-and-Language Models (VLMs) in achieving fine-grained spatial and physical grounding. It employs a skill library and an evolving context memory to support long-horizon reasoning, making it task-agnostic and model-agnostic. Empirical results show significant improvements over one-shot baselines on benchmarks like BlenderGym and SlideBench. Additionally, the paper introduces BlenderBench, a new benchmark to evaluate heterogeneous foundation VLMs.",157.9,Phi-4,Nvidia B200 (Cloud Native)
2601.11124v1_Learn Before Represent Bridging Generative and Con.pdf,Learn Before Represent: Bridging Generative and Contrastive Learning for Domain-Specific LLM Embeddings,"['Xiaoyu Liang', 'Yuchen Peng', 'Jiale Luo', 'Wenhao Wang', 'Haoji Hu', 'Xincheng Zhou']","This paper addresses the challenge of adapting Large Language Models (LLMs) for domain-specific tasks, such as those in chemistry and law, where traditional contrastive learning methods fall short due to a lack of domain-specific knowledge. The authors propose a novel two-stage framework called Learn Before Represent (LBR), which first injects domain knowledge through an Information Bottleneck-Constrained Generative Learning stage, and then refines this knowledge using Generative-Refined Contrastive Learning. This approach aims to enhance the LLM's ability to handle specialized terminology and improve performance on tasks in vertical domains. Extensive experiments demonstrate that LBR significantly outperforms existing methods, establishing a new paradigm for building accurate and robust representations in specialized fields.",157.42,Phi-4,Nvidia B200 (Cloud Native)
2601.11135v1_Context-aware Graph Causality Inference for Few-Sh.pdf,Context-aware Graph Causality Inference for Few-Shot Molecular Property Prediction,"['Van Thuy Hoang', 'O-Joun Lee']","The paper addresses the challenge of predicting molecular properties in few-shot scenarios using a novel framework called CaMol. This framework leverages causal inference to identify latent causal structures within molecules that determine specific properties. It introduces a context graph to encode chemical knowledge, a learnable atom masking strategy to isolate causal substructures, and a distribution intervener for backdoor adjustment. The approach demonstrates superior accuracy and sample efficiency on diverse molecular datasets, aligning discovered causal substructures with established chemical knowledge, thus enhancing model interpretability.",154.5,Phi-4,Nvidia B200 (Cloud Native)
2601.11143v1_Learning Quadrupedal Locomotion for a Heavy Hydrau.pdf,Learning Quadrupedal Locomotion for a Heavy Hydraulic Robot Using an Actuator Model,"['Minho Lee', 'Hyeonseok Kim', 'Jin Tak Kim', 'Sangshin Park', 'Jeong Hyun Lee', 'Jungsan Cho', 'Jemin Hwangbo']","This paper addresses the challenge of sim-to-real transfer for large-scale hydraulic robots, focusing on the slow control response and complex fluid dynamics inherent in such systems. The authors propose an analytical actuator model based on hydraulic dynamics to represent the complex actuators, predicting joint torques for 12 actuators in under 1 microsecond. This rapid processing capability is crucial for reinforcement learning (RL) environments. The model is compared with neural network-based models, showing advantages in data-limited scenarios. The locomotion policy trained with this model is successfully deployed on a hydraulic quadruped robot weighing over 300 kg, marking the first demonstration of stable and robust command-tracking locomotion with RL on a heavy hydraulic quadruped robot. This work highlights advanced sim-to-real transferability.",154.02,Phi-4,Nvidia B200 (Cloud Native)
2601.11144v2_Deep GraphRAG A Balanced Approach to Hierarchical .pdf,Deep GraphRAG: A Balanced Approach to Hierarchical Retrieval and Adaptive Integration,"['Yuejie Li', 'Ke Yang', 'Tao Wang', 'Bolin Chen', 'Bowen Li', 'Chengjun Mao']","Graph-based Retrieval-Augmented Generation (GraphRAG) frameworks face a trade-off between global search comprehensiveness and local search efficiency. Existing methods struggle with large-scale hierarchical graphs, retrieval path optimization, and exploration-exploitation dynamics, often lacking robust multi-stage re-ranking. Deep GraphRAG addresses these issues with a hierarchical global-to-local retrieval strategy, integrating macroscopic inter-community and microscopic intra-community contextual relations. It employs a three-stage process: inter-community filtering, community-level refinement, and entity-level fine-grained search, guided by a beam search-optimized dynamic re-ranking module. Additionally, it features a Knowledge Integration Module using a compact LLM trained with Dynamic Weighting Reward GRPO (DW-GRPO) to balance relevance, faithfulness, and conciseness. Evaluations show Deep GraphRAG outperforms baseline methods in accuracy and efficiency.",153.82,Phi-4,Nvidia B200 (Cloud Native)
2601.11147v1_Do We Always Need Query-Level Workflows Rethinking.pdf,Do We Always Need Query-Level Workflows? Rethinking Agentic Workflow Generation for Multi-Agent Systems,"['Zixu Wang', 'Bingbing Xu', 'Yige Yuan', 'Huawei Shen', 'Xueqi Cheng']","The paper explores the necessity of query-level workflow generation in Multi-Agent Systems (MAS) built on large language models. It argues that query-level generation is not always necessary, as a small set of top-K task-level workflows can cover equivalent or more queries. The authors propose a low-cost task-level generation framework, SCALE, which reduces token usage by up to 83% while maintaining competitive performance. The paper highlights the inefficiencies of exhaustive execution-based task-level evaluation and introduces a self-prediction and calibration approach for evaluation instead.",154.39,Phi-4,Nvidia B200 (Cloud Native)
2601.11151v1_Cross-Modal Attention Network with Dual Graph Lear.pdf,Cross-Modal Attention Network with Dual Graph Learning in Multimodal Recommendation,"['JI DAI', 'QUAN FANG', 'JUN HU', 'DESHENG CAI', 'YANG YANG', 'CAN ZHAO']","This paper introduces the Cross-modal Recursive Attention Network with dual graph Embedding (CRANE) to address limitations in multimedia recommendation systems. It proposes a Recursive Cross-Modal Attention mechanism to capture high-order intra- and inter-modal dependencies and a symmetric dual-graph framework to unify behavioral and semantic signals. CRANE achieves high computational efficiency and scalability, showing a 5% improvement in key metrics over state-of-the-art baselines in experiments on four public datasets.",154.57,Phi-4,Nvidia B200 (Cloud Native)
2601.11160v1_Clustering High-dimensional Data Balancing Abstrac.pdf,Clustering High-dimensional Data: Balancing Abstraction and Representation,"['Claudia Plant', 'Lena G. M. Bauer', 'Christian Böhm']","This tutorial discusses the challenge of clustering high-dimensional data by balancing abstraction and representation. It explores how different clustering algorithms implement this balance, with examples like K-means, subspace clustering, and deep clustering. The tutorial highlights the need for abstraction to avoid mere representation learning and discusses methods to enforce abstraction through clustering losses. It concludes with future research directions aimed at adaptively balancing abstraction and representation for improved performance, energy efficiency, and interpretability.",154.28,Phi-4,Nvidia B200 (Cloud Native)
2601.11178v1_TANDEM Temporal-Aware Neural Detection for Multimo.pdf,TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech,"['Girish A. Koushik', 'Helen Treharne', 'Diptesh Kanojia']","This paper introduces TANDEM, a framework designed to transform audio-visual hate detection from a binary classification task into a structured reasoning problem. By employing a novel tandem reinforcement learning strategy, TANDEM optimizes vision-language and audio-language models through self-constrained cross-modal context, enhancing reasoning over extended temporal sequences without dense frame-level supervision. The approach significantly outperforms existing baselines, achieving notable improvements in target identification and temporal grounding across benchmark datasets. The findings suggest that structured, interpretable alignment is feasible in complex multimodal settings, providing a blueprint for transparent and actionable online safety moderation tools.",154.39,Phi-4,Nvidia B200 (Cloud Native)
2601.11189v1_Policy-Based Deep Reinforcement Learning Hyperheur.pdf,Policy-Based Deep Reinforcement Learning Hyperheuristics for Job-Shop Scheduling Problems,"['Sofiene Lassoued', 'Asrat Gobachew', 'Stefan Lier', 'Andreas Schwung']","This paper introduces a policy-based deep reinforcement learning hyper-heuristic framework designed to tackle the Job Shop Scheduling Problem (JSSP). The framework features a hyper-heuristic agent that dynamically switches scheduling rules based on the system state. Two key mechanisms are integrated: action prefiltering, which limits decision-making to feasible low-level actions for unbiased heuristic evaluation, and a commitment mechanism that controls the frequency of heuristic switching. The study explores various commitment strategies and compares deterministic greedy selection with stochastic sampling for action selection. Computational experiments on standard JSSP benchmarks show that this approach surpasses traditional heuristics, metaheuristics, and recent neural network-based scheduling methods.",154.42,Phi-4,Nvidia B200 (Cloud Native)
2601.11196v1_Artificial Intelligence and the US Economy An Acco.pdf,Artificial Intelligence and the US Economy: An Accounting Perspective on Investment and Production,"['Luisa Carpinelli', 'Filippo Natoli', 'Marco Taboga']","This paper explores the macroeconomic impact of the current wave of artificial intelligence (AI) in the US, focusing on how it is reflected in national accounts. It emphasizes the significant role of data centers as the backbone of the AI ecosystem, which have seen substantial investment due to the growing global demand for AI services. The paper notes that while AI-related capital expenditure has significantly boosted aggregate demand, its net contribution to GDP growth is less pronounced due to the high import content of AI hardware. It also suggests that the production of services from new AI data centers could soon contribute to GDP on a scale comparable to current investment spending. However, short reinvestment cycles and uncertainty about future AI demand pose medium-term macroeconomic risks.",154.83,Phi-4,Nvidia B200 (Cloud Native)
2601.11199v1_SD-RAG A Prompt-Injection-Resilient Framework for .pdf,SD-RAG: A Prompt-Injection-Resilient Framework for Selective Disclosure in Retrieval-Augmented Generation,"['Aiman Al Masoud', 'Marco Arazzi', 'Antonino Nocera']","The paper introduces SD-RAG, a novel approach to selective disclosure in retrieval-augmented generation (RAG) that enhances privacy and security by decoupling these constraints from the generation process. Unlike existing methods that rely on prompt-level safeguards, SD-RAG applies sanitization and disclosure controls during the retrieval phase, before augmenting the language model's input. It incorporates a semantic mechanism for dynamic security and privacy constraints and utilizes an optimized graph-based data model for fine-grained, policy-aware retrieval. Experimental evaluations show SD-RAG's superiority over baseline approaches, with up to a 58% improvement in privacy scores and strong resilience to prompt injection attacks.",154.51,Phi-4,Nvidia B200 (Cloud Native)
2601.11200v1_FAQ Mitigating Quantization Error via Regenerating.pdf,FAQ: Mitigating Quantization Error via Regenerating Calibration Data with Family-Aware Quantization,"['Haiyang Xiao', 'Weiqing Li', 'Jinyue Guo', 'Guochao Jiang', 'Guohua Liu', 'Yuewei Zhang']","This paper addresses the challenge of representativeness and universality of calibration data in post-training quantization (PTQ) for deploying large language models (LLMs) on resource-constrained devices. Traditional PTQ methods often rely on limited samples, leading to biases in quantization parameters. The authors propose FAQ (Family-Aware Quantization), a framework that regenerates calibration data using prior knowledge from larger LLMs of the same family. This approach generates high-fidelity calibration samples that align better with the target model's activation distribution. Experiments on models like Qwen3-8B demonstrate that FAQ can reduce accuracy loss by up to 28.5% compared to baseline methods, showcasing its effectiveness in enhancing PTQ.",154.4,Phi-4,Nvidia B200 (Cloud Native)
2601.11202v1_Epistemic Control and the Normativity of Machine L.pdf,EPISTEMIC CONTROL AND THE NORMATIVITY OF MACHINE LEARNING-BASED SCIENCE,['Emanuele Ratti'],"This chapter explores the concept of 'epistemic control' in the context of machine learning (ML) systems used in science. It addresses concerns raised by Paul Humphreys about the displacement of human scientists due to ML systems. The author introduces two conditions for epistemic control, 'tracking' and 'tracing', and argues against Humphreys' pessimistic view. A nuanced perspective on epistemic control in ML-based science is developed, considering the challenges of maintaining meaningful control over automated computational tools in various contexts, including science.",155.04,Phi-4,Nvidia B200 (Cloud Native)
2601.11207v1_LoRA as Oracle.pdf,LoRA as Oracle,"['Marco Arazzi', 'Antonino Nocera']","This paper introduces a novel LoRA-based oracle framework designed for backdoor detection and membership inference in deep neural networks. The framework utilizes low-rank adaptation modules as lightweight, model-agnostic probes. By attaching task-specific LoRA adapters to a frozen backbone, the authors analyze optimization dynamics and representation shifts when exposed to suspicious samples. The study demonstrates that poisoned and member samples induce distinctive low-rank updates, which can be detected using simple ranking and energy-based statistics. This method allows for reliable inference without needing access to the original training data or modifying the deployed model.",154.86,Phi-4,Nvidia B200 (Cloud Native)
2601.11219v1_SDFLoRA Selective Dual-Module LoRA for Federated F.pdf,SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients,"['Zhikang Shen', 'Jianrong Lu', 'Haiyuan Wan', 'Jianhai Chen']","This paper introduces SDFLoRA, a novel approach for federated learning with large language models (LLMs) that addresses the challenges of rank heterogeneity and privacy preservation. By decomposing client adapters into global and local modules, SDFLoRA selectively aligns and aggregates global modules across clients while keeping local modules private. This method enhances robust learning under rank heterogeneity and optimizes privacy by injecting differential privacy noise exclusively into the global module. Experimental results on GLUE benchmarks demonstrate that SDFLoRA outperforms existing federated LoRA baselines and achieves a better utility–privacy trade-off.",154.6,Phi-4,Nvidia B200 (Cloud Native)
2601.11232v1_FactCorrector A Graph-Inspired Approach to Long-Fo.pdf,A Graph-Inspired Approach to Long-Form Factuality Correction of Large Language Models,"['Javier Carnerero-Cano', 'Massimiliano Pronesti', 'Radu Marinescu', 'Tigran Tchrakian', 'James Barry', 'Jasmina Gajcin', 'Yufang Hou', 'Alessandra Pascale', 'Elizabeth Daly']","This paper introduces FACTCORRECTOR, a novel post-hoc correction method for improving the factual accuracy of Large Language Models (LLMs) without the need for retraining. FACTCORRECTOR leverages structured feedback to correct factual errors in LLM outputs across various domains. The paper also presents the VELI5 benchmark, a dataset with systematically injected factual errors and ground-truth corrections, to facilitate rigorous evaluation of factuality correction methods. Experiments demonstrate that FACTCORRECTOR significantly enhances factual precision while maintaining relevance, outperforming existing baselines.",153.96,Phi-4,Nvidia B200 (Cloud Native)
2601.11252v1_Beyond Model Scaling Test-Time Intervention for Ef.pdf,BEYONDMODELSCALING: TEST-TIME INTERVENTION FOR EFFICIENT DEEP REASONING,"['Qianyue Wang', 'Jinwu Hu', 'Yufeng Wang', 'Huanxiang Lin', 'Bolin Chen', 'Zhiquan Wen', 'Yaofo Chen', 'Mingkui Tan']","This paper addresses inefficiencies in Large Reasoning Models (LRMs) such as overthinking and overshoot, which lead to increased computational costs and degraded performance. The authors propose a novel test-time interactive reasoning paradigm called Think-with-Me, which introduces external feedback intervention at transitional conjunctions during the reasoning process. This approach aims to adaptively extend or terminate reasoning to reduce redundancy while maintaining accuracy. The feedback is generated through multi-criteria evaluation and can come from human or LLM proxies. The model is trained using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments demonstrate that Think-with-Me achieves a superior balance between accuracy and reasoning length, significantly outperforming existing models in terms of accuracy and efficiency under limited context windows.",154.02,Phi-4,Nvidia B200 (Cloud Native)
2601.11258v1_Knowledge is Not Enough Injecting RL Skills for Co.pdf,Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation,"['Pingzhi Tang', 'Yiding Wang', 'Muhan Zhang']","The paper addresses the challenge of updating Large Language Models (LLMs) with new information due to their 'knowledge cutoff' problem. While Supervised Fine-Tuning (SFT) is commonly used, it often fails to improve the model's ability to utilize new information effectively. The authors propose a novel framework, Parametric Skill Transfer (PaST), which leverages the orthogonality of SFT and Reinforcement Learning (RL) parameter updates. PaST extracts a domain-agnostic Skill Vector from a source domain and injects it into a target model after lightweight SFT. This approach demonstrates significant improvements in knowledge-incorporation QA and tool-use benchmarks, showcasing its effectiveness and scalability.",154.69,Phi-4,Nvidia B200 (Cloud Native)
2601.11269v1_X-Distill Cross-Architecture Vision Distillation f.pdf,X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning,"['Maanping Shao', 'Feihong Zhang', 'Gu Zhang', 'Baiye Cheng', 'Zhengrong Xue', 'Huazhe Xu']","This paper introduces X-Distill, a method that combines the strengths of Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs) for visuomotor learning. By using cross-architecture knowledge distillation, X-Distill transfers visual representations from a large, frozen DINOv2 teacher to a compact ResNet-18 student. This distilled encoder is then fine-tuned with a diffusion policy head for robotic manipulation tasks. The approach demonstrates superior performance over from-scratch ResNet and fine-tuned DINOv2 encoders, as well as 3D encoders with point cloud observations or Vision-Language Models, across simulated and real-world tasks.",154.16,Phi-4,Nvidia B200 (Cloud Native)
2601.11282v1_From SERPs to Sound How Search Engine Result Pages.pdf,From SERPs to Sound: How Search Engine Result Pages and AI-generated Podcasts Interact to Influence User Attitudes on Controversial Topics,"['Junjie Wang', 'Gaole He', 'Alisa Rieger', 'Ujwal Gadiraju']","This study explores the interaction between search engine result pages (SERPs) and AI-generated podcasts in shaping user attitudes on controversial topics. Through a controlled user study with 483 participants, the research investigates how the sequence and modality of information exposure influence user opinions. The findings indicate that a majority of users experienced attitude change, with sequence playing a significant role. The study also examines the impact of viewpoint bias and topic controversiality on attitude change, finding no effect from individual moderators.",154.22,Phi-4,Nvidia B200 (Cloud Native)
2601.11286v1_XChoice Explainable Evaluation of AI-Human Alignme.pdf,XChoice: Explainable Evaluation of AI–Human Alignment in LLM-based Constrained Choice Decision Making,"['Weihong Qi', 'Fan Huang', 'Rasika Muralidharan', 'Jisun An', 'Haewoon Kwak']","The paper introduces XCHOICE, an explainable framework for evaluating AI-human alignment in constrained decision making. It moves beyond traditional outcome agreement metrics like accuracy and F1 score by fitting a mechanism-based decision model to both human data and LLM-generated decisions. This approach recovers interpretable parameters that reflect the importance of decision factors, constraint sensitivity, and trade-offs. The framework assesses alignment by comparing these parameters across models, options, and subgroups. The authors demonstrate XCHOICE using data from the American Time Use Survey, revealing alignment variations across models and activities, with notable misalignment in Black and married groups. The robustness of XCHOICE is validated through invariance analysis, and a retrieval-augmented generation intervention is evaluated for targeted mitigation. Overall, XCHOICE provides mechanism-based metrics that diagnose misalignment and support informed improvements beyond surface outcome matching.",153.94,Phi-4,Nvidia B200 (Cloud Native)
2601.11344v1_How Much Would a Clinician Edit This Draft Evaluat.pdf,Evaluating LLM Alignment for Patient Message Response Drafting,"['Parker Seegmiller', 'Joseph Gatto', 'Sarah E. Greer', 'Ganza Belise Isingizwe', 'Rohan Ray', 'Timothy Burdick', 'Sarah M. Preum']","This paper investigates the alignment of large language models (LLMs) with clinicians in drafting responses to patient portal messages. It introduces a novel taxonomy of thematic elements in clinician responses and proposes an evaluation framework to assess the editing load of LLM-drafted responses. The study evaluates various adaptation techniques, including thematic prompting and retrieval-augmented generation, using an expert-annotated dataset. Results indicate significant uncertainty in aligning LLM drafts with clinician responses, highlighting the need for theme-driven adaptation strategies to improve alignment and support reliable use in clinical workflows.",154.45,Phi-4,Nvidia B200 (Cloud Native)
2601.11350v1_FEATHer Fourier-Efficient Adaptive Temporal Hierar.pdf,FEATHer: Fourier-Efficient Adaptive Temporal Hierarchy Forecaster for Time-Series Forecasting,"['Jaehoon Lee', 'Seungwoo Lee', 'Younghwi Kim', 'Dohee Kim', 'Sunghyun Sim']","Time-series forecasting is crucial for industrial applications like manufacturing and energy management. As systems evolve towards cyber-physical automation, models must operate on edge devices with strict constraints on latency, memory, and energy. Conventional deep forecasting models become impractical under these constraints. This paper introduces FEATHer, a multiscale temporal model designed for accurate long-term forecasting with minimal resources. FEATHer features an ultra-lightweight multiscale temporal decomposition, a shared Dense Temporal Kernel for efficient temporal mixing, a frequency-aware branch gating mechanism, and a Sparse Period Kernel for capturing periodic structures. FEATHer achieves strong predictive performance with as few as 400 trainable parameters, outperforming existing baselines across eight benchmarks. These results demonstrate the feasibility of reliable long-range forecasting under constrained edge conditions, suggesting a practical direction for next-generation industrial systems.",154.62,Phi-4,Nvidia B200 (Cloud Native)
2601.11354v1_AstroReason-Bench Evaluating Unified Agentic Plann.pdf,AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems,"['Weiyi Wang', 'Xinchi Chen', 'Jingjing Gong', 'Xuanjing Huang', 'Xipeng Qiu']","AstroReason-Bench is introduced as a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), which are characterized by heterogeneous objectives, strict physical constraints, and long-horizon decision-making. The benchmark integrates multiple scheduling regimes, such as ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluations on various state-of-the-art agentic LLM systems reveal that current agents substantially underperform specialized solvers, highlighting the limitations of generalist planning under realistic constraints. AstroReason-Bench serves as a challenging and diagnostic testbed for future agentic research.",154.87,Phi-4,Nvidia B200 (Cloud Native)
2601.11359v1_Think-Clip-Sample Slow-Fast Frame Selection for Vi.pdf,THINK-CLIP-SAMPLE: SLOW-FAST FRAME SELECTION FOR VIDEO UNDERSTANDING,"['Wenhui Tan', 'Ruihua Song', 'Jiaze Li', 'Jianzhong Ju', 'Zhenbo Luo']","This paper introduces Think-Clip-Sample (TCS), a training-free framework designed to enhance long video understanding by addressing computational constraints and suboptimal frame selection in multi-modal large language models (MLLMs). TCS incorporates two key components: Multi-Query Reasoning, which generates multiple queries to capture various aspects of the question and video, and Clip-level Slow-Fast Sampling, which adaptively balances dense local details and sparse global context. The framework demonstrates improved performance across different MLLMs, achieving up to 6.9% accuracy increase and reducing inference time by 50%, as evidenced by experiments on MLVU, LongVideoBench, and VideoMME datasets.",154.61,Phi-4,Nvidia B200 (Cloud Native)
2601.11369v2_Institutional AI Governing LLM Collusion in Multi-.pdf,Institutional AI: Governing LLM Collusion in Multi-Agent Cournot Markets via Public Governance Graphs,"['M. Bracale Syrnikov', 'F. Pierucci', 'M. Galisai', 'M. Prandi', 'P. Bisconti', 'F. Giarrusso', 'O. Sorokoletova', 'V. Suriani', 'D. Nardi']","This paper introduces an experimental framework for evaluating Institutional AI, a system-level approach to AI alignment that shifts focus from preference engineering in agent-space to mechanism design in institution-space. The core contribution is the governance graph, a public, immutable manifest that outlines legal states, transitions, sanctions, and restorative paths. An Oracle/Controller runtime interprets this manifest, enforcing consequences for coordination and maintaining a cryptographically keyed governance log. The framework is applied to govern Cournot collusion, comparing Ungoverned, Constitutional, and Institutional regimes. Results show that the Institutional regime significantly reduces collusion, highlighting the potential of framing multi-agent alignment as an institutional design problem.",154.63,Phi-4,Nvidia B200 (Cloud Native)
2601.11379v1_Evaluating LLM Behavior in Hiring Implicit Weights.pdf,"Evaluating LLM Behavior in Hiring: Implicit Weights, Fairness Across Groups, and Alignment with Human Preferences","['Morgane Hoffmann', 'Emma Jouffroy', 'Warren Jouanneau', 'Marc Palyart', 'Charles Pebereau']","This paper explores the use of General-purpose Large Language Models (LLMs) in recruitment, focusing on how these models assign importance to various attributes in hiring decisions. The authors propose a framework to evaluate LLM decision logic by using synthetic datasets derived from real freelancer profiles and project descriptions. The study aims to understand which attributes LLMs prioritize and how these priorities vary across different project contexts and demographic groups. The findings indicate that LLMs emphasize core productivity signals like skills and experience but may interpret certain features beyond their explicit matching value. Although the models show minimal average discrimination against minority groups, intersectional effects suggest that productivity signals carry different weights between demographic groups. The paper also discusses how to align LLM decision logic with human recruiters' preferences.",154.54,Phi-4,Nvidia B200 (Cloud Native)
2601.11389v1_Hyperparameter Optimization of Constraint Programm.pdf,Hyperparameter Optimization of Constraint Programming Solvers,"['Hedieh Haddad', 'Thibault Falque', 'Pierre Talbot', 'Pascal Bouvry']","The paper introduces a novel two-phase framework, 'probe and solve algorithm', for automated hyperparameter optimization of constraint programming solvers, integrated into the CPMpy library. The framework divides the time budget into a probing phase for exploring hyperparameters and a solving phase for applying the best configuration. Two optimization methods, Bayesian optimization and Hamming distance search, are implemented and compared. The algorithm is evaluated on ACE and Choco solvers across 114 problem instances, showing that Bayesian optimization significantly outperforms default configurations and Hamming distance search, enhancing solution quality in a substantial number of instances.",154.69,Phi-4,Nvidia B200 (Cloud Native)
2601.11400v1_Wetland mapping from sparse annotations with satel.pdf,Wetland mapping from sparse annotations with satellite image time series and temporal-aware segment anything model,"['Shuai Yuan', 'Tianwu Lin', 'Shuang Chen', 'Yu Xia', 'Peng Qin', 'Xiangyu Liu', 'Xiaoqing Xu', 'Nan Xu', 'Hongsheng Zhang', 'Jie Wang', 'Peng Gong']","Accurate wetland mapping is critical for ecosystem monitoring and management, yet acquiring dense pixel-level annotations is prohibitively costly. This paper introduces WetSAM, a novel framework that leverages satellite image time series to enhance wetland mapping from sparse point annotations. WetSAM uses a dual-branch design: a temporal branch that distinguishes wetland features from phenological variations and a spatial branch that reconstructs distinct boundaries via a temporal-constrained region-growing strategy. The framework also employs bidirectional consistency regularization to minimize discrepancies between predictions from two segmentation heads. Validated across eight diverse global locations, WetSAM achieves an average F1-score of 85.58%, outperforming other state-of-the-art algorithms. The results demonstrate that WetSAM provides accurate, structurally consistent segmentation from sparse labels, showing strong generalization ability and promise for scalable, low-cost wetland mapping at high spatial resolutions.",154.59,Phi-4,Nvidia B200 (Cloud Native)
2601.11409v1_Topology-Guaranteed Image Segmentation Enforcing C.pdf,"Topology-Guaranteed Image Segmentation: Enforcing Connectivity, Genus, and Width Constraints","['Wenxiao Li', 'Xue-Cheng Tai', 'Jun Liu']","This paper addresses the limitations of traditional topological methods in image segmentation by proposing a novel framework that integrates width information into topological structures. By combining persistent homology with smoothing concepts from partial differential equations, the method modifies local extrema of upper-level sets to capture width properties. This enhanced topological description is incorporated into variational image segmentation models, allowing for the design of neural networks that segment images with specific topological and width properties. The approach ensures the preservation of essential topological invariants such as connectivity and genus counts, while maintaining critical width attributes like line thickness and length. Numerical experiments demonstrate the method's effectiveness in maintaining topological fidelity and embedding width characteristics into segmented image structures.",154.43,Phi-4,Nvidia B200 (Cloud Native)
2601.11421v1_The Great March 100 100 Detail-oriented Tasks for .pdf,THE GREAT MARCH 100: 100 DETAIL-ORIENTED TASKS FOR EVALUATING EMBODIED AI AGENTS,"['Ziyu Wang', 'Chenyuan Liu', 'Yushun Xiang', 'Runhao Zhang', 'Yu Zhang', 'Qingbo Hao', 'Hongliang Lu', 'Houyu Chen', 'Zhizhong Feng', 'Kaiyue Zheng', 'Dehao Ye', 'Xianchao Zeng', 'Xinyu Zhou', 'Boran Wen', 'Jiaxin Li', 'Mingyu Zhang', 'Kecheng Zheng', 'Qian Zhu', 'Ran Cheng', 'Yong-Lu Li']","The paper introduces the Great March 100 (GM-100), a set of 100 carefully designed tasks aimed at evaluating the capabilities of robotic agents. These tasks cover a wide range of interactions and long-tail behaviors, addressing the limitations of current datasets and task designs that often focus on common tasks and lack systematic consideration. GM-100 aims to provide a diverse and challenging set of tasks to promote diversity and complexity in robot dataset task designs. The tasks are developed through systematic analysis and expansion of existing task designs, combined with insights from human-object interaction primitives and object affordances. The paper presents experimental results demonstrating the feasibility and challenge of the GM-100 tasks in differentiating the performance of current Vision-Language Alignment (VLA) models. Data and code are available at https://rhos.ai/research/gm-100.",153.29,Phi-4,Nvidia B200 (Cloud Native)
2601.11429v1_Relational Linearity is a Predictor of Hallucinati.pdf,Relational Linearity is a Predictor of Hallucinations,"['Yuetian Lu', 'Yihong Liu', 'Hinrich Schütze']","This paper investigates the phenomenon of hallucinations in large language models (LLMs) when answering questions about synthetic entities. The authors hypothesize that the linearity of the relation between subject and object influences the likelihood of hallucinations. They propose that linear relations are stored more abstractly, making it difficult for LLMs to assess their knowledge, whereas nonlinear relations are stored more directly, facilitating knowledge assessment. To test this hypothesis, they create a dataset called SyntHal, consisting of synthetic entities for six relations, and find a strong correlation between relational linearity and hallucination rate. This suggests that the way factual knowledge is represented in LLMs affects their ability to self-assess and manage hallucinations.",154.29,Phi-4,Nvidia B200 (Cloud Native)
2601.11440v1_GenDA Generative Data Assimilation on Complex Urba.pdf,GENDA: GENERATIVEDATAASSIMILATION ONCOMPLEX URBANAREAS VIA CLASSIFIER-FREE DIFFUSION GUIDANCE,"['Francisco Giral', 'Álvaro Manzano', 'Ignacio Gómez', 'Ricardo Vinuesa', 'Soledad Le Clainche']","This paper introduces GenDA, a generative data assimilation framework designed to reconstruct high-resolution wind fields in complex urban areas from sparse sensor data. The framework utilizes a multiscale graph-based diffusion architecture trained on computational fluid dynamics simulations. It employs classifier-free guidance to interpret observational constraints during sampling, allowing for obstacle-aware reconstruction and generalization across various geometries and wind conditions without retraining. Evaluated against graph neural network baselines and classical data assimilation methods, GenDA significantly reduces error and improves structural similarity in wind field reconstructions. The framework is tested on Reynolds-averaged Navier-Stokes simulations of a real urban neighborhood, demonstrating its potential for scalable, geometry-aware environmental monitoring.",154.27,Phi-4,Nvidia B200 (Cloud Native)
2601.11441v1_Hierarchical Orthogonal Residual Spread for Precis.pdf,Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models,"['Xiaojie Gu', 'Guangxu Chen', 'Yuheng Yang', 'Jingxin Han', 'Andi Zhang']","This paper introduces a novel method called HORSE for model editing in large language models (LLMs). Unlike existing methods that focus on optimizing an information matrix, HORSE employs a hierarchical orthogonal residual spread approach. This method reduces noisy gradients and allows for more stable edits by operating at the token level and adapting layer-wise weights in real time. The approach mitigates overfitting and cross-knowledge interference, aligning better with how knowledge is distributed in transformers. Extensive experiments on datasets with models like GPT, LLaMA, and Mistral demonstrate that HORSE achieves state-of-the-art performance in precise massive editing across diverse scenarios.",153.61,Phi-4,Nvidia B200 (Cloud Native)
2601.11442v1_Map2Thought Explicit 3D Spatial Reasoning via Metr.pdf,Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps,"['Xiangjun Gao', 'Zhensong Zhang', 'Dave Zhenyu Chen', 'Songcen Xu', 'Long Quan', 'Eduardo P´erez-Pellitero', 'Youngkyoon Jang']","Map2Thought is a framework designed to enable explicit and interpretable spatial reasoning for 3D Vision-Language Models (3D-VLMs). It introduces two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap integrates a discrete grid for relational reasoning with a continuous, metric-scale representation for precise geometric understanding. Cog-CoT performs explicit geometric reasoning through deterministic operations, producing interpretable inference traces grounded in 3D structure. The framework achieves high accuracy with reduced supervision and outperforms state-of-the-art methods in various training scenarios.",154.47,Phi-4,Nvidia B200 (Cloud Native)
2601.11451v1_PRISM-CAFO Prior-conditioned Remote-sensing Infras.pdf,PRISM-CAFO: Prior-conditioned Remote-sensing Infrastructure Segmentation and Mapping for CAFOs,"['Oishee Bintey Hoque', 'Nibir Chandra Mandal', 'Kyle Luong', 'Amanda Wilson', 'Samarth Swarup', 'Madhav Marathe', 'Abhijin Adiga']","This paper introduces PRISM-CAFO, an infrastructure-first pipeline for identifying and characterizing Concentrated Animal Feeding Operations (CAFOs) using aerial and satellite imagery. The method employs a domain-tuned YOLOv8 detector to identify candidate infrastructure, such as barns and manure lagoons, and uses SAM2 masks to filter these detections. It extracts structured descriptors and combines them with deep visual features using a spatial cross-attention classifier. The approach outputs CAFO type predictions and mask-level attributions, linking decisions to visible infrastructure. The method achieves state-of-the-art performance, with Swin-B+PRISM-CAFO surpassing the best baseline by up to 15%. The paper also discusses the impact of domain priors on classification decisions and releases code and data to support scalable monitoring of livestock infrastructure.",154.27,Phi-4,Nvidia B200 (Cloud Native)
2601.11459v1_Interactive Narrative Analytics Bridging Computati.pdf,Interactive Narrative Analytics: Bridging Computational Narrative Extraction and Human Sensemaking,['BRIAN KEITH'],"This paper introduces the field of Interactive Narrative Analytics (INA), which combines computational narrative extraction with interactive visual analytics to support sensemaking. INA addresses the challenges of information overload and misinformation by enabling the interactive exploration of narrative structures through computational methods and visual interfaces. The field faces challenges in scalability, interactivity, knowledge integration, and evaluation standardization, but offers promising opportunities in news analysis, intelligence, scientific literature exploration, and social media analysis. INA emphasizes temporal, causal, and relational aspects of information, capturing how events unfold and connect over time to form coherent stories.",154.66,Phi-4,Nvidia B200 (Cloud Native)
2601.11464v1_MHA2MLA-VLM Enabling DeepSeeks Economical Multi-He.pdf,MHA2MLA-VLM: Enabling DeepSeek’s Economical Multi-Head Latent Attention across Vision-Language Models,"['Xiaoran Fan', 'Zhichao Sun', 'Tao Ji', 'Lixing Shen', 'Tao Gui']","This paper introduces MHA2MLA-VLM, a framework designed to convert existing vision-language models (VLMs) to a more efficient Multi-Head Latent Attention (MLA) architecture. The framework addresses the challenges of memory and computational bottlenecks in VLMs by employing two core techniques: a modality-adaptive partial-RoPE strategy and a modality-decoupled low-rank approximation method. These techniques allow for effective compression of the Key-Value (KV) cache, reducing its footprint while maintaining model performance. The paper demonstrates that minimizing output activation error, rather than parameter distance, significantly reduces performance loss during adaptation. Extensive experiments show that MHA2MLA-VLM can restore original model performance with minimal supervised data and integrates seamlessly with KV quantization.",153.92,Phi-4,Nvidia B200 (Cloud Native)
2601.11468v1_Exploring LLM Features in Predictive Process Monit.pdf,Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs,"['ALESSANDRO PADELLA', 'MASSIMILIANO DE LEONI', 'MARLON DUMAS']","This paper extends prior research on using Large Language Models (LLMs) for Predictive Process Monitoring (PPM) in data-scarce environments. The study evaluates the generality, semantic leverage, and reasoning mechanisms of LLMs across multiple Key Performance Indicators (KPIs) such as Total Time and Activity Occurrence prediction. Empirical evaluations on three distinct event logs demonstrate that LLMs outperform benchmark methods in settings with only 100 traces. The paper also explores the reasoning strategies of LLMs, showing that they perform higher-order reasoning rather than merely replicating existing predictive methods.",154.21,Phi-4,Nvidia B200 (Cloud Native)
2601.11479v1_Health Facility Location in Ethiopia Leveraging LL.pdf,Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning,"['Yohai Trabelsi', 'Guojun Xiong', 'Fentabil Getnet', 'Stéphane Verguet', 'Milind Tambe']","This paper presents a hybrid framework developed in collaboration with the Ethiopian Public Health Institute and Ministry of Health to optimize the location of health facilities in Ethiopia. The framework integrates expert knowledge with optimization techniques, addressing the challenge of limited resources and the need for prioritization in upgrading health posts. It combines a provable approximation algorithm for population coverage optimization with a Large Language Model (LLM)-driven iterative refinement process. This approach ensures that solutions align with expert qualitative guidance while maintaining coverage guarantees. The effectiveness of the framework is demonstrated through experiments on real-world data from three Ethiopian regions, highlighting its potential for equitable, data-driven health system planning.",154.29,Phi-4,Nvidia B200 (Cloud Native)
2601.11492v1_BoxMind Closed-loop AI strategy optimization for e.pdf,BoxMind: Closed-loop AI strategy optimization for elite boxing validated in the 2024 Olympics,"['Kaiwen Wang', 'Kaili Zheng', 'Rongrong Deng', 'Qingmin Fan', 'Milin Zhang', 'Zongrui Li', 'Xuesi Zhou', 'Bo Han', 'Liren Chen', 'Chenyi Guo', 'Ji Wu']","BoxMind is a closed-loop AI expert system designed for elite boxing, validated during the 2024 Olympics. It addresses the complexity of boxing analytics by defining atomic punch events and parsing match footage into hierarchical technical-tactical indicators. A graph-based predictive model is proposed, combining technical-tactical profiles with latent embeddings to predict match outcomes. The system achieved 69.8% accuracy on the BoxerGraph test set and 87.5% on Olympic matches, providing strategic recommendations comparable to human experts. BoxMind contributed to the Chinese National Team's success, establishing a paradigm for transforming video data into strategic intelligence in sports.",154.56,Phi-4,Nvidia B200 (Cloud Native)
2601.11496v1_The Poisoned Apple Effect Strategic Manipulation o.pdf,The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents,"['Eilam Shapira', 'Moshe Tennenholtz', 'Roi Reichart']","This paper explores the economic implications of integrating AI agents into markets, focusing on how expanding available technologies can alter strategic interactions and regulatory outcomes. The authors introduce the 'Poisoned Apple' effect, where agents release new technologies not for use but to manipulate market design in their favor, impacting equilibrium payoffs and fairness objectives. The study highlights the vulnerability of static regulatory frameworks to such manipulations and calls for dynamic market designs that adapt to evolving AI capabilities.",154.84,Phi-4,Nvidia B200 (Cloud Native)
2601.11505v1_MetaboNet The Largest Publicly Available Consolida.pdf,METABONET: THE LARGEST PUBLICLY AVAILABLE CONSOLIDATED DATASET FOR TYPE1 DIABETES MANAGEMENT,"['Miriam K. Wolff', 'Peter Calhoun', 'Eleonora Maria Aiello', 'Yao Qin', 'Sam F. Royston']","This paper introduces the MetaboNet dataset, a consolidated resource for Type 1 Diabetes (T1D) management algorithm development. It addresses the fragmentation and lack of standardization in existing T1D datasets by unifying multiple publicly available datasets into a single resource. The MetaboNet dataset includes continuous glucose monitoring (CGM) data, insulin pump dosing records, and auxiliary information such as carbohydrate intake and physical activity for 3135 subjects over 1228 patient-years. This dataset is significantly larger than existing standalone datasets and is available in both a fully public subset and a Data Use Agreement (DUA)-restricted subset. The paper describes the dataset's broad range of glycemic profiles and demographics, which can enhance the generalizability of algorithmic performance. The dataset is accessible for immediate download, with processing pipelines provided for converting data into the standardized MetaboNet format.",154.76,Phi-4,Nvidia B200 (Cloud Native)
2601.11516v2_Building Production-Ready Probes For Gemini.pdf,Building Production-Ready Probes For Gemini,"['János Kramár', 'Joshua Engels', 'Zheng Wang', 'Bilal Chughtai', 'Rohin Shah', 'Neel Nanda', 'Arthur Conmy']","This paper discusses the development of activation probes as a misuse mitigation technique for Gemini, Google's frontier language model. The authors identify challenges in generalizing probes under production distribution shifts, particularly from short-context to long-context inputs. They propose new probe architectures to address these challenges and evaluate their robustness in the cyber-offensive domain. The study demonstrates that combining architecture choice with diverse training distributions enhances generalization. Additionally, the paper highlights the successful deployment of these probes in user-facing instances of Gemini and explores the use of AlphaEvolve for automating improvements in probe architecture and adaptive red teaming. The findings contribute to the broader application of probes as a misuse mitigation in production environments.",154.51,Phi-4,Nvidia B200 (Cloud Native)
2601.11517v1_Do explanations generalize across large reasoning .pdf,DO EXPLANATIONS GENERALIZE ACROSS LARGE REASONING MODELS?,"['Koyena Pal', 'David Bau', 'Chandan Singh']","This paper investigates whether explanations generated by large reasoning models (LRMs) generalize across different models. The study evaluates if a chain of thought (CoT) produced by one LRM can guide other LRMs to the same conclusion, thereby assessing the generalizability of these explanations. The findings suggest that CoT explanations often increase consistency between LRMs and that this generalization correlates with human preference rankings and reinforcement learning post-training. The paper also proposes a sentence-level ensembling strategy to improve consistency and highlights the need for caution when using LRM explanations for new insights. The study provides a framework for characterizing the generalization of LRM explanations, emphasizing their potential utility in scientific discovery.",154.42,Phi-4,Nvidia B200 (Cloud Native)
2601.11625v1_Reasoning Stabilization Point A Training-Time Sign.pdf,Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance,['Sahil Rajesh Dhayalkar'],"This paper introduces a training-time interpretability view that tracks token-level attributions across fine-tuning epochs to monitor how decision evidence evolves. It defines 'explanation drift' as the change in token attributions between epochs and introduces the 'Reasoning Stabilization Point' (RSP) as the earliest epoch after which drift remains consistently low. The study finds that explanation drift stabilizes early in training, even as validation accuracy continues to improve marginally. This approach helps identify when models rely on shortcuts, such as label-correlated trigger tokens, which may not be apparent from validation accuracy alone. The paper provides a simple, low-cost diagnostic for selecting checkpoints in a stable-evidence regime.",154.53,Phi-4,Nvidia B200 (Cloud Native)
2601.11643v1_Syllabic Agglutinative Tokenizations for Indonesia.pdf,Syllabic Agglutinative Tokenizations for Indonesian LLM: A Study from “Gasing Literacy Learning System”,"['Hokky Situngkir', 'Andhika Bernard Lumbantobing', 'Yohanes Surya']","This paper introduces a novel syllable-based tokenization approach for Indonesian large language models, inspired by the Gasing Literacy Learning System. The method segments Indonesian text at syllable boundaries before applying byte-pair encoding, creating a vocabulary that aligns with the language's morphophonological structure. It identifies high-frequency syllables through rule-based segmentation and constructs a compact vocabulary of 3,500 tokens. Empirical evaluation shows substantial improvements over conventional methods, achieving higher R´enyi efficiency and maintaining longer average token lengths. The approach reduces computational burden while respecting Indonesian's agglutinative morphology, offering a promising paradigm for linguistically-informed tokenization strategies, especially for morphologically rich and underrepresented languages.",152.4,Phi-4,Nvidia B200 (Cloud Native)
2601.11644v1_Predicting When to Trust Vision-Language Models fo.pdf,Predicting When to Trust Vision-Language Models for Spatial Reasoning,"['Muhammad Imran', 'Yugyung Lee']","Vision-Language Models (VLMs) are powerful for multimodal tasks but struggle with spatial reasoning, achieving only 49-54% accuracy on basic directional tasks. This paper introduces a vision-based confidence estimation framework to predict when to trust VLM spatial predictions. The framework uses independent geometric verification through object detection, improving accuracy and reliability over text-based methods. It achieves significant improvements in AUROC scores and coverage, demonstrating the effectiveness of external geometric verification over self-assessment. The approach is crucial for applications requiring precise spatial understanding, such as robotics and autonomous systems.",154.21,Phi-4,Nvidia B200 (Cloud Native)
2601.11647v1_Reinforcement Learning for Dynamic Workflow Optimi.pdf,Reinforcement Learning for Dynamic Workflow Optimization in CI/CD Pipelines,"['Aniket Abhishek Soni', 'Milan Parikh', 'Rashi Nimesh Kumar Dhenia', 'Jubin Abhishek Soni', 'Ayush Raj Jha', 'Sneja Mitinbhai Shah']","This paper proposes a reinforcement learning (RL) approach to optimize CI/CD pipeline workflows dynamically. The pipeline is modeled as a Markov Decision Process, and an RL agent is trained to make runtime decisions to maximize throughput and minimize testing overhead. A simulated CI/CD environment is used to evaluate the approach, showing up to a 30% improvement in throughput and a 25% reduction in test execution overhead compared to a static baseline. The RL agent learns to skip or abbreviate tests when appropriate, accelerating delivery without significantly increasing the risk of undetected failures. This work demonstrates the potential of RL to adapt DevOps workflows for greater efficiency, providing novel insights into intelligent pipeline automation.",153.36,Phi-4,Nvidia B200 (Cloud Native)
2601.11650v1_Large Language Model Agent for User-friendly Chemi.pdf,LARGE LANGUAGE MODEL AGENT FOR USER-FRIENDLY CHEMICAL PROCESS SIMULATIONS,"['Jingkang Liang', 'Niklas Groll', 'Gürkan Sin']","This paper introduces a large language model (LLM) agent integrated with A VEV A Process Simulation (APS) via Model Context Protocol (MCP) to facilitate natural language interaction with rigorous process simulations. The integration allows users to execute complex simulation tasks using plain-language instructions. The framework is evaluated through two water-methanol separation case studies, demonstrating its utility in both educational and professional settings. It aids in autonomously analyzing and optimizing flowsheets, extracting data, and presenting results. The framework supports both step-by-step dialogue and single-prompt modes, catering to novices and experts. Despite current limitations like oversimplification and calculation errors, the LLM-based agent shows promise as a valuable tool for analysis, optimization, and guided construction in chemical process simulations.",154.58,Phi-4,Nvidia B200 (Cloud Native)
2601.11651v1_Aesthetics as Structural Harm Algorithmic Lookism .pdf,Aesthetics as Structural Harm: Algorithmic Lookism Across Text-to-Image Generation and Classification,"['Miriam Doh', 'Aditya Gulati', 'Corina Canali', 'Nuria Oliver']","This paper investigates algorithmic lookism in text-to-image (T2I) generative AI and gender classification tasks. By analyzing 26,400 synthetic faces generated with Stable Diffusion models, the study demonstrates how these models encode biases associating facial attractiveness with positive attributes, reflecting socially constructed biases. The research identifies significant gender biases in gender classification algorithms, with women's faces, especially those with negative attributes, experiencing higher misclassification rates. The study highlights three critical harms: the encoding of attractiveness-positive attribute associations in T2I models, gender disparities in classification systems, and intensified aesthetic constraints in newer models. These findings reveal algorithmic lookism as a systematic infrastructure across AI vision systems, exacerbating existing inequalities through representation and recognition.",154.56,Phi-4,Nvidia B200 (Cloud Native)
2601.11652v1_WISP Waste- and Interference-Suppressed Distribute.pdf,WISP: Waste- and Interference-Suppressed Distributed Speculative LLM Serving at the Edge via Dynamic Drafting and SLO-Aware Batching,"['XIANGCHEN LI', 'JIAKUN FAN', 'QINGYUAN WANG', 'DIMITRIOS SPATHARAKIS', 'SAEID GHAFOURI', 'HANS VANDIERENDONCK', 'DEEPU JOHN', 'BO JI', 'ALI R. BUTT', 'DIMITRIOS S. NIKOLOPOULOS']","This paper addresses the challenges of integrating edge devices into the Large Language Model (LLM) inference process to balance workloads between edge and cloud. It identifies two bottlenecks: Wasted Drafting Time and Verification Interference. The proposed solution, WISP, is an efficient and SLO-aware distributed LLM inference system that includes an intelligent speculation controller, a verification time estimator, and a verification batch scheduler. These components enhance drafting efficiency and optimize verification request scheduling, significantly improving system capacity and goodput compared to centralized serving and SLED.",152.67,Phi-4,Nvidia B200 (Cloud Native)
2601.11657v1_Size is Not the Solution Deformable Convolutions f.pdf,Size is Not the Solution: Deformable Convolutions for Effective Physics Aware Deep Learning,"['Jack T. Beerman', 'Shobhan Roy', 'H.S. Udaykumar', 'Stephen S. Baek']","This paper introduces deformable physics-aware recurrent convolutions (D-PARC) to address the limitations of traditional convolutional neural networks (CNNs) in modeling highly nonlinear physical systems. Inspired by Hybrid Lagrangian-Eulerian (HLE) numerical methods, D-PARC demonstrates superior performance in capturing complex phenomena such as Burgers’ equation, Navier-Stokes, and reactive flows. Unlike larger architectures, D-PARC employs an 'active filtration' strategy, focusing computational resources on high-strain regions and coarsening elsewhere, akin to adaptive refinement in computational mechanics. This approach highlights the potential of strategic architectural design over mere parameter scaling in physics-aware deep learning.",154.48,Phi-4,Nvidia B200 (Cloud Native)
2601.11658v1_Towards AGI A Pragmatic Approach Towards Self Evol.pdf,Towards AGI: A Pragmatic Approach Towards Self Evolving Agent,"['Indrajit Kar', 'Sammy Zonunpuia', 'Zonunfeli Ralte']","This paper introduces a hierarchical self-evolving multi-agent framework that integrates a Base LLM, an operational SLM agent, a Code-Generation LLM, and a Teacher-LLM to enable continuous adaptation. The framework allows agents to autonomously expand capabilities, generate new tools, and evolve their reasoning. The workflow involves task execution using existing tools, tool synthesis through the Code-Gen LLM, and evolution via Curriculum Learning (CL), Reward-Based Learning (RL), or Genetic Algorithm (GA) when failures persist. Evaluated using the TaskCraft dataset, the study finds that CL provides fast recovery and strong generalization, RL excels in high-difficulty tasks, and GA offers high behavioral diversity. Evolved agents outperform their originals, demonstrating robust, autonomous, self-improving agentic evolution.",154.23,Phi-4,Nvidia B200 (Cloud Native)
2601.11663v1_Activation Sensitivity as a Unifying Principle for.pdf,Activation Sensitivity as a Unifying Principle for Post-Training Quantization,['Bruce Changlong Xu'],"This paper presents a unified theoretical framework for post-training quantization (PTQ) of large language models by introducing the concept of activation sensitivity. Activation sensitivity is defined as the expected impact of channel-wise perturbations on the loss, which is formalized using a first-order Taylor expansion of the loss. This results in a measure of channel importance that captures both activation magnitude and downstream error propagation. The paper shows that existing methods like AWQ and GPTQ can be seen as approximations of this sensitivity under different assumptions. The work connects various sensitivity metrics to classical pruning methods and highlights challenges in PTQ, such as cross-layer error accumulation and calibration distribution mismatch. The paper aims to provide a conceptual foundation for understanding and extending PTQ methods.",153.82,Phi-4,Nvidia B200 (Cloud Native)
2601.11664v1_Serverless AI Security Attack Surface Analysis and.pdf,Serverless AI Security: Attack Surface Analysis and Runtime Protection Mechanisms for FaaS-Based Machine Learning,"['Chetan Pathade', 'Vinod Dhimam', 'Ilsa Lareb', 'Sheheryar Ahmad']","This paper presents a comprehensive security analysis of machine learning workloads in serverless environments, focusing on Function-as-a-Service (FaaS) platforms. It characterizes the attack surface across five categories: function-level vulnerabilities, model-specific threats, infrastructure attacks, supply chain risks, and IAM complexity. Through empirical assessments on AWS Lambda, Azure Functions, and Google Cloud Functions, the paper demonstrates real-world attack scenarios and quantifies their security impact. It proposes the Serverless AI Shield (SAS), a multi-layered defense framework that includes pre-deployment validation, runtime monitoring, and post-execution forensics. The evaluation shows SAS achieves 94% detection rates with a performance overhead below 9% for inference latency. An open-source security toolkit is released to help practitioners assess and harden their serverless AI deployments, advancing the field toward more resilient cloud-native machine learning systems.",157.66,Phi-4,Nvidia B200 (Cloud Native)
2601.11666v1_MATEX Multi-scale Attention and Text-guided Explai.pdf,MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models,"['Muhammad Imran', 'Chi Lee', 'Yugyung Lee']","MATEX (Multi-scale Attention and Text-guided Explainability) is a novel framework designed to enhance interpretability in medical vision-language models by incorporating anatomically informed spatial reasoning. It combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to produce precise, stable, and clinically meaningful gradient attribution maps. MATEX addresses limitations of prior methods, such as spatial imprecision and lack of anatomical grounding, and outperforms the state-of-the-art M2IB approach in spatial precision and alignment with expert-annotated findings on the MS-CXR dataset. This framework aims to improve trust and transparency in radiological AI applications.",157.71,Phi-4,Nvidia B200 (Cloud Native)
2601.11667v1_Distill-then-Replace Efficient Task-Specific Hybri.pdf,Distill-then-Replace: Efficient Task-Specific Hybrid Attention Model Construction,"['Xiaojie Xia', 'Huigang Zhang', 'Chaoliang Zhong', 'Jun Sun', 'Yusuke Oishi']","Transformer architectures, while delivering state-of-the-art accuracy, are limited by their quadratic time and memory complexity with respect to sequence length. Linear attention mechanisms offer a more scalable alternative but often at the cost of performance. This paper introduces a method to construct efficient task-specific hybrid attention models by transferring weights from pretrained full-attention modules to linear attention counterparts using blockwise local distillation. Additionally, a greedy layer replacement strategy is employed to iteratively substitute full attention blocks with linear ones, optimizing for validation performance on the target task. This approach allows for the creation of a task-specific hybrid model in a single efficient pass, without the need for costly re-training or neural architecture search, and can be applied to any pretrained full-attention backbone for various downstream tasks.",158.13,Phi-4,Nvidia B200 (Cloud Native)
2601.11670v1_A Confidence-Variance Theory for Pseudo-Label Sele.pdf,A Confidence-Variance Theory for Pseudo-Label Selection in Semi-Supervised Learning,"['Jinshi Liu', 'Pan Liu']","This paper introduces a Confidence-Variance (CoVar) theory framework for pseudo-label selection in semi-supervised learning. It addresses the issue of overconfidence in deep networks by proposing a joint reliability criterion that combines maximum confidence (MC) with residual-class variance (RCV). The framework suggests that reliable pseudo-labels should exhibit high MC and low RCV, with the influence of RCV increasing as confidence grows. This approach corrects overconfident but unstable predictions. The paper presents a threshold-free selection mechanism and integrates CoVar into existing semi-supervised methods, demonstrating consistent improvements across various datasets and label ratios. The study highlights the importance of combining confidence with residual-class variance for more reliable pseudo-label selection.",158.02,Phi-4,Nvidia B200 (Cloud Native)
2601.11674v1_Pigment Network Detection and Classification in De.pdf,Pigment Network Detection and Classification in Dermoscopic Images Using Directional Imaging Algorithms and Convolutional Neural Networks,"['M. A. Rasel', 'Sameem Abdul Kareem', 'Unaizah Obaidellah']","This study focuses on automating the detection and classification of pigment networks (PN) in dermoscopic images, which is crucial for early melanoma diagnosis. The research employs a directional imaging algorithm incorporating Principal Component Analysis (PCA), contrast enhancement, filtering, and noise reduction, achieving a 96% success rate on the PH2 dataset, which improved to 100% after pixel intensity adjustments. A new dataset of PN images was created, and two classifiers, Convolutional Neural Network (CNN) and Bag of Features (BoF), were used to classify PN into atypical and typical categories. The CNN, designed with two convolutional layers and two batch normalization layers, achieved 90% accuracy, 90% sensitivity, and 89% specificity. The study demonstrates the potential of the proposed CNN model for effective PN classification and suggests future research should focus on expanding datasets and incorporating additional dermatological features to enhance melanoma diagnosis.",158.14,Phi-4,Nvidia B200 (Cloud Native)
2601.11675v1_Generating metamers of human scene understanding.pdf,GENERATING METAMERS OF HUMAN SCENE UNDERSTANDING,"['Ritik Raina', 'Abe Leite', 'Alexandros Graikos', 'Seoyoung Ahn', 'Dimitris Samaras', 'Gregory J. Zelinsky']","This paper introduces MetamerGen, a tool designed to generate images that align with latent human scene representations. MetamerGen is a latent diffusion model that synthesizes images by combining low-resolution 'gist' information from peripheral vision with high-resolution details from fixated areas. This novel image-to-image synthesis problem is addressed using a dual-stream representation of foveated scenes, integrating DINOv2 tokens to merge detailed features from fixated regions with peripherally degraded features capturing scene context. The perceptual alignment of MetamerGen-generated images to human scene representations is evaluated through a behavioral experiment, where participants judge whether generated images are 'same' or 'different' from the original. The study reveals that high-level semantic alignment is crucial for metamerism, particularly when scenes are conditioned on viewers' own fixated regions. MetamerGen provides insights into the features contributing to human judgments at various levels of visual processing and highlights the importance of understanding scene metamerism in cognitive science.",153.89,Phi-4,Nvidia B200 (Cloud Native)
2601.11676v1_HALO Semantic-Aware Distributed LLM Inference in L.pdf,HALO: Semantic-Aware Distributed LLM Inference in Lossy Edge Network,"['Peirong Zheng', 'Wenchao Xu', 'Haozhao Wang', 'Jinyu Chen', 'Xuemin (Sherman) Shen']","The paper addresses the challenge of deploying large language models (LLMs) for inference at the edge, which is constrained by the limited resources of individual edge nodes. Distributed inference is proposed as a solution to leverage computational resources across multiple devices. However, existing methods face issues with strict synchronization due to unreliable network conditions. The authors introduce HALO, a novel framework designed to enhance distributed LLM inference in lossy edge networks by allowing relaxed synchronization. HALO strategically allocates less critical neuron groups to unstable devices to minimize delays caused by packet loss. The framework includes three key mechanisms: a semantic-aware predictor to evaluate neuron group significance, a parallel execution scheme for neuron group loading, and a load-balancing scheduler for managing devices with heterogeneous resources. Experimental results demonstrate that HALO achieves a 3.41x speedup for LLaMA-series LLMs under unreliable network conditions, maintaining performance comparable to optimal conditions and outperforming existing methods.",154.23,Phi-4,Nvidia B200 (Cloud Native)
2601.11683v1_Attesting Model Lineage by Consisted Knowledge Evo.pdf,Attesting Model Lineage by Consisted Knowledge Evolution with Fine-Tuning Trajectory,"['Zhuoyi Shang', 'Jiasen Li', 'Pengzhen Chen', 'Yanwei Liu', 'Xiaoyan Gu', 'Weiping Wang']","The paper addresses the emerging lineage relationship among models due to fine-tuning in deep learning, focusing on security concerns like unauthorized model redistribution and false claims of model provenance. Existing methods rely on static architectural similarities, which are insufficient for capturing the dynamic knowledge evolution in model lineage. The authors propose a novel framework for model lineage attestation by verifying the trajectory of knowledge evolution and parameter modification. This involves quantifying parameter-level changes through model editing and using a knowledge vectorization mechanism with probe samples to refine evolved knowledge into compact representations. The framework adapts probing strategies to different model families and verifies arithmetic consistency of knowledge relationships, demonstrating effectiveness in various adversarial scenarios. The method achieves reliable lineage verification across classifiers, diffusion models, and large language models.",153.31,Phi-4,Nvidia B200 (Cloud Native)
2601.11684v1_Mobile-friendly Image de-noising Hardware Consciou.pdf,Mobile-friendly Image de-noising: Hardware Conscious Optimization for Edge Application,"['Srinivas Miriyala', 'Sowmya Vajrala', 'Hitesh Kumar', 'Sravanth Kodavanti', 'Vikram Rajendiran']","This paper presents a novel mobile-friendly network for image de-noising using Entropy-Regularized differentiable Neural Architecture Search (NAS) on a hardware-aware search space for a U-Net architecture. The designed model achieves a 12% reduction in parameters, a ~2-fold improvement in on-device latency, and a 1.5-fold improvement in memory footprint with a 0.7% drop in PSNR when deployed on Samsung Galaxy S24 Ultra. Compared to the state-of-the-art Swin-Transformer for Image Restoration, the proposed network offers competitive accuracy with an ~18-fold reduction in GMACs. The network was tested for Gaussian de-noising with three intensities on four benchmarks and real-world de-noising on one benchmark, demonstrating its generalization ability.",154.28,Phi-4,Nvidia B200 (Cloud Native)
2601.11685v1_Towards Efficient Image Deblurring for Edge Deploy.pdf,Towards Efficient Image Deblurring for Edge Deployment,"['Srinivas Soumitri Miriyala', 'Sowmya Lahari Vajrala', 'Rama Sravanth Kodavanti']","This paper addresses the challenge of deploying efficient image deblurring models on edge devices, where real-time constraints are critical. While recent deep networks like transformers and activation-free architectures achieve high accuracy, their efficiency is often measured in FLOPs or parameters, which do not correlate with latency on embedded hardware. The authors propose a hardware-aware adaptation framework that restructures existing models through sensitivity-guided block substitution, surrogate distillation, and training-free multi-objective search driven by device profiling. Applied to the 36-block NAFNet baseline, the optimized variants achieve up to 55% reduction in GMACs compared to recent transformer-based state-of-the-art models while maintaining competitive accuracy. On-device deployment yields a 1.25× latency improvement over the baseline. Experiments on motion deblurring (GoPro), defocus deblurring (DPDD), and auxiliary benchmarks (RealBlur-J/R, HIDE) demonstrate the generality of the approach, confirming its accuracy-efficiency trade-off. These results establish feedback-driven adaptation as a principled strategy for bridging the gap between algorithmic design and deployment-ready deblurring models.",154.66,Phi-4,Nvidia B200 (Cloud Native)
2601.11686v1_Proof of Concept Multi-Target Wildfire Risk Predic.pdf,Proof of Concept: Multi-Target Wildfire Risk Prediction and Large Language Model Synthesis,"['Nicolas Caron', 'Hassan Noura', 'Christophe Guyeux', 'Benjamin Aynes']","This paper presents a proof of concept for a hybrid framework that combines predictive models for various wildfire risk dimensions with large language models (LLMs) to synthesize heterogeneous outputs into structured, actionable reports. The framework aims to address the operational needs of first responders and firefighting services by providing multi-target wildfire risk assessments, including meteorological danger, ignition activity, intervention complexity, and resource mobilization. The approach is designed to support early attack and resource pre-positioning with appropriate temporal and spatial resolution.",154.56,Phi-4,Nvidia B200 (Cloud Native)
2601.11687v1_Semantic Caching and Intent-Driven Context Optimiz.pdf,Semantic Caching and Intent-Driven Context Optimization for Multi-Agent Natural Language to Code Systems: A Production Study in Enterprise Analytics,['Harmohit Singh'],"This paper presents a production-optimized multi-agent system designed to translate natural language queries into executable Python code for structured data analytics. Unlike systems that rely on expensive frontier models, this approach achieves high accuracy and cost efficiency through three key innovations: a semantic caching system with LLM-based equivalence detection and structured adaptation hints that provides cache hit rates of 67% on production queries; a dual-threshold decision mechanism that separates exact-match retrieval from reference-guided generation; and an intent-driven dynamic prompt assembly system that reduces token consumption by 40-60% through table-aware context filtering. The system has been deployed in production for enterprise inventory management, processing over 10,000 queries with an average latency of 8.2 seconds and 94.3% semantic accuracy. The paper describes the architecture, presents empirical results from production deployment, and discusses practical considerations for deploying LLM-based analytics systems at scale.",154.35,Phi-4,Nvidia B200 (Cloud Native)
2601.11688v1_SpecMap Hierarchical LLM Agent for Datasheet-to-Co.pdf,SpecMap: Hierarchical LLM Agent for Datasheet-to-Code,"['Vedant Nipane', 'Pulkit Agrawal', 'Amit Singh']","The paper introduces a hierarchical methodology for mapping datasheets to code in embedded systems engineering, leveraging large language models (LLMs) for semantic analysis. This approach addresses the challenge of establishing precise traceability between datasheets and code implementations, particularly in low-level software where manual mapping is infeasible. By structuring the traceability process across multiple abstraction levels, the method narrows the search space through repository-level structure inference, file-level relevance estimation, and fine-grained symbol-level alignment. It extends beyond function-centric mapping to include macros, structs, constants, configuration parameters, and register definitions. The approach is evaluated on open-source embedded systems repositories, showing significant improvements over traditional baselines with up to 73.3% file mapping accuracy. The methodology reduces computational overhead and runtime, supporting automated analysis and enabling applications like training data generation for machine learning models, standards compliance verification, and specification coverage analysis.",154.36,Phi-4,Nvidia B200 (Cloud Native)
2601.11700v1_Telling Human and Machine Handwriting Apart.pdf,Telling Human and Machine Handwriting Apart,"['Luis A. Leiva', 'Moises Diaz', 'Nuwan T. Attygalle', 'Miguel A. Ferrer', 'Réjean Plamondon']","This paper explores the use of handwriting movements as a form of behavioral biometrics to verify human presence in digital interactions. The study involves ten public datasets of handwritten symbols, reproduced using various synthesizers, and employs a shallow recurrent neural network to distinguish between human and machine-generated inputs. The classifier achieves high accuracy, demonstrating robust performance even in few-shot and out-of-domain settings. The research highlights the potential for enhancing security systems by verifying human presence, thereby mitigating risks associated with synthetic data and online fraud.",154.43,Phi-4,Nvidia B200 (Cloud Native)
2601.11702v1_PASTA A Scalable Framework for Multi-Policy AI Com.pdf,PASTA: A Scalable Framework for Multi-Policy AI Compliance,"['YU YANG', 'IG-JAE KIM', 'DONGWOOK YOON']","The paper introduces PASTA, a scalable compliance tool designed to address the challenges of multi-policy AI compliance. It integrates four key innovations: a comprehensive model-card format, a policy normalization scheme, an efficient LLM-powered pairwise evaluation engine with cost-saving strategies, and an interface that provides interpretable evaluations through compliance heatmaps and actionable recommendations. PASTA aligns closely with human expert judgments and offers a novel framework for scalable automated AI governance, making it easier for practitioners to understand and act upon compliance evaluations.",154.43,Phi-4,Nvidia B200 (Cloud Native)
2601.11713v1_Inter-Cell Interference Rejection Based on Ultrawi.pdf,Inter-Cell Interference Rejection Based on Ultrawideband Walsh-Domain Wireless Autoencoding,"['Rodney Martinez Alonso', 'Cel Thys', 'Sofie Pollin', 'Cedric Dehos', 'Yuneisy Esthela Garcia Guzman']","This paper introduces a novel technique for rejecting partial-in-band inter-cell interference (ICI) in ultrawideband communication systems. It presents an end-to-end wireless autoencoder architecture that optimizes transmitter and receiver encoding/decoding in the Walsh domain to mitigate interference from coexisting narrower-band 5G base stations. By leveraging the orthogonality and self-inverse properties of Walsh functions, the system encodes bit-words across parallel Walsh branches. Analytical modeling and simulation characterize how 5G CP-OFDM interference maps into the Walsh domain, identifying optimal transmission frequency and sampling rate ratios for maximum ICI rejection. Experimental results demonstrate up to 12 dB of ICI rejection while maintaining a low block error rate (BLER) under baseline channel noise conditions.",154.32,Phi-4,Nvidia B200 (Cloud Native)
2601.11746v1_LIME-LLM Probing Models with Fluent Counterfactual.pdf,LIME-LLM: Probing Models with Fluent Counterfactuals,"['George Mihaila', 'Suleyman Olcay Polat', 'Poli Nemkova', 'Himanshu Sharma', 'Namratha V. Urs', 'Mark V. Albert']","LIME-LLM introduces a framework that replaces random token masking with hypothesis-driven, controlled perturbations for generating local explanations in NLP. By using a 'Single Mask–Single Sample' protocol and employing neutral and boundary infill strategies, LIME-LLM constructs fluent, on-manifold neighborhoods that isolate feature effects. The method is evaluated against traditional perturbation-based methods and generative alternatives, demonstrating significant improvements in local explanation fidelity across benchmarks like CoLA, SST-2, and HateXplain.",154.29,Phi-4,Nvidia B200 (Cloud Native)
2601.11747v1_PRISM Learning Design Knowledge from Data for Styl.pdf,PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement,"['Huaxiaoyue Wang', 'Sunav Choudhary', 'Franck Dernoncourt', 'Yu Shen', 'Stefano Petrangeli']","The paper introduces PRISM (PRior-Informed Stylistic Modification), a method to improve graphic design based on natural language instructions. It addresses the limitations of Vision Language Models (VLMs) in capturing specific design styles by leveraging real-world design data to learn and apply design knowledge. PRISM clusters designs to capture style diversity, summarizes clusters into actionable knowledge, and retrieves relevant knowledge during inference for style-aware improvements. Experiments on the Crello dataset demonstrate PRISM's superior performance in style alignment, validated by user studies.",154.28,Phi-4,Nvidia B200 (Cloud Native)
2601.11758v1_Early Linguistic Pattern of Anxiety from Social Me.pdf,Early Linguistic Pattern of Anxiety from Social Media: Using Interpretable Linguistic Features: A Multi-Faceted Validation Study with Author-Disjoint Evaluation,"['Arnab Das', 'Utsa']","This study presents a transparent approach to detecting anxiety through social media language using interpretable linguistic features. By analyzing Reddit posts, the research employs a logistic regression classifier trained on curated subreddits. The evaluation includes feature ablation, keyword masking, and cross-domain validation with clinically interviewed participants. The model demonstrates strong performance and robustness, even with sentiment removal or keyword masking. The findings suggest that linguistic features can reliably support anxiety detection across diverse online contexts, providing a reproducible baseline for interpretable mental health screening.",154.33,Phi-4,Nvidia B200 (Cloud Native)
2601.11762v1_Industry-Aligned Granular Topic Modeling.pdf,Industry-Aligned Granular Topic Modeling,"['Sae Young Moon', 'Myeongjun Erik Jang', 'Haoyan Luo', 'Chunyang Xiao', 'Antonios Georgiadis', 'Fran Silavong']","This paper introduces the TIDE framework, which provides a novel granular topic modeling method based on large language models (LLMs). It addresses the need for deeper insights in business applications by offering functionalities such as summarizing long documents, topic parenting, and distillation. Through experiments on various datasets, TIDE demonstrates superior performance over modern topic modeling methods and supports industrial business scenarios. The framework is in the process of being open-sourced.",154.24,Phi-4,Nvidia B200 (Cloud Native)
2601.11768v1_Lightweight Self-Supervised Detection of Fundament.pdf,Lightweight Self-Supervised Detection of Fundamental Frequency and Accurate Probability of Voicing in Monophonic Music,"['Venkat Suprabath Bitra', 'Homayoon Beigi']","This paper introduces a lightweight, fully self-supervised framework for joint fundamental frequency (F0) estimation and voicing inference, designed for rapid single-instrument training from limited audio. The framework employs transposition-equivariant learning on CQT features and an EM-style iterative reweighting scheme using Shift Cross-Entropy (SCE) consistency as a reliability signal to suppress uninformative noisy/unvoiced frames. The resulting weights provide confidence scores for pseudo-labeling a separate lightweight voicing classifier without manual annotations. The method, trained on MedleyDB and evaluated on MDB-stem-synth ground truth, achieves competitive cross-corpus performance and demonstrates cross-instrument generalization.",153.31,Phi-4,Nvidia B200 (Cloud Native)
2601.11776v1_Cleansing the Artificial Mind A Self-Reflective De.pdf,Cleansing the Artificial Mind: A Self-Reflective Detoxification Framework for Large Language Models,"['Kaituo Zhang', 'Zhimeng Jiang', 'Na Zou']","This paper introduces a self-reflective detoxification framework for Large Language Models (LLMs) that leverages their inherent self-regulatory mechanisms to detect and correct toxic content without relying on external modules or data annotation. The proposed framework includes a Toxic Signal Detector for internal self-identification and a systematic intervention process to transform toxic text into non-toxic counterparts. Experiments on benchmark datasets demonstrate superior detoxification performance compared to state-of-the-art methods while maintaining semantic fidelity. The findings highlight the potential for self-regulated language models, contributing to more responsible and ethically guided text generation systems.",154.37,Phi-4,Nvidia B200 (Cloud Native)
2601.11778v1_Translation as a Scalable Proxy for Multilingual E.pdf,Translation as a Scalable Proxy for Multilingual Evaluation,"['Sheriff Issaka', 'Erick Rosas Gonzalez', 'Lieqi Liu', 'Evans Kofi Agyei', 'Lucas Bandarkar', 'Nanyun Peng', 'David Ifeoluwa Adelani', 'Francisco Guzmán', 'Saadia Gabriel']","The paper addresses the challenge of evaluating large language models (LLMs) across the world's 7,000 languages, noting that comprehensive benchmarks exist for fewer than 30 languages. It proposes using translation quality as a scalable, cost-effective proxy for assessing a model's multilingual capabilities. The study evaluates 14 models across 9 benchmarks and 7 translation metrics, finding a strong correlation between translation performance and downstream task success. This suggests that translation quality can serve as a reliable indicator of multilingual understanding, facilitating a translation-first screening approach with targeted follow-up evaluations.",153.69,Phi-4,Nvidia B200 (Cloud Native)
2601.11781v1_Risk-Aware Human-in-the-Loop Framework with Adapti.pdf,Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles,"['Dawood Wasif', 'Terrence J. Moore', 'Seunghyun Yoon', 'Hyuk Lim', 'Dan Dongseong Kim', 'Frederica F. Nelson', 'Jin-Hee Cho']","The paper introduces RAIL, a risk-aware human-in-the-loop framework designed to enhance the safety and effectiveness of autonomous vehicles (AVs) when encountering rare long-tailed scenarios or cyber-physical intrusions. RAIL integrates three cues—curvature actuation integrity, time-to-collision proximity, and observation-shift consistency—into an Intrusion Risk Score (IRS) using a weighted Noisy-OR model. When the IRS exceeds a threshold, RAIL blends actions with cue-specific shields and allows for human override, while a contextual bandit arbitrates among shields to improve mitigation choices. The framework combines Soft Actor–Critic (SAC) with risk-prioritized replay and dual rewards, ensuring that takeovers and near misses guide learning. RAIL outperforms various baselines in MetaDrive and CARLA simulations, demonstrating improved success rates and reduced disturbance rates under cyber-attacks.",153.06,Phi-4,Nvidia B200 (Cloud Native)
2601.11792v1_A self-evolving multi-role collaborative framework.pdf,A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation,"['Yifei Sun', 'Yongan Li', 'A.K. Qin', 'Sicheng Hua', 'Tamas Pflanzner']","This paper addresses the task of innovative mathematical problem generation (IMPG) by proposing a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance. The framework includes a multi-role collaborative mechanism with components such as a sampler, generator, evaluator, state machine, and memory, ensuring problem correctness through iterative optimization. An improved difficulty model is introduced to quantify difficulty and provide fine-grained guidance, utilizing the data-driven association-guided path sampling (DAPS) algorithm. The HSM3K-CN dataset, comprising high-quality high school math problems, is constructed. A multi-stage training pipeline involving continual pre-training (CPT), supervised fine-tuning (SFT), and group relative policy optimization (GRPO) enhances the model's generation and evaluation capabilities. System self-evolution is achieved by transferring evaluation capabilities from the expert model to the apprentice model via distillation. Experiments demonstrate that the proposed method significantly improves the innovation of generated problems while maintaining a high correctness rate.",154.15,Phi-4,Nvidia B200 (Cloud Native)
2601.11801v1_RobotDesignGPT Automated Robot Design Synthesis us.pdf,RobotDesignGPT: Automated Robot Design Synthesis using Vision Language Models,"['Nitish Sontakke', 'K. Niranjan Kumar', 'Sehoon Ha']","The paper introduces RobotDesignGPT, a novel framework that automates the robot design synthesis process using large pre-trained vision-language models. This approach leverages the general knowledge and reasoning capabilities of these models to create initial robot designs from simple user prompts and reference images. The framework improves design quality through a novel visual feedback mechanism, reducing the need for manual intervention. It demonstrates the ability to design visually appealing and kinematically valid robots inspired by nature, such as legged animals and flying creatures. The effectiveness of the framework is validated through an ablation study and a user study.",154.19,Phi-4,Nvidia B200 (Cloud Native)
2601.11809v1_Multi-agent DRL-based Lane Change Decision Model f.pdf,Multi-agent DRL-based Lane Change Decision Model for Cooperative Planning in Mixed Traffic,"['Zeyu Mu', 'Shangtong Zhang', 'B. Brian Park']","This study introduces a hybrid multi-agent lane change decision model to enhance cooperative platooning among connected automated vehicles (CAVs) in mixed traffic. The model leverages the QMIX framework and a convolutional neural network (CNN-QMIX) to process traffic data, enabling optimal decision-making regardless of the number of CAVs present. It includes a trajectory planner and a model predictive controller to ensure smooth and safe lane changes. Evaluated in a microsimulation environment, the model significantly outperforms baseline rule-based models, increasing cooperative platooning rates by up to 26.2%, thus optimizing CAV cooperation and traffic dynamics during early deployment stages.",154.13,Phi-4,Nvidia B200 (Cloud Native)
2601.11816v1_POLARIS Typed Planning and Governed Execution for .pdf,POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation,"['Zahra Moslemi', 'Keerthi Koneru', 'Yen-Ting Lee', 'Sheethal Kumar', 'Ramesh Radhakrishnan']","The paper introduces POLARIS, a framework designed for policy-aware, auditable, and operationally predictable agentic systems in enterprise back-office automation. POLARIS addresses the limitations of generic multi-agent setups by proposing a governed orchestration framework that involves typed plan synthesis and validated execution over LLM agents. The framework includes a planner that generates type-checked directed acyclic graphs (DAGs), a reasoning module that selects compliant plans, and execution that is safeguarded by validator-gated checks, a bounded repair loop, and compiled policy guardrails. POLARIS is applied to document-centric finance tasks, producing decision-grade artifacts and full execution traces with reduced human intervention. Empirical evaluations show high performance on the SROIE dataset and a synthetic suite, establishing POLARIS as a benchmark for governed Agentic AI.",153.02,Phi-4,Nvidia B200 (Cloud Native)
2601.11825v1_AI Co-Scientist for Knowledge Synthesis in Medical.pdf,AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept,"['Arya Rahgozara', 'Pouria Mortezaagha']","This paper introduces an AI co-scientist designed to enhance scalable and transparent knowledge synthesis in biomedical science. The AI system formalizes Population, Intervention, Comparator, Outcome, and Study design (PICOS) to address research waste caused by redundant studies and limited scalability of traditional evidence synthesis workflows. The platform integrates relational databases, semantic retrieval, and a Neo4j knowledge graph, evaluated on dementia-sport and non-communicable disease corpora. It employs a transformer-based classifier for PICOS compliance and retrieval-augmented generation for full-text synthesis, outperforming non-retrieval methods in structured queries and cross-study integration. The study demonstrates the potential of AI in improving the efficiency and accuracy of medical knowledge synthesis.",154.6,Phi-4,Nvidia B200 (Cloud Native)
2601.11840v1_Imandra CodeLogician Neuro-Symbolic Reasoning for .pdf,Neuro-Symbolic Reasoning for Precise Analysis of Software Logic,"['Hongyu Lin', 'Samer Abdallah', 'Makar Valentinov', 'Paul Brennan', 'Elijah Kagan', 'Christoph M. Wintersteiger', 'Denis Ignatovich', 'Grant Passmore']","This paper introduces CodeLogician, a neurosymbolic agent and framework designed for precise analysis of software logic. It integrates with ImandraX, an industrial automated reasoning engine, to enhance the capabilities of Large Language Models (LLMs) in constructing explicit formal models of software systems. The paper presents a new benchmark dataset, code-logic-bench, to evaluate mathematical reasoning about software logic, bridging the gap between theorem proving and software engineering benchmarks. The results demonstrate that augmenting LLMs with CodeLogician significantly improves reasoning accuracy and coverage, highlighting the importance of neurosymbolic integration for rigorous software understanding and formal verification.",154.46,Phi-4,Nvidia B200 (Cloud Native)
2601.11850v1_Human-AI Collaborative Inductive Thematic Analysis.pdf,Human–AI Collaborative Inductive Thematic Analysis: How AI Guides Analysis and Researchers Reclaim Interpretive Authority,"['Matthew Nyaaba', 'Min SungEun', 'Mary Abiswin Apam', 'Kwame Owoahene Acheampong', 'Emmanuel Dwamena', 'Xiaoming Zhai']","The study explores the integration of generative artificial intelligence (GenAI) into qualitative research, specifically through a tool called Inductive Thematic Analysis GPT (ITA–GPT). This tool assists researchers in conducting inductive thematic analysis by guiding them through established procedures. The research, conducted within the framework of Human–Artificial Intelligence Collaborative Inductive Thematic Analysis (HACITA), examines how researchers interact with ITA–GPT to analyze interview transcripts from education research in Ghana. The findings indicate that while ITA–GPT provides a procedural scaffold and enhances transparency, researchers maintain epistemic authority by modifying, deleting, rejecting, inserting, and commenting on AI-generated outputs. This ensures the restoration of contextual and emotional nuances and aligns interpretations with professional standards. The study highlights the importance of human intervention in refining AI-generated abstractions and maintaining audit trails.",154.2,Phi-4,Nvidia B200 (Cloud Native)
2601.11854v1_ATOD An Evaluation Framework and Benchmark for Age.pdf,ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System,"['Yifei Zhang', 'Hooshang Nayyeri', 'Rinat Khaziev', 'Emine Yilmaz', 'Gokhan Tur', 'Dilek Hakkani-Tür', 'Hari Thadakamalla']","This paper introduces ATOD, a benchmark and synthetic dialogue generation pipeline designed to evaluate advanced task-oriented dialogue (TOD) systems. These systems are characterized by their ability to handle multi-goal coordination, dependency management, long-term memory, adaptability, and proactive behavior. The paper also proposes ATOD-Eval, a comprehensive evaluation framework that translates these capabilities into fine-grained metrics for both offline and online assessments. Experiments demonstrate that ATOD-Eval provides a thorough assessment of task completion, agentic capabilities, and response quality, offering a better accuracy-efficiency trade-off compared to existing methods.",156.95,Phi-4,Nvidia B200 (Cloud Native)
2601.11859v1_Cascaded Transformer for Robust and Scalable SLA D.pdf,Cascaded Transformer for Robust and Scalable SLA Decomposition via Amortized Optimization,['Cyril Shih-Huan Hsu'],"The paper introduces Casformer, a cascaded Transformer architecture designed for fast, optimization-free SLA decomposition in 6G networks. Casformer leverages historical domain feedback and integrates cross-domain dependencies to achieve improved SLA decomposition quality, scalability, and robustness under volatile network conditions. The model is trained using a learning paradigm inspired by Domain-Informed Neural Networks (DINNs), incorporating risk-informed modeling and amortized optimization. This approach reduces runtime complexity and simplifies deployment, offering a scalable and efficient solution for real-time SLA management in advanced 5G-and-beyond network environments.",158.38,Phi-4,Nvidia B200 (Cloud Native)
2601.11863v1_Utilizing Metadata for Better Retrieval-Augmented .pdf,Utilizing Metadata for Better Retrieval-Augmented Generation,"['Raquib Bin Yousuf', 'Shengzhe Xu', 'Mandar Sharma', 'Andrew Neeser', 'Chris Latimer', 'Naren Ramakrishnan']","This paper investigates retrieval-augmented generation systems that rely on retrieving semantically relevant document chunks to support accurate outputs from large language models. The study focuses on structured and repetitive corpora, such as regulatory filings, where chunk similarity alone is insufficient. The authors explore metadata-aware retrieval strategies, comparing plain-text baselines with methods that embed metadata directly. The evaluation includes metadata-as-text (prefix and suffix), dual-encoder unified embedding, dual-encoder late-fusion retrieval, and metadata-aware query reformulation. The findings indicate that prefixing and unified embeddings consistently outperform plain-text baselines, with unified embeddings sometimes exceeding prefixing in performance while being easier to maintain. The study also analyzes embedding space, demonstrating that metadata integration enhances effectiveness by increasing intra-document cohesion, reducing inter-document confusion, and widening the separation between relevant and irrelevant chunks. Structural cues from field-level ablations provide strong disambiguating signals. The paper's code, evaluation framework, and the RAGMATE-10K dataset are publicly available.",158.24,Phi-4,Nvidia B200 (Cloud Native)
2601.11868v1_Terminal-Bench Benchmarking Agents on Hard Realist.pdf,"TERMINAL-BENCH: BENCHMARKING AGENTS ON HARD, REALISTIC TASKS IN COMMAND LINE INTERFACES","['Mike A. Merrill', 'Alexander G. Shaw', 'Nicholas Carlini', 'Boxuan Li', 'Harsh Raj', 'Ivan Bercovich', 'Lin Shi', 'Jeong Yeon Shin', 'Thomas Walshe', 'E. Kelly Buchanan', 'Junhong Shen', 'Guanghao Ye', 'Haowei Lin', 'Jason Poulos', 'Maoyu Wang', 'Marianna Nezhurina', 'Jenia Jitsev', 'Di Lu', 'Orfeas Menis Mastromichalakis', 'Zhiwei Xu', 'Zizhao Chen', 'Yue Liu', 'Robert Zhang', 'Leon Liangyu Chen', 'Anurag Kashyap', 'Jan-Lucas Uslu', 'Jeffrey Li', 'Jianbo Wu', 'Minghao Yan', 'Song Bian', 'Vedang Sharma', 'Ke Sun', 'Steven Dillmann', 'Akshay Anand', 'Andrew Lanpouthakoun', 'Bardia Koopah', 'Changran Hu', 'Etash Guha', 'Gabriel H. S. Dreiman', 'Jiacheng Zhu', 'Karl Krauth', 'Li Zhong', 'Niklas Muennighoff', 'Robert Amanfu', 'Shangyin Tan', 'Shreyas Pimpalgaonkar', 'Tushar Aggarwal', 'Xiangning Lin', 'Xin Lan', 'Xuandong Zhao', 'Yiqing Liang', 'Yuanli Wang', 'Zilong Wang', 'Changzhi Zhou', 'David Heineman', 'Hange Liu', 'Harsh Trivedi', 'John Yang', 'Junhong Lin', 'Manish Shetty', 'Michael Yang', 'Nabil Omi', 'Negin Raoof', 'Shanda Li', 'Terry Yue Zhuo', 'Wuwei Lin', 'Yiwei Dai', 'Yuxin Wang', 'Wenhao Chai', 'Shang Zhou', 'Dariush Wahdany', 'Ziyu She', 'Jiaming Hu', 'Zhikang Dong', 'Yuxuan Zhu', 'Sasha Cui', 'Ahson Saiyed', 'Arinbjorn Kolbeinsson', 'Jesse Hu', 'Christopher Michael Rytting', 'Ryan Marten', 'Yixin Wang', 'Andy Konwinski']","Terminal-Bench 2.0 is a benchmark designed to evaluate AI agents on challenging, realistic tasks within command line environments. It consists of 89 tasks inspired by real-world workflows, each with unique environments, human-written solutions, and comprehensive tests. The benchmark reveals that current frontier models and agents achieve less than 65% success, highlighting areas for improvement. The dataset and evaluation tools are made available to support future research and development at tbench.ai.",154.31,Phi-4,Nvidia B200 (Cloud Native)
2601.11876v1_AI for Green Spaces Leveraging Autonomous Navigati.pdf,Autonomous Trash Pickup Robots on Grass Fields: A Viable Solution,['Author(s) not explicitly listed in the provided text'],"The paper addresses the significant problem of litter in the U.S., particularly in grass fields, where traditional cleaning methods are ineffective. The authors propose an autonomous robot capable of navigating, identifying, and picking up trash in parks. The robot uses a Spanning Tree Coverage algorithm for path planning and Real-Time Kinematic GPS for navigation, achieving centimeter-level accuracy. For trash detection, the ResNet50 Convolutional Neural Network is employed, achieving 94.52% accuracy. The robot's pickup mechanism is specifically designed for grass field litter, resulting in an overall success rate of 80%. This demonstrates the viability of autonomous trash pickup robots in grass fields.",157.26,Phi-4,Nvidia B200 (Cloud Native)
2601.11880v1_TF-CoDiT Conditional Time Series Synthesis with Di.pdf,TF-CoDiT: Conditional Time Series Synthesis with Diffusion Transformers for Treasury Futures,"['Yingxiao Zhang', 'Jiaxin Duan', 'Junfu Zhang', 'Ke Feng']","This paper introduces TF-CoDiT, a novel framework utilizing Diffusion Transformers (DiT) for synthesizing treasury futures data. It addresses the challenges of low data volume, market dependencies, and grouped correlations among variables in treasury futures. TF-CoDiT transforms 1-D time series into Discrete Wavelet Transform coefficient matrices and employs a U-shape Variational Autoencoder (VAE) to encode cross-channel dependencies into a latent variable. The framework uses the Financial Market Attribute Protocol (FinMAP) to derive prompts covering essential market conditions. Experiments demonstrate TF-CoDiT's ability to generate highly authentic data with minimal error, showcasing its robustness across different contracts and temporal horizons.",157.44,Phi-4,Nvidia B200 (Cloud Native)
2601.11885v1_MyGram Modality-aware Graph Transformer with Globa.pdf,MyGram: Modality-aware Graph Transformer with Global Distribution for Multi-modal Entity Alignment,"['Zhifei Li', 'Ziyue Qin', 'Xiangyu Luo', 'Xiaoju Hou', 'Yue Zhao', 'Miao Zhang', 'Zhifang Huang', 'Kui Xiao', 'Bing Yang']","This paper introduces MyGram, a modality-aware graph transformer designed for multi-modal entity alignment. The approach addresses the challenge of integrating multi-modal data, such as images and text, to enhance the semantic representations of entities in knowledge graphs. Existing methods often neglect structural contextual information within each modality, leading to susceptibility to interference from shallow features. MyGram incorporates a modality diffusion learning module to capture deep structural contextual information and facilitate fine-grained multi-modal fusion. Additionally, it introduces a Gram Loss to ensure global distribution consistency across modalities by minimizing the volume of a 4-dimensional parallelotope formed by multi-modal features. Experimental results on five public datasets demonstrate that MyGram outperforms baseline models, achieving significant improvements in Hits@1 across various datasets.",156.47,Phi-4,Nvidia B200 (Cloud Native)
2601.11895v1_DevBench A Realistic Developer-Informed Benchmark .pdf,"DevBench: A Realistic, Developer-Informed Benchmark for Code Generation Models","['Pareesa Ameneh Golnari', 'Adarsh Kumarappan', 'Wen Wen', 'Xiaoyu Liu', 'Gabriel Ryan', 'Yuting Sun', 'Shengyu Fu', 'Elsie Nallipogu']","DevBench is a telemetry-driven benchmark designed to evaluate Large Language Models (LLMs) on realistic code completion tasks. It includes 1,800 evaluation instances across six programming languages and six task categories derived from real developer telemetry. The benchmark emphasizes ecological validity, avoids training data contamination, and enables detailed diagnostics. It assesses 9 state-of-the-art models, revealing differences in syntactic precision, semantic reasoning, and practical utility. DevBench provides actionable insights for model selection and improvement, focusing on realistic and challenging completion scenarios identified from over one billion developer code completion interactions.",157.97,Phi-4,Nvidia B200 (Cloud Native)
2601.11903v1_AEMA Verifiable Evaluation Framework for Trustwort.pdf,AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems,"['Yen-Ting Lee', 'Keerthi Koneru', 'Zahra Moslemi', 'Sheethal Kumar', 'Ramesh Radhakrishnan']","The paper introduces AEMA (Adaptive Evaluation Multi-Agent), a framework designed to evaluate large language model (LLM)-based multi-agent systems. AEMA addresses the challenges of evaluating these systems by providing a process-aware and auditable framework that ensures reliable coordination, transparent decision-making, and verifiable performance. Unlike traditional single-response scoring methods, AEMA offers greater stability, human alignment, and traceable records, supporting accountable automation. The framework is demonstrated through enterprise-style agent workflows, showing improved score consistency, human alignment, and verifiable consistency compared to single LLM-as-a-Judge methods. The contributions of AEMA include a unified process-aware, verifiable framework for multi-agent evaluation, enhancing trust and control in agentic AI systems.",158.07,Phi-4,Nvidia B200 (Cloud Native)
2601.11905v1_LIBRA Language Model Informed Bandit Recourse Algo.pdf,LIBRA: Language Model Informed Bandit Recourse Algorithm for Personalized Treatment Planning,"['Junyu Cao', 'Ruijiang Gao', 'Esmaeil Keyvanshokooh', 'Jianhao Ma']","This paper introduces a unified framework integrating algorithmic recourse, contextual bandits, and large language models (LLMs) for sequential decision-making in high-stakes settings like personalized medicine. The authors propose the Generalized Linear Recourse Bandit (GLRB) algorithm and the LIBRA algorithm, which combines domain knowledge from LLMs with bandit learning. LIBRA provides guarantees for warm-start performance, limited LLM consultation, and robustness against unreliable LLMs. The paper establishes matching lower bounds for the recourse bandit problem and demonstrates the near-optimality of the proposed algorithms through experiments. Results show improvements in regret, treatment quality, and sample efficiency over standard contextual bandits and LLM-only benchmarks, highlighting the potential of LLM-assisted bandit algorithms in personalized high-stakes decision-making.",158.03,Phi-4,Nvidia B200 (Cloud Native)
2601.11907v1_Towards Airborne Object Detection A Deep Learning .pdf,Towards Airborne Object Detection: A Deep Learning Analysis,"['Prosenjit Chatterjee', 'ANK Zaman']","This paper introduces a dual-task model based on EfficientNetB4 for airborne object classification and threat-level prediction. It addresses the scarcity of clean, balanced training data by constructing the AODTA Dataset from multiple public sources. The model achieves 96% accuracy in object classification and 90% accuracy in threat-level prediction, outperforming a ResNet-50 baseline. The study focuses on classification and threat-level inference using pre-localized airborne object images, complementing existing object detection pipelines.",158.09,Phi-4,Nvidia B200 (Cloud Native)
2601.11913v1_LSTM-MAS A Long Short-Term Memory Inspired Multi-A.pdf,LSTM-MAS: A Long Short-Term Memory Inspired Multi-Agent System for Long-Context Understanding,"['Yichen Jiang', 'Peng Ye', 'Jiakang Yuan', 'Chongjun Tu', 'Lei Bai', 'Tao Chen']","This paper introduces LSTM-MAS, a Multi-Agent System inspired by the Long Short-Term Memory (LSTM) architecture, designed to address the challenge of long-context understanding in large language models (LLMs). Traditional single-LLM-based methods face limitations due to computational costs and constrained context lengths. LSTM-MAS mitigates these issues by organizing agents in a chained architecture, incorporating worker, filter, judge, and manager agents to emulate LSTM's hierarchical information flow and gated memory mechanisms. This design enables controlled information transfer and selective long-term dependency modeling, effectively avoiding error accumulation and hallucination propagation. The model demonstrates significant improvements over the previous best multi-agent approach, CoA, across various benchmarks.",156.05,Phi-4,Nvidia B200 (Cloud Native)
2601.11920v1_Enhancing LLM-Based Data Annotation with Error Dec.pdf,Enhancing LLM-Based Data Annotation with Error Decomposition,"['Zhen Xu', 'Vedant Khatri', 'Yijun Dai', 'Xiner Liu', 'Siyan Li', 'Xuanming Zhang', 'Renzhe Yu']","This paper addresses the challenges of using large language models (LLMs) for data annotation, particularly in subjective tasks involving psychological constructs. While LLMs achieve near-human accuracy in objective tasks, their performance in subjective tasks is inconsistent. The authors propose a diagnostic evaluation paradigm that distinguishes between task-inherent ambiguity and model-driven inaccuracies. This paradigm includes a diagnostic taxonomy, a human annotation test, and a computational method to decompose LLM annotation errors. The approach is validated on educational annotation tasks, demonstrating its conceptual validity and practical utility. The paper argues that high alignment metrics do not fully capture the quality of LLM annotations and offers a low-cost diagnostic tool for assessing task suitability and guiding technical optimization.",158.03,Phi-4,Nvidia B200 (Cloud Native)
2601.11935v1_Big Data Workload Profiling for Energy-Aware Cloud.pdf,Big Data Workload Profiling for Energy-Aware Cloud Resource Management,"['Milan Parikh', 'Aniket Abhishek Soni', 'Sneja Mitinbhai Shah', 'Ayush Raj Jha']","This paper presents a workload-aware scheduling framework for cloud data centers to reduce operational energy consumption amidst growing big data workloads. The framework profiles CPU usage, memory demand, and storage I/O behavior to guide energy-efficient virtual machine placement. By leveraging historical execution logs and real-time telemetry, it predicts the energy and performance impact of placement decisions, adaptively consolidating workloads without violating service-level agreements. Evaluated on a multi-node cloud testbed using Hadoop MapReduce, Spark MLlib, and ETL workloads, the framework achieved a 15–20% reduction in energy consumption while maintaining SLA compliance. The study underscores the effectiveness of data-driven workload profiling in enhancing the sustainability of cloud computing environments.",157.65,Phi-4,Nvidia B200 (Cloud Native)
2601.11940v1_Thinking Traps in Long Chain-of-Thought A Measurab.pdf,Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart,"['Kang Chen', 'Fan Yu', 'Junjie Nian', 'Shihan Zhao', 'Zhuoka Feng', 'Zijun Yao', 'Heng Wang', 'Minshen Yu', 'Yixin Cao']","The paper investigates the limitations of Long Chain-of-Thought (Long-CoT) in enhancing reasoning capabilities, particularly focusing on 'thinking traps' where models continue to elaborate incorrect reasoning paths. The authors introduce TAAR (Trap-Aware Adaptive Restart), a framework that predicts and mitigates these traps by truncating and adaptively restarting the reasoning process. Experiments demonstrate TAAR's effectiveness in improving reasoning performance across various benchmarks without fine-tuning base model parameters.",158.05,Phi-4,Nvidia B200 (Cloud Native)
2601.11956v1_Double-Calibration Towards Trustworthy LLMs via Ca.pdf,Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and Reasoning Confidence,"['Yuyin Lu', 'Ziran Liang', 'Yanghui Rao', 'Wenqi Fan', 'Fu Lee Wang', 'Qing Li']","The paper addresses the challenge of hallucination in Large Language Models (LLMs) by introducing a framework called DoublyCal. This framework employs a double-calibration principle to enhance the trustworthiness of LLMs. It uses a lightweight proxy model to generate Knowledge Graph (KG) evidence with calibrated confidence, which then guides the LLM to produce more accurate and well-calibrated predictions. The approach significantly improves both the accuracy and confidence calibration of LLMs with minimal token cost, addressing both epistemic and aleatoric uncertainties.",157.71,Phi-4,Nvidia B200 (Cloud Native)
2601.11960v1_R2PO Decoupling Training Trajectories from Inferen.pdf,R2PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning,"['Jingchu Wang', 'Bingbing Xu', 'Yige Yuan', 'Bin Xie', 'Xiaoqian Sun', 'Huawei Shen']","This paper introduces R2PO (Residual Rollout Policy Optimization), a novel approach to decouple training trajectories from inference responses in large language models (LLMs) using reinforcement learning. Traditional methods use a single policy for both inference and training, leading to conflicts between the need for stable inference responses and diverse training trajectories. R2PO addresses this by adding a lightweight residual module atop the policy, allowing for controlled trajectory diversification during training while maintaining stable inference generation. Experiments demonstrate that R2PO outperforms baseline methods, achieving significant accuracy improvements and reducing errors in various benchmarks.",157.72,Phi-4,Nvidia B200 (Cloud Native)
2601.11969v1_textttMemoryRewardBench Benchmarking Reward Models.pdf,MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models,"['Zecheng Tang', 'Baibei Ji', 'Ruoxi Sun', 'Haitian Wang', 'Wangjie You', 'Yijun Zhang', 'Wenpeng Zhu', 'Ji Qi', 'Juntao Li', 'Min Zhang']","This paper introduces MemRewardBench, the first benchmark for evaluating the ability of reward models (RMs) to assess long-term memory management in large language models (LLMs). The benchmark covers tasks involving long-context comprehension and long-form generation, with context lengths ranging from 8K to 128K tokens. Evaluations on 13 state-of-the-art RMs reveal a narrowing performance gap between open-source and proprietary models, with newer models outperforming older ones. The study also highlights the capabilities and limitations of current RMs in evaluating memory management across various settings.",157.61,Phi-4,Nvidia B200 (Cloud Native)
2601.11974v1_Learn Like Humans Use Meta-cognitive Reflection fo.pdf,Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement,"['Xinmeng Hou', 'Peiliang Gong', 'Bohao Qu', 'Wuqi Wang', 'Qing Guo', 'Yang Liu']","The paper introduces the Metacognitive Agent Reflective Self-improvement (MARS) framework, which aims to enhance the adaptability and efficiency of Large Language Models (LLMs) by mimicking human learning processes. Unlike existing self-improving frameworks that rely on inefficient multi-turn recursive loops, MARS achieves self-evolution within a single recurrence cycle. It integrates principle-based reflection to abstract normative rules and procedural reflection to derive step-by-step strategies, allowing agents to refine their reasoning logic without continuous online feedback. The framework is inspired by educational psychology and demonstrates superior performance over state-of-the-art self-evolving systems with reduced computational overhead, as evidenced by experiments on six benchmarks.",156.99,Phi-4,Nvidia B200 (Cloud Native)
2601.11977v1_One-Shot Price Forecasting with Covariate-Guided E.pdf,One-Shot Price Forecasting with Covariate-Guided Experts under Privacy Constraints,"['He Ren', 'Yinliang Xu', 'Jinfeng Wang', 'Jeremy Watson', 'Jian Song']","This paper addresses the challenges of forecasting in power systems, which involves complex multivariate time series with strict privacy constraints. Traditional methods struggle with generalization across diverse scenarios. The authors propose a novel MoE-Encoder module to enhance pre-trained forecasting models by adding a sparse mixture-of-experts layer. This module transforms multivariate forecasting into an expert-guided univariate task, capturing inter-variable relations and supporting localized training in federated settings. Experiments show that MoE-Encoder improves forecasting accuracy and allows efficient adaptation to new regions with minimal performance loss, offering a scalable and privacy-aware solution.",158.16,Phi-4,Nvidia B200 (Cloud Native)
2601.11979v1_Process In-Context Learning Enhancing Mathematical.pdf,Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion,"['Ang Gao', 'Changshuo Zhang', 'Xiao Zhang', 'Deyang Li', 'Minjun Zhao', 'Fangchao Liu', 'Xinyu Zhang']","This paper introduces Process In-Context Learning (PICL), a framework designed to enhance mathematical reasoning in large language models by dynamically inserting demonstrations during inference. Traditional in-context learning (ICL) methods use static demonstrations, which do not adapt to the evolving needs of multi-step reasoning tasks. PICL addresses this by identifying confusion points in real-time and retrieving relevant demonstrations to guide the reasoning process. Experiments demonstrate that PICL outperforms baseline methods by reducing errors during inference, highlighting the importance of adaptive demonstration insertion in complex reasoning tasks.",157.65,Phi-4,Nvidia B200 (Cloud Native)
2601.11995v1_Learning Audio-Visual Embeddings with Inferred Lat.pdf,Learning Audio–Visual Embeddings with Inferred Latent Interaction Graphs,"['Donghuo Zeng', 'Hao Niu', 'Yanan Wang', 'Masato Taya']","This paper addresses the challenge of learning robust audio–visual embeddings by proposing a framework that leverages soft-label predictions and inferred latent interactions. The framework includes three main components: (1) Audio–Visual Semantic Alignment Loss (AV-SAL) trains a teacher network to produce aligned soft-label distributions across modalities, enriching the supervision signal by assigning nonzero probability to co-occurring but unannotated events. (2) Inferred Latent Interaction Graph (ILI) uses the GRaSP algorithm to infer a sparse, directed dependency graph among classes, highlighting directional dependencies that expose likely semantic or conditional relationships. (3) Latent Interaction Regularizer (LIR) trains a student network with both metric loss and a regularizer guided by the ILI graph, enhancing robustness and semantic coherence. Experiments on AVE and VEGAS benchmarks demonstrate consistent improvements in mean average precision (MAP).",158.48,Phi-4,Nvidia B200 (Cloud Native)
2601.11998v1_Hybrid IDS Using Signature-Based and Anomaly-Based.pdf,Hybrid IDS Using Signature-Based and Anomaly-Based Detection,"['Messaouda Boutassetta', 'Amina Makhlouf', 'Newfel Messaoudi', 'Abdelmadjid Benmachiche', 'Ines Boutabia']","This paper presents a comprehensive survey and conceptual overview of Hybrid Intrusion Detection Systems (IDS), which integrate signature-based and anomaly-based detection techniques to enhance attack detection capabilities. The survey examines recent research on Hybrid IDS, classifies existing models into functional categories, and discusses their advantages, limitations, and application domains, including financial systems, air traffic control, and social networks. Recent trends in Hybrid IDS research, such as machine learning-based approaches and cloud-based deployments, are reviewed. The paper outlines potential future research directions aimed at developing more cost-effective Hybrid IDS solutions with improved ability to detect emerging and sophisticated cyberattacks.",157.94,Phi-4,Nvidia B200 (Cloud Native)
2601.12002v1_Kernel-Based Learning of Safety Barriers.pdf,Kernel-Based Learning of Safety Barriers,"['Oliver Schöno', 'Zhengang Zhong', 'Sadegh Soudjani']","The paper presents a data-driven approach for safety verification and synthesis of black-box systems with discrete-time stochastic dynamics. It introduces the use of control barrier certificates to ensure system safety and employs conditional mean embeddings to embed system data into a reproducing kernel Hilbert space (RKHS). An RKHS ambiguity set is constructed to robustify results against out-of-distribution behavior. The approach is theoretically extended to various temporal logic specifications beyond safety. For computing safety barriers, a finite Fourier expansion is used to transform a semi-infinite optimization problem into a linear program, enabling efficient problem generation via the fast Fourier transform. This provides a scalable and robust framework for safety verification, applicable to systems with complex dynamics and uncertainties, as demonstrated in case studies involving neural network controllers.",158.05,Phi-4,Nvidia B200 (Cloud Native)
2601.12003v1_Robust Verification of Concurrent Stochastic Games.pdf,Robust Verification of Concurrent Stochastic Games,"['Angel Y. He', 'David Parker']","This paper introduces robust Concurrent Stochastic Games (CSGs) and their subclass, interval CSGs (ICSGs), to address the challenge of epistemic uncertainty in transition probabilities within CSGs. The authors propose a novel framework for robust verification under worst-case assumptions about transition uncertainty. They develop theoretical foundations and efficient algorithms for both finite- and infinite-horizon objectives in zero-sum and nonzero-sum settings, with the latter based on Nash equilibria. An implementation in the PRISM-games model checker demonstrates the feasibility of robust verification of ICSGs across large benchmarks.",158.38,Phi-4,Nvidia B200 (Cloud Native)
2601.12014v1_Are LLMs Ready for TOON Benchmarking Structural Co.pdf,Are LLMs Ready for TOON? Benchmarking Structural Correctness–Sustainability Trade-offs in Novel Structured Output Formats,"['Elio Masciari', 'Vincenzo Moscato', 'Enea Vincenzo Napolitano', 'Gian Marco Orlando', 'Marco Perillo', 'Diego Russo']","This paper introduces a sustainability-aware evaluation framework for assessing structured output formats generated by Large Language Models (LLMs). It proposes the Environment-Aware Generation Correctness Score (GCSenv), which integrates structural correctness with carbon-aware efficiency. The paper benchmarks the novel TOON format against established formats like JSON, XML, and YAML, revealing a trade-off between compactness and structural correctness. The results suggest that increased model capacity can mitigate this trade-off, and that sustainability-inclusive benchmarking is crucial for carbon-conscious LLM deployments.",157.95,Phi-4,Nvidia B200 (Cloud Native)
2601.12019v1_Acting Flatterers via LLMs Sycophancy Combating Cl.pdf,Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs Opposing-Stance Reasoning,"['Chaowei Zhang', 'Xiansheng Luo', 'Zewei Zhang', 'Yi Zhu', 'Jipeng Qiang', 'Longwei Wang']","This paper addresses the issue of clickbait in online content by leveraging the sycophantic tendencies of Large Language Models (LLMs). Instead of eliminating this behavior, the authors propose a novel Self-renewal Opposing-stance Reasoning Generation (SORG) framework. This framework prompts LLMs to generate 'agree' and 'disagree' reasoning pairs for news titles without needing ground-truth labels. The generated reasoning is then used in a local Opposing Reasoning-based Clickbait Detection (ORCD) model, which employs three BERT encoders and contrastive learning with soft labels derived from LLM-generated credibility scores. The method outperforms existing LLM prompting, fine-tuned smaller language models, and state-of-the-art clickbait detection baselines across three benchmark datasets.",157.1,Phi-4,Nvidia B200 (Cloud Native)
2601.12024v1_A Multi-Agent System for Generating Actionable Bus.pdf,A Multi-Agent System for Generating Actionable Business Advice,"['Kartikey Singh Bhandari', 'Tanish Jain', 'Archit Agrawal', 'Dhruv Kumar', 'Praveen Kumar', 'Pratik Narang']","This paper introduces a multi-agent, LLM-based framework designed to transform large-scale customer review corpora into actionable business advice. The framework integrates clustering, advice generation, iterative evaluation, and feasibility-based ranking to produce specific, actionable, and practical outputs. Experiments across various service domains demonstrate that this framework outperforms single model baselines in terms of actionability, specificity, and non-redundancy, with medium-sized models approaching the performance of larger models.",157.59,Phi-4,Nvidia B200 (Cloud Native)
2601.12030v1_ARC Active and Reflection-driven Context Managemen.pdf,Active and Reflection-driven Context Management for Long-Horizon Information Seeking Agents,"['Yilun Yao', 'Shan Huang', 'Elsie Dai', 'Zhewen Tan', 'Zhenyu Duan', 'Shousheng Jia', 'Yanbing Jiang', 'Tong Yang']","The paper introduces ARC, a framework for managing context in long-horizon information-seeking tasks using large language models. It addresses the issue of 'context rot' by treating context as a dynamic internal state, allowing for active monitoring and revision. ARC outperforms passive context compression methods, showing significant improvements in accuracy on benchmarks like BrowseComp-ZH with Qwen2.5-32B-Instruct.",158.03,Phi-4,Nvidia B200 (Cloud Native)
2601.12038v1_Abstract Argumentation with Subargument Relations.pdf,Abstract Argumentation with Subargument Relations,['Beishui Liao'],"This paper explores abstract argumentation frameworks enriched with explicit subargument relations, alongside attack relations. It analyzes how subargument relations interact with attacks and their impact on fundamental semantic properties. The study aims to provide a principled abstraction of structural information, clarifying the role of subarguments in abstract acceptability reasoning. The paper highlights the limitations of existing frameworks that conflate conflict and structure, and proposes treating subargument relations as a primitive component to better capture structural dependencies and improve explainability, modularity, and dynamic updates in argumentation.",158.44,Phi-4,Nvidia B200 (Cloud Native)
2601.12040v1_Partial Reasoning in Language Models Search and Re.pdf,Partial Reasoning in Language Models: Search and Refinement Guided by Uncertainty,"['Murilo da Luz', 'Bruno Brandão', 'Luana Martins', 'Gustavo Oliveira', 'Bryan de Oliveira', 'Luckeciano Melo', 'Telma Soares']","The paper introduces PREGU (Partial Reasoning Guided by Uncertainty), a method that enhances the reasoning capabilities of Large Language Models (LLMs) by monitoring the entropy of the output distribution during autoregressive generation. When entropy exceeds a certain threshold, indicating uncertainty, PREGU performs a localized search in the latent space to refine the reasoning process. This approach, tested on models like LLaMA-3-8B, Mistral-7B, and Qwen2-7B across benchmarks such as GSM8K, GSM-Hard, SVAMP, and StrategyQA, shows performance improvements over traditional Soft Reasoning methods. The study highlights the potential of using entropy as a signal for selective refinement in LLM reasoning tasks.",158.01,Phi-4,Nvidia B200 (Cloud Native)
2601.12042v1_Less Is More -- Until It Breaks Security Pitfalls .pdf,Less Is More — Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models,"['Xiaomei Zhang', 'Zhaoxi Zhang', 'Leo Yu Zhang', 'Yanjun Zhang', 'Guanhong Tao', 'Shirui Pan']","This paper investigates the security implications of visual token compression in Large Vision-Language Models (LVLMs). While compression improves inference efficiency, it significantly degrades model robustness, introducing vulnerabilities that are hidden and difficult to diagnose. The study identifies instability in token importance ranking as the primary cause of this degradation. The authors propose a Compression-Aware Attack (CAA) to exploit these vulnerabilities and introduce Transfer CAA (T-CAA) for black-box settings. Experimental results show persistent security risks across various models and datasets, highlighting a trade-off between efficiency and security.",157.85,Phi-4,Nvidia B200 (Cloud Native)
2601.12049v1_textitFocaLogic Logic-Based Interpretation of Visu.pdf,FocaLogic: Logic-Based Interpretation of Visual Model Decisions,"['Chenchen Zhao', 'Muxi Chen', 'Qiang Xu']","FocaLogic is a novel model-agnostic framework designed to interpret and quantify visual model decision-making through logic-based representations. It identifies minimal interpretable subsets of visual regions, termed 'visual focuses,' that decisively influence model predictions. These visual focuses are translated into precise logical expressions, enabling transparent and structured interpretations. FocaLogic also introduces quantitative metrics such as focus precision, recall, and divergence to objectively evaluate model behavior. Empirical analyses demonstrate its capability to uncover critical insights, including training-induced concentration, focus accuracy through generalization, and anomalous focuses under biases and adversarial attacks. Overall, FocaLogic provides a systematic, scalable, and quantitative solution for interpreting visual models.",158.15,Phi-4,Nvidia B200 (Cloud Native)
2601.12053v1_A New Strategy for Artificial Intelligence Trainin.pdf,A New Strategy for Artificial Intelligence: Training Foundation Models Directly on Human Brain Data,['Maël Donoso'],"This paper explores a novel strategy for artificial intelligence by proposing the training of foundation models directly on human brain data. The author argues that neuroimaging data could provide insights into human cognition that are not accessible through traditional data sources. The paper identifies current limitations of foundation models and suggests leveraging specific brain regions and cognitive processes to address these limitations. Two methods, reinforcement learning from human brain (RLHB) and chain of thought from human brain (CoTHB), are proposed to strategically use neuroimaging data in foundation model training. The paper discusses the potential implications for AI development, including agents, artificial general intelligence, and artificial superintelligence, while also considering ethical, social, and technical challenges. The author suggests that brain-trained foundation models could serve as a middle ground between scaling current architectures and exploring neuroscience-inspired solutions, with future discoveries in neuroscience potentially enhancing this strategy.",157.83,Phi-4,Nvidia B200 (Cloud Native)
2601.12055v1_Automating Parameter Selection in Deep Image Prior.pdf,Automating Parameter Selection in Deep Image Prior for Fluorescence Microscopy Image Denoising via Similarity-Based Parameter Transfer,"['Lina Meyer', 'Felix Wissel', 'Tobias Knopp', 'Susanne Pfefferle', 'Ralf Fliegert', 'Maximilian Sandmann', 'Liana Uebler', 'Franziska Mückl', 'Björn-Philipp Diercks', 'David Lohr', 'René Werner']","This paper introduces AUTO-DIP, a pipeline for automatic parameter transfer in deep image prior (DIP) for fluorescence microscopy image denoising. The study hypothesizes that similar images share optimal parameter configurations for DIP-based denoising, enabling optimization-free DIP application. The authors generated calibration and validation sets from an open-source dataset to assess image similarity criteria and implemented AUTO-DIP. The results show that parameter transfer based on image metadata similarity outperforms quantitative image similarity measures and other denoising approaches, particularly for very noisy inputs. AUTO-DIP enhances DIP-based denoising speed and quality, facilitating routine applications in fluorescence microscopy imaging.",153.5,Phi-4,Nvidia B200 (Cloud Native)
2601.12061v1_Codebook-Injected Dialogue Segmentation for Multi-.pdf,Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs,"['Jinsook Lee', 'Kirk Vanacore', 'Zhuqian Zhou', 'Bakhtawar Ahtisham', 'Jeanine Grütter', 'René F. Kizilcec']","This paper addresses the challenges in Dialogue Act (DA) annotation, particularly the issue of boundary-driven disagreement among annotators. The authors propose a method called codebook-injected segmentation, which conditions boundary decisions on downstream annotation criteria. They evaluate LLM-based segmenters against standard and retrieval-augmented baselines using new evaluation metrics for span consistency, distinctiveness, and human-AI distributional agreement. The study finds that DA-awareness improves internal consistency of segments compared to text-only baselines, though coherence-based baselines are better at detecting global shifts in dialogue flow. The results suggest that segmentation should be optimized for downstream objectives rather than a single performance score.",154.32,Phi-4,Nvidia B200 (Cloud Native)
2601.12068v1_Bridging the Gap in Bangla Healthcare Machine Lear.pdf,Bridging the Gap in Bangla Healthcare: Machine Learning Based Disease Prediction Using a Symptoms-Disease Dataset,"['Rowzatul Zannat', 'Abdullah Al Shafi', 'Abdul Muntakim']","This study addresses the limited resources for disease prediction in Bangla by developing a comprehensive Bangla symptoms-disease dataset containing 758 unique symptom-disease relationships spanning 85 diseases. The dataset, made publicly available, supports disease prediction based on Bangla symptom inputs, enhancing healthcare accessibility for Bengali-speaking populations. Multiple machine learning models were evaluated using this dataset, with both soft and hard voting ensemble approaches achieving 98% accuracy. This work establishes a foundational resource for disease prediction in Bangla, promoting equitable access to health information for Bangla-speaking communities, particularly for early disease detection and healthcare interventions.",153.25,Phi-4,Nvidia B200 (Cloud Native)
2601.12082v1_Conditional Random Fields for Interactive Refineme.pdf,CONDITIONAL RANDOM FIELDS FOR INTERACTIVE REFINEMENT OF HISTOPATHOLOGICAL PREDICTIONS,"['Tiffanie Godelaine', 'Maxime Zanella', 'Karim El Khoury', 'Saïd Mahmoudi', 'Benoît Macq', 'Christophe De Vleeschouwer']","This paper introduces HistoCRF, a framework that refines zero-shot predictions from Vision-Language Models (VLMs) in histopathological image analysis using Conditional Random Fields (CRFs). The framework does not require additional model training and leverages expert annotations to enhance prediction accuracy. Experiments demonstrate significant accuracy improvements in patch-level classification tasks across various datasets, with notable gains achieved through human-in-the-loop annotations. The approach emphasizes label diversity and consistency across predictions, providing a valuable tool for assisting pathologists in cancer detection and staging.",157.49,Phi-4,Nvidia B200 (Cloud Native)
2601.12095v1_Neural Isomorphic Fields A Transformer-based Algeb.pdf,Neural Isomorphic Fields: A Transformer-Based Algebraic Numeric Embedding,"['Hamidreza Sadeghi', 'Saeedeh Momtazi', 'Reza Safabakhsh']","This paper addresses the challenges neural network models face when processing very small or very large numbers, such as overflow, underflow, and unstable output variations. The authors propose using embedding vectors for numbers to retain essential algebraic properties while preventing numerical instabilities. They introduce a novel concept called Neural Isomorphic Fields (NIF), which are fixed-length number embedding vectors that preserve algebraic operations—addition, multiplication, and comparison—within the rational numbers field. The experiments show that addition performs exceptionally well, achieving over 95% accuracy on key algebraic tests, while multiplication shows lower accuracy, indicating areas for further improvement. The paper highlights the potential of embedding spaces in preserving the algebraic properties of numbers, inspired by advances in deep learning and embedding techniques.",154.61,Phi-4,Nvidia B200 (Cloud Native)
2601.12099v1_Large language models struggle with ethnographic t.pdf,Large Language Models Struggle with Ethnographic Text Annotation,"['Leonardo S. Goodall', 'Dor Shilton', 'Daniel Austin Mullins', 'Harvey Whitehouse']","This paper evaluates the performance of seven state-of-the-art large language models (LLMs) in annotating 121 ritual features across 567 ethnographic excerpts. The study finds that LLMs struggle with reliable automated annotation, particularly with longer texts, features requiring ordinal distinctions, and ambiguous constructs. Human inter-coder reliability sets a ceiling on LLM accuracy, indicating that even for features where human coders agree, LLMs fall short. The findings suggest that LLMs cannot yet replace human expertise in ethnographic annotation, highlighting the methodological and epistemological challenges in cross-cultural anthropology.",152.84,Phi-4,Nvidia B200 (Cloud Native)
2601.12104v1_Powerful Training-Free Membership Inference Agains.pdf,Powerful Training-Free Membership Inference Against Autoregressive Language Models,"['David Ilić', 'David Stanojević', 'Kostadin Cvejoski']","This paper introduces EZ-MIA, a novel membership inference attack that leverages the observation that memorization in language models is most evident at error positions. The Error Zone (EZ) score, a key component of EZ-MIA, measures the probability shifts at these error positions relative to a pretrained reference model. This method requires only two forward passes per query and no additional model training. EZ-MIA significantly outperforms existing methods, achieving higher detection rates with lower false-positive rates, as demonstrated on datasets like WikiText with GPT-2 and AG News with Llama-2-7B. The findings highlight the substantial privacy risks associated with fine-tuned language models, emphasizing the need for improved privacy auditing and deployment strategies.",154.1,Phi-4,Nvidia B200 (Cloud Native)
2601.12124v1_SynQP A Framework and Metrics for Evaluating the Q.pdf,SYNQP: A FRAMEWORK AND METRICS FOR EVALUATING THE QUALITY AND PRIVACY RISK OF SYNTHETIC DATA,"['Bing Hu', 'Yixin Li', 'Asma Bahamyirou', 'Helen Chen']","The paper introduces SYNQP, an open framework for benchmarking privacy in synthetic data generation (SDG) using simulated sensitive data. It addresses the challenge of evaluating privacy risks due to the lack of accessible benchmark datasets. The framework uses simulated data to ensure confidentiality of original data. The authors propose a new identity disclosure risk metric, demonstrating its application with CTGAN and comparing it to existing methods. The study shows that differential privacy (DP) reduces identity disclosure and membership-inference attack risks, with all DP-augmented models staying below a regulatory threshold. This work aims to enhance the transparency and reliability of privacy evaluations, facilitating safer use of synthetic data in health applications.",154.44,Phi-4,Nvidia B200 (Cloud Native)
2601.12126v1_UniMo Unified Motion Generation and Understanding .pdf,UniMo: Unified Motion Generation and Understanding with Chain of Thought,"['Guocun Wang', 'Kenkun Liu', 'Jing Lin', 'Guorui Song', 'Jian Li', 'Xiaoguang Han']","UniMo is a novel framework designed to enhance the interpretability and coherence of 3D human motion generation and understanding. It integrates motion-language information and interpretable chain of thought reasoning into large language models through supervised fine-tuning. Additionally, it employs reinforcement learning with Group Relative Policy Optimization to optimize token groups, ensuring structural correctness and semantic alignment. This approach addresses the limitations of existing methods, such as semantic misalignment and cumulative prediction errors, resulting in superior performance in both motion generation and understanding tasks.",153.77,Phi-4,Nvidia B200 (Cloud Native)
2601.12132v1_Bengali Text Classification An Evaluation of Large.pdf,Bengali Text Classification: An Evaluation of Large Language Model Approaches,"['Md Mahmudul Hoque', 'Md Mehedi Hassain', 'Md Hojaifa Tanvir', 'Rahul Nandy']","This study evaluates the effectiveness of large language models (LLMs) in Bengali text classification, focusing on newspaper articles from Prothom Alo. The research compares three instruction-tuned LLMs: LLaMA 3.1 8B Instruct, LLaMA 3.2 3B Instruct, and Qwen 2.5 7B Instruct. Qwen 2.5 achieved the highest accuracy of 72%, particularly excelling in the 'Sports' category, while LLaMA 3.1 and LLaMA 3.2 achieved 53% and 56%, respectively. The findings underscore the potential of LLMs in Bengali NLP despite limited resources, with future research aimed at exploring more models, addressing class imbalance, and refining fine-tuning methods.",154.4,Phi-4,Nvidia B200 (Cloud Native)
2601.12134v1_Human-Human-AI Triadic Programming Uncovering the .pdf,Human-Human-AI Triadic Programming: Uncovering the Role of AI Agent and the Value of Human Partner in Collaborative Learning,"['Taufiq Daryanto', 'Xiaohan Ding', 'Kaike Ping', 'Lance T. Wilhelm', 'Yan Chen', 'Chris Brown', 'Eugenia H. Rho']","This paper explores the concept of human-human-AI (HHAI) triadic programming, where two humans collaborate with an AI agent. The study investigates whether AI can augment human-human pair programming while maintaining the social and pedagogical benefits of collaboration. It compares this triadic approach to human-AI pairs and examines design considerations, such as whether AI should act as a shared collaborator or personal support. A within-subjects study with 20 participants is conducted to evaluate the effectiveness of this approach.",154.66,Phi-4,Nvidia B200 (Cloud Native)
2601.12138v1_DriveSafe A Hierarchical Risk Taxonomy for Safety-.pdf,DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants,"['Abhishek Kumar', 'Riya Tapwal', 'Carsten Maple']","This paper introduces DriveSafe, a hierarchical risk taxonomy specifically designed for LLM-based driving assistants. It addresses the limitations of general-purpose safety frameworks by providing a domain-specific taxonomy with 129 fine-grained risk categories. These categories cover technical, legal, societal, and ethical dimensions, grounded in real-world driving regulations and safety principles. The paper evaluates the refusal behavior of six widely deployed LLMs, revealing their frequent failure to appropriately refuse unsafe or non-compliant driving-related queries. This underscores the need for specialized safety alignment in driving contexts.",153.38,Phi-4,Nvidia B200 (Cloud Native)
2601.12141v1_TIDE A Trace-Informed Depth-First Exploration for .pdf,TIDE: A Trace-Informed Depth-First Exploration for Planning with Temporally Extended Goals,"['Yuliia Suprun', 'Khen Elimelech', 'Lydia E. Kavraki', 'Moshe Y. Vardi']","The paper introduces TIDE (Trace-Informed Depth-first Exploration), a novel approach for task planning with temporally extended goals (TEGs) using Linear Temporal Logic on finite traces (LTLf). Traditional methods often lack informed heuristics for guiding the search for temporal goals. TIDE addresses this by decomposing the problem into smaller reach-avoid subproblems, using cost-driven heuristics to prioritize promising automaton traces. It features an adaptive backtracking mechanism to recover from failed plans, ensuring completeness and efficiency. Experimental results show TIDE's promising performance, making it a valuable addition to planning methods for TEGs.",154.55,Phi-4,Nvidia B200 (Cloud Native)
2601.12147v1_Segment and Matte Anything in a Unified Model.pdf,Segment and Matte Anything in a Unified Model,"['Zezhong Fan', 'Xiaohan Li', 'Topojoy Biswas', 'Kaushiki Nag', 'Kannan Achan']","This paper introduces Segment And Matte Anything (SAMA), a lightweight extension of the Segment Anything Model (SAM) that enhances interactive image segmentation and matting. SAMA incorporates a Multi-View Localization Encoder (MVLE) and a Localization Adapter (Local-Adapter) to improve mask prediction accuracy and boundary detail recovery. It features dual prediction heads for segmentation and matting tasks, achieving state-of-the-art performance on various benchmarks. The model is trained on a diverse dataset, demonstrating its adaptability and effectiveness in numerous applications.",153.88,Phi-4,Nvidia B200 (Cloud Native)
2601.12150v1_Enhanced Diagnostic Performance via Large-Resoluti.pdf,Enhanced Diagnostic Performance via Large-Resolution Inference Optimization for Pathology Foundation Models,"['Mengxuan Hu', 'Zihan Guan', 'John Kang', 'Sheng Li', 'Zhongliang Zhou']","This paper addresses the inefficiencies in applying pathology foundation models to whole-slide images (WSIs) due to their constraint to specific input sizes, such as 224×224. The authors propose a space- and time-efficient inference strategy that sparsifies attention using spatially aware neighboring blocks and filters out non-informative tokens through global attention scores. This approach reduces GPU memory and runtime during high-resolution WSI inference while preserving or improving downstream performance. The method enables inference at higher resolutions under the same GPU budget, achieving up to a 7.67% improvement in ROI classification and compatible results in segmentation.",155.58,Phi-4,Nvidia B200 (Cloud Native)
2601.12186v1_Aletheia What Makes RLVR For Code Verifiers Tick.pdf,Aletheia: What Makes RLVR For Code Verifiers Tick?,"['Vatsal Venkatkrishna', 'Indraneil Paul', 'Iryna Gurevych']","This paper explores the use of Reinforcement Learning from Verifiable Rewards (RLVR) for training multi-domain thinking verifiers, particularly in the context of code generation. While execution feedback is commonly used, the authors propose using code verifiers to evaluate model outputs when execution feedback is unavailable. They introduce Aletheia, a testbed for evaluating the robustness of code verifiers across different policy models and shifts. The study examines key components of RLVR-based verifier training, such as intermediate thinking traces, learning from negative samples, and on-policy training. Findings suggest that while RLVR is optimal, simplifications are possible, with on-policy learning being crucial for small verifier sizes and thinking-based training for larger scales. The paper highlights the potential of code verifiers to enhance code generation post-training pipelines by leveraging the capabilities of Large Language Models without requiring code execution.",156.9,Phi-4,Nvidia B200 (Cloud Native)
2601.12205v1_Do Neural Codecs Generalize A Controlled Study Acr.pdf,Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks,"['Shih-Heng Wang', 'Jiatong Shi', 'Jinchuan Tian', 'Haibin Wu', 'Shinji Watanabe']","This paper explores the generalization capabilities of Neural Audio Codecs (NACs) by addressing three key questions: (i) Can NACs generalize to unseen languages during pre-training? (ii) Can speech-only pre-trained NACs effectively generalize to non-speech applications? (iii) Does incorporating non-speech pre-training data enhance performance on both speech and non-speech tasks? The study involves training NACs from scratch with controlled configurations and curated pre-trained data to ensure fair comparisons. The performance is evaluated using 11 metrics, revealing that NACs can generalize to unseen languages, speech-only pre-trained NACs underperform on non-speech tasks, and including non-speech data during pre-training improves non-speech task performance while maintaining speech task performance.",157.47,Phi-4,Nvidia B200 (Cloud Native)
2601.12212v1_Speculative Sampling with Reinforcement Learning.pdf,Speculative Sampling with Reinforcement Learning,"['Chenan Wang', 'Daniel H. Shi', 'Haipeng Chen']","This paper addresses the challenge of inference time latency in large language models (LLMs) by introducing Reinforcement Learning for Speculative Sampling (Re-SpS). Re-SpS dynamically optimizes draft tree hyperparameters in real-time, enhancing generation speed by balancing speculative aggression with computational overhead. The method leverages efficient state representations from target model hidden states and introduces multi-step action persistence for better context modeling. Evaluation across five benchmarks shows significant speed improvements over the state-of-the-art method EAGLE-3, achieving up to 5.45× speedup over the backbone LLM and 1.12× speedup compared to EAGLE-3, without compromising output fidelity.",157.84,Phi-4,Nvidia B200 (Cloud Native)
2601.12215v1_Wavelet-Driven Masked Multiscale Reconstruction fo.pdf,Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation Models,"['Megha Thukral', 'Cyrus Tanade', 'Simon A. Lee', 'Juhyeon Lee', 'Hao Zhou', 'Keum San Chun', 'Migyeong Gwak', 'Viswam Nathan', 'Md Mahbubur Rahman', 'Li Zhu', 'Mehrab Bin Morshed', 'Subramaniam Venkatraman', 'Sharanya Arcot Desai']","This paper introduces a novel self-supervised pretraining framework called Masked Multiscale Reconstruction (MMR) for photoplethysmography (PPG) signals. The framework leverages wavelet-based multiresolution decomposition to reconstruct masked coefficients, enabling the model to learn hierarchical time–frequency scales. Pretrained on approximately 17 million unlabeled 10-second PPG segments from around 32,000 smartwatch users, the MMR model demonstrates superior or comparable performance on 17 out of 19 diverse health-related tasks compared to existing PPG foundation models and other self-supervised baselines. The study highlights the robustness and physiological relevance of wavelet-based representations, suggesting their potential in developing generalizable PPG foundation models.",157.95,Phi-4,Nvidia B200 (Cloud Native)
2601.12224v1_Where It Moves It Matters Referring Surgical Instr.pdf,"Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion","['Meng Wei', 'Kun Yuan', 'Shi Li', 'Yue Zhou', 'Long Bai', 'Nassir Navab', 'Hongliang Ren', 'Hong Joo Lee', 'Tom Vercauteren', 'Nicolas Padoy']","This paper introduces SurgRef, a novel motion-guided framework for referring segmentation of surgical instruments in videos, leveraging motion rather than static visual cues. The framework addresses challenges such as occlusion and unfamiliar terminology by grounding language expressions in instrument motion. The authors present Ref-IMotion, a diverse dataset for training and evaluating SurgRef, achieving state-of-the-art accuracy and generalization across various surgical procedures. This work advances the field of surgical video segmentation by providing robust, language-driven capabilities.",155.71,Phi-4,Nvidia B200 (Cloud Native)
2601.12234v1_Proc3D Procedural 3D Generation and Parametric Edi.pdf,Proc3D: Procedural 3D Generation and Parametric Editing of 3D Shapes with Large Language Models,"['Fadlullah Raji', 'Stefano Petrangeli', 'Matheus Gadelha', 'Yu Shen', 'Uttaran Bhattacharya', 'Gang Wu']","Proc3D introduces a system for generating editable 3D models using a procedural compact graph (PCG) representation. This approach allows for real-time modifications through intuitive manual controls and text-based editing with Large Language Models (LLMs). The system demonstrates significant improvements in editing efficiency and alignment with text prompts, outperforming existing methods by achieving a speedup of over 400× and improving ULIP scores by 28%.",155.79,Phi-4,Nvidia B200 (Cloud Native)
2601.12242v1_Optimal Power Allocation and Sub-Optimal Channel A.pdf,Optimal Power Allocation and Sub-Optimal Channel Assignment for Downlink NOMA System Using Deep Reinforcement Learning,"['WooSeok Kim', 'Jeonghoon Lee', 'Sangho Kim', 'Taesun An', 'WonMin Lee', 'Dowon Kim', 'Kyungseop Shin']","This paper addresses the challenges in Non-Orthogonal Multiple Access (NOMA) systems, particularly focusing on power allocation and channel assignment. The authors propose a deep reinforcement learning framework that incorporates replay memory with an on-policy algorithm to optimize network resource allocation in NOMA systems. The study includes extensive simulations to evaluate the impact of various parameters such as learning rate, batch size, model type, and state features. The goal is to enhance the utilization of network resources amidst the growing demands of the Internet of Things (IoT).",157.92,Phi-4,Nvidia B200 (Cloud Native)
2601.12243v1_Less is More Label-Guided Summarization of Procedu.pdf,Less is More: Label-Guided Summarization of Procedural and Instructional Videos,"['Shreya Rajpal', 'Michal Golovanesky', 'Carsten Eickhoff']","The paper introduces PRISM, a three-stage framework for video summarization that focuses on procedural and instructional videos. PRISM integrates adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model to produce semantically grounded summaries. The method ensures meaningful and procedural transitions in selected frames, filtering out generic content. Evaluated on instructional and activity datasets, PRISM retains 84% semantic content while sampling fewer than 5% of original frames, outperforming baselines by up to 33%. The approach generalizes across procedural and domain-specific video tasks, achieving strong performance in semantic alignment and precision.",157.91,Phi-4,Nvidia B200 (Cloud Native)
2601.12247v1_Plan Verify and Fill A Structured Parallel Decodin.pdf,"Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion Language Models","['Miao Li', 'Hanyang Jiang', 'Sikai Cheng', 'Hengyu Fu', 'Yuhang Cai', 'Baihe Huang', 'Tinghan Ye', 'Xuanzhou Chen', 'Pascal Van Hentenryck']","This paper introduces the Plan-Verify-Fill (PVF) approach for Diffusion Language Models (DLMs), which aims to improve text generation efficiency by utilizing a global bidirectional context. Unlike current reactive decoding strategies, PVF constructs a hierarchical skeleton prioritizing semantic anchors and employs a verification protocol to optimize structural stopping. The approach significantly reduces the Number of Function Evaluations (NFE) by up to 65% without compromising accuracy, as demonstrated on benchmark datasets like LLaDA-8B-Instruct and Dream-7B-Instruct.",157.44,Phi-4,Nvidia B200 (Cloud Native)
2601.12248v1_AQUA-Bench Beyond Finding Answers to Knowing When .pdf,AQUA-BENCH: BEYOND FINDING ANSWERS TO KNOWING WHEN THERE ARE NONE IN AUDIO QUESTION ANSWERING,"['Chun-Yi Kuan', 'Hung-yi Lee']","Recent advances in audio-aware large language models have shown strong performance on audio question answering. However, existing benchmarks mainly cover answerable questions and overlook the challenge of unanswerable ones, where no reliable answer can be inferred from the audio. To address this gap, the authors present AQUA-Bench, a benchmark for Audio Question Unanswerability Assessment. It evaluates three scenarios: Absent Answer Detection, Incompatible Answer Set Detection, and Incompatible Audio Question Detection. AQUA-Bench aims to measure model reliability and promote the development of more robust audio-language systems. Experiments suggest that while models excel on standard answerable tasks, they face challenges with unanswerable ones, indicating a blind spot in current audio-language understanding.",157.97,Phi-4,Nvidia B200 (Cloud Native)
2601.12249v1_An Innovative Framework for Breast Cancer Detectio.pdf,"An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion","['Ehsan Sadeghi Pour', 'Mahdi Esmaeili', 'Morteza Romoozi']","This paper presents an innovative framework for detecting malignant masses in mammographic images by integrating Pyramid Adaptive Atrous Convolution (PAAC) and Transformer architectures. The approach utilizes Multi-Scale Feature Fusion to enhance feature extraction and combines Dice Loss and Focal Loss functions to improve learning and reduce errors in binary breast cancer classification. The model was trained on a comprehensive dataset from INbreast, MIAS, and DDSM, achieving high accuracy, sensitivity, specificity, F1-score, and precision. It outperforms foundational models and demonstrates potential as a reliable tool for breast cancer diagnosis.",158.27,Phi-4,Nvidia B200 (Cloud Native)
2601.12256v1_Improving Large Molecular Language Model via Relat.pdf,Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration,"['Jinyoung Park', 'Minseong Bae', 'Jeehye Na', 'Hyunwoo J. Kim']","This paper introduces CoLLaMo, a large language model-based molecular assistant designed to enhance the integration of diverse molecular modalities, including 1D sequences, 2D graphs, and 3D conformations. The proposed relation-aware modality-collaborative attention mechanism facilitates fine-grained information exchange between atoms, addressing the hallucination and robustness issues in existing large molecular language models (LMLMs). The paper also presents new evaluation metrics for molecular comprehension, demonstrating CoLLaMo's superior performance across various tasks such as molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction.",157.86,Phi-4,Nvidia B200 (Cloud Native)
2601.12257v1_Soft Shadow Diffusion SSD Physics-inspired Learnin.pdf,Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy,"['Fadlullah Raji', 'John Murray Bruce']","This paper addresses the challenge of non-line-of-sight (NLOS) imaging, which reconstructs scenes from indirect measurements when a direct line of sight is impractical or impossible. The authors propose a novel approach called Soft Shadow Diffusion (SSD) for 3D reconstruction of hidden scenes from ordinary NLOS photographs. By reformulating the light transport model, they decompose the scene into light-occluding and non-light-occluding components, resulting in a separable non-linear least squares (SNLLS) inverse problem. Two solutions are developed: a gradient-based optimization method and a physics-inspired neural network approach. The SSD method demonstrates effectiveness in real experimental scenarios and generalizes well to unseen classes in both simulation and real-world NLOS scenes, showing robustness to noise and ambient illumination.",158.07,Phi-4,Nvidia B200 (Cloud Native)
2601.12259v1_FutureX-Pro Extending Future Prediction to High-Va.pdf,FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains,"['ByteDance Seed', 'Hong Kong University of Science and Technology', 'Georgia Institute of Technology', 'Stanford University', 'Princeton University']","FutureX-Pro builds upon the FutureX benchmark for general-purpose future prediction by introducing specialized frameworks for high-value vertical domains: Finance, Retail, Public Health, and Natural Disaster. It evaluates the performance of agentic Large Language Models (LLMs) on foundational prediction tasks within these domains, assessing their readiness for industrial deployment. The study highlights the performance gap between generalist reasoning and the precision required for high-stakes applications, emphasizing the need for domain-specific grounding in LLMs.",158.26,Phi-4,Nvidia B200 (Cloud Native)
2601.12260v1_Docs2Synth A Synthetic Data Trained Retriever Fram.pdf,Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding,"['Yihao Ding', 'Qiang Sun', 'Puzhen Wu', 'Sirui Li', 'Siwen Luo', 'Wei Liu']","The paper introduces Docs2Synth, a synthetic-supervision framework designed to enhance the understanding of scanned visually rich documents in regulated domains. It addresses challenges such as the lack of manual annotations and the difficulty for pretrained models to stay updated with domain-specific facts. Docs2Synth leverages a synthetic data approach to generate and verify diverse question-answer pairs, training a lightweight visual retriever to extract domain-relevant evidence. This retriever collaborates with Multimodal Large Language Models (MLLMs) through an iterative retrieval-generation loop, reducing hallucination and improving response consistency. The framework is presented as an easy-to-use Python package, demonstrating significant improvements in grounding and domain generalization across various benchmarks without requiring human annotations.",156.89,Phi-4,Nvidia B200 (Cloud Native)
2601.12263v1_Multimodal Generative Engine Optimization Rank Man.pdf,Multimodal Generative Engine Optimization: Rank Manipulation for Vision–Language Model Rankers,"['Yixuan Du', 'Chenxiao Yu', 'Haoyan Xu', 'Ziyi Wang', 'Yue Zhao', 'Xiyang Hu']","This paper investigates a critical vulnerability in Vision-Language Models (VLMs) used in modern retrieval and recommendation systems. The authors introduce a novel adversarial framework called Multimodal Generative Engine Optimization (MGEO), which allows a malicious actor to unfairly promote a target product by optimizing both image perturbations and textual suffixes. Unlike previous attacks that treated modalities separately, MGEO uses an alternating gradient-based optimization strategy to exploit the deep cross-modal coupling within VLMs. Experiments on real-world datasets demonstrate that this coordinated attack significantly outperforms text-only and image-only baselines, highlighting the potential for multimodal synergy to be weaponized against the integrity of search rankings.",156.03,Phi-4,Nvidia B200 (Cloud Native)
2601.12269v1_Simulated Annealing Enhances Theory-of-Mind Reason.pdf,Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models,"['Xucong Hu', 'Jian-Qiao Zhu']","This paper addresses the limitations of autoregressive language models in maintaining globally coherent latent-state representations, particularly in Theory of Mind (ToM) tasks. The authors propose a method that enhances ToM performance by using Markov chain Monte Carlo (MCMC) power sampling techniques, specifically incorporating simulated annealing. This approach allows for sampling from sequence-level probability distributions, improving the model's ability to reason about latent mental states without additional training. The results suggest that sampling-based optimization can effectively extract latent capabilities from language models.",157.85,Phi-4,Nvidia B200 (Cloud Native)
2601.12276v1_Predictive Prototyping Evaluating Design Concepts .pdf,PREDICTIVE PROTOTYPING: EVALUATING DESIGN CONCEPTS WITH GPT,['Hilsann Yong'],"This paper explores the potential of using generative pretrained transformers (GPTs), specifically OpenAI’s GPT-4o, to predict outcomes typically gained through prototyping, such as cost, performance, and usability. The study introduces a novel approach using retrieval augmented generation (RAG) to emulate design feedback. It leverages prototyping data from 'Instructables.com' to enhance model relevance. Two main efforts are reported: a controlled study comparing GPT predictions with human designers against physical prototyping results, and an experimental application where a prototype based on GPT-RAG recommendations outperformed commercial and topology optimized models. The study finds that GPT-RAG predictions are more accurate than human estimates for cost and performance, and that averaging multiple GPT-RAG responses improves accuracy, demonstrating the emulation of crowd behavior.",157.98,Phi-4,Nvidia B200 (Cloud Native)
2601.12282v1_CytoCLIP Learning Cytoarchitectural Characteristic.pdf,CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training,"['Pralaypati Ta', 'Sriram Venkatesaperumal', 'Keerthi Ram', 'Mohanasankar Sivaprakasam']","The paper introduces CytoCLIP, a novel approach leveraging vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint visual-text representations of brain cytoarchitecture. CytoCLIP includes two model variants: one trained on low-resolution whole-region images to capture overall cytoarchitectural patterns, and another on high-resolution image tiles for detailed cellular-level representation. The training dataset comprises NISSL-stained histological sections of developing fetal brains, featuring 86 distinct regions for low-resolution images and 384 regions for high-resolution tiles. The model's performance is evaluated through region classification and cross-modal retrieval tasks, demonstrating superior performance with an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification, outperforming existing methods.",157.3,Phi-4,Nvidia B200 (Cloud Native)
2601.12286v1_Conversational Context Classification A Representa.pdf,Conversational Context Classification: A Representation Engineering Approach,['Jonathan Pan'],"This paper explores the use of Representation Engineering (RepE) and One-Class Support Vector Machine (OCSVM) to detect when Large Language Models (LLMs) generate out-of-context responses. By training OCSVM on in-context examples, the study establishes a boundary within the LLM’s hidden state latent space to identify specific context subspaces. The approach was evaluated using Llama and Qwen models, showing promising results in detecting context-specific subspaces. This research contributes to AI safety by improving the detection of in or out-of-context conversation threads and aids in better interpreting LLMs.",157.97,Phi-4,Nvidia B200 (Cloud Native)
2601.12288v1_TimeGMM Single-Pass Probabilistic Forecasting via .pdf,TIMEGMM: SINGLE-PASS PROBABILISTIC FORECASTING VIA ADAPTIVE GAUSSIAN MIXTURE MODELS WITH REVERSIBLE NORMALIZATION,"['Lei Liu', 'Tengyuan Liu', 'Hongwei Zhao', 'Jiahui Huang', 'Ruibo Guo', 'Bin Li']","This paper introduces TimeGMM, a novel probabilistic forecasting framework based on Gaussian Mixture Models (GMM) designed to capture complex future distributions in a single forward pass. A key innovation is the GMM-adapted Reversible Instance Normalization (GRIN) module, which dynamically adapts to temporal-probabilistic distribution shifts. The framework combines a Temporal Encoder (TE-Module) with a Conditional Temporal-Probabilistic Decoder (CTPD-Module) to jointly capture temporal dependencies and mixture distribution parameters. Extensive experiments show that TimeGMM outperforms state-of-the-art methods, achieving significant improvements in CRPS and NMAE metrics.",157.68,Phi-4,Nvidia B200 (Cloud Native)
2601.12294v1_ToolPRMBench Evaluating and Advancing Process Rewa.pdf,ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents,"['Dawei Li', 'Yuguang Yao', 'Zhen Tan', 'Huan Liu', 'Ruocheng Guo']","This paper introduces ToolPRMBench, a large-scale benchmark designed to evaluate process reward models (PRMs) for tool-using agents. It addresses the lack of systematic evaluation benchmarks for PRMs in tool-using settings by converting agent trajectories into step-level test cases. The benchmark includes offline and online sampling methods to capture single-step errors and multi-step failures, respectively. A multi-LLM verification pipeline is proposed to ensure data quality. Extensive experiments demonstrate the effectiveness of specialized PRMs for tool-using agents, highlighting their potential over general PRMs.",156.43,Phi-4,Nvidia B200 (Cloud Native)
2601.12304v1_A Two-Stage Globally-Diverse Adversarial Attack fo.pdf,A TWO-STAGE GLOBALLY-DIVERSE ADVERSARIAL ATTACK FOR VISION-LANGUAGE PRE-TRAINING MODELS,"['Wutao Chen', 'Huaqin Zou', 'Chen Wan', 'Lifeng Huang']","This paper introduces a two-stage globally-diverse attack framework, 2S-GDA, designed to enhance the adversarial attack success rates on vision-language pre-training (VLP) models, particularly in black-box scenarios. The framework addresses the limitations of existing multimodal attacks by introducing textual perturbations through a globally-diverse strategy and enhancing visual diversity with multi-scale resizing and block-shuffle rotation. Extensive experiments demonstrate that 2S-GDA significantly improves attack success rates over state-of-the-art methods, with gains of up to 11.17% in black-box settings. The modular nature of the framework allows for easy integration with existing methods to further enhance adversarial transferability.",157.34,Phi-4,Nvidia B200 (Cloud Native)
2601.12310v1_Survival is the Only Reward Sustainable Self-Train.pdf,Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection,"['Jennifer Dodgson', 'Alfath Daryl Alhajir', 'Michael Joedhitya', 'Akira Rafhael Janson Pattirane', 'Surender Suresh Kumar', 'Joseph Lim', 'C.H. Peh', 'Adith Ramdas', 'Steven Zhang Zhexu']","This paper introduces a self-training architecture where learning is mediated by environmental viability rather than rewards or external criteria. It demonstrates that candidate behaviors are selected based on their ability to persist and allow future interactions, without semantic feedback or dense rewards. The study shows that improvement arises through persistence of effective strategies under a regime of consolidation and pruning, termed negative-space learning (NSL). The work establishes that environment-grounded selection enables sustainable self-improvement, paving the way for robust autonomous systems without human-curated data or complex reward shaping.",158.12,Phi-4,Nvidia B200 (Cloud Native)
2601.12316v1_GazeFormer-MoE Context-Aware Gaze Estimation via C.pdf,GAZEFORMER-MOE: CONTEXT-AWARE GAZE ESTIMATION VIA CLIP AND MOE TRANSFORMER,"['Xinyuan Zhao', 'Xianrui Chen', 'Ahmad Chaddad']","The paper introduces GazeFormer-MoE, a novel framework for 3D gaze estimation that leverages CLIP global features conditioned with learnable prototype banks (illumination, head pose, background, direction). It integrates these enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space, replacing several feed-forward network (FFN) blocks with routed/shared Mixture of Experts (MoE) to enhance conditional capacity. The model achieves state-of-the-art angular errors on multiple datasets, demonstrating significant improvements over previous methods. The contributions include a semantics-modulated multi-scale pipeline and a routed/shared MoE Transformer architecture, enhancing robustness against variations in illumination, pose, and background.",157.77,Phi-4,Nvidia B200 (Cloud Native)
2601.12317v1_Explanova Automatically Discover Data Insights in .pdf,Explanova: Automatically Discover Data Insights in N×M Table via XAI Combined LLM Workflow,['Yiming Huang'],"Explanova is a framework designed to automate data analysis by leveraging a preset AutoML-like workflow combined with explainable AI (XAI) and local small language models (LLMs). It aims to enhance LLM-based data science by focusing on data analytics rather than improving LLMs themselves. The framework explores various analytical tasks such as single-feature statistics, feature-to-feature relationships, and feature modeling, making data analysis automation feasible even with limited GPU resources.",157.95,Phi-4,Nvidia B200 (Cloud Native)
2601.12318v1_Beyond Human Annotation Recent Advances in Data Ge.pdf,Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence,"['Dehao Ying', 'Fengchang Yu', 'Haihua Chen', 'Changjiang Jiang', 'Yurong Li', 'Wei Lu']","This survey addresses the bottleneck of manual annotation in Document Intelligence (DI) by establishing a comprehensive technical map for data generation. It introduces a novel taxonomy based on the availability of data and labels, organizing methodologies into four paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction. A multi-level evaluation framework is proposed to assess intrinsic quality and extrinsic utility, compiling performance gains across DI benchmarks. The survey highlights critical challenges such as fidelity gaps and explores frontiers like co-evolutionary ecosystems, positioning data generation as central to next-generation DI.",157.61,Phi-4,Nvidia B200 (Cloud Native)
2601.12323v1_MARO Learning Stronger Reasoning from Social Inter.pdf,MARO: Learning Stronger Reasoning from Social Interaction,"['Yin Cai', 'Zhouhong Gu', 'JunTao Zhang', 'Ping Chen']","This paper introduces Multi-Agent Reward Optimization (MARO), a method designed to enhance the reasoning abilities of large language models (LLMs) by training them in multi-agent social environments. MARO addresses challenges such as sparse learning signals, uneven role distribution, and environmental instability by decomposing outcomes into specific behaviors, balancing training sample weights, and evaluating the utility of each behavior. The method demonstrates significant improvements in social reasoning capabilities and effective transferability to other tasks like mathematical reasoning and instruction following, highlighting the potential of multi-agent social learning in advancing LLMs' general reasoning skills.",157.43,Phi-4,Nvidia B200 (Cloud Native)
2601.12327v1_The Expert Validation Framework EVF Enabling Domai.pdf,The Expert Validation Framework (EVF): Enabling Domain Expert Control in AI Engineering,"['Lucas Gren', 'Felix Dobslaw']","The paper presents an Expert Validation Framework (EVF) designed to integrate domain experts into the development of software with Generative AI (GenAI) components. The framework aims to ensure quality and maintain expert control over system behavior through structured processes of specification, testing, validation, and continuous monitoring. It addresses the gap between AI capabilities and organizational trust by providing a rigorous, expert-driven methodology for quality assurance in diverse GenAI applications. The framework's four-stage implementation process includes specification, system creation, validation, and production monitoring, enabling organizations to leverage GenAI while maintaining oversight and quality standards.",157.34,Phi-4,Nvidia B200 (Cloud Native)
2601.12330v1_IceWatch Forecasting Glacial Lake Outburst Floods .pdf,IceWatch: Forecasting Glacial Lake Outburst Floods (GLOFs) using Multimodal Deep Learning,"['Zuha Fatima', 'Muhammad Anser Sohaib', 'Muhammad Talha', 'Ayesha Kanwal', 'Sidra Sultana', 'Nazia Perwaiz']","This paper introduces IceWatch, a novel deep learning framework designed to predict Glacial Lake Outburst Floods (GLOFs) by integrating spatial and temporal data. The framework includes a vision component, RiskFlow, which uses a CNN-based classifier to analyze Sentinel-2 multispectral satellite imagery for spatial patterns of snow, ice, and meltwater. Additionally, TerraFlow and TempFlow components model glacier velocity and forecast near-surface temperature, respectively, using NASA ITS_LIVE and MODIS LST records. These components are harmonized through preprocessing and synchronization, enabling multimodal, physics-informed GLOF prediction. IceWatch aims to provide rapid, real-time data processing with robustness to noise and missing information, paving the way for scalable GLOF warning systems and potential integration with diverse sensor inputs for global glacier monitoring.",156.91,Phi-4,Nvidia B200 (Cloud Native)
2601.12331v1_Efficient Privacy-Preserving Retrieval Augmented G.pdf,Efficient Privacy-Preserving Retrieval Augmented Generation with Distance-Preserving Encryption,"['Huanyi Ye', 'Jiale Guo', 'Ziyao Liu', 'Kwok-Yan Lam']","This paper introduces an efficient privacy-preserving Retrieval-Augmented Generation (RAG) framework (ppRAG) designed for untrusted cloud environments. It addresses privacy vulnerabilities in traditional RAG systems, such as vector-to-text reconstruction and vector analysis attacks, by proposing a Conditional Approximate Distance-Comparison-Preserving Symmetric Encryption (CAPRISE). CAPRISE encrypts embeddings while allowing similarity computations between encrypted query and database embeddings, preserving only relative distance ordering. Additionally, differential privacy is applied to mitigate query analysis risks. The framework demonstrates efficient processing, high retrieval accuracy, and strong privacy guarantees, making it suitable for resource-constrained users seeking secure, cloud-augmented LLMs.",157.23,Phi-4,Nvidia B200 (Cloud Native)
2601.12338v1_Actionable Advice from Reviews via Mixture of LoRA.pdf,Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM Pipeline for Issue Extraction and Business Recommendations,"['Kartikey Singh Bhandari', 'Manav Ganesh', 'Yashwant Viswanathan', 'Archit Agrawal', 'Dhruv Kumar', 'Pratik Narang']","This paper introduces a modular two-LLM framework designed to convert unstructured customer reviews into actionable business recommendations. The framework consists of an Issue model that extracts key issues and assigns themes, and an Advice model that generates specific operational fixes based on these issues. The Advice model is enhanced using a mixture-of-LoRA experts strategy, which involves training multiple low-rank adapters and using a lightweight gating mechanism for token-level expert mixing. The approach is evaluated using synthetic review-issue-advice triples from Yelp reviews in the airlines and restaurants domains, demonstrating superior performance in actionability and specificity compared to prompting-only and single-adapter baselines.",157.48,Phi-4,Nvidia B200 (Cloud Native)
2601.12341v1_Time-Continuous Modeling for Temporal Affective Pa.pdf,Time-Continuous Modeling for Temporal Affective Pattern Recognition in LLM’s,"['Rezky M. Kam', 'Coddy N. Siswanto']","This paper addresses the limitations of traditional text modality decoder models that rely on discrete token generation, which often results in a lack of true understanding and introduces black-box interpretations of affective dynamics in conversations. The authors propose a hybrid encoder-decoder architecture that utilizes time-aware models based on time-continuous patterns and their correlations with variability in affective states. This approach leverages physics-informed neural networks to allow for temporal and longitudinal adaptation, enabling machines to mimic psychological plausibility from user interactions while maintaining independent expressiveness, computational efficiency, and interpretability.",157.7,Phi-4,Nvidia B200 (Cloud Native)
2601.12343v1_How Well Do LLMs Predict Human Behavior A Measure .pdf,How Well Do LLMs Predict Human Behavior? A Measure of their Pretrained Knowledge,"['Wayne Gao', 'Sukjin Han', 'Annie Liang']","The paper introduces a measure called 'its equivalent sample size' to evaluate the predictive capabilities of pretrained large language models (LLMs) in predicting human behavior. This measure is defined as the amount of task-specific data needed to match the predictive accuracy of an LLM. The authors compare the prediction error of a fixed LLM with that of flexible machine learning models trained on increasing amounts of domain-specific data. They develop a new asymptotic theory for cross-validated prediction error to provide a statistical inference procedure. The method is applied to the Panel Study of Income Dynamics, revealing that LLMs encode significant predictive information for some economic variables but much less for others, indicating varying utility as substitutes for domain-specific data across different settings.",157.63,Phi-4,Nvidia B200 (Cloud Native)
2601.12349v1_Zero-Permission Manipulation Can We Trust Large Mu.pdf,Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?,"['Yi Qian', 'Kunwei Qian', 'Xingbang He', 'Ligeng Chen', 'Jikang Zhang', 'Tiantai Zhang', 'Haiyang Wei', 'Linzhang Wang', 'Hao Wu', 'Bing Mao']","The paper investigates the security vulnerabilities of large multimodal model-powered GUI agents on mobile platforms. It challenges the assumption of Visual Atomicity, demonstrating that the UI state can change between observation and action, creating a critical attack surface. The authors introduce Action Rebinding, a novel attack that allows benign applications to rebind an agent's execution without dangerous permissions. They also present an Intent Alignment Strategy to manipulate the agent's reasoning process, enabling it to bypass verification gates. The study shows a high success rate in executing these attacks across various Android GUI agents, highlighting a fundamental flaw in current agent-OS integration and providing insights for secure future designs.",157.14,Phi-4,Nvidia B200 (Cloud Native)
2601.12357v1_SimpleMatch A Simple and Strong Baseline for Seman.pdf,SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence,"['Hailong Jin', 'Huiying Li']","SimpleMatch addresses a fundamental limitation in current semantic correspondence methods caused by deep downsampling operations, which lead to the irreversible fusion of adjacent keypoint features. The proposed framework, SimpleMatch, effectively handles semantic correspondence even at low resolutions by introducing a lightweight upsample decoder that recovers spatial detail and a multi-scale supervised loss to maintain discriminative features. Additionally, it optimizes training memory usage through sparse matching and window-based localization, reducing memory usage by 51%. At a resolution of 252×252, SimpleMatch achieves superior performance with 84.1% PCK@0.1 on the SPair-71k benchmark, providing a practical and efficient baseline for future research in semantic correspondence.",157.9,Phi-4,Nvidia B200 (Cloud Native)
2601.12358v1_From Prompts to Pavement LMMs-based Agentic Behavi.pdf,From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles,"['Omar Y. Goba', 'Ahmed Y. Gado', 'Catherine M. Elias', 'Ahmed Hussein']","This paper introduces an innovative framework for autonomous vehicles that leverages large language models (LLMs) and multi-modal vision models (LVMs) to dynamically generate and adapt behavior trees (BTs). The framework includes a Descriptor agent for assessing scene criticality, a Planner agent for constructing high-level sub-goals, and a Generator agent for synthesizing executable BT sub-trees in XML format. Integrated into a CARLA+Nav2 simulation, the system activates upon baseline BT failure, successfully navigating around unexpected obstacles without human intervention. This approach demonstrates a proof-of-concept for adaptable behavior planning in diverse driving scenarios, extending beyond the limitations of traditional static BTs.",157.63,Phi-4,Nvidia B200 (Cloud Native)
2601.12374v1_A Scalable Entity-Based Framework for Auditing Bia.pdf,A Scalable Entity-Based Framework for Auditing Bias in LLMs,"['Akram Elbouanani', 'Aboubacar Tuo', 'Adrian Popescu']","This paper introduces a scalable framework for auditing bias in large language models (LLMs) using named entities as probes. The framework allows for large-scale analysis by generating synthetic data that mirrors bias patterns found in natural text. The study conducts the largest bias audit to date, analyzing 1.9 billion data points across various entity types, tasks, languages, models, and prompting strategies. Results reveal systematic biases, such as penalizing right-wing politicians and favoring Western nations. The study finds that while instruction tuning reduces bias, increasing model scale amplifies it, and prompting in Chinese or Russian does not reduce Western-aligned preferences. The findings underscore the need for rigorous auditing of LLMs before their deployment in high-stakes applications.",157.49,Phi-4,Nvidia B200 (Cloud Native)
2601.12389v1_NADIR Differential Attention Flow for Non-Autoregr.pdf,NADIR: Differential Attention Flow for Non-Autoregressive Transliteration in Indic Languages,"['Lakshya Tomar', 'Vinayak Abrol', 'Puneet Agarwal']","This paper introduces NADIR, a novel non-autoregressive (NAR) architecture designed for multilingual transliteration in Indic languages. NADIR addresses the trade-off between speed and accuracy in NAR models by integrating a Differential Transformer and a Mixture-of-Experts mechanism. This allows NADIR to effectively handle complex character mappings without sequential dependencies, achieving a significant speed-up over autoregressive (AR) models while maintaining competitive accuracy. The model reduces various error types, such as repetition, substitution, omission, and insertion errors, making it a practical solution for fast and reliable NAR systems suitable for real-time, large-scale deployment.",157.68,Phi-4,Nvidia B200 (Cloud Native)
2601.12392v1_PsychēChat An Empathic Framework Focused on Emotio.pdf,Psych¯eChat: An Empathic Framework Focused on Emotion Shift Tracking and Safety Risk Analysis in Psychological Counseling,"['Zhentao Xia', 'Yongqi Fan', 'Yuxiang Chu', 'Yichao Yin', 'Liangliang Chen', 'Tong Ruan', 'Weiyan Zhang']","Psych¯eChat is a proposed framework designed to enhance psychological counseling by explicitly integrating emotion shift tracking and safety risk analysis. It addresses the limitations of existing large language models (LLMs) in modeling emotion shifts across counseling sessions and aligning counselor responses with these shifts while mitigating safety risks. The framework employs interactive role-playing to synthesize counselor-seeker dialogues, featuring two key modules: the Emotion Management Module and the Risk Control Module. Additionally, it introduces two modeling paradigms: the Agent Mode and the LLM Mode, which structure the processes into a collaborative multi-agent pipeline and a unified chain-of-thought, respectively. Extensive experiments demonstrate that Psych¯eChat outperforms existing methods in providing emotional insights and ensuring safety control.",157.14,Phi-4,Nvidia B200 (Cloud Native)
2601.12401v1_Beyond the Dirac Delta Mitigating Diversity Collap.pdf,Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation,"['Jinmei Liu', 'Haoru Li', 'Zhenhong Sun', 'Chaofeng Chen', 'Yatao Bian', 'Bo Wang', 'Daoyi Dong', 'Chunlin Chen', 'Zhi Wang']","This paper addresses the challenge of diversity collapse in reinforcement learning (RL) fine-tuning for image generation models. The authors propose DRIFT (DiveRsity-Incentivized Reinforcement Fine-Tuning), a framework that encourages output diversity while maintaining task alignment. DRIFT employs three strategies: sampling a reward-concentrated subset, prompting with stochastic variations, and optimizing intra-group diversity using a potential-based reward shaping mechanism. Experimental results demonstrate that DRIFT achieves superior performance in balancing task alignment and generation diversity, significantly improving both metrics compared to existing methods.",156.86,Phi-4,Nvidia B200 (Cloud Native)
2601.12402v1_Weaknesses of Facial Emotion Recognition Systems.pdf,Weaknesses of Facial Emotion Recognition Systems,"['Aleksandra Jamróz', 'Patrycja Wysocka', 'Piotr Garbat']","This study reviews facial emotion recognition systems, focusing on their weaknesses. It evaluates the performance of selected neural networks trained on specific datasets and tested on others, highlighting issues such as dataset variability, difficulty in recognizing certain emotions, and challenges in differentiating closely related emotions. The study underscores the importance of emotion recognition in enhancing human-computer interaction by improving system responses to users' emotional states.",157.88,Phi-4,Nvidia B200 (Cloud Native)
2601.12405v1_Explainable Machine Learning for Pediatric Dental .pdf,Explainable Machine Learning for Pediatric Dental Risk Stratification Using Socio-Demographic Determinants,"['Manasi Kanade', 'Abhi Thakkar', 'Gabriela Fernandes']","This study develops an explainable artificial intelligence (XAI) framework for pediatric dental risk stratification, emphasizing interpretability and ethical deployment over maximal predictive accuracy. Using supervised machine learning, the model incorporates socio-demographic factors such as age, income-to-poverty ratio, race/ethnicity, gender, and medical history. The model's performance is evaluated through ROC analysis and calibration curves, with SHAP used for explainability. Results show modest discriminative performance and identify key socio-demographic contributors to dental risk. The framework supports population screening and equitable resource allocation, demonstrating responsible AI application in pediatric dentistry.",157.14,Phi-4,Nvidia B200 (Cloud Native)
2601.12410v1_Are LLMs Smarter Than Chimpanzees An Evaluation on.pdf,Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation,"['Dingyi Yang', 'Junqi Zhao', 'Xue Li', 'Ce Li', 'Boyang Li']","This paper evaluates the performance of Large Language Models (LLMs) in tasks related to knowledge state tracking and estimation, comparing them to human capabilities and chimpanzee cognition. The study designs tasks to test if LLMs can detect when story characters demonstrate knowledge they should not possess and predict their next actions based on their own knowledge versus objective truths. Results indicate that current state-of-the-art LLMs perform near-randomly on these tasks, significantly underperforming compared to humans. The paper suggests that future LLM research should focus more on enhancing knowledge estimation and intention understanding abilities.",156.32,Phi-4,Nvidia B200 (Cloud Native)
2601.12415v1_Orthogonalized Policy OptimizationDecoupling Sampl.pdf,Orthogonalized Policy Optimization: Decoupling Sampling Geometry from Optimization Geometry in RLHF,['Wang Zixian'],"This paper explores the implicit conflation of sampling geometry and optimization geometry in alignment methods for large language models, such as PPO, DPO, and IPO. It introduces Orthogonalized Policy Optimization (OPO), which decouples these two aspects by using 𝛼-weighted importance sampling and 𝜒2-induced quadratic regularization. This approach results in a stable optimization process with linear gradient dynamics, avoiding issues like numerical instability and gradient saturation in high-confidence regimes. The paper positions OPO as a unifying framework for existing alignment methods, offering a principled foundation for robust reasoning-oriented training.",158.2,Phi-4,Nvidia B200 (Cloud Native)
2601.12436v1_Purification Before Fusion Toward Mask-Free Speech.pdf,Purification Before Fusion: Toward Mask-Free Speech Enhancement for Robust Audio-Visual Speech Recognition,"['Linzhi Wu', 'Xingyu Zhang', 'Hao Yuan', 'Yakun Zhang', 'Changyan Zheng', 'Liang Xie', 'Tiejun Liu', 'Erwei Yin']","This paper introduces an end-to-end noise-robust audio-visual speech recognition (A VSR) framework that eliminates the need for explicit noise mask generation. By leveraging a Conformer-based bottleneck fusion module, the framework refines noisy audio features with video assistance, reducing modality redundancy and enhancing inter-modal interactions. This approach preserves speech semantic integrity and achieves robust recognition performance, outperforming prior mask-based methods under noisy conditions, as demonstrated on the LRS3 benchmark.",157.44,Phi-4,Nvidia B200 (Cloud Native)
2601.12442v1_Constraint-Aware Neurosymbolic Uncertainty Quantif.pdf,Constraint-Aware Neurosymbolic Uncertainty Quantification with Bayesian Deep Learning for Scientific Discovery,"['Shahnawaz Alam', 'Mohammed Mudassir Uddin', 'Mohammed Kaif Pasha']","This paper introduces the Constraint-Aware Neurosymbolic Uncertainty Framework (CANUF), which integrates Bayesian deep learning with differentiable symbolic reasoning to provide trustworthy uncertainty estimates while respecting domain constraints. The framework consists of automated constraint extraction, a probabilistic neural backbone with variational inference, and a differentiable constraint satisfaction layer. Experiments demonstrate that CANUF significantly reduces Expected Calibration Error and maintains high constraint satisfaction across various scientific benchmarks. The framework offers an end-to-end differentiable pipeline for uncertainty quantification, constraint satisfaction, and interpretable explanations in scientific predictions.",158.1,Phi-4,Nvidia B200 (Cloud Native)
2601.12443v1_Adversarial Defense in Vision-Language Models An O.pdf,Adversarial Defense in Vision-Language Models: An Overview,"['Xiaowei Fu', 'Lei Zhang']","The paper provides an overview of adversarial defense strategies for Vision Language Models (VLMs), such as CLIP, which are vulnerable to sophisticated and imperceptible adversarial attacks. These attacks can significantly degrade model performance in cross-modal tasks. The paper reviews three main defense paradigms: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves adversarial fine-tuning during training, which is computationally intensive and may not generalize well. Test-time Adaptation Defense adapts the model at inference time, offering flexibility but with increased complexity. Training-free Defense focuses on altering adversarial inputs or their embeddings without modifying the model, thus mitigating attack impacts without additional training. The paper highlights the strengths and limitations of these approaches and discusses ongoing challenges in enhancing VLM robustness.",157.97,Phi-4,Nvidia B200 (Cloud Native)
2601.12444v1_Large Language Model for OWL Proofs.pdf,Large Language Model for OWL Proofs,"['Hui Yang', 'Jiaoyan Chen', 'Uli Sattler']","This paper investigates the capability of Large Language Models (LLMs) to generate proofs within the context of OWL ontologies. The authors develop an automated dataset construction and evaluation framework to assess proof generation through tasks such as Extraction, Simplification, and Explanation, along with evaluating Logic Completeness. The study reveals that while some models perform well overall, they struggle with complex cases. Logical complexity, rather than the format of representation, significantly influences LLM performance. Additionally, noise and incompleteness in input data notably reduce performance. These findings highlight the potential of LLMs in generating rigorous logical explanations and the challenges they face under complex or imperfect conditions.",158.05,Phi-4,Nvidia B200 (Cloud Native)
2601.12449v1_AgenTRIM Tool Risk Mitigation for Agentic AI.pdf,AgenTRIM: Tool Risk Mitigation for Agentic AI,"['Roy Betser', 'Shamik Bose', 'Amit Giloni', 'Chiara Picardi', 'Sindhu Padakandla', 'Roman Vainshtein']","The paper introduces AGENTRIM, a framework designed to mitigate security risks associated with tool-driven agency in AI agents. These agents, which combine large language models (LLMs) with external tools, face risks such as indirect prompt injection and tool misuse due to improper tool permissions. AGENTRIM addresses these issues by enforcing least-privilege tool access through offline and online phases, ensuring safer tool use without altering the agent's internal reasoning. The framework is evaluated on the AgentDojo benchmark, demonstrating reduced attack success and maintained task performance.",157.68,Phi-4,Nvidia B200 (Cloud Native)
2601.12465v1_Incentivizing In-depth Reasoning over Long Context.pdf,INCENTIVIZING IN-DEPTH REASONING OVER LONG CONTEXTS WITH PROCESS ADVANTAGE SHAPING,"['Miao Peng', 'Weizhou Shen', 'Nuo Chen', 'Chenliang Li', 'Ming Yan', 'Jia Li']","This paper addresses the challenge of enhancing long-context reasoning in large language models (LLMs) using Reinforcement Learning with Verifiable Rewards (RLVR). It identifies the 'almost-there' phenomenon, where trajectories are mostly correct but fail at the final step, due to insufficient reasoning density and loss of learning signals from partially correct trajectories. The authors propose DEEPREASONQA, a framework for creating high-difficulty, multi-hop QA pairs, and LONGPAS, a method for fine-grained credit assignment by evaluating reasoning steps for validity and relevance. Experiments demonstrate that these methods significantly improve long-context reasoning performance compared to RLVR baselines and match leading LLMs with fewer parameters.",157.32,Phi-4,Nvidia B200 (Cloud Native)
2601.12467v1_Patch-Level Tokenization with CNN Encoders and Att.pdf,Patch-Level Tokenization with CNN Encoders and Attention for Improved Transformer Time-Series Forecasting,['Saurish Nagrath'],"This paper introduces a two-stage forecasting framework that separates local temporal representation learning from global dependency modeling for time-series forecasting. In the first stage, a convolutional neural network (CNN) processes fixed-length temporal patches to extract short-range dynamics and non-linear feature interactions, producing compact patch-level token embeddings. These embeddings are refined using token-level self-attention. In the second stage, a Transformer encoder processes the token sequence to model inter-patch dependencies and generate forecasts. Experiments on synthetic multivariate time-series data show that this patch-based tokenization strategy achieves competitive performance compared to convolutional and patch-based Transformer baselines, highlighting the importance of structured temporal representations and the benefits of decoupling local encoding from global attention-based modeling.",154.68,Phi-4,Nvidia B200 (Cloud Native)
2601.12471v1_Knowing When to Abstain Medical LLMs Under Clinica.pdf,Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty,"['Sravanthi Machcha', 'Sushrita Yerra', 'Sahil Gupta', 'Aishwarya Sahoo', 'Sharmin Sultana', 'Hong Yu', 'Zonghai Yao']","This paper introduces MedAbstain, a benchmark and evaluation protocol for assessing the ability of large language models (LLMs) to abstain when uncertain in medical multiple-choice question answering (MCQA). The study highlights the importance of abstention mechanisms for trustworthy LLM deployment in high-stakes applications. It reveals that even high-accuracy models often fail to abstain appropriately, and that providing explicit abstention options significantly improves model uncertainty and safer abstention. The findings suggest that scaling model size or advanced prompting offers limited benefits compared to integrating abstention mechanisms.",153.33,Phi-4,Nvidia B200 (Cloud Native)
2601.12494v1_Harmonizing the Arabic Audio Space with Data Sched.pdf,Harmonizing the Arabic Audio Space with Data Scheduling,"['Hunzalah Hassan Bhatti', 'Firoj Alam', 'Shammur Absar Chowdhury']","This paper presents a systematic study on adapting Arabic-centric audio large language models (LLMs) for multi-task instruction tuning, addressing the challenges in linguistically complex, dialect-rich settings. It introduces AraMega-SSum, a novel dataset for Arabic speech summarization, and evaluates the adaptation of Qwen2.5-Omni (7B) using Task-Progressive Curriculum (TPC) and Aligner-Based Diverse Sampling (ADS). The study reveals a trade-off between efficiency and robustness, showing that while ADS accelerates convergence and improves paralinguistic F1-scores, it can destabilize generative decoding. TPC stabilizes acoustic mapping but may cause negative transfer in downstream tasks. A Hybrid TPC+ADS Strategy is proposed as an optimal training approach, balancing robust foundation establishment with diversity-aware refinement to capture nuanced details. These findings provide practical guidance for efficiently adapting Omni-models in complex, low-resource multimodal environments.",153.41,Phi-4,Nvidia B200 (Cloud Native)
2601.12499v1_Failure Modes in Multi-Hop QA The Weakest Link Law.pdf,Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck,"['Meiru Zhang', 'Zaiqiao Meng', 'Nigel Collier']","This paper investigates the limitations of Large Language Models (LLMs) in multi-hop reasoning tasks due to position bias, which leads to overlooking information at certain positions. The authors introduce Multi-Focus Attention Instruction (MFAI) to address these issues by steering attention towards specific positions. They establish the 'Weakest Link Law', showing that multi-hop reasoning performance is determined by the least visible evidence, governed by absolute position rather than the distance between facts. The study reveals that while matched MFAI can resolve recognition bottlenecks, misleading MFAI can cause confusion. The paper also demonstrates that models employing System-2 reasoning can effectively locate and integrate necessary information, even in noisy, long-context settings.",154.03,Phi-4,Nvidia B200 (Cloud Native)
2601.12518v1_Cooperative Multi-agent RL with Communication Cons.pdf,Cooperative Multi-agent RL with Communication constraints,"['Nuoya Xiong', 'Aarti Singh']","This paper addresses the challenge of limited communication in decentralized Multi-agent Reinforcement Learning (MARL) systems. It proposes a novel technique called base policy prediction to handle outdated information due to communication constraints. This method predicts policy updates using old gradients, allowing for effective learning with fewer communication rounds. Theoretical analysis shows convergence to an ε-Nash equilibrium in potential games with reduced communication and sample complexity. Empirical tests in simulated games and MAPPO environments demonstrate significant reductions in communication costs while maintaining performance, outperforming standard algorithms under similar constraints.",154.7,Phi-4,Nvidia B200 (Cloud Native)
2601.12522v1_Improved Bug Localization with AI Agents Leveragin.pdf,Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition,"['Asif Mohammed Samir', 'Mohammad Masudur Rahman']","This paper introduces CogniGent, a novel agentic technique for bug localization that leverages multiple AI agents capable of causal reasoning, call-graph-based root cause analysis, and context engineering. It emulates developers-inspired debugging practices and conducts hypothesis testing to support bug localization. Evaluated on a dataset of 591 bug reports, CogniGent outperformed traditional and LLM-based techniques, achieving significant improvements in Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). The technique addresses reasoning, dependency, and context limitations, advancing the state of bug localization by bridging human-like cognition with agentic automation.",154.64,Phi-4,Nvidia B200 (Cloud Native)
2601.12534v1_Encoding Emotion Through Self-Supervised Eye Movem.pdf,ENCODING EMOTION THROUGH SELF-SUPERVISED EYE MOVEMENT RECONSTRUCTION,"['Marcus Ma', 'Jordan Prescott', 'Emily Zhou', 'Tiantian Feng', 'Kleanthis Avramidis', 'Gabor Mihaly Toth', 'Shrikanth Narayanan']","This paper explores the relationship between eye movement and emotional expression, focusing on predicting emotional markers from low-resolution videos. The study utilizes a novel gaze detection model inspired by self-supervised learning methods from language models. The model is applied to a dataset of Holocaust survivor interviews, demonstrating its ability to predict emotional behaviors such as laughing, crying/sobbing, and sighing. The findings suggest that self-supervised eye movement reconstruction is an effective method for encoding emotional signals, even from low-quality video data.",154.47,Phi-4,Nvidia B200 (Cloud Native)
2601.12535v1_Improving Low-Resource Machine Translation via Rou.pdf,Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning,"['Ahmed Attia', 'Alham Fikri']","This paper addresses the challenge of low-resource machine translation (MT) by introducing a self-supervised reinforcement-learning-based fine-tuning approach using round-trip bootstrapping. The method leverages the No Language Left Behind (NLLB) models to translate English into a low-resource language and back to English, optimizing the process with a reward function based on chrF++ and BLEU metrics. The approach shows consistent improvements in translation quality for languages such as Central Aymara, Friulian, Wolof, and Russian, enhancing fluency and semantic fidelity without requiring parallel corpora. The paper argues that this method can benefit from scaling, allowing models to leverage their pretrained knowledge for further self-improvement.",154.34,Phi-4,Nvidia B200 (Cloud Native)
2601.12538v1_Agentic Reasoning for Large Language Models.pdf,Agentic Reasoning for Large Language Models,"['Tianxin Wei', 'Ting-Wei Li', 'Zhining Liu', 'Xuying Ning', 'Ze Yang', 'Jiaru Zou', 'Zhichen Zeng', 'Ruzhong Qiu', 'Xiao Lin', 'Dongqi Fu', 'Zihao Li', 'Mengting Ai', 'Duo Zhou', 'Wenxuan Bao', 'Yunzhe Li', 'Gaotang Li', 'Cheng Qian', 'Yu Wang', 'Xiangru Tang', 'Yin Xiao', 'Liri Fang', 'Hui Liu', 'Xianfeng Tang', 'Yuji Zhang', 'Chi Wang', 'Jiaxuan You', 'Heng Ji', 'Hanghang Tong', 'Jingrui He']","This paper surveys agentic reasoning in large language models (LLMs), highlighting their capabilities and challenges in open-ended and dynamic environments. It introduces a systematic roadmap organized along three dimensions: foundational, self-evolving, and collective multi-agent reasoning. The paper discusses system constraints and optimization settings, reviews real-world applications, and outlines open challenges and future directions in agentic reasoning.",154.41,Phi-4,Nvidia B200 (Cloud Native)
2601.12539v1_MemeLens Multilingual Multitask VLMs for Memes.pdf,MemeLens: Multilingual Multitask VLMs for Memes,"['Ali Ezzat Shahroor', 'Mohamed Bayan Kmainasi', 'Abul Hasnat', 'Dimitar Dimitrov', 'Giovanni Da San Martino', 'Preslav Nakov', 'Firoj Alam']","This paper introduces MEMELENS, a unified multilingual and multitask explanation-enhanced Vision Language Model (VLM) designed for meme understanding. It consolidates 38 public meme datasets into a shared taxonomy of 20 tasks, addressing the limitations of existing research that is often task- and language-specific. The paper presents an empirical analysis across different modeling paradigms, task categories, and datasets, highlighting the need for multimodal training and the challenges of over-specialization. The findings suggest that robust meme understanding requires a unified training approach rather than fine-tuning on individual datasets. The authors plan to make the experimental resources and datasets publicly available for community use.",153.82,Phi-4,Nvidia B200 (Cloud Native)
2601.12542v1_Rethinking the AI Scientist Interactive Multi-Agen.pdf,Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery,"['Lukas Weidener', 'Marko Brkić', 'Mihailo Jovanović', 'Ritvik Singh', 'Chiara Baccin', 'Emre Ulgac', 'Alex Dobrin', 'Aakaash Meduri']","This paper introduces Deep Research, a multi-agent system designed for interactive scientific investigation with rapid turnaround times. The system integrates specialized agents for planning, data analysis, literature search, and novelty detection, all coordinated through a persistent world state. It supports semi-autonomous and fully autonomous operational modes, catering to different research workflows. Evaluated on the BixBench computational biology benchmark, Deep Research achieved state-of-the-art performance, with significant improvements over existing baselines. The paper also discusses architectural constraints and practical deployment considerations for AI-assisted scientific workflows.",153.27,Phi-4,Nvidia B200 (Cloud Native)
2601.12547v1_How Clinicians Think and What AI Can Learn From It.pdf,"Ordinal-First, Robust Decision Algorithms for Medicine","['Dr. Dipayan Sengupta, MD (Dermatology)', 'Dr. Saumya Panda, MD (Dermatology)']","The paper argues that clinical AI systems should shift from being prediction engines to robust decision-making tools. It highlights that clinicians often use ordinal non-compensatory decision-making, relying on fast-and-frugal heuristics to make decisions under uncertainty. The authors propose that ordinal decision rules, which consider the order of outcomes rather than their absolute values, are more suitable for clinical settings due to the inherent uncertainty and measurement challenges. They suggest a blueprint for AI systems that align with clinician reasoning by using robust ordinal decision rules and treating complex models as tools for tie-breaking when necessary.",154.38,Phi-4,Nvidia B200 (Cloud Native)
2601.12549v1_Benchmarking Concept-Spilling Across Languages in .pdf,Benchmarking Concept-Spilling Across Languages in LLMs,"['Ilia Badanin', 'Daniil Dzenhaliou', 'Imanol Schlag']","This paper introduces a novel framework for evaluating the semantic robustness of multilingual Large Language Models (LLMs) by examining how they handle polysemous words across different languages. The study identifies a phenomenon known as 'language spilling,' where models exhibit a bias towards dominant languages, leading to semantic interference in non-English content generation. The authors propose a methodology to measure model performance based on the sequence in which true meanings from the target language are generated before resorting to dominant-language meanings. The research evaluates various multilingual LLMs using a structured task across nine languages, employing a benchmark of 100 high-polysemy English words. The findings highlight significant variations in semantic robustness among models and languages, offering a ranking system for model comparison. The paper contributes a scalable benchmark and a validation pipeline for assessing multilingual semantic evaluation, essential for developing more balanced AI systems.",157.81,Phi-4,Nvidia B200 (Cloud Native)
2601.12554v1_Artificial Intelligence in Materials Science and E.pdf,"Artificial Intelligence in Materials Science and Engineering: Current Landscape, Key Challenges, and Future Trajectories","['Iman Peivaste', 'Salim Belouettar', 'Francesco Mercuri', 'Nicholas Fantuzzi', 'Hamidreza Dehghani', 'Razieh Izadi', 'Halliru Ibrahim', 'Jakub Lengiewicz', 'Maël Belouettar-Mathis', 'Kouider Bendine', 'Ahmed Makradi', 'Martin Hörsch', 'Peter Klein', 'Mohamed El Hachemi', 'Heinz A. Preisig', 'Yacine Rezgui', 'Natalia Konchakova', 'Ali Daouadji']","This review provides a comprehensive overview of the current landscape of Artificial Intelligence in materials science and engineering. It synthesizes recent advancements and methodologies, surveying machine learning approaches from traditional algorithms to advanced deep learning architectures like CNNs, GNNs, and Transformers. The review also highlights emerging generative AI and probabilistic models such as Gaussian Processes for uncertainty quantification. It emphasizes the pivotal role of data, focusing on effective representation and featurization strategies, including compositional, structural, image-based, and language-inspired approaches, combined with appropriate preprocessing, which are fundamental to the performance of machine learning models in this field.",157.7,Phi-4,Nvidia B200 (Cloud Native)
2601.12557v1_Life Machine Learning and the Search for Habitabil.pdf,"Life, Machine Learning, and the Search for Habitability: Predicting Biosignature Fluxes for the Habitable Worlds Observatory","['Mark Moussa', 'Amber V. Young', 'Brianna Isola', 'Vasuda Trehan', 'Michael D. Himes', 'Nicholas Wogan', 'Giada Arney']","This paper introduces two advanced machine-learning architectures, a Bayesian Convolutional Neural Network (BCNN) and a novel Spectral Query Adaptive Transformer (SQuAT), designed to predict biosignature species fluxes from exoplanetary reflected-light spectra. The BCNN quantifies uncertainties robustly, while SQuAT enhances interpretability by associating spectral features with specific biosignatures. Both models demonstrate high predictive accuracy on a diverse dataset, offering tools for optimizing observation schedules and maximizing scientific returns for missions like NASA's Habitable Worlds Observatory.",158.21,Phi-4,Nvidia B200 (Cloud Native)
2601.12560v1_Agentic Artificial Intelligence AI Architectures T.pdf,"Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents","['Arunkumar V', 'Gangadharan G.R.', 'Rajkumar Buyya']","The paper explores the transition from traditional AI models to Agentic AI, where systems act as autonomous entities capable of perception, reasoning, planning, and action. It introduces a unified taxonomy for agent architectures, categorizing them into Perception, Brain, Planning, Action, Tool Use, and Collaboration. The paper discusses the evolution from linear reasoning to native inference models and from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. It reviews environments for agent operation, including digital operating systems and embodied robotics, and evaluates current practices. The paper also identifies challenges such as hallucination in action and prompt injection, proposing future research directions for robust and reliable autonomous systems.",158.3,Phi-4,Nvidia B200 (Cloud Native)
2601.12577v1_Primate-like perceptual decision making emerges th.pdf,Primate-like perceptual decision making emerges through deep recurrent reinforcement learning,"['Nathan J. Wispinski', 'Scott A. Stone', 'Anthony Singhal', 'Patrick M. Pilarski', 'Craig S. Chapman']","This paper explores the emergence of primate-like decision-making mechanisms through deep recurrent reinforcement learning. By training a neural network on a noisy perceptual discrimination task, the study demonstrates that the network develops key decision-making abilities similar to those observed in primates, such as trading off speed for accuracy and adapting to new information. The findings support the theory that these mechanisms evolved to optimize decision-making in uncertain environments, providing experimental evidence for the evolutionary pressures that shaped primate decision-making abilities.",158.26,Phi-4,Nvidia B200 (Cloud Native)
2601.12582v1_Ontology-aligned structuring and reuse of multimod.pdf,Ontology-aligned structuring and reuse of multimodal materials data and workflows towards automatic reproduction,"['Sepideh Baghaee Ravari', 'Abril Azocar Guzman', 'Sarath Menon', 'Stefan Sandfeld', 'Tilmann Hickel', 'Markus Stricker']","The paper introduces an ontology-driven, large language model (LLM)-assisted framework for the automated extraction and structuring of computational workflows from the literature. It focuses on density functional theory-based stacking fault energy (SFE) calculations in hexagonal close-packed magnesium and its binary alloys. The approach uses a multi-stage filtering strategy and prompt-engineered LLM extraction applied to method sections and tables. Extracted information is unified into a canonical schema and aligned with established materials ontologies (CMSO, ASMO, and PLDO), enabling the construction of a knowledge graph using atomRDF. This knowledge graph facilitates systematic comparison of reported SFE values and supports the structured reuse of computational protocols. While full computational reproducibility is still constrained by missing or implicit metadata, the framework enhances the organization and contextualization of published results in a semantically interoperable form, improving transparency and reusability of computational materials data.",158.11,Phi-4,Nvidia B200 (Cloud Native)
2601.12585v1_Do MLLMs See What We See Analyzing Visualization L.pdf,Analyzing Visualization Literacy Barriers in AI Systems,"['Mengli (Dawn) Duan', 'Yuhe (Sissi) Jiang', 'Matthew Varona', 'Carolina Nobre']","This paper presents the first systematic analysis of barriers to visualization literacy in Multimodal Large Language Models (MLLMs). Using a modified Visualization Literacy Assessment Test (reVLAT) with synthetic data, the authors analyzed 309 erroneous responses from four state-of-the-art MLLM models. The study identified a taxonomy of MLLM failures, highlighting two machine-specific barriers that extend existing human-participation frameworks. The results indicate that while MLLMs perform well on simple charts, they struggle with color-intensive, segment-based visualizations and often fail to form consistent comparative reasoning. These findings are crucial for guiding the future evaluation and design of reliable AI-driven visualization assistants.",158.19,Phi-4,Nvidia B200 (Cloud Native)
2601.12594v1_SLAP Scalable Language-Audio Pretraining with Vari.pdf,SLAP: SCALABLE LANGUAGE-AUDIO PRETRAINING WITH VARIABLE-DURATION AUDIO AND MULTI-OBJECTIVE TRAINING,"['Xinhao Mei', 'Gael Le Lan', 'Haohe Liu', 'Zhaoheng Ni', 'V arun Nagaraja', 'Yang Liu', 'Yangyang Shi', 'Vikas Chandra']","This paper introduces Scalable Language-Audio Pretraining (SLAP), which addresses limitations of current Contrastive Language-Audio Pretraining (CLAP) models by scaling to 109 million audio-text pairs with variable audio durations and incorporating multiple training objectives. SLAP combines contrastive loss with self-supervised and captioning losses in a single-stage training, enhancing the learning of dense audio representations. The model achieves state-of-the-art performance on audio-text retrieval and zero-shot audio classification tasks, demonstrating its effectiveness across diverse benchmarks.",158.25,Phi-4,Nvidia B200 (Cloud Native)
2601.12607v1_A Cloud-based Multi-Agentic Workflow for Science.pdf,A Cloud-based Multi-Agentic Workflow for Science,"['Anurag Acharya', 'Timothy Vega', 'Rizwan A. Ashraf', 'Anshu Sharma', 'Derek Parker', 'Robert Rallo']","This paper presents a domain-agnostic, model-independent workflow for an agentic framework designed to function as a scientific assistant on the cloud. The framework integrates a supervisor agent with various specialized agents to handle tasks ranging from literature review and data analysis to complex simulation runs. A proof-of-concept system was developed to accelerate research in Catalysts, a critical area in Chemistry and Material Science. The study evaluates the framework's performance on synthetic and real-world tasks, demonstrating high task routing and completion accuracy. The framework's cost and service breakdown are also reported, showcasing its viability for other scientific domains.",157.89,Phi-4,Nvidia B200 (Cloud Native)
2601.12617v1_Creating Disability Story Videos with Generative A.pdf,"Creating Disability Story Videos with Generative AI: Motivation, Expression, and Sharing","['Shuo Niu', 'Dylan Clements', 'Hyungsin Kim']","This research explores how people with disabilities (PwDs) use Generative AI (GenAI) to create and share videos about their experiences. It examines the motivations, expression, and sharing of PwD-created GenAI story videos, grounded in digital storytelling theory. The study identifies four core affordances of GenAI that either facilitate or require improvements for disability storytelling: non-capturable depiction, identity concealment and representation, contextual realism and consistency, and emotional articulation. The paper concludes with a framework and discusses design implications for GenAI in relation to story completion, media formats, and corrective mechanisms.",158.18,Phi-4,Nvidia B200 (Cloud Native)
2601.12637v1_Topology-Aware Multiscale Mixture of Experts for E.pdf,Topology-Aware Multiscale Mixture of Experts for Efficient Molecular Property Prediction,"['Long D. Nguyen', 'Kelin Xia', 'Binh P. Nguyen']","This paper introduces a novel approach, Multiscale Interaction Mixture of Experts (MI-MoE), to enhance molecular property prediction by adapting interaction modeling across different geometric regimes. The contributions include: (1) a distance-cutoff expert ensemble to capture short-, mid-, and long-range interactions without a fixed cutoff; (2) a topological gating encoder that uses filtration-based descriptors, such as persistent homology features, to route inputs to experts based on evolving connectivity; and (3) demonstrating that MI-MoE improves performance across various 3D molecular backbones and benchmark datasets for both regression and classification tasks. The results emphasize the effectiveness of topology-aware multiscale routing in 3D molecular graph learning.",158.21,Phi-4,Nvidia B200 (Cloud Native)
2601.12638v1_Mixed Precision PointPillars for Efficient 3D Obje.pdf,Mixed Precision PointPillars for Efficient 3D Object Detection with TensorRT,"['Ninnart Fuengfusin', 'Keisuke Yoneda', 'Naoki Suganuma']","This paper addresses the challenge of real-time LIDAR 3D object detection for autonomous vehicles by proposing a mixed precision framework for PointPillars. The framework identifies sensitive layers using post-training quantization (PTQ) and assigns them floating point precision, while other layers are quantized to 8-bit integer (INT8). This approach, combined with quantization-aware training (QAT), mitigates performance degradation due to LIDAR's wide numerical distributions and extreme outliers. The proposed methods achieve competitive performance to floating point models and, when deployed with TensorRT, significantly reduce latency and model size.",158.02,Phi-4,Nvidia B200 (Cloud Native)
2601.12641v1_STEP-LLM Generating CAD STEP Models from Natural L.pdf,Generating CAD STEP Models from Natural Language with Large Language Models,"['Xiangyu Shi', 'Junyang Ding', 'Xu Zhao', 'Sinong Zhan', 'Payal Mohapatra', 'Daniel Quispe', 'Kojo Welbeck', 'Jian Cao', 'Wei Chen', 'Ping Guo', 'Qi Zhu']","This paper addresses the challenge of creating CAD models from natural language using large language models (LLMs). Traditional methods focus on command sequences or script-based formats, which are not universally compatible with manufacturing. The authors propose using the STEP file format, a neutral boundary representation format, to overcome these limitations. They introduce a dataset of approximately 40,000 STEP-caption pairs and novel preprocessing techniques tailored for the graph-structured format of STEP. These include a depth-first search (DFS)-based reserialization to linearize cross-references and chain-of-thought (CoT)-style structural annotations for global coherence. The integration of retrieval-augmented generation (RAG) and reinforcement learning (RL) with a Chamfer Distance-based geometric reward further refines the model. Experiments show that the proposed STEP-LLM framework achieves higher geometric fidelity compared to the Text2CAD baseline, demonstrating the potential for LLM-driven STEP model generation from natural language to democratize CAD design for manufacturing.",157.9,Phi-4,Nvidia B200 (Cloud Native)
2601.12646v1_Unbounded Harms Bounded Law Liability in the Age o.pdf,"Unbounded Harms, Bounded Law: Liability in the Age of Borderless AI",['Ha-Chi Tran'],"The paper addresses the challenges posed by the rapid advancement of artificial intelligence (AI) in the context of risk governance and liability, particularly focusing on transboundary AI harms. It highlights the inadequacies of current legal frameworks in dealing with AI-induced risks that cross national borders and regulatory jurisdictions. The author argues that these harms are inherent to AI supply chains and are likely to increase due to global deployment and cross-border data infrastructures. The paper explores compensation and liability mechanisms from other high-risk and transnational domains to propose legal design principles for AI-related harms. It emphasizes the need for a global AI compensation and accountability architecture amidst geopolitical rivalries and the necessity for collective action in governing transboundary AI risks.",158.43,Phi-4,Nvidia B200 (Cloud Native)
2601.12648v1_Intelligent Documentation in Medical Education Can.pdf,Intelligent Documentation in Medical Education: Can AI Replace Manual Case Logging?,"['Nafiz Imtiaz Khan, MSc', 'Kylie Cleland, BSc', 'Vladimir Filkov, PhD', 'Roger Eric Goldman, PhD']","This study explores the potential of large language models (LLMs) to automate procedural case log documentation in radiology training. By analyzing 414 curated radiology reports from interventional radiology residents, the study evaluates the performance of both local and commercial LLMs in replacing manual logging. The results indicate that LLMs can significantly reduce the manual annotation burden, with models like Qwen-2.5 and Claude-3.5 achieving high F1-scores. The study suggests that LLMs could save over 35 hours of manual work per resident annually, though it emphasizes the need for broader validation and integration into clinical workflows.",154.4,Phi-4,Nvidia B200 (Cloud Native)
2601.12654v1_Explanation Multiplicity in SHAP Characterization .pdf,Explanation Multiplicity in SHAP: Characterization and Assessment,"['HYUNSEUNG HWANG', 'SEUNGEUN LEE', 'LUCAS ROSENBLATT', 'JULIA STOYANOVICH', 'STEVEN EUIJONG WHANG']","This paper investigates the phenomenon of explanation multiplicity in SHAP, a widely used method for post-hoc feature attribution in explainable AI. Explanation multiplicity refers to the occurrence of multiple, internally valid but substantively different explanations for the same decision, even when the individual, prediction task, and trained model are held constant. The authors present a methodology to characterize this multiplicity, distinguishing sources from model training and selection versus intrinsic stochasticity in the explanation pipeline. They demonstrate that commonly used metrics can mask instability in explanations, and propose using randomized baseline values for better interpretation. The study finds that explanation multiplicity is prevalent across various datasets and model classes, highlighting it as a normative concern for responsible AI deployment.",154.43,Phi-4,Nvidia B200 (Cloud Native)
2601.12658v1_Augmenting Question Answering with A Hybrid RAG Ap.pdf,Augmenting Question Answering with A Hybrid RAG Approach,"['Tianyi Yang', 'Nashrah Haque', 'Vaishnave Jonnalagadda', 'Yuya Jeremy Ong', 'Zhehui Chen', 'Yanzhao Wu', 'Lei Yu', 'Divyesh Jadav', 'Wenqi Wei']","This paper introduces Structured-Semantic RAG (SSRAG), a hybrid architecture designed to enhance the quality of responses in Question-Answering (QA) tasks. SSRAG integrates query augmentation, agentic routing, and a structured retrieval mechanism that combines vector and graph-based techniques with context unification. The approach aims to improve answer accuracy and informativeness by refining retrieval processes and enhancing contextual grounding. Extensive evaluations on QA datasets such as TruthfulQA, SQuAD, and WikiQA demonstrate that SSRAG consistently improves response quality over standard RAG implementations.",154.38,Phi-4,Nvidia B200 (Cloud Native)
2601.12661v1_MedConsultBench A Full-Cycle Fine-Grained Process-.pdf,"MedConsultBench: A Full-Cycle, Fine-Grained, Process-Aware Benchmark for Medical Consultation Agents","['Chuhan Qiao', 'Jianghua Huang', 'Daxing Zhao', 'Ziding Liu', 'Yanjun Shen', 'Bing Cheng', 'Wei Lin', 'Kai Wu']","MedConsultBench is a comprehensive framework designed to evaluate medical consultation agents by covering the entire clinical workflow from history taking to follow-up Q&A. It introduces Atomic Information Units (AIUs) to track clinical information acquisition at a sub-turn level, enabling precise monitoring through 22 fine-grained metrics. The benchmark addresses the underspecification and ambiguity in online consultations, evaluating uncertainty-aware inquiry and medication regimen compatibility. Systematic evaluation of 19 large language models reveals that high diagnostic accuracy often masks deficiencies in information-gathering efficiency and medication safety, highlighting a gap between theoretical medical knowledge and clinical practice ability.",154.11,Phi-4,Nvidia B200 (Cloud Native)
2601.12664v1_Generalizable Hyperparameter Optimization for Fede.pdf,Generalizable Hyperparameter Optimization for Federated Learning on Non-IID Cancer Images,"['Elisa Gonçalves Ribeiro', 'Rodrigo Moreira', 'Larissa Ferreira Rodrigues Moreira', 'André Ricardo Backes']","This paper addresses the challenge of optimizing hyperparameters for Federated Learning (FL) in non-IID settings, specifically for cancer histopathology imaging. It explores whether hyperparameters optimized on one dataset can generalize across different non-IID federated scenarios. The study introduces a cross-dataset aggregation heuristic that combines configurations by averaging learning rates and considering the modal optimizers and batch sizes. This approach achieves competitive classification performance in binary histopathology tasks for ovarian and colorectal cancers.",154.42,Phi-4,Nvidia B200 (Cloud Native)
2601.12667v1_Empowering All-in-Loop Health Management of Spacec.pdf,Empowering All-in-Loop Health Management of Spacecraft Power System in the Mega-Constellation Era via Human-AI Collaboration,"['Yi Di', 'Zhibin Zhao', 'Fujin Wang', 'Xue Liu', 'Jiafeng Tang', 'Jiaxin Ren', 'Zhi Zhai', 'Xuefeng Chen']","This paper addresses the challenges of health management for spacecraft power systems (SPS) in the era of satellite mega-constellations (SMC). It proposes the AUC principle and introduces SpaceHMchat, an open-source Human-AI collaboration framework for all-in-loop health management (AIL HM). SpaceHMchat supports tasks such as work condition recognition, anomaly detection, fault localization, and maintenance decision making. The framework achieves high performance across various metrics and introduces the first-ever AIL HM dataset for SPS, containing four sub-datasets with diverse fault types and extensive data points.",154.06,Phi-4,Nvidia B200 (Cloud Native)
2601.12671v1_Exploiting Test-Time Augmentation in Federated Lea.pdf,Exploiting Test-Time Augmentation in Federated Learning for Brain Tumor MRI Classification,"['Thamara Leandra de Deus Melo', 'Rodrigo Moreira', 'Larissa Ferreira Rodrigues Moreira', 'Andr´e R. Backes']","This paper evaluates the use of convolutional neural networks (CNNs) in a federated learning (FL) setting for brain tumor MRI classification. It compares models trained on original versus preprocessed MRI images, including resizing, grayscale conversion, normalization, filtering, and histogram equalization. The study finds that preprocessing alone yields negligible gains, but when combined with test-time augmentation (TTA), it results in consistent, statistically significant improvements in federated MRI classification (p<0.001). The authors suggest that TTA should be the default inference strategy in FL-based medical imaging, and when computational resources allow, combining TTA with light preprocessing can provide additional reliable gains.",154.39,Phi-4,Nvidia B200 (Cloud Native)
2601.12688v1_Logic-Guided Multistage Inference for Explainable .pdf,Logic-Guided Multistage Inference for Explainable Multidefendant Judgment Prediction,"['Xu Zhang', 'Qinghua Wang', 'Mengyang Zhao', 'Fang Wang', 'Cunquan Qu']","This paper addresses the complexity of assigning responsibility in multidefendant cases by incorporating sentencing logic into a pretrained Transformer encoder framework. The proposed masked multistage inference (MMSI) framework enhances AI-driven judicial assistance by clarifying defendant roles and improving sensitivity to culpability distinctions. The framework, evaluated on the IMLJP dataset for intentional injury cases, demonstrates significant accuracy improvements over baselines in role-based culpability differentiation, offering a robust solution for intelligent judicial systems.",154.28,Phi-4,Nvidia B200 (Cloud Native)
2601.12711v1_Neurosymbolic LoRA Why and When to Tune Weights vs.pdf,Neurosymbolic LoRA: Why and When to Tune Weights vs. Rewrite Prompts,"['Kevin Wang', 'Neel P. Bhatt', 'Cong Liu', 'Junbo Li', 'Runjin Chen', 'Yihan Xi', 'Timothy Barclay', 'Alvaro Velasquez', 'Ufuk Topcu', 'Zhangyang Wang']","This paper introduces a neurosymbolic LoRA framework that combines numerical updates and symbolic manipulations to adapt large language models (LLMs). Numerical fine-tuning is used for injecting factual knowledge, while symbolic updates control style and alignment without retraining. The framework employs a monitoring signal and a reward-based classifier to decide when to use LoRA for factual reconstruction and TextGrad for token-level edits. This approach is memory-efficient and produces high-quality, reusable prompts, especially beneficial in data-scarce domains like mathematical reasoning. Experiments show that neurosymbolic LoRA outperforms purely numerical or symbolic methods, demonstrating superior adaptability and performance.",153.48,Phi-4,Nvidia B200 (Cloud Native)
2601.12715v1_RSOD Reliability-Guided Sonar Image Object Detecti.pdf,RSOD: Reliability-Guided Sonar Image Object Detection with Extremely Limited Labels,"['Chengzhou Li', 'Ping Guo', 'Guanchen Meng', 'Qi Jia', 'Jinyuan Liu', 'Zhu Liu', 'Xiaokang Liu', 'Yu Liu', 'Zhongxuan Luo', 'Xin Fan']","This paper addresses the challenge of object detection in sonar images, which are characterized by fewer texture details and higher noise levels compared to natural images. The authors propose a teacher-student framework called RSOD to effectively learn from sonar images with extremely limited labeled data. RSOD calculates a reliability score based on the consistency of the teacher's predictions across different views and introduces an object mixed pseudo-label method to mitigate the shortage of labeled data. The student model is optimized using a reliability-guided adaptive constraint, allowing it to perform well even with minimal labeled data. The method demonstrates competitive performance on the UATD dataset using only 5% of labeled data, compared to a baseline trained on 100% labeled data. Additionally, a new dataset is collected to support further research in sonar image detection.",153.73,Phi-4,Nvidia B200 (Cloud Native)
2601.12720v1_Teaching Large Reasoning Models Effective Reflecti.pdf,Teaching Large Reasoning Models Effective Reflection,"['Hanbin Wang', 'Jingwei Song', 'Jinpeng Li', 'Qi Zhu', 'Fei Mi', 'Ganqu Cui', 'Yasheng Wang', 'Lifeng Shang']","This paper addresses the issue of superficial reflection in Large Reasoning Models (LRMs) by proposing two novel approaches: Self-Critique Fine-Tuning (SCFT) and Reinforcement Learning with Effective Reflection Rewards (RLERR). SCFT enhances reflective reasoning by prompting models to critique their own outputs and fine-tuning them using high-quality critiques. RLERR builds on SCFT by using these critiques to create reward signals for reinforcement learning, thereby improving the model's self-correction process. Experiments on benchmarks AIME2024 and AIME2025 demonstrate significant improvements in reasoning accuracy and reflection quality over state-of-the-art baselines.",152.87,Phi-4,Nvidia B200 (Cloud Native)
2601.12723v1_An Evolutionary Framework for Automatic Optimizati.pdf,An Evolutionary Framework for Automatic Optimization Benchmark Generation via Large Language Models,"['Yuhiro Ono', 'Tomohiro Harada', 'Yukiya Miura']","This paper introduces an evolutionary framework for automatic optimization benchmark generation using large language models (LLMs). The proposed LLM-driven evolutionary benchmark generator (LLM-EBG) leverages LLMs as generative operators to create and evolve benchmark problems within a flexible representation space. The framework aims to address the limitations of existing artificial benchmarks and the challenges of constructing benchmarks from real-world problems. As a case study, the authors generate unconstrained single-objective continuous minimization problems represented as mathematical expressions. These problems are designed to highlight performance differences between genetic algorithms (GA) and differential evolution (DE). Experimental results demonstrate that LLM-EBG successfully produces benchmarks where the target algorithm (GA) outperforms DE in over 80% of trials. Additionally, exploratory landscape analysis shows that benchmarks favoring GA exhibit high sensitivity to variable scaling, indicating that the framework can generate problems with distinct geometric characteristics that reflect the intrinsic search behaviors of different optimization algorithms.",154.4,Phi-4,Nvidia B200 (Cloud Native)
2601.12727v1_AI-exhibited Personality Traits Can Shape Human Se.pdf,AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations,"['Jingshu Li', 'Tianqi Song', 'Nattapat Boonprakong', 'Zicheng Zhu', 'Yitian Yang', 'Yi-Chieh Lee']","This study investigates how AI chatbots, specifically those based on Large Language Models like GPT-4o, can influence human self-concept through their exhibited personality traits. By engaging in conversations with an AI exhibiting specific personality traits, individuals' self-concepts tend to align with those traits, leading to a homogenization of self-concepts among different individuals. The research highlights the potential impact of AI personality traits on human self-perception and the importance of understanding these interactions.",154.93,Phi-4,Nvidia B200 (Cloud Native)
2601.12731v1_A Shared Geometry of Difficulty in Multilingual La.pdf,A Shared Geometry of Difficulty in Multilingual Language Models,"['Stefano Civelli', 'Pietro Bernardelle', 'Nicolò Brunello', 'Gianluca Demartini']","This paper investigates the multilingual geometry of problem-difficulty in large language models (LLMs) by training linear probes on the AMC subset of the Easy2Hard benchmark, translated into 21 languages. The study reveals that difficulty-related signals emerge at two distinct stages within the model's internal representations: shallow (early-layers) and deep (later-layers). Probes trained on deep representations show high accuracy within the same language but poor cross-lingual generalization. Conversely, probes trained on shallow representations generalize better across languages, albeit with lower within-language performance. These findings suggest that LLMs initially form a language-agnostic representation of problem difficulty, which later becomes language-specific. This aligns with existing research indicating that LLMs operate in an abstract conceptual space before producing language-specific outputs, extending this process to high-level meta-cognitive properties like problem-difficulty estimation.",154.44,Phi-4,Nvidia B200 (Cloud Native)
2601.12740v1_TreeWriter AI-Assisted Hierarchical Planning and W.pdf,TreeWriter: AI-Assisted Hierarchical Planning and Writing for Long-Form Documents,"['Zijian Zhang', 'Fangshi Du', 'Xingjian Liu', 'Pan Chen', 'Oliver Huang', 'Runlong Ye', 'Michael Liut', 'Alán Aspuru-Guzik']","TreeWriter is an AI-assisted hierarchical writing system designed to address the challenges of creating long-form documents. It represents documents as trees, allowing authors to create, save, and refine outlines at multiple levels. This facilitates drafting, understanding, and iterative editing. TreeWriter integrates a built-in AI agent that provides context-aware editing suggestions and supports document navigation. Studies show that TreeWriter enhances idea exploration, AI helpfulness, and authorial control compared to traditional tools like Google Docs. It also supports collaborative writing through its hierarchical organization. The paper provides design guidelines for future AI-assisted writing tools that balance automation with user agency.",154.35,Phi-4,Nvidia B200 (Cloud Native)
2601.12742v1_AirHunt Bridging VLM Semantics and Continuous Plan.pdf,AirHunt: Bridging VLM Semantics and Continuous Planning for Efficient Aerial Object Navigation,"['Xuecheng Chen', 'Zongzhuo Liu', 'Jianfa Ma', 'Bang Du', 'Tiantian Zhang', 'Xueqian Wang', 'Boyu Zhou']","AirHunt is an aerial object navigation system that integrates Vision-Language Models (VLMs) with continuous path planning to efficiently locate open-set objects in outdoor environments. It addresses the challenges of frequency mismatch between VLM inference and real-time planning, and limited 3D scene understanding by using a dual-pathway asynchronous architecture. This architecture synergizes VLM reasoning with path planning, allowing adaptive semantic guidance. Additionally, AirHunt includes an active dual-task reasoning module for selective VLM querying and a semantic-geometric coherent planning module to balance semantic priorities and motion efficiency. The system demonstrates higher success rates, lower navigation errors, and reduced flight times compared to existing methods, validated through diverse tasks and real-world experiments.",154.33,Phi-4,Nvidia B200 (Cloud Native)
2601.12744v1_Vision Language Models for Optimization-Driven Int.pdf,Vision Language Models for Optimization-Driven Intent Processing in Autonomous Networks,"['Tasnim Ahmed', 'Yifan Zhu', 'Salimur Choudhury']","This paper explores the use of Vision-Language Models (VLMs) for generating optimization code in Intent-Based Networking (IBN) systems. It presents a benchmark, IntentOpt, consisting of 85 optimization problems across 17 categories, to evaluate the performance of four VLMs under different prompting strategies. The study finds that visual parameter extraction and program-of-thought prompting can reduce the success rate of code generation. It also highlights the performance gap between open-source and closed-source models. The paper demonstrates the practical feasibility of deploying VLM-generated code in network testbeds using the Model Context Protocol.",154.7,Phi-4,Nvidia B200 (Cloud Native)
2601.12745v1_A Graph Prompt Fine-Tuning Method for WSN Spatio-T.pdf,A Graph Prompt Fine-Tuning Method for Spatio-Temporal Correlation Anomaly Detection,"['Miao Ye', 'Jing Cui', 'Yuan Huang', 'Yong Wang', 'Qian He', 'Jiwen Zhang']","This paper addresses anomaly detection in multi-temporal modal data within Wireless Sensor Networks (WSNs), focusing on the challenges of extracting spatio-temporal correlation features, high annotation costs, and sample imbalance. The authors propose a novel graph neural network backbone that integrates a multi-scale strategy and inter-modal fusion with a variational graph convolution module to enhance feature extraction. A multi-task self-supervised training strategy, termed 'pre-training - graph prompting - fine-tuning,' is designed to leverage the WSN graph structure data. The method includes a 'pre-training' phase with no-negative comparative learning, prediction, and reconstruction to learn from unlabeled data, followed by a 'graph prompting-fine-tuning' phase to refine the model. Experimental results on public and actual datasets demonstrate superior performance with F1 scores of 91.30% and 92.31%, respectively, outperforming existing methods.",153.66,Phi-4,Nvidia B200 (Cloud Native)
2601.12754v1_PAIR-SAFE A Paired-Agent Approach for Runtime Audi.pdf,PAIR-SAFE: A Paired-Agent Approach for Runtime Auditing and Refining AI-Mediated Mental Health Support,"['Jiwon Kim', 'Violeta J. Rodriguez', 'Dong Whi Yoo', 'Eshwar Chandrasekharan', 'Koustuv Saha']","The paper introduces PAIR-SAFE, a paired-agent framework designed to audit and refine AI-generated mental health support. It integrates a Responder agent with a supervisory Judge agent, using the Motivational Interviewing Treatment Integrity (MITI-4) framework. The Judge agent audits responses and provides structured decisions to guide runtime refinement. Simulated counseling interactions show significant improvements in key MITI dimensions, supported by qualitative expert evaluation. This approach aims to enhance the clinical grounding and safety of AI-assisted mental health support.",153.06,Phi-4,Nvidia B200 (Cloud Native)
2601.12758v1_VISPA Pluralistic Alignment via Automatic Value Se.pdf,VISPA: Pluralistic Alignment via Automatic Value Selection and Activation,"['Shenyan Zheng', 'Jiayou Zhong', 'Anudeex Shetty', 'Heng Ji', 'Preslav Nakov', 'Usman Naseem']","The paper introduces VISPA, a framework for pluralistic alignment in large language models (LLMs) that allows for dynamic control over value expression through internal model activation steering. This approach addresses the challenge of representing diverse perspectives in high-stakes domains such as healthcare, law, and education. VISPA is shown to be effective across various models and settings, offering a scalable solution for achieving pluralistic alignment without relying on prompt-level interventions. The framework demonstrates adaptability with different steering initiations, models, and values, suggesting a path toward LLMs that can serve a broader range of perspectives.",153.98,Phi-4,Nvidia B200 (Cloud Native)
2601.12762v1_Teaching LLMs to Learn Tool Trialing and Execution.pdf,Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction,"['Xingjie Gao', 'Pengcheng Huang', 'Zhenghao Liu', 'Yukun Yan', 'Shuo Wang', 'Zulong Chen', 'Chen Qian', 'Ge Yu', 'Yu Gu']","This paper introduces ToolMaster, a framework designed to enhance the tool-use capabilities of Large Language Models (LLMs) by shifting from static solution paths to active learning through environment interaction. ToolMaster employs a trial-and-execution paradigm, where LLMs learn tool usage by imitating teacher-generated trajectories and engaging in reinforcement learning to coordinate trial and execution phases. This approach allows LLMs to autonomously explore and correct tool usage, significantly improving their generalization and robustness when dealing with novel or unfamiliar tools. Experimental results show that ToolMaster outperforms existing baselines in these aspects.",154.05,Phi-4,Nvidia B200 (Cloud Native)
2601.12781v1_VIRO Robust and Efficient Neuro-Symbolic Reasoning.pdf,VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension,"['Hyejin Park', 'Junhyuk Kwon', 'Suha Kwak', 'Jungseul Ok']","The paper introduces Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework designed to enhance the robustness and efficiency of Referring Expression Comprehension (REC). VIRO addresses the issue of cascading errors in existing neuro-symbolic approaches by embedding lightweight verifiers within reasoning steps. These verifiers validate outputs such as object existence or spatial relationships, allowing the system to handle cases where no target is present more effectively. The framework achieves state-of-the-art performance, demonstrating strong generalization to real-world data, high computational efficiency, and low program failure rates.",154.34,Phi-4,Nvidia B200 (Cloud Native)
2601.12785v1_Distilling Time Series Foundation Models for Effic.pdf,DISTILLING TIME SERIES FOUNDATION MODELS FOR EFFICIENT FORECASTING,"['Yuqi Li', 'Kuiye Ding', 'Chuanguang Yang', 'Szu-Yu Chen', 'Yingli Tian']","This paper introduces DistilTS, a novel distillation framework specifically designed for Time Series Foundation Models (TSFMs). TSFMs, known for their strong forecasting performance, suffer from large parameter sizes that hinder deployment. DistilTS addresses two main challenges: task difficulty discrepancy and architecture discrepancy. It introduces horizon-weighted objectives to balance learning across different forecasting horizons and a temporal alignment strategy to reduce architectural mismatch. Experiments demonstrate that DistilTS achieves forecasting performance comparable to full-sized TSFMs while significantly reducing parameters by up to 1/150 and accelerating inference by up to 6000×.",154.5,Phi-4,Nvidia B200 (Cloud Native)
2601.12804v1_SL-CBM Enhancing Concept Bottleneck Models with Se.pdf,SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability,"['Hanwei Zhang', 'Luo Cheng', 'Rui Wen', 'Yang Zhang', 'Lijun Zhang', 'Holger Hermanns']","This paper introduces SL-CBM, an enhancement to Concept Bottleneck Models (CBMs) aimed at improving interpretability through semantic locality. SL-CBM addresses the issue of poor locality faithfulness in existing CBMs by generating spatially coherent saliency maps at both concept and class levels. It incorporates a 1×1 convolutional layer and a cross-attention mechanism to better align concepts with image regions and predictions. The proposed method demonstrates improved locality faithfulness, explanation quality, and intervention efficacy while maintaining competitive classification accuracy. The paper highlights the importance of contrastive and entropy-based regularization in achieving a balance between accuracy, sparsity, and faithfulness. SL-CBM sets a new standard for interpretable and trustworthy concept-based models.",153.71,Phi-4,Nvidia B200 (Cloud Native)
2601.12805v1_SciHorizon-GENE Benchmarking LLM for Life Sciences.pdf,SciHorizon-Gene: Benchmarking LLM for Life Sciences Inference from Gene Knowledge to Functional Understanding,"['Xiaohan Huang', 'Meng Xiao', 'Chuan Qin', 'Qingqing Long', 'Jinmiao Chen', 'Yuanchun Zhou', 'Hengshu Zhu']","This paper introduces SciHorizon-Gene, a large-scale gene-centric benchmark designed to evaluate the ability of large language models (LLMs) to reason from gene-level knowledge to functional understanding. The benchmark, constructed from authoritative biological databases, includes over 540K questions related to gene-to-function reasoning scenarios. It assesses LLMs across four critical perspectives: research attention sensitivity, hallucination tendency, answer completeness, and literature influence. The study reveals significant variability in gene-level reasoning capabilities among different LLMs and highlights challenges in generating accurate, complete, and literature-grounded interpretations. The benchmark provides a foundation for analyzing LLM behavior at the gene level and offers insights for model selection and development in biological interpretation.",154.09,Phi-4,Nvidia B200 (Cloud Native)
2601.12809v1_Left-Right Symmetry Breaking in CLIP-style Vision-.pdf,Left–Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data,"['Takaki Yamamoto', 'Chihiro Noguchi', 'Toshihiro Tanizawa']","This paper investigates how left-right relational understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. Using a controllable 1D image-text testbed, the authors explore the impact of label and layout diversity on generalization to unseen object pairs. They find that contrastive training effectively learns left-right relations, with label diversity being a key factor for generalization. Mechanistic insights are provided through attention decomposition, revealing that interactions between positional and token embeddings create a horizontal attention gradient that breaks left-right symmetry. Ablating this gradient significantly reduces left-right discrimination, offering insights into the acquisition of relational competence in CLIP-style models.",154.74,Phi-4,Nvidia B200 (Cloud Native)
2601.12816v1_Fisher-Orthogonal Projected Natural Gradient Desce.pdf,Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning,"['Ishir Garg', 'Neel Kolhe', 'Andy Peng', 'Rohan Gopalam']","The paper introduces the Fisher-Orthogonal Projected Natural Gradient Descent (FOPNG) optimizer to address the challenge of catastrophic forgetting in continual learning. FOPNG enforces Fisher-orthogonal constraints on parameter updates, preserving performance on previously learned tasks while learning new ones. Unlike existing methods that operate in Euclidean parameter space, FOPNG projects gradients onto the Fisher-orthogonal complement of previous task gradients, unifying natural gradient descent with orthogonal gradient methods within an information-geometric framework. This approach ensures reparameterization invariance, guarantees descent in the Fisher metric, and helps maintain prior task outputs. The paper provides theoretical analysis, practical implementations using the diagonal Fisher, and demonstrates strong results on standard continual learning benchmarks.",154.39,Phi-4,Nvidia B200 (Cloud Native)
2601.12822v1_MirrorGuard Toward Secure Computer-Use Agents via .pdf,MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real Reasoning Correction,"['Wenqi Zhang', 'Yulin Shen', 'Changyue Jiang', 'Jiarun Dai', 'Geng Hong', 'Xudong Pan*']","This paper introduces MirrorGuard, a defense framework designed to enhance the security of Computer Use Agents (CUAs) by using simulation-based training to correct unsafe reasoning patterns. The framework employs a neural-symbolic simulation pipeline to generate realistic GUI interaction trajectories in a text-based environment, capturing unsafe reasoning without executing real operations. MirrorGuard effectively intercepts and rectifies insecure reasoning chains in CUAs, significantly reducing security risks in real-world applications. Extensive evaluations demonstrate its effectiveness, notably reducing the unsafe rate on the ByteDance UI-TARS system from 66.5% to 13.0%, while maintaining a low false refusal rate. The study highlights the potential of simulation-derived defenses to provide robust protection while preserving agent utility.",154.2,Phi-4,Nvidia B200 (Cloud Native)
2601.12837v1_Cognition spaces natural artificial and hybrid.pdf,"Cognition spaces: natural, artificial, and hybrid","['Ricard Solé', 'Luis F Seoane', 'Jordi Pla-Mauri', 'Michael Timothy Bennett', 'Michael E. Hochberg', 'Michael Levin']","The paper proposes a cognition space approach to compare cognitive processes across natural, artificial, and hybrid systems. It introduces three cognition spaces—basal aneural, neural, and human–AI hybrid—and discusses the uneven occupation of these spaces, attributing the voids to evolutionary contingencies, physical constraints, and design limitations. The approach emphasizes the diversity of cognitive systems and highlights hybrid cognition as a promising area for exploring novel forms of complexity beyond biological evolution.",154.28,Phi-4,Nvidia B200 (Cloud Native)
2601.12842v1_SCULPT Constraint-Guided Pruned MCTS that Carves E.pdf,SCULPT: Constraint-Guided Pruned MCTS that Carves Efficient Paths for Mathematical Reasoning,"['Qitong Fang', 'Haotian Li', 'Xu Wang']","This paper introduces SCULPT, a constraint-guided approach for Monte Carlo Tree Search (MCTS) that enhances the problem-solving ability of large language models (LLMs) by integrating domain-aware scoring into the search process. SCULPT addresses the inefficiencies of stochastic exploration by scoring and pruning actions based on symbolic checks and structural pattern guidance, steering the search towards plausible reasoning paths. The approach demonstrates stable improvements across multiple datasets and assesses executor transferability and performance on advanced reasoning models. By incorporating domain-specific constraints, SCULPT improves accuracy, efficiency, and reasoning stability in mathematical problem-solving.",154.28,Phi-4,Nvidia B200 (Cloud Native)
2601.12849v1_The Cost of EFX Generalized-Mean Welfare and Compl.pdf,The Cost of EFX: Generalized-Mean Welfare and Complexity Dichotomies with Few Surplus Items,"['Eugene Lim', 'Tzeh Yuan Neoh', 'Nicholas Teh']","This paper explores the interaction between envy-freeness up to any good (EFX) and generalized-mean welfare in the context of allocating indivisible goods with few surplus items. It establishes complexity dichotomies for different welfare objectives, showing NP-hardness for certain p-values and polynomial-time solutions for others. The study also examines the welfare loss due to EFX and the computational challenges of combining EFX with Pareto-optimality.",153.81,Phi-4,Nvidia B200 (Cloud Native)
2601.12856v1_Mining Citywide Dengue Spread Patterns in Singapor.pdf,Mining Citywide Dengue Spread Patterns in Singapore Through Hotspot Dynamics from Open Web Data,"['Liping Huang', 'Gaoxi Xiao', 'Stefan Ma', 'Hechang Chen', 'Shisong Tang', 'Flora Salim']","This study introduces a novel framework to uncover and exploit latent transmission links between urban regions for dengue spread, using publicly available case data. The framework models hotspot formation influenced by epidemic dynamics in neighboring regions, aligning with human commuting flows. It optimizes hidden links through gradient descent for forecasting hotspot status and verifying spreading patterns. Case studies from Singapore (2013–2018, 2020) demonstrate the model's robustness, achieving high F-scores even during COVID-19 mobility disruptions. The work transforms open web-based case data into a predictive and explanatory resource, advancing epidemic modeling and providing a scalable tool for public health planning.",153.87,Phi-4,Nvidia B200 (Cloud Native)
2601.12879v1_Hierarchical Sparse Circuit Extraction from Billio.pdf,Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition,"['Mohammed Mudassir Uddin', 'Shahnawaz Alam', 'Mohammed Kaif Pasha']","The paper introduces the Hierarchical Attribution Graph Decomposition (HAGD) framework to address the challenge of extracting sparse computational circuits from billion-parameter language models. The framework reduces the complexity of circuit discovery from exponential to polynomial time through multi-resolution abstraction hierarchies and differentiable circuit search. It integrates cross-layer transcoders, graph neural network meta-learning, and causal intervention protocols. The methodology is empirically evaluated on GPT-2 variants and Llama models, achieving high behavioral preservation and interpretable subgraph sizes. The results suggest shared computational patterns across model families and highlight limitations in current attribution methodologies, providing a foundation for future interpretability research.",154.28,Phi-4,Nvidia B200 (Cloud Native)
2601.12882v1_YOLO26 An Analysis of NMS-Free End to End Framewor.pdf,YOLO26: AN ANALYSIS OF NMS-FREE END-TO-END FRAMEWORK FOR REAL-TIME OBJECT DETECTION,['Sudip Chakrabarty'],"This paper analyzes YOLO26, a new architecture in the YOLO framework that eliminates Non-Maximum Suppression (NMS) in favor of an end-to-end learning strategy. Key innovations include the MuSGD optimizer for stabilizing lightweight backbones, STAL for small-target-aware assignment, and ProgLoss for dynamic supervision. YOLO26 outperforms previous versions and competitors like RTMDet and DAMO-YOLO in both inference speed and detection accuracy, resolving the trade-off between latency and precision in real-time object detection.",153.91,Phi-4,Nvidia B200 (Cloud Native)
2601.12886v1_Communication Methods in Multi-Agent Reinforcement.pdf,Communication Methods in Multi-Agent Reinforcement Learning,['Christoph Wittner'],"This paper provides an overview of communication techniques in multi-agent reinforcement learning (MARL). It analyzes 29 publications to evaluate the strengths and weaknesses of various communication methods, including explicit, implicit, attention-based, graph-based, and hierarchical/role-based communication. The study concludes that there is no universally optimal communication framework, as the choice depends on the specific problem. It also emphasizes the need for communication methods with low computational overhead for scalability and highlights current research gaps, such as the need for standardized benchmarking and improved robustness under realistic conditions to enhance real-world applicability.",158.45,Phi-4,Nvidia B200 (Cloud Native)
2601.12893v1_AdaNODEs Test Time Adaptation for Time Series Fore.pdf,ADANODES: TEST TIME ADAPTATION FOR TIME SERIES FORECASTING USING NEURAL ODES,"['Ting Dang', 'Soumyajit Chatterjee', 'Hong Jia', 'Yu Wu', 'Flora Salim', 'Fahim Kawsar']","This paper introduces AdaNODEs, a source-free test time adaptation (TTA) method specifically designed for time series forecasting. By utilizing Neural Ordinary Differential Equations (NODEs), AdaNODEs addresses the unique challenges of distribution shifts in time series data. The method proposes a novel loss function tailored for forecasting tasks and updates only a limited number of model parameters, effectively capturing temporal dependencies while minimizing memory usage. Experimental results show that AdaNODEs achieves relative improvements of 5.88% and 28.4% over state-of-the-art baselines, particularly excelling in scenarios with higher severity distribution shifts.",158.03,Phi-4,Nvidia B200 (Cloud Native)
2601.12904v1_From Prefix Cache to Fusion RAG Cache Accelerating.pdf,From Prefix Cache to Fusion RAG Cache: Accelerating LLM Inference in Retrieval-Augmented Generation,"['JIAHAO WANG', 'WEIYU XIE', 'MINGXING ZHANG', 'BOXING ZHANG', 'JIANWEI DONG', 'YUENING ZHU', 'CHEN LIN', 'JINQI TANG', 'YAOCHEN HAN', 'ZHIYUAN AI', 'XIANGLIN CHEN', 'YONGWEI WU', 'CONGFENG JIANG']","This paper introduces FusionRAG, a novel inference framework designed to optimize the preprocessing and reprocessing stages of Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs). The framework addresses the challenge of maintaining generation quality while reducing computational costs and Time to First Token (TTFT) by embedding cross-chunk contextual information during preprocessing and selectively recomputing KVCache during reprocessing. Experimental results demonstrate that FusionRAG significantly improves generation quality and efficiency, achieving up to 70% higher normalized-F1 scores and reducing TTFT by 2.66-9.39× compared to Full Attention, with less than 15% of tokens recomputed.",156.62,Phi-4,Nvidia B200 (Cloud Native)
2601.12910v1_SciCoQA Quality Assurance for Scientific Paper--Co.pdf,SCICOQA: Quality Assurance for Scientific Paper–Code Alignment,"['Tim Baumgärtner', 'Iryna Gurevych']","SCICOQA is a dataset designed to detect discrepancies between scientific publications and their associated codebases, ensuring faithful implementations. The dataset is constructed from GitHub issues and reproducibility papers, with a synthetic data generation method to scale the dataset. It includes 611 paper-code discrepancies, both real and synthetic, across various computational science disciplines. The evaluation of 21 Large Language Models (LLMs) reveals the challenges in detecting these discrepancies, with GPT-5 achieving a 45.7% detection rate for real-world cases. The study highlights the importance of aligning scientific text with code to address the reproducibility crisis in science.",157.85,Phi-4,Nvidia B200 (Cloud Native)
2601.12912v1_Human Emotion Verification by Action Languages via.pdf,Human Emotion Verification by Action Languages via Answer Set Programming,"['ANDREAS BR ¨ANNSTR ¨OM', 'JUAN CARLOS NIEVES']","This paper introduces the action language C-MT (Mind Transition Language), which is built on answer set programming (ASP) and transition systems to model the evolution of human mental states in response to observable actions. It formalizes mental states, such as emotions, as multi-dimensional configurations, drawing on psychological theories like the Appraisal Theory of Emotion. The language is extended with a novel causal rule, 'forbids to cause', and expressions for mental state dynamics, enabling the modeling of valid transitions between mental states. These principles are evaluated using transition systems and trajectories, supporting controlled reasoning about the dynamic evolution of human mental states. The framework allows for comparing different dynamics of change by analyzing trajectories adhering to various psychological principles. The action language is applied to design models for emotion verification, with applications in health and wellbeing, such as depression support and therapy.",158.01,Phi-4,Nvidia B200 (Cloud Native)
2601.12913v1_Actionable Interpretability Must Be Defined in Ter.pdf,Position: Actionable Interpretability Must Be Defined in Terms of Symmetries,"['Pietro Barbiero', 'Mateo Espinosa Zarlenga', 'Francesco Giannini', 'Alberto Termine', 'Filippo Bonchi', 'Mateja Jamnik', 'Giuseppe Marra']","This paper argues that existing definitions of interpretability in AI are not actionable because they lack formal principles for deriving concrete modeling and inferential rules. The authors propose that interpretability should be defined in terms of symmetries, positing that four specific symmetries can formalize interpretability principles. These include inference equivariance, information invariance, concept-closure invariance, and structural invariance. The paper suggests that these symmetries can motivate core interpretability properties, characterize interpretable models, and provide a unified formulation of interpretable inference.",157.5,Phi-4,Nvidia B200 (Cloud Native)
2601.12922v1_Your Privacy Depends on Others Collusion Vulnerabi.pdf,Your Privacy Depends on Others: Collusion Vulnerabilities in Individual Differential Privacy,"['Johannes Kaiser', 'Alexander Ziller', 'Eleni Triantafillou', 'Daniel Rückert', 'Georgios Kaissis']","This paper identifies a vulnerability in individual differential privacy (iDP) mechanisms, where an individual's privacy risk is influenced by the privacy choices of all data contributors, not just their own. This creates a discrepancy between the promise of individual privacy control and the collective determination of risk. The authors demonstrate that certain distributions of privacy preferences can unintentionally increase privacy risks, even when formal guarantees are met. They propose a new privacy contract, (εi, δi, ∆)-iDP, to provide users with a hard upper bound on excess vulnerability while allowing flexibility in mechanism design. The findings highlight the need for a re-evaluation of iDP systems to make excess risks transparent and controllable.",157.96,Phi-4,Nvidia B200 (Cloud Native)
2601.12925v1_ForeDiffusion Foresight-Conditioned Diffusion Poli.pdf,ForeDiffusion: Foresight-Conditioned Diffusion Policy via Future View Construction for Robot Manipulation,"['Weize Xie', 'Yi Ding', 'Ying He', 'Leilei Wang', 'Binwen Bai', 'Zheyi Zhao', 'Chenyang Wang', 'F. Richard Yu']","This paper introduces ForeDiffusion, a novel approach to robot manipulation that addresses limitations in existing diffusion strategies by incorporating predicted future view representations into the diffusion process. This foresight-conditioned method aims to improve the success rate of complex tasks by guiding the policy to be forward-looking and correcting trajectory deviations. ForeDiffusion employs a dual loss mechanism, combining traditional denoising loss with a consistency loss for future observations, resulting in a unified optimization. Extensive evaluations demonstrate that ForeDiffusion significantly outperforms existing methods, achieving an average success rate of 80% across tasks, with a notable 23% improvement in complex tasks.",157.76,Phi-4,Nvidia B200 (Cloud Native)
2601.12929v1_Membership Inference Test Auditing Training Data i.pdf,Membership Inference Test: Auditing Training Data in Object Classification Models,"['Gonzalo Mancera', 'Daniel DeAlcala', 'Aythami Morales', 'Ruben Tolosana', 'Julian Fierrez']","This research investigates the performance of Membership Inference Tests (MINT) in the context of object recognition models. The study proposes and develops specialized architectures for MINT models to optimize performance and efficiency in data utilization. Experiments were conducted using an object detection model, an embedding extractor, and a MINT module across three public databases with over 174K images. The proposed architecture employs convolutional layers to capture activation patterns during training, achieving precision rates between 70% and 80% depending on the detection module layer depth. The study also analyzes factors influencing the MINT module and the transparency of training processes, addressing ethical and legal challenges in AI deployment.",157.76,Phi-4,Nvidia B200 (Cloud Native)
2601.12931v1_Online Continual Learning for Time Series a Natura.pdf,ONLINE CONTINUAL LEARNING FOR TIME SERIES: A NATURAL SCORE-DRIVEN APPROACH,"['Edoardo Urettini', 'Daniele Atzeni', 'Ioanna-Yvonni Tsaknaki', 'Antonio Carta']","This paper explores the application of online continual learning (OCL) methods to online time series forecasting (OTSF), addressing the need for models that can adapt rapidly to changing environments while retaining long-term memory. The authors reframe neural network optimization as a parameter filtering problem, demonstrating that natural gradient descent is a score-driven method with information-theoretic optimality. They introduce a robust optimizer using a Student’s t likelihood and natural gradient, which provides bounded updates to enhance robustness against outliers. The paper also presents Natural Score-driven Replay (NatSR), combining the robust optimizer with a replay buffer and a dynamic scale heuristic to improve adaptation during regime shifts. Empirical results show that NatSR outperforms more complex state-of-the-art methods in forecasting tasks.",157.87,Phi-4,Nvidia B200 (Cloud Native)
2601.12937v1_On the Evidentiary Limits of Membership Inference .pdf,On the Evidentiary Limits of Membership Inference for Copyright Auditing,"['Murat Bilgehan Ertan', 'Emirhan Boge', 'Min Chen', 'Kaleel Mahmood', 'Marten van Dijk']","The paper explores the reliability of membership inference attacks (MIAs) for auditing copyrighted texts in large language models (LLMs) under adversarial conditions. It introduces SAGE, a paraphrasing framework that alters lexical structure while preserving semantic content, to test the robustness of MIAs. The findings suggest that MIAs are not robust to semantics-preserving transformations and are insufficient as standalone mechanisms for copyright auditing in adversarial settings.",158.14,Phi-4,Nvidia B200 (Cloud Native)
2601.12938v1_The Post-Turing Condition Conceptualising Artifici.pdf,The Post-Turing Condition: Conceptualising Artificial Subjectivity and Synthetic Sociality,"['Thorsten Jelinek', 'Patrick Glauner', 'Alvin Wang Graylin', 'Yubao Qiu']","This paper explores the impact of artificial intelligence in the Post-Turing era, where AI influences social coordination and meaning formation beyond mere cognitive task automation. It introduces the PRMO framework, which connects AI design to human subjectivity dimensions: Perception, Representation, Meaning, and the Real. The concept of Synthetic Sociality is discussed as a future where AI agents manage social coherence, potentially excluding humans from meaning formation. To mitigate this risk, the paper proposes Quadrangulation as a design principle, ensuring AI systems consider humans as integral to shared meaning contexts. This work provides a structural vocabulary for analyzing AI's role at the intersection of computation and society.",158.9,Phi-4,Nvidia B200 (Cloud Native)
2601.12939v1_Active Inference-Driven World Modeling for Adaptiv.pdf,ACTIVE INFERENCE-DRIVEN WORLD MODELING FOR ADAPTIVE UAV SWARM TRAJECTORY DESIGN,"['Kaleem Arshid', 'Ali Krayani', 'Lucio Marcenaro', 'David Martin Gomez', 'Carlo Regazzoni']","This paper introduces an Active Inference-based framework for autonomous trajectory design in UAV swarms. The method integrates probabilistic reasoning and self-learning to enable distributed mission allocation, route ordering, and motion planning. Expert trajectories generated using a Genetic Algorithm with Repulsion Forces (GA–RF) are employed to train a hierarchical World Model capturing swarm behavior across mission, route, and motion levels. During online operation, UAVs infer actions by minimizing divergence between current beliefs and model-predicted states, enabling adaptive responses to dynamic environments. Simulation results show faster convergence, higher stability, and safer navigation than Q-Learning, demonstrating the scalability and cognitive grounding of the proposed framework for intelligent UAV swarm control.",158.78,Phi-4,Nvidia B200 (Cloud Native)
2601.12946v1_AI-generated data contamination erodes pathologica.pdf,AI-generated data contamination erodes pathological variability and diagnostic reliability,"['Hongyu He', 'Shaowen Xiang', 'Ye Zhang', 'Yingtao Zhu', 'Jin Zhang', 'Hao Deng', 'Emily Alsentzer', 'Qingyu Chen', 'Kun-Hsing Yu', 'Andrew Marmenshall', 'Tingting Chen', 'Srinivas Anumasa', 'Daniel Ebner', 'Dean Ho', 'Kee Yuan Ngiam', 'Ching-Yu Cheng', 'Dianbo Liu']","The study investigates the impact of AI-generated data contamination on pathological variability and diagnostic reliability in medical records. It reveals that without human verification, AI models converge towards generic phenotypes, eroding critical diagnostic features and skewing demographic representations. This leads to false diagnostic confidence, with models failing to detect life-threatening conditions. Blinded physician evaluations confirm the clinical uselessness of AI-generated documentation after two generations. The study evaluates mitigation strategies, finding that mixing real data with quality-aware filtering preserves diversity. The findings highlight the need for policy-mandated human oversight to prevent the degradation of healthcare data ecosystems by generative AI.",158.67,Phi-4,Nvidia B200 (Cloud Native)
2601.12951v1_Beyond Accuracy Characterizing Code Comprehension .pdf,Beyond Accuracy: Characterizing Code Comprehension Capabilities in (Large) Language Models,"['Felix Mächtle', 'Jan-Niclas Serr', 'Nils Loose', 'Thomas Eisenbarth']","This paper explores the alignment of Large Language Models' (LLMs) code-comprehension performance with traditional human-centric software metrics. It introduces a diagnostic framework to evaluate LLMs as binary input-output consistency tasks, correlating model performance with metrics like lexical size, control-flow complexity, and abstract syntax tree structure. The study finds minimal correlation between human-defined metrics and LLM success, suggesting that LLM comprehension reflects model-specific regularities. The findings emphasize the need for benchmark methodologies that focus on instance-level diagnostics and acknowledge the limits in predicting correct outcomes.",157.41,Phi-4,Nvidia B200 (Cloud Native)
2601.13007v1_ArchAgent Scalable Legacy Software Architecture Re.pdf,ARCHAGENT: SCALABLE LEGACY SOFTWARE ARCHITECTURE RECOVERY WITH LLMS,"['Rusheng Pan', 'Bingcheng Mao', 'Tianyi Ma', 'Zhenhua Ling']","ArchAgent is a scalable agent-based framework designed to recover accurate software architecture from large-scale legacy systems. It addresses challenges such as architectural drift, missing relationships, and the limited context of Large Language Models (LLMs) by combining static analysis, adaptive code segmentation, and LLM-powered synthesis. The framework reconstructs multiview, business-aligned architectures from cross-repository codebases, introducing scalable diagram generation with contextual pruning and integrating cross-repository data to identify business-critical modules. Evaluations on large-scale GitHub projects demonstrate significant improvements over existing benchmarks, and an ablation study confirms the importance of dependency context for accuracy. A real-world case study further illustrates the effective recovery of critical business logics from legacy projects.",154.95,Phi-4,Nvidia B200 (Cloud Native)
2601.13013v1_HT-GNN Hyper-Temporal Graph Neural Network for Cus.pdf,HT-GNN: Hyper-Temporal Graph Neural Network for Customer Lifetime Value Prediction in Baidu Ads,"['Xiaohui Zhao', 'Xinjian Zhao', 'Jiahui Zhang', 'Guoyu Liu', 'Houzhi Wang', 'Shu Wu']","The paper introduces a Hyper-Temporal Graph Neural Network (HT-GNN) designed to address challenges in predicting Customer Lifetime Value (LTV) for advertising platforms. It focuses on demographic heterogeneity and temporal dynamics, using a hypergraph-supervised module, a transformer-based temporal encoder, and a task-adaptive mixture-of-experts for multi-horizon LTV forecasting. Experiments on Baidu Ads with 15 million users show HT-GNN's superior performance over existing methods.",157.67,Phi-4,Nvidia B200 (Cloud Native)
2601.13018v1_Bi-Attention HateXplain  Taking into account the s.pdf,Bi-Attention HateXplain: Taking into account the sequential aspect of data during explainability in a multi-task context,['Ghislain Dorian Tchuente Mondjo'],"The paper addresses the challenge of hate speech detection and the need for explainability in AI models. It critiques the variability in attention mechanisms of existing models and proposes a new model, BiAtt-BiRNN-HateXplain, which incorporates a BiRNN layer to better handle the sequential nature of data. This model aims to improve detection performance, explainability, and reduce unintentional bias. The experimental results on HateXplain data demonstrate improvements in these areas.",158.22,Phi-4,Nvidia B200 (Cloud Native)
2601.13020v1_PASs-MoE Mitigating Misaligned Co-drift among Rout.pdf,PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway Activation Subspaces for Continual Learning,"['Zhiyan Hou', 'Haiyun Guo', 'Haokai Ma', 'Yandu Sun', 'Yonghui Yang', 'Jinqiao Wang']","This paper addresses the issue of Misaligned Co-drift in Mixture-of-Experts (MoE) methods for Continual Instruction Tuning (CIT) of Multimodal Large Language Models (MLLMs). The authors introduce the Pathway Activation Subspace (PASs) to mitigate the co-drift between router preferences and expert adaptation pathways. PASs provide a capability-aligned coordinate system for routing and preservation, enhancing both accuracy and anti-forgetting in continual learning without adding parameters. The proposed PASs-based MoE–LoRA method includes PAS-guided Reweighting and PAS-aware Rank Stabilization, showing superior performance over conventional baselines and MoE–LoRA variants.",156.88,Phi-4,Nvidia B200 (Cloud Native)
2601.13048v1_Analysis of Long Range Dependency Understanding in.pdf,ANALYSIS OF LONG RANGE DEPENDENCY UNDERSTANDING IN STATE SPACE MODELS,"['Srividya Ravikumar', 'Abhinav Anand', 'Shweta Verma', 'Mira Mezini']","This paper presents a systematic kernel interpretability study of the diagonalized state-space model (S4D) trained on a real-world task of vulnerability detection in source code. Through time and frequency domain analysis, it is shown that the long-range modeling capability of S4D varies significantly under different model architectures, affecting performance. The study reveals that the S4D kernel can behave as a low-pass, band-pass, or high-pass filter depending on the architecture. These insights can guide future work in designing better S4D-based models.",158.49,Phi-4,Nvidia B200 (Cloud Native)
2601.13054v1_TinyML-Enabled IoT for Sustainable Precision Irrig.pdf,TinyML-Enabled IoT for Sustainable Precision Irrigation,"['Kamogelo Taueatsoala', 'Caitlyn Daniels', 'Angelina J. Ramsunar', 'Petrus Bronkhorst', 'Absalom E. Ezugwu']","This paper introduces a novel IoT framework that integrates Tiny Machine Learning (TinyML) for precision irrigation, targeting small-scale farming communities affected by water scarcity and limited access to advanced technologies. The framework employs a four-layer architecture using low-cost hardware, including an ESP32 microcontroller and a Raspberry Pi, to facilitate autonomous decision-making without cloud dependency. Environmental monitoring is achieved through various sensors, and a Gradient Boosting model is identified as the most effective for predicting irrigation needs with high accuracy. The system's local communication is managed via an MQTT-based LAN protocol, ensuring functionality in areas with limited internet access. Experimental validation shows significant water usage reduction compared to traditional methods, highlighting the system's potential for sustainable deployment in resource-constrained rural settings.",158.67,Phi-4,Nvidia B200 (Cloud Native)
2601.13060v1_MagicGUI-RMS A Multi-Agent Reward Model System for.pdf,MAGICGUI-RMS: A MULTI-AGENT REWARD MODEL SYSTEM FOR SELF-EVOLVING GUI AGENTS VIA AUTOMATED FEEDBACK REFLUX,"['Zecheng Li', 'Zhihui Cao', 'Wenke Huang', 'Yudong Zhang', 'Keying Qi', 'Rui Wang', 'Zeyu Zheng', 'Jian Zhao', 'Hao Zhu', 'Hengxin Wu', 'Yuran Wang', 'Guitao Fan', 'Guokun Wu', 'Yicong Liu', 'Zhilin Gao', 'Haikun Xu', 'He Yang', 'Minqi Xiang', 'Xingyu Liu', 'Zuojian Wang']","MagicGUI-RMS is a multi-agent reward model system designed to enhance the autonomous interaction and task execution capabilities of GUI agents. It addresses challenges in automating trajectory evaluation and generating high-quality training data by integrating a Domain-Specific Reward Model (DS-RM) with a General-Purpose Reward Model (GP-RM). This integration allows for fine-grained action assessment and robust generalization across various GUI tasks. The system supports reward learning at scale through a structured data construction pipeline that automatically produces balanced and diverse reward datasets, reducing annotation costs while maintaining sample fidelity. During execution, MagicGUI-RMS identifies erroneous actions, proposes refined alternatives, and continuously enhances agent behavior through an automated data-reflux mechanism. Extensive experiments demonstrate significant improvements in task accuracy and behavioral robustness, establishing MagicGUI-RMS as a foundational system for building self-improving GUI agents driven by reward-based adaptation.",157.85,Phi-4,Nvidia B200 (Cloud Native)
2601.13075v1_METIS Mentoring Engine for Thoughtful Inquiry  Sol.pdf,METIS: Mentoring Engine for Thoughtful Inquiry & Solutions,"['Abhinav Rajeev Kumar', 'Dhruv Trehan', 'Paras Chopra']","This paper introduces METIS, an AI tool designed to mentor undergraduates in transforming initial research ideas into publishable papers. METIS is a stage-aware assistant that provides literature search, curated guidelines, methodology checks, and memory support. The system is evaluated against GPT-5 and Claude Sonnet 4.5, showing preference in LLM judgments and higher student scores in clarity, actionability, and constraint-fit. METIS demonstrates effectiveness in document-grounded stages, with open materials provided for reproducibility. The paper contributes a practical mentoring workflow, a simple system with inspectable tools, and empirical comparisons, emphasizing structured progression in research mentorship.",157.92,Phi-4,Nvidia B200 (Cloud Native)
2601.13111v1_CORE-T COherent REtrieval of Tables for Text-to-SQ.pdf,CORE-T: COherent REtrieval of Tables for Text-to-SQL,"['Hassan Soliman', 'Vivek Gupta', 'Dan Roth', 'Iryna Gurevych']","CORE-T is a scalable, training-free framework designed to improve the retrieval of relevant and joinable tables in text-to-SQL workflows. It addresses the challenge of retrieving tables from large, heterogeneous collections without predefined schema graphs. By enriching tables with LLM-generated metadata and pre-computing a lightweight table-compatibility cache, CORE-T enhances table-selection accuracy and reduces the number of tables retrieved. This results in improved multi-table execution accuracy and efficiency, outperforming existing methods in terms of F1 score and token usage.",158.78,Phi-4,Nvidia B200 (Cloud Native)
2601.13114v1_IntAgent NWDAF-Based Intent LLM Agent Towards Adva.pdf,IntAgent: NWDAF-Based Intent LLM Agent Towards Advanced Next Generation Networks,"['Abdelrahman Soliman', 'Ahmed Refaey', 'Aiman Erbad', 'Amr Mohamed']","This paper introduces IntAgent, an intelligent intent LLM agent that integrates NWDAF analytics and tools to fulfill network operator intents. Unlike previous approaches, IntAgent develops an intent tools engine within the NWDAF analytics engine, utilizing live network analytics for reasoning and tool selection. The framework is demonstrated through use cases like ML-based traffic prediction and scheduled policy enforcement, showcasing its ability to autonomously fulfill complex network intents. The paper emphasizes the role of LLMs in translating high-level network intents into specific configurations, enhancing automation and efficiency in next-generation networks.",159.14,Phi-4,Nvidia B200 (Cloud Native)
2601.13122v1_Responsible AI for General-Purpose Systems Overvie.pdf,"Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward","['Gourab K. Patro', 'Himanshi Agrawal', 'Himanshu Gharat', 'Supriya Panigrahi', 'Nim Sherpa', 'Vishal Vaddina', 'Dagnachew Birru']","This paper reviews the risks and vulnerabilities of modern general-purpose AI systems, which are capable of performing a wide range of tasks but often produce untrustworthy outputs due to issues like hallucinations, toxicity, and stereotypes. The authors compare these risks with those of traditional task-specific AI systems, highlighting the higher Degree of Freedom in output of general-purpose AI. They propose the C 2V2 (Control, Consistency, Value, Veracity) desiderata to address responsible AI requirements and discuss recent efforts in AI alignment and other enhancements. The paper argues for a system design approach to meet these desiderata by modeling application- or domain-dependent responsible AI requirements.",159.08,Phi-4,Nvidia B200 (Cloud Native)
2601.13142v1_TVWorld Foundations for Remote-Control TV Agents.pdf,TVWorld: Foundations for Remote-Control TV Agents,"['Zhantao Ma', 'Quanfeng Lu', 'Shuai Zhong', 'Dahai Yu', 'Ping Luo', 'Michael K. Ng']","This paper introduces TVWorld, an offline graph-based abstraction for evaluating remote-control TV navigation, addressing the gap in existing research focused on point-and-click interactions. It presents two benchmarks, TVWorld-N and TVWorld-G, to assess topology-aware navigation and focus-aware grounding, respectively. The paper identifies a key limitation in current agents: insufficient topology awareness for long-horizon TV navigation. To address this, a Topology-Aware Training framework is proposed, leading to the development of TVTheseus, a foundation model achieving state-of-the-art performance in TV navigation tasks.",158.89,Phi-4,Nvidia B200 (Cloud Native)
2601.13160v1_Training instability in deep learning follows low-.pdf,Training instability in deep learning follows low-dimensional dynamical principles,"['Zhipeng Zhang', 'Zhenjie Yao', 'Kai Li', 'Lei Yang']","The paper explores the stability of the training process in deep learning systems, which are high-dimensional dynamical systems. Small perturbations in optimization, data, parameters, or learning signals can lead to abrupt and irreversible failures. The authors propose a unified dynamical perspective to characterize training stability across four dimensions: optimization, environmental/data, parametric, and learning-signal stability. Through controlled perturbation auditing, they identify three key regularities: high final performance is often decoupled from training stability; controlled stochasticity buffers learning dynamics; and deviations in low-dimensional latent meta-states precede performance collapse. These findings suggest that training stability is a measurable and comparable property, providing a foundation for studying learning dynamics beyond final performance outcomes.",159.48,Phi-4,Nvidia B200 (Cloud Native)
2601.13166v1_From 100000 images to winning the first brain MRI .pdf,"From 100,000+ images to winning the first brain MRI foundation model challenges: Sharing lessons and models","['Pedro M. Gordaliza', 'Jaume Banus', 'Benoît Gérin', 'Maxence Wynen', 'Nataliia Molchanova', 'Jonas Richiardi', 'Meritxell Bach Cuadra']","This paper discusses the development and success of foundation models for medical image analysis, specifically brain MRI, in the context of the first challenges of this kind held at MICCAI 2025. The authors' solution, which ranked first in both the SSL3D and FOMO25 contests, leverages a U-Net CNN architecture combined with anatomical priors and neuroimaging domain knowledge. Their models trained significantly faster and were smaller than competing transformer-based approaches. The paper highlights the potential of foundation models to address challenges in radiology, such as data sparsity and protocol variability, by enabling fine-tuning with minimal labeled data.",156.36,Phi-4,Nvidia B200 (Cloud Native)
2601.13186v1_Prompt Injection Mitigation with Agentic AI Nested.pdf,"Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching","['Diego Gosmar', 'Deborah A. Dahl']","This paper addresses the challenge of prompt injection in large language models, particularly in multi-agent settings, by extending the Total Injection Vulnerability Score (TIVS) framework with semantic similarity-based caching, a dedicated fourth-agent evaluator, and an Observability Score Ratio (OSR). The proposed system integrates a three-stage agentic pipeline with Continuum Memory Systems, using semantic caching across 301 prompts from ten attack families. A fourth agent conducts security analysis with five key performance indicators, including OSR, to balance mitigation and auditability. Experiments demonstrate secure responses with reduced high-risk breaches and significant computational savings, leading to real-time responses, cost reduction, and energy savings. The system achieves a 41.6% reduction in computational load, translating to proportional decreases in energy consumption and carbon emissions. The results highlight the benefits of Observability-aware evaluation in multi-agent pipelines and the potential for memory-augmented agents to enhance security, performance, and sustainability without altering model weights.",159.04,Phi-4,Nvidia B200 (Cloud Native)
2601.13187v1_Scientific production in the era of Large Language.pdf,Scientific production in the era of Large Language Models,"['Keigo Kusumegi', 'Xinyu Yang', 'Paul Ginsparg', 'Mathijs de Vaan', 'Toby Stuart', 'Yian Yin']","This paper analyzes the impact of Large Language Models (LLMs) on scientific research using large-scale datasets. It finds that scientists using LLMs to draft manuscripts have significantly increased paper production, with a 23.7-89.3% increase depending on the field and author background. The use of LLMs has altered the relationship between writing complexity and paper quality, resulting in manuscripts that are linguistically complex but often substantively lacking. Additionally, LLM adopters tend to access and cite a more diverse range of prior work, including books and less-cited, newer documents. These findings suggest a major shift in scientific production, indicating potential changes needed in how journals, funding agencies, and tenure committees evaluate scientific works.",159.16,Phi-4,Nvidia B200 (Cloud Native)
2601.13197v1_Diffusion-Driven Synthetic Tabular Data Generation.pdf,Diffusion-Driven Synthetic Tabular Data Generation for Enhanced DoS/DDoS Attack Classification,"['Aravind B', 'Anirud R.S.', 'Sai Surya Teja N', 'Bala Subrahmanya Sriranga Navaneeth A', 'Karthika R', 'Mohankumar N']","This paper addresses class imbalance in network intrusion detection using Tabular Denoising Diffusion Probability Models (TabDDPM) for data augmentation. The approach synthesizes high-fidelity minority-class samples from the CIC-IDS2017 dataset through iterative denoising processes. By generating synthetic samples for underrepresented classes and merging them with the original dataset, the augmented training data enables an ANN classifier to achieve near-perfect recall on previously underrepresented attack classes. The results demonstrate the effectiveness of diffusion models for addressing tabular data imbalance in security domains, with potential applications in fraud detection and medical diagnostics.",159.02,Phi-4,Nvidia B200 (Cloud Native)
2601.13206v1_Real-Time Deadlines Reveal Temporal Awareness Fail.pdf,Real-Time Deadlines Reveal Temporal Awareness Failures in LLM Strategic Dialogues,"['Neil Sehgal', 'Sharath Chandra Guntuku', 'Lyle Ungar']","This paper investigates how Large Language Models (LLMs) perform under real-time deadlines in simulated negotiation scenarios. The study reveals that LLMs struggle with temporal awareness, as evidenced by significantly lower deal closure rates when not provided with real-time updates. While LLMs perform well under turn-based limits, their inability to track elapsed time affects their strategic decision-making in time-sensitive applications. This limitation poses challenges for deploying LLMs in real-world settings where continuous time constraints are critical.",159.3,Phi-4,Nvidia B200 (Cloud Native)
2601.13217v1_Beyond Single-shot Writing Deep Research Agents ar.pdf,Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn Report Revision,"['Bingsen Chen', 'Boyan Li', 'Ping Nie', 'Yuyu Zhang', 'Xi Ye', 'Chen Zhao']","This paper introduces MRDRE, an evaluation suite for assessing Deep Research Agents (DRAs) in multi-turn report revision tasks. Unlike existing benchmarks that treat report generation as a single-shot task, MRDRE simulates iterative revision processes akin to human practices. The study reveals that while DRAs can address user feedback, they often regress on previously covered content and citation quality. Over multiple revision turns, even the best-performing agents struggle to maintain content integrity and preserve earlier edits. The paper highlights that these issues are not easily resolved through inference-time fixes like prompt engineering or dedicated sub-agents.",158.83,Phi-4,Nvidia B200 (Cloud Native)
2601.13222v1_Incorporating QA Nuggets into Retrieval-Augmented .pdf,Incorporating Q&A Nuggets into Retrieval-Augmented Generation,"['Laura Dietz', 'Bryan Li', 'Gabrielle Liu', 'Jia-Huei Ju', 'Eugene Yang', 'Dawn Lawrie', 'William Walden', 'James Mayfield']","The paper introduces Crucible, a Nugget-Augmented Generation System that integrates Q&A nuggets into Retrieval-Augmented Generation (RAG) frameworks. Crucible constructs a bank of Q&A nuggets from retrieved documents to guide extraction, selection, and report generation, ensuring explicit citation provenance. The system leverages nuggets to avoid repeated information and maintain clear, interpretable Q&A semantics. Evaluated on the TREC NECIR 2024 collection, Crucible outperforms the existing nugget-based RAG system, Ginger, in terms of nugget recall, density, and citation grounding.",159.31,Phi-4,Nvidia B200 (Cloud Native)
2601.13227v1_Insider Knowledge How Much Can RAG Systems Gain fr.pdf,Insider Knowledge: How Much Can RAG Systems Gain from Evaluation Secrets?,"['Laura Dietz', 'Bryan Li', 'Eugene Yang', 'Dawn Lawrie', 'William Walden', 'James Mayfield']","The paper investigates the risk of faulty measurements in RAG systems due to circularity when using LLM judges for evaluation. It highlights how near-perfect evaluation scores can be achieved by optimizing outputs for LLM judges, emphasizing the need for blind evaluation settings and methodological diversity to avoid mistaking metric overfitting for genuine system progress.",159.25,Phi-4,Nvidia B200 (Cloud Native)
2601.13228v1_Autoregressive Models Rival Diffusion Models at AN.pdf,Autoregressive Models Rival Diffusion Models at Any-Order Generation,"['Tianqi Du', 'Lizhe Fang', 'Weijie Yang', 'Chenheng Zhang', 'Zeming Wei', 'Yifei Wang', 'Yisen Wang']","This paper addresses the limitations of diffusion language models in capturing deep, hierarchical dependencies by proposing a new framework called Any-order Any-subset Autoregressive modeling (A3). A3 extends autoregressive modeling to allow flexible, parallel, and bidirectional generation while maintaining the probabilistic rigor and multi-layer dependency modeling of traditional autoregressive models. The authors implement A3 using a two-stream attention architecture and a progressive adaptation strategy, demonstrating its superiority over diffusion-based models in tasks like question answering, commonsense reasoning, and story infilling.",159.24,Phi-4,Nvidia B200 (Cloud Native)
2601.13233v1_RAG A Random-Forest-Based Generative Design Framew.pdf,RAG: A RANDOM-FOREST-BASED GENERATIVE DESIGN FRAMEWORK FOR UNCERTAINTY-AWARE DESIGN OF METAMATERIALS WITH COMPLEX FUNCTIONAL RESPONSE REQUIREMENTS,"['Bolin Chen', 'Dex Doksoo Lee', ""Wei 'Wayne' Chen"", 'Wei Chen']","This paper introduces a novel framework, RAG (RAndom-forest-based Generative approach), for the uncertainty-aware design of metamaterials with complex functional response requirements. Traditional design methods often struggle with high-dimensional, nonlinear, and condition-dependent responses. RAG leverages the small-data compatibility of random forests to efficiently predict high-dimensional functional responses and estimate the likelihood of solutions conditioned on design requirements. This approach quantifies the trustworthiness of generated designs and addresses the one-to-many mapping problem through single-shot design generation. The framework is demonstrated on acoustic and mechanical metamaterials, showcasing its data efficiency compared to neural networks. RAG provides a lightweight and reliable pathway for inverse design involving functional responses, expensive simulations, and complex requirements.",158.84,Phi-4,Nvidia B200 (Cloud Native)
2601.13235v1_RubRIX Rubric-Driven Risk Mitigation in Caregiver-.pdf,RubRIX: Rubric-Driven Risk Mitigation in Caregiver-AI Interactions,"['Drishti Goel', 'Jeongah Lee', 'Qiuyue Joy Zhong', 'Violeta J. Rodriguez', 'Daniel S. Brown', 'Ravi Karkar', 'Dong Whi Yoo', 'Koustuv Saha']","This paper introduces RubRIX, a rubric-based framework for evaluating risks in large language model (LLM) responses within caregiving contexts. It addresses the limitations of existing AI evaluation frameworks by focusing on nuanced risks specific to caregiving, such as Inattention, Bias & Stigma, Information Inaccuracy, Uncritical Affirmation, and Epistemic Arrogance. The framework was validated by clinicians and tested on over 20,000 caregiver queries, showing significant risk reduction across six state-of-the-art LLMs. The study emphasizes the need for domain-sensitive, user-centered evaluation frameworks for AI deployment in high-burden contexts like caregiving.",158.85,Phi-4,Nvidia B200 (Cloud Native)
2601.13236v1_Pixelwise Uncertainty Quantification of Accelerate.pdf,Pixelwise Uncertainty Quantification of Accelerated MRI Reconstruction,"['Ilias I. Giannakopoulos', 'Lokesh B Gautham Muthukumar', 'Yvonne W. Lui', 'Riccardo Lattanzi']","This paper introduces a framework for pixel-wise uncertainty quantification in parallel MRI reconstructions, which allows for the automatic identification of unreliable regions without needing ground-truth reference images. The method combines conformal quantile regression with image reconstruction techniques to estimate pixel-wise uncertainty intervals. The approach was tested on Cartesian undersampled brain and knee data from the fastMRI dataset, using acceleration factors from 2 to 10. An end-to-end Variational Network was employed for image reconstruction. The results showed a strong correlation between predicted uncertainty maps and actual reconstruction errors, with Pearson correlation coefficients exceeding 90% at four-fold acceleration and above. In contrast, simpler heuristic methods resulted in lower correlation coefficients. The framework facilitates the evaluation of reconstruction quality without fully-sampled reference images, paving the way for adaptive MRI acquisition protocols that balance scan time and diagnostic reliability.",158.61,Phi-4,Nvidia B200 (Cloud Native)
2601.13238v1_A Semantic Decoupling-Based Two-Stage Rainy-Day At.pdf,A Semantic Decoupling–Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision–Language Models,"['Chengyin Hu', 'Xiang Chen', 'Zhe Jia', 'Weiwen Shi', 'Fengyu Zhang', 'Jiujiang Guo', 'Yiwei Wei']","This paper introduces an adversarial framework to assess the robustness of Vision-Language Models (VLMs) under rainy conditions. The framework employs a two-stage, parameterized perturbation model based on semantic decoupling to analyze rain-induced shifts in decision-making. In the first stage, global effects of rainfall are modeled by applying a low-dimensional global modulation to condition the embedding space, weakening original semantic decision boundaries. The second stage introduces structured rain variations by modeling multi-scale raindrop appearance and rainfall-induced illumination changes, optimizing the resulting non-differentiable weather space to induce stable semantic shifts. The study demonstrates that even physically plausible weather perturbations can cause substantial semantic misalignment in VLMs, highlighting potential safety and reliability risks in real-world deployment. Key findings include the importance of illumination modeling and multi-scale raindrop structures in driving these semantic shifts.",158.3,Phi-4,Nvidia B200 (Cloud Native)
2601.13240v1_KOCO-BENCH Can Large Language Models Leverage Doma.pdf,KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?,"['Xue Jiang', 'Jiaru Qian', 'Xianjie Shi', 'Chenjie Li', 'Hao Zhu', 'Ziyu Wang', 'Jielun Zhang', 'Zheyu Zhao', 'Kechi Zhang', 'Jia Li', 'Wenpin Jiao', 'Zhi Jin', 'Ge Li', 'Yihong Dong']","This paper introduces KOCO-BENCH, a novel benchmark designed to evaluate domain specialization methods for Large Language Models (LLMs) in software development. Unlike existing benchmarks, KOCO-BENCH focuses on how LLMs acquire and apply domain knowledge, featuring curated knowledge corpora and multi-granularity evaluation tasks. The benchmark includes 6 emerging domains with 11 software frameworks and 25 projects, challenging state-of-the-art LLMs. Despite applying domain specialization methods like SFT, RAG, and kNN-LM, improvements are marginal, with the best-performing agent achieving only 34.2%. This highlights the need for more effective domain specialization methods. The benchmark, evaluation code, and baselines are released to advance further research.",158.75,Phi-4,Nvidia B200 (Cloud Native)
2601.13247v1_Aligning Agentic World Models via Knowledgeable Ex.pdf,Aligning Agentic World Models via Knowledgeable Experience Learning,"['Baochang Ren', 'Yunzhi Yao', 'Rui Sun', 'Shuofei Qiao', 'Ningyu Zhang', 'Huajun Chen']","The paper addresses the disconnect between the semantic reasoning capabilities of Large Language Models (LLMs) and their lack of procedural grounding in physical laws. It introduces WorldMind, a framework that constructs a symbolic World Knowledge Repository using environmental feedback. This framework uses Process Experience to ensure physical feasibility and Goal Experience to guide task optimality. Experiments on EB-ALFRED and EB-Habitat show that WorldMind outperforms baselines with significant cross-model and cross-environment transferability.",159.02,Phi-4,Nvidia B200 (Cloud Native)
2601.13260v1_Stop Taking Tokenizers for Granted They Are Core D.pdf,Stop Taking Tokenizers for Granted: They Are Core Design Decisions in Large Language Models,"['Sawsan Alqahtani', 'Mir Tafseer Nayeem', 'Md Tahmid Rahman Laskar', 'Tasnim Mohiuddin', 'M Saiful Bari']","This paper argues that tokenization is a core design decision in large language models (LLMs), not just a preprocessing step. It critiques common subword approaches like Byte Pair Encoding (BPE) for their misalignment with linguistic structures, bias amplification, and inefficiency across languages and domains. The authors propose a context-aware framework that integrates tokenizer and model co-design, considering linguistic, domain, and deployment factors. They emphasize the need for standardized evaluation and transparent reporting to make tokenization choices accountable and comparable. By treating tokenization as a core design problem, the paper suggests that language technologies can become fairer, more efficient, and more adaptable.",157.97,Phi-4,Nvidia B200 (Cloud Native)
2601.13262v1_CURE-Med Curriculum-Informed Reinforcement Learnin.pdf,CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning,"['Eric Onyame', 'Akash Ghosh', 'Subhadip Baidya', 'Sriparna Saha', 'Xiuying Chen', 'Chirag Agarwal']","This paper introduces CURE-MED, a curriculum-informed reinforcement learning framework designed to enhance multilingual medical reasoning in large language models (LLMs). The authors present CUREMED-BENCH, a multilingual dataset with open-ended reasoning queries in thirteen languages, including underrepresented ones like Amharic, Yoruba, and Swahili. CURE-MED integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to improve logical correctness and language stability. The framework outperforms strong baselines, achieving significant improvements in language consistency and logical correctness across different parameter sizes. This work addresses the challenges of deploying LLMs in multilingual healthcare settings by ensuring reliable and equitable medical reasoning.",159.0,Phi-4,Nvidia B200 (Cloud Native)
2601.13268v1_Improving the Safety and Trustworthiness of Medica.pdf,Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation Loops,"['Zainab Ghafoor', 'Md Shafiqul Islam', 'Koushik Howlader', 'Md Rasel Khondokar', 'Tanusree Bhattacharjee', 'Sayantan Chakraborty', 'Adrito Roy', 'Ushashi Bhattacharjee', 'Tirtho Roy']","This paper introduces a multi-agent refinement framework aimed at enhancing the safety and reliability of medical Large Language Models (LLMs). The framework employs two generative models, DeepSeek R1 and Med-PaLM, alongside two evaluation agents, LLaMA 3.1 and Phi-4, to assess responses based on the American Medical Association’s Principles of Medical Ethics and a five-tier Safety Risk Assessment protocol. The study evaluates the system's performance across 900 clinically diverse queries, demonstrating significant improvements in convergence efficiency, ethical violation reduction, and domain-specific risk behavior. The results show an 89% reduction in ethical violations and a 92% risk downgrade rate, highlighting the framework's effectiveness in ensuring the safety and trustworthiness of medical AI applications.",158.24,Phi-4,Nvidia B200 (Cloud Native)
2601.13286v1_AI Skills Improve Job Prospects Causal Evidence fr.pdf,AI Skills Improve Job Prospects: Causal Evidence from a Hiring Experiment,"['Fabian Stephany', 'Ole Teutloff', 'Angelo Leone']","This study investigates the impact of AI-related skills on hiring decisions and their ability to offset traditional disadvantages such as older age or lower formal education. Conducted through an experimental survey with 1,700 recruiters from the UK and the US, the research uses a paired conjoint design to evaluate hypothetical candidates represented by synthetically designed résumés across three occupations: graphic designer, office assistant, and software engineer. The findings reveal that AI skills significantly increase interview invitation probabilities by approximately 8 to 15 percentage points and can partially or fully mitigate disadvantages related to age and education. The effects are strongest for office assistants, where formal AI certification plays an additional compensatory role. The study also notes that recruiters' backgrounds and AI usage moderate these effects. Overall, AI skills serve as a powerful hiring signal and can alleviate traditional labor market disadvantages, with implications for skill acquisition strategies and recruitment practices.",153.72,Phi-4,Nvidia B200 (Cloud Native)
2601.13295v1_CooperBench Why Coding Agents Cannot be Your Teamm.pdf,CooperBench: Why Coding Agents Cannot be Your Teammates Yet,"['Arpandeep Khatua', 'Hao Zhu', 'Peter Tran', 'Arya Prabhudesai', 'Frederic Sadrieh', 'Johann K. Lieberwirth', 'Xinkai Yu', 'Yicheng Fu', 'Michael J. Ryan', 'Jiaxin Pei', 'Diyi Yang']","The paper introduces CooperBench, a benchmark designed to evaluate the collaborative capabilities of AI coding agents. It highlights the challenges these agents face in teamwork, such as communication issues, deviation from commitments, and incorrect expectations about others' actions. The study reveals that current AI agents perform significantly worse in collaborative tasks compared to individual tasks, contrasting with human teams where collaboration typically enhances productivity. The benchmark includes over 600 tasks across 12 libraries in 4 programming languages, grounded in real open-source repositories with expert-written tests.",151.56,Phi-4,Nvidia B200 (Cloud Native)
2601.13317v1_Paid Voices vs. Public Feeds Interpretable Cross-P.pdf,Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme Modeling of Climate Discourse,"['Samantha Sudhoff', 'Pranav Perumal', 'Zhaoqing Wu', 'Tunazzina Islam']","This paper presents a comparative analysis of climate discourse across paid advertisements on Meta and public posts on Bluesky from July 2024 to September 2025. It introduces a thematic discovery and assignment framework that clusters texts by semantic similarity and uses large language models to generate interpretable theme labels. The study evaluates these themes against traditional topic modeling baselines and validates their coherence through downstream tasks. The findings highlight systematic differences in thematic structure, stance alignment, and temporal responsiveness between paid and public climate messaging, reflecting platform-level incentives. The framework supports comparative narrative analysis across diverse communication environments.",154.09,Phi-4,Nvidia B200 (Cloud Native)
2601.13327v1_PepEDiff Zero-Shot Peptide Binder Design via Prote.pdf,PepEDiﬀ: Zero-Shot Peptide Binder Design via Protein Embedding Diffusion,"['Po-Yu Liang', 'Tibo Duran', 'Jun Bai']","PepEDiﬀ is a novel peptide binder generator that designs binding sequences for a target receptor protein sequence and its pocket residues. Unlike existing methods that rely on intermediate structure prediction, PepEDiﬀ generates binder sequences directly in a continuous latent space derived from a pretrained protein embedding model. This approach enhances structural and sequence diversity by avoiding reliance on predicted structures. The method employs latent-space exploration and diffusion-based sampling to generate peptides beyond the limited distribution of known binders, leveraging the global protein embedding manifold as a semantic prior. Evaluated on TIGIT, a challenging target with a large, flat protein–protein interaction interface, PepEDiﬀ outperforms state-of-the-art approaches, demonstrating its potential as a general, structure-free framework for zero-shot peptide binder design.",154.39,Phi-4,Nvidia B200 (Cloud Native)
2601.13348v1_The AI Genie Phenomenon and Three Types of AI Chat.pdf,"The AI Genie Phenomenon and Three Types of AI Chatbot Addiction: Escapist Roleplays, Pseudosocial Companions, and Epistemic Rabbit Holes","['M. Karen Shen', 'Jessica Huang', 'Olivia Liang', 'Ig-Jae Kim', 'Dongwook Yoon']","This study investigates the addictive potential of AI chatbots, focusing on the 'AI Genie' phenomenon where users can obtain anything with minimal effort. Through thematic analysis of Reddit entries, the study identifies three types of AI chatbot addiction: Escapist Roleplay, Pseudosocial Companion, and Epistemic Rabbit Hole. It also explores the involvement of sexual content and the varying effectiveness of recovery strategies across addiction types. The findings provide empirical insights to inform future prevention, diagnosis, and intervention strategies for AI chatbot addiction.",154.27,Phi-4,Nvidia B200 (Cloud Native)
2601.13352v1_LLM-as-RNN A Recurrent Language Model for Memory U.pdf,LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction,"['Yuxing Lu', 'J. Ben Tamo', 'Weichen Zhao', 'Nan Sun', 'Yishan Zhong', 'Wenqi Shi', 'Jinzhuo Wang', 'May D. Wang']","This paper introduces LLM-as-RNN, a framework that transforms a frozen Large Language Model (LLM) into a recurrent predictor by using a natural-language memory state. This state is updated at each timestep through feedback-driven text rewrites, allowing the model to learn without parameter updates. The method is evaluated on sequential benchmarks in healthcare, meteorology, and finance, showing significant improvements over existing baselines. LLM-as-RNN corrects errors and retains task-relevant patterns, effectively performing online learning through language.",155.75,Phi-4,Nvidia B200 (Cloud Native)
2601.13358v1_The Geometry of Thought How Scale Restructures Rea.pdf,The Geometry of Thought: How Scale Restructures Reasoning in Large Language Models,['Samuel Cyrenius Anderson'],"This paper explores how scaling in large language models affects reasoning, revealing that it restructures rather than uniformly improves reasoning capabilities. By analyzing over 25,000 chain-of-thought trajectories across domains like Law, Science, Code, and Math, the study finds that scaling induces domain-specific geometric changes in reasoning processes. Legal reasoning, for example, undergoes a 'crystallization' phase, while scientific and mathematical reasoning remain invariant. The paper introduces Neural Reasoning Operators to predict reasoning endpoints and identifies a universal oscillatory signature across domains. These findings suggest that the cost of reasoning is determined by manifold geometry, offering insights for inference acceleration.",154.46,Phi-4,Nvidia B200 (Cloud Native)
2601.13376v1_Bounded Minds Generative Machines Envisioning Conv.pdf,"Bounded Minds, Generative Machines: Envisioning Conversational AI that Works with Human Heuristics and Reduces Bias Risk",['JIQUN LIU'],"This article explores the design of conversational AI systems that align with human heuristics and bounded rationality, rather than assuming idealized users. It highlights the need for AI systems to detect cognitive vulnerabilities, support judgment under uncertainty, and evaluate beyond factual accuracy to enhance decision quality and cognitive robustness. The paper emphasizes the risks of AI systems exploiting human biases for behavioral manipulation and calls for a shift in design practices to address these challenges.",154.63,Phi-4,Nvidia B200 (Cloud Native)
2601.13383v1_A Lightweight Modular Framework for Constructing A.pdf,"A LIGHTWEIGHT MODULAR FRAMEWORK FOR CONSTRUCTING AUTONOMOUS AGENTS DRIVEN BY LARGE LANGUAGE MODELS: DESIGN, IMPLEMENTATION, AND APPLICATIONS IN AGENTFORGE","['A. A. Jafari', 'C. Ozcinar', 'G. Anbarjafari']","This paper introduces AgentForge, a lightweight, open-source Python framework designed to facilitate the construction of LLM-driven autonomous agents. It addresses the limitations of existing frameworks by offering a modular architecture with composable skill abstraction, a unified LLM backend interface, and a declarative YAML-based configuration system. The framework supports seamless integration of various LLM APIs and local inference engines, and its skill composition mechanism is formalized as a directed acyclic graph (DAG). Experimental evaluations show that AgentForge achieves competitive task completion rates and significantly reduces development time compared to other methods. The framework is suitable for real-time applications and supports extension with built-in skills and custom skill development.",154.21,Phi-4,Nvidia B200 (Cloud Native)
2601.13385v1_Organ-Aware Attention Improves CT Triage and Class.pdf,Organ-Aware Attention Improves CT Triage and Classification,"['Lavsen Dahal', 'Yubraj Bhandari', 'Geoffrey D. Rubin', 'Joseph Y. Lo']","This study addresses the need for efficient triage and classification of high-volume CT scans to improve patient care and reduce radiologist workload. It highlights the limitations of existing Vision-Language Models (VLM) in handling 3D anatomy and protocol variations. The paper introduces ORACLE-CT, an encoder-agnostic, organ-aware model that combines Organ-Masked Attention and Organ-Scalar Fusion to enhance classification performance. The model achieves state-of-the-art results on chest and abdomen CT datasets, demonstrating superior AUROC scores compared to existing methods. The study underscores the potential of organ-aware approaches in medical imaging.",154.05,Phi-4,Nvidia B200 (Cloud Native)
2601.13392v1_Beyond Memorization Testing LLM Reasoning on Unsee.pdf,Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation Tasks,"['Shlok Shelat', 'Jay Raval', 'Souvik Roy', 'Manas Gaur']","This paper investigates whether large language models (LLMs) perform genuine symbolic reasoning or rely on pattern matching for formal language tasks, specifically in the context of deterministic finite automata (DFA) construction from regular languages. The authors introduce a benchmark that includes factual knowledge questions, seen construction problems, and unseen problems with multiple constraints or generated via Arden’s theorem. While models perform well on factual and seen tasks, their accuracy significantly drops on unseen problems due to systematic misinterpretations and failures in maintaining global consistency. The study evaluates various prompting strategies and finds persistent errors, highlighting a gap between syntactically plausible DFA generation and semantically correct formal reasoning.",154.14,Phi-4,Nvidia B200 (Cloud Native)
2601.13398v1_Can LLMs Compress and Decompress Evaluating Code U.pdf,Can LLMs Compress (and Decompress)? Evaluating Code Understanding and Execution via Invertibility,"['Nickil Maveli', 'Antonio Vergari', 'Shay B. Cohen']","The paper evaluates the ability of Large Language Models (LLMs) to maintain consistent reasoning across forward and backward code execution. It introduces the ROUNDTRIPCODEEVAL (RTCE) benchmark, which tests round-trip consistency through four distinct code execution reasoning tasks. The study finds that while state-of-the-art Code-LLMs show modest improvements with various techniques, they still struggle with true round-trip consistency, indicating a lack of internal coherence necessary for reliable code reasoning. The RTCE benchmark reveals new insights not captured by existing benchmarks, emphasizing the importance of reversible and coherent reasoning in code understanding.",153.72,Phi-4,Nvidia B200 (Cloud Native)
2601.13400v1_Deep Image Prior with L0 Gradient Regularizer for .pdf,DEEP IMAGE PRIOR WITH L0 GRADIENT REGULARIZER FOR IMAGE SMOOTHING,"['Nhat Thanh Tran', 'Kevin Bui', 'Jack Xin']","This paper introduces DIP-ℓ0, a deep image prior framework incorporating the ℓ0 gradient regularizer for image smoothing. Unlike traditional methods that rely on local window statistics or optimization problems, DIP-ℓ0 leverages deep learning without requiring a curated training dataset. The framework addresses the challenge of constructing a proper training dataset for image smoothing by utilizing an alternating direction method of multipliers algorithm with an off-the-shelf ℓ0 gradient minimization solver. Numerical experiments demonstrate that DIP-ℓ0 outperforms existing image smoothing algorithms in edge-preserving smoothing and JPEG artifact removal.",154.62,Phi-4,Nvidia B200 (Cloud Native)
2601.13401v1_Reasoning with Pixel-level Precision QVLM Architec.pdf,Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics,"['Peter A. Massih', 'Eric Cosatto']","This paper addresses the limitations of current Vision-Language Models (VLMs) in quantitative spatial reasoning due to their loss of pixel-level information. The authors introduce the SQuID (Satellite Quantitative Intelligence Dataset) to evaluate quantitative spatial reasoning and propose the QVLM (Quantitative Vision-Language Model) architecture. QVLM maintains pixel precision by generating executable code that operates on pixel-level masks, preserving spatial indexing. Experiments show QVLM achieves 42.0% accuracy on SQuID, significantly outperforming traditional VLMs. The paper highlights the importance of architectural decoupling for improving accuracy in quantitative tasks.",154.91,Phi-4,Nvidia B200 (Cloud Native)
2601.13404v1_Local-to-Global Logical Explanations for Deep Visi.pdf,Local-to-Global Logical Explanations for Deep Vision Models,"['Bhavan Vasu', 'Giuseppe Raffa', 'Prasad Tadepalli']","The paper introduces local and global explanation methods for deep neural networks, which are typically opaque. These methods generate explanations in terms of human-recognizable primitive concepts, using logical formulas in monotone disjunctive-normal-form (MDNF). The local explanations provide instance-specific rationales for single images, while global explanations offer class-level summaries for sets of images. The explanations maintain high fidelity and coverage, addressing transparency and accountability in AI decision-making, in line with GDPR requirements.",154.5,Phi-4,Nvidia B200 (Cloud Native)
2601.13406v1_Integrating Virtual Reality and Large Language Mod.pdf,Integrating Virtual Reality and Large Language Models for Team-Based Non-Technical Skills Training and Evaluation in the Operating Room,"['Jacob Barker', 'Doga Demirel', 'Cullen Jackson', 'Anna Johansson', 'Robbin Miraglia', 'Darian Hoagland', 'Stephanie B. Jones', 'John Mitchell', 'Daniel B. Jones', 'Suvranu De']","The paper introduces the Virtual Operating Room Team Experience (VORTeX), a multi-user virtual reality platform that integrates immersive team simulation with large language model analytics to train and evaluate non-technical skills such as communication, decision-making, teamwork, and leadership in surgical settings. The platform uses structured prompts from the Non-Technical Skills for Surgeons (NOTSS) framework to analyze team dialogue, enabling automated classification of behaviors and generation of interaction graphs. Two laparoscopic emergency scenarios, pneumothorax and intra-abdominal bleeding, were used to simulate realistic stress and collaboration. The pilot sessions conducted at the 2024 SAGES conference involved twelve surgical professionals who provided feedback on the VORTeX system.",154.7,Phi-4,Nvidia B200 (Cloud Native)
2601.13412v1_Using deep learning for predicting cleansing quali.pdf,Using deep learning for predicting cleansing quality of colon capsule endoscopy images,"['Puneet Sharma', 'Kristian Dalsbø Hindberg', 'Benedicte Schelde-Olesen', 'Ulrik Deding', 'Esmaeil S. Nadimi', 'Jan-Matthias Braun']","This study investigates the use of deep learning to predict the cleansing quality of colon capsule endoscopy (CCE) images. A dataset of 500 images, labeled by 14 clinicians using the Leighton–Rex scale, was used to train a ResNet-18 model. The model underwent structured pruning to achieve high sparsity while maintaining accuracy. Explainability was assessed using various CAM techniques and the ROAD method. The pruned model achieved 88% cross-validation accuracy with 79% sparsity, demonstrating the efficiency of pruning. The study also discusses the challenges in evaluating CCE image quality and the importance of model explainability in clinical settings.",155.03,Phi-4,Nvidia B200 (Cloud Native)
2601.13422v1_TrustEnergy A Unified Framework for Accurate and R.pdf,TrustEnergy: A Unified Framework for Accurate and Reliable User-level Energy Usage Prediction,"['Dahai Yu', 'Rongchao Xu', 'Dingyi Zhuang', 'Yuheng Bu', 'Shenhao Wang', 'Guang Wang']","This paper introduces TrustEnergy, a unified framework designed for accurate and reliable user-level energy usage prediction. It addresses the limitations of existing deep learning approaches by incorporating a Hierarchical Spatiotemporal Representation module and a Sequential Conformalized Quantile Regression module. The former captures both macro and micro energy usage patterns using a novel memory-augmented spatiotemporal graph neural network, while the latter dynamically adjusts uncertainty bounds to ensure valid prediction intervals over time. Implemented and evaluated with an electricity provider in Florida, TrustEnergy demonstrates a 5.4% increase in prediction accuracy and a 5.7% improvement in uncertainty quantification compared to state-of-the-art baselines.",154.71,Phi-4,Nvidia B200 (Cloud Native)
2601.13435v1_A Learnable Wavelet Transformer for Long-Short Equ.pdf,A Learnable Wavelet Transformer for Long-Short Equity Trading and Risk-Adjusted Return Optimization,"['Shuozhe Li', 'Du Cheng', 'Leqi Liu']","This paper introduces WaveLSFormer, a learnable wavelet-based long-short Transformer designed for multi-scale decomposition and return-oriented decision learning in financial trading. The model employs a learnable wavelet front-end to generate low- and high-frequency components, guided by spectral regularizers for stable frequency bands. A novel low-guided high-frequency injection (LGHI) module refines low-frequency representations with high-frequency cues, enhancing training stability. The model outputs a portfolio of long/short positions optimized for trading objectives and risk-aware regularization. Extensive experiments demonstrate that WaveLSFormer outperforms traditional models like MLP, LSTM, and Transformer backends, achieving significant improvements in profitability and risk-adjusted returns across various industries.",154.64,Phi-4,Nvidia B200 (Cloud Native)
2601.13437v1_MOSLD-Bench Multilingual Open-Set Learning and Dis.pdf,MOSLD-Bench: Multilingual Open-Set Learning and Discovery Benchmark for Text Categorization,"['Adriana-Valentina Costache', 'Daria-Nicoleta Dragomir', 'Silviu-Florin Gheorghe', 'Eduard Poesina', 'Paul Irofti', 'Radu Tudor Ionescu']","This paper introduces MOSLD-Bench, the first multilingual open-set learning and discovery benchmark for text categorization by topic, comprising 960K data samples across 12 languages. The benchmark is constructed by rearranging existing datasets and collecting new data samples from the news domain. A novel framework for the OSLD task is proposed, integrating multiple stages to continuously discover and learn new classes. The paper evaluates several language models, including a new one, to provide reference results for future work. The benchmark is released at a specified GitHub repository.",153.45,Phi-4,Nvidia B200 (Cloud Native)
2601.13443v1_Explicit Cognitive Allocation A Principle for Gove.pdf,Explicit Cognitive Allocation: A Principle for Governed and Auditable Inference in Large Language Models,"['Héctor Manuel Manzanilla-Granados', 'Zaira Navarrete-Cazales', 'Miriam Pescador-Rojas', 'Tonahtiu Ramírez-Romero']","The paper introduces the principle of Explicit Cognitive Allocation to structure AI-assisted inference in large language models (LLMs). It proposes the Cognitive Universal Agent (CUA) architecture, which organizes inference into distinct stages, enhancing traceability, epistemic control, and reproducibility. The paper evaluates CUA against baseline LLM inference, demonstrating improved epistemic alignment and exposure of instrumental structures in the agricultural domain. The findings suggest that explicit cognitive and instrumental allocation can enhance the controllability, auditability, and epistemic scope of AI-assisted reasoning, providing a foundation for systematic evaluation in domains requiring transparency and governance.",154.84,Phi-4,Nvidia B200 (Cloud Native)
2601.13458v1_Labels or Preferences Budget-Constrained Learning .pdf,Labels or Preferences? Budget-Constrained Learning with Human Judgments over AI-Generated Outputs,"['Zihan Dong', 'Ruijia Wu', 'Linjun Zhang']","This paper addresses the challenge of optimally allocating a fixed annotation budget between ground-truth labels and pairwise preferences in AI. The authors introduce Preference-Calibrated Active Learning (PCAL), a novel method that optimizes data acquisition strategies and develops a statistically efficient estimator for functionals of the data distribution. The method is theoretically proven to be asymptotically optimal and robust, even with poorly estimated nuisance models. The framework is flexible and applies to a broad class of problems by optimizing the estimator’s variance. Simulations and real-data analysis demonstrate the practical benefits and superior performance of PCAL.",155.37,Phi-4,Nvidia B200 (Cloud Native)
2601.13462v1_SpatialBench-UC Uncertainty-Aware Evaluation of Sp.pdf,SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt Following in Text-to-Image Generation,['Amine Rostane'],"The paper introduces SpatialBench-UC, a benchmark for evaluating text-to-image models' adherence to spatial instructions. It addresses the challenge of automating spatial evaluation due to uncertainties in object detection and ambiguous geometry. The benchmark includes 200 prompts organized into 100 counterfactual pairs, with reproducible configurations and human audits to calibrate confidence thresholds. The evaluation framework allows for selective prediction with PASS/FAIL/UNDECIDABLE verdicts, providing interpretable confidence scores. Results on three baselines show varying PASS rates and coverage, highlighting the importance of confidence in interpreting results.",154.87,Phi-4,Nvidia B200 (Cloud Native)
2601.13464v1_Context and Transcripts Improve Detection of Deepf.pdf,Context and Transcripts Improve Detection of Deepfake Audios of Public Figures,"['Chongyang Gao', 'Marco Postiglione', 'Julian Baldwin', 'Natalia Denisenko', 'Isabel Gortner', 'Luke Fosdick', 'Chiara Pulice', 'Sarit Kraus', 'V. S. Subrahmanian']",This paper introduces a novel Context-based Audio Deepfake Detector (CADD) that enhances audio deepfake detection by incorporating context and transcripts. The authors created the Journalist-provided Deepfake Dataset (JDD) and a synthetic audio dataset (SYN) to evaluate their approach. They demonstrate that using context and transcripts significantly improves detection performance across various metrics and datasets. CADD also shows robustness against adversarial evasion strategies. The study highlights the importance of context in improving the efficacy of audio deepfake detectors.,154.5,Phi-4,Nvidia B200 (Cloud Native)
2601.13465v1_Graph Neural Networks are Heuristics.pdf,Graph Neural Networks are Heuristics,"['Yimeng Min', 'Carla P. Gomes']","This paper demonstrates that a graph neural network can be transformed into an unsupervised heuristic for combinatorial optimization, specifically for the Travelling Salesman Problem (TSP). By encoding global structural constraints as an inductive bias, the model can generate solutions through direct forward passes without the need for search, supervision, or sequential decision-making. Techniques like dropout and snapshot ensembling enable a single model to act as an implicit ensemble, enhancing solution diversity and reducing optimality gaps. The study shows that graph neural networks can internalize global combinatorial structures and function as strong, learned heuristics, reframing the role of learning in combinatorial optimization from augmenting classical algorithms to directly creating new heuristics.",154.63,Phi-4,Nvidia B200 (Cloud Native)
2601.13474v1_Preconditioning Benefits of Spectral Orthogonaliza.pdf,Preconditioning Benefits of Spectral Orthogonalization in Muon,"['Jianhao Ma', 'Yu Huang', 'Yuejie Chi', 'Yuxin Chen']","The paper investigates the Muon optimizer, which uses spectral orthogonalization of gradients to enhance the pretraining of large language models. The study focuses on a simplified variant of Muon, analyzing its effectiveness through matrix factorization and in-context learning of linear transformers. The authors prove that this variant converges linearly with iteration complexities independent of the condition number, outperforming gradient descent and Adam. The analysis shows that Muon dynamics decouple into independent scalar sequences in the spectral domain, each with similar convergence behavior. The paper formalizes the preconditioning effect of spectral orthogonalization, providing insights into Muon's effectiveness in matrix optimization problems.",155.04,Phi-4,Nvidia B200 (Cloud Native)
2601.13476v1_A Unified Variational Imputation Framework for Ele.pdf,A Unified Variational Imputation Framework for Electric Vehicle Charging Data Using Retrieval-Augmented Language Model,"['Jinhao Li', 'Hao Wang']","This paper introduces a novel probabilistic variational imputation framework, PRAIM, designed to address the challenge of missing records in electric vehicle (EV) charging data. By leveraging a pre-trained language model and retrieval-augmented memory, PRAIM encodes heterogeneous data into a unified representation, enabling a single imputation model to utilize variational neural architecture. This approach effectively captures inter-station correlations and overcomes data sparsity, significantly improving imputation accuracy and preserving the original data's statistical distribution. Extensive experiments on four public datasets demonstrate PRAIM's superior performance over established baselines, enhancing downstream forecasting tasks.",154.96,Phi-4,Nvidia B200 (Cloud Native)
2601.13481v1_Towards Efficient and Robust Linguistic Emotion Di.pdf,Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement,"['Jian Zhang', 'Zhangqi Wang', 'Zhiyuan Wang', 'Weiping Fu', 'Yu He', 'Haiping Zhu', 'Qika Lin', 'Jun Liu']","This paper addresses the challenges of diagnosing emotions in mental health contexts using linguistic expressions. It highlights the limitations of existing methods in handling emotional comorbidity and inefficient exploration of clinically relevant cues. The authors propose APOLO, a framework that uses a multi-agent system to optimize prompt design for large language models (LLMs). APOLO models the instruction refinement process as a Partially Observable Markov Decision Process (POMDP) and employs a closed-loop design with Planner, Teacher, Critic, Student, and Target roles to iteratively refine prompts. The framework aims to improve diagnostic accuracy and robustness, demonstrating its effectiveness across various benchmarks. The study contributes a scalable and generalizable approach for enhancing the reliability of LLMs in mental healthcare applications.",154.61,Phi-4,Nvidia B200 (Cloud Native)
2601.13487v1_The Hidden Toll of Social Media News Causal Effect.pdf,The Hidden Toll of Social Media News: Causal Effects on Psychosocial Wellbeing,"['Olivia Pal', 'Agam Goyal', 'Eshwar Chandrasekharan', 'Koustuv Saha']","This study investigates the psychosocial effects of news consumption on social media, using a large-scale dataset from the Bluesky platform. By conducting a quasi-experimental study with matched users, the research reveals that news engagement on social media leads to increased depression, stress, and anxiety, while also decreasing loneliness and increasing social interaction. The study finds that bookmarking news feeds is associated with greater psychosocial deterioration compared to commenting or quoting, with significant cumulative effects from repeated exposure. These findings suggest that different types of engagement with news content on social media have distinct psychological impacts, extending beyond crisis-centric frameworks and highlighting the need for tools and interventions to mitigate these effects.",154.65,Phi-4,Nvidia B200 (Cloud Native)
2601.13508v1_CatMaster An Agentic Autonomous System for Computa.pdf,CatMaster: An Agentic Autonomous System for Computational Heterogeneous Catalysis Research,"['Honghao Chen', 'Jiangjie Qiu', 'Yi Shen Tew', 'Xiaonan Wang']","CatMaster is an autonomous system driven by large-language-models designed to streamline computational heterogeneous catalysis research. It addresses the challenges of long, costly, and iterative workflows in density functional theory (DFT) studies by converting natural language requests into complete calculation workspaces. These workspaces include structures, inputs, outputs, logs, and concise run records. CatMaster maintains a persistent project record to support inspection and restartability, and it integrates a multi-fidelity tool library for rapid surrogate relaxations and high-fidelity DFT calculations. The system is demonstrated through four increasingly complex tasks, showcasing its ability to handle O2 spin-state checks, BCC Fe surface energies, and high-throughput Pt–Ni–Cu alloy screening.",154.77,Phi-4,Nvidia B200 (Cloud Native)
2601.13515v1_Automatic Adjustment of HPA Parameters and Attack .pdf,Automatic Adjustment of HPA Parameters and Attack Prevention in Kubernetes Using Random Forests,"['Huah Yong Chan', 'Hanlin Zhou', 'Jingfei Ni', 'Mengchun Wu', 'Qing Deng']","This paper explores the use of HTTP status codes as custom metrics within the Horizontal Pod Autoscaler (HPA) in Kubernetes. By integrating the Random Forest classification algorithm, the study assesses and predicts attacks, dynamically adjusting the maximum pod parameter in the HPA to manage attack traffic. This method redirects attack traffic to honeypot pods, reducing the incidence of 5XX status codes and preventing excessive HPA expansion. Experiments demonstrate the importance of setting appropriate thresholds for HPA adjustments in targeted attack scenarios.",154.73,Phi-4,Nvidia B200 (Cloud Native)
2601.13518v1_AgenticRed Optimizing Agentic Systems for Automate.pdf,AGENTICRED: Optimizing Agentic Systems for Automated Red-teaming,"['Jiayi Yuan', 'Jonathan Nöther', 'Natasha Jaques', 'Goran Radanović']","This paper introduces AGENTICRED, an automated pipeline that leverages large language models' (LLMs) in-context learning to iteratively design and refine red-teaming systems without human intervention. Unlike traditional methods that rely on manually designed workflows, AGENTICRED treats red-teaming as a system design problem, evolving agentic systems using evolutionary selection. The approach consistently outperforms state-of-the-art methods, achieving high attack success rates on various models, including Llama-2-7B, Llama-3-8B, GPT-3.5-Turbo, GPT-4o-mini, and Claude-Sonnet-3.5. This work underscores the potential of automated system design as a powerful paradigm for AI safety evaluation.",154.7,Phi-4,Nvidia B200 (Cloud Native)
2601.13528v1_Eliciting Harmful Capabilities by Fine-Tuning On S.pdf,ELICITING HARMFUL CAPABILITIES BY FINE-TUNING ON SAFEGUARDED OUTPUTS,"['Jackson Kaunismaa', 'Avery Griffin', 'John Hughes', 'Christina Q Knight', 'Mrinank Sharma', 'Erik Jones']","This paper explores how robustly safeguarded AI models can inadvertently contribute to the development of harmful capabilities in open-source models through elicitation attacks. The authors demonstrate that by constructing prompts in adjacent domains to a target harmful task, obtaining responses from safeguarded models, and fine-tuning open-source models on these responses, adversaries can recover a significant portion of the capability gap between open-source and unrestricted models. The study highlights the challenges of mitigating ecosystem-level risks with output-level safeguards and suggests that even ostensibly harmless outputs from safeguarded models can be exploited to enhance the dangerous capabilities of open-source models.",154.75,Phi-4,Nvidia B200 (Cloud Native)
2601.13533v1_Reasoning While Recommending Entropy-Guided Latent.pdf,Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models,['Changshuo Zhang'],"This paper introduces the Entropy-Guided Latent Reasoning (EGLR) recommendation model, which integrates reasoning capabilities into generative re-ranking scenarios. The model addresses the challenge of adapting to dynamic entropy changes in model difficulty during list generation. It offers three core advantages: real-time reasoning during generation, entropy-guided variable-length reasoning with dynamic temperature adjustment, and a lightweight integration design. Experimental results on two real-world datasets demonstrate the model's effectiveness and compatibility with existing generative re-ranking models, enhancing their performance.",154.86,Phi-4,Nvidia B200 (Cloud Native)
2601.13534v1_MN-TSGContinuous Time Series Generation with Irreg.pdf,MN-TSG: CONTINUOUSTIMESERIESGENERATION WITH IRREGULAROBSERVATIONS,"['Xu Zhang', 'Junwei Deng', 'Chang Xu', 'Hao Li', 'Jiang Bian']","This paper introduces MN-TSG, a novel framework for continuous time series generation with irregular observations. It addresses the limitations of existing methods that assume regularly sampled data by integrating Mixture-of-Experts (MoE)-based Neural Controlled Differential Equations (NCDEs) with existing time series generation models. The framework dynamically parameterizes expert functions and decouples the design for effective optimization, allowing for refined continuous generation. MN-TSG is demonstrated to outperform strong baselines in both irregular-to-regular and irregular-to-continuous generation tasks across ten datasets.",154.91,Phi-4,Nvidia B200 (Cloud Native)
2601.13537v1_When Wording Steers the Evaluation Framing Bias in.pdf,When Wording Steers the Evaluation: Framing Bias in LLM Judges,"['Yerin Hwang', 'Dongryeol Lee', 'Taegwan Kang', 'Minwoo Lee', 'Kyomin Jung']","This paper investigates the impact of framing bias on large language models (LLMs) when used as evaluative agents. It explores how subtle changes in prompt phrasing can lead to significant discrepancies in model outputs across various high-stakes evaluation tasks. The study demonstrates that LLMs exhibit susceptibility to framing, with different model families showing distinct tendencies. The findings highlight framing bias as a structural issue in current LLM-based evaluation systems, emphasizing the need for framing-aware protocols to ensure reliability and fairness.",154.7,Phi-4,Nvidia B200 (Cloud Native)
2601.13545v1_TruthTensor Evaluating LLMs Human Imitation throug.pdf,TRUTHTENSOR: EVALUATING LLM SHUMAN IIMITATION THROUGH PREDICTION MARKET DRIFT AND HOLISTIC REASONING,"['Shirin Shahabi', 'Spencer Graham', 'Haruna Isah']","This paper introduces TruthTensor, a novel evaluation paradigm for Large Language Models (LLMs) that assesses them as human-imitation systems in socially-grounded, high-entropy environments. Unlike static benchmarks, TruthTensor uses live prediction markets to provide a holistic view of model behavior, incorporating probabilistic scoring and drift-centric diagnostics. The framework emphasizes reproducibility, calibration, narrative stability, and resource efficiency, demonstrating that models with similar forecast accuracy can differ significantly in these aspects. TruthTensor operationalizes modern evaluation best practices, ensuring defensible assessments of LLMs in real-world decision contexts.",153.28,Phi-4,Nvidia B200 (Cloud Native)
2601.13546v1_ChatAD Reasoning-Enhanced Time-Series Anomaly Dete.pdf,ChatAD: Reasoning-Enhanced Time-Series Anomaly Detection with Multi-Turn Instruction Evolution,"['Hui Sun', 'Chang Xu', 'Haonan Xie', 'Hao Li', 'Yuhao Huang', 'Chuheng Zhang', 'Ming Jin', 'Xiaoguang Liu', 'Gang Wang', 'Jiang Bian']","This paper introduces a novel approach to enhance time-series anomaly detection (AD) using large language models (LLMs). The authors propose a multi-agent-based TS Evolution algorithm (TSEvol) and introduce the AD reasoning & multi-turn dialogue Dataset (TSEData-20K). They also contribute the Chatbot family for AD, including models like ChatAD-Llama3-8B, Qwen2.5-7B, and Mistral-7B. Additionally, the TS Kahneman-Tversky Optimization (TKTO) is proposed to improve cross-task generalization. The paper evaluates the performance of ChatAD and nine baselines across seven datasets and tasks, showing significant improvements in accuracy, F1 score, and reduction in false positives. The optimized ChatAD demonstrates competitive performance in reasoning and cross-task generalization for classification, forecasting, and imputation tasks.",153.52,Phi-4,Nvidia B200 (Cloud Native)
2601.13547v1_HateXScore A Metric Suite for Evaluating Reasoning.pdf,HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations,"['Yujia Hu', 'Roy Ka-Wei Lee']","This paper introduces HateXScore, a metric suite designed to evaluate the reasoning quality of explanations in hate speech detection. It assesses four key components: conclusion explicitness, faithfulness and causal grounding of quoted spans, identification of protected groups, and logical consistency among these elements. Evaluated on six diverse hate speech datasets, HateXScore aims to complement standard metrics by revealing interpretability failures and annotation inconsistencies. Human evaluation shows strong agreement with HateXScore, validating it as a practical tool for trustworthy and transparent moderation.",154.94,Phi-4,Nvidia B200 (Cloud Native)
2601.13558v1_Leveraging ChatGPT and Other NLP Methods for Ident.pdf,Leveraging ChatGPT and Other NLP Methods for Identifying Risk and Protective Behaviors in MSM: Social Media and Dating Apps Text Analysis,"['Mehrab Beikzadeh', 'Chenglin Hong', 'Cory J Cascalheira', 'Callisto Boka', 'Majid Sarrafzadeh', 'Ian W Holloway']","This study explores the potential of using text data from social media and dating apps to predict risk and protective behaviors among men who have sex with men (MSM). The research focuses on identifying behaviors such as condomless anal sex, number of sexual partners, binge drinking, and heavy drinking, as well as pre-exposure prophylaxis (PrEP) use. Machine learning models were trained using ChatGPT embeddings, BERT embeddings, LIWC analysis, and a custom dictionary-based approach. The models demonstrated high predictive accuracy for certain behaviors, particularly monthly binge drinking and having over five sexual partners. The study concludes that text data can provide valuable insights for personalized public health interventions, with potential improvements through increased data collection.",154.76,Phi-4,Nvidia B200 (Cloud Native)
2601.13559v1_AgentGC Evolutionary Learning-based Lossless Compr.pdf,AgentGC: Evolutionary Learning-based Lossless Compression for Genomics Data with LLM-driven Multiple Agent,"['Hui Sun', 'Yanfeng Ding', 'Huidong Ma', 'Chang Xu', 'Keyan Jin', 'Lizheng Zu', 'Cheng Zhong', 'Xiaoguang Liu', 'Gang Wang', 'Wentong Cai']","AgentGC introduces an evolutionary, agent-based approach to lossless compression for genomics data, addressing limitations of current learning-based methods. It features a multi-layer architecture with a user-friendly interface, cognitive optimization, and automated compression. The system supports diverse scenarios with modes prioritizing compression ratio, throughput, or balance. Compared to 14 baselines across 9 datasets, AgentGC achieves significant improvements in compression ratios and throughput.",154.01,Phi-4,Nvidia B200 (Cloud Native)
2601.13562v1_Reasoning is a Modality.pdf,Reasoning is a Modality,"['Zhiguang Liu', 'Yi Shang']","The paper hypothesizes that reasoning is a distinct modality separate from the low-level workspace where rules are applied. To test this, the authors designed a novel role-separated transformer block for solving ARC tasks, which separates global controller tokens from grid workspace tokens. This design enables iterative rule execution and aims to mimic human-like reasoning by maintaining a distinct internal channel for reasoning. The method achieved 62.6% accuracy on ARC-1, surpassing average human performance and outperforming previous methods. The approach emphasizes a shift from dense ViT baselines to controller-driven reasoning, aligning more closely with human cognitive processes.",154.88,Phi-4,Nvidia B200 (Cloud Native)
2601.13563v1_ButterflyMoE Sub-Linear Ternary Experts via Struct.pdf,ButterflyMoE: Sub-Linear Ternary Experts via Structured Butterfly Orbits,['Aryan Karmore'],"The paper introduces ButterflyMoE, a method that addresses the memory scaling bottleneck in Mixture of Experts (MoE) models by using a shared ternary-quantized substrate and learned rotations. This approach reduces memory requirements from O(N·d^2) to O(d^2 + N·dlogd), achieving a 150x memory reduction at 256 experts with negligible accuracy loss. The method allows for more experts to fit on edge devices, such as a 4GB memory limit, compared to standard MoE models. The key benefits include improved memory compression, suppression of activation outliers, and significant reduction in quantization error. ButterflyMoE demonstrates dense accuracy on language modeling benchmarks and enables stable training without recovery stages.",154.84,Phi-4,Nvidia B200 (Cloud Native)
2601.13564v1_Multi-objective fluorescent molecule design with a.pdf,Multi-objective fluorescent molecule design with a data-physics dual-driven generative framework,"['Yanheng Li', 'Zhichen Pu', 'Lijiang Yang', 'Zehao Zhou', 'Yi Qin Gao']","This paper presents a novel framework for the design of fluorescent molecules, integrating both data-driven and physics-driven approaches. The dual-driven generative framework aims to optimize multiple objectives in fluorescent molecule design, enhancing both the efficiency and effectiveness of the process. The authors demonstrate the framework's capability through various examples, showcasing its potential to significantly advance the field of molecular design.",155.37,Phi-4,Nvidia B200 (Cloud Native)
2601.13566v1_Self-Improvement as Coherence Optimization A Theor.pdf,Self-Improvement as Coherence Optimization: A Theoretical Account,"['Tianyi Qiu', 'Ahmed Hani Ismail', 'Zhonghao He', 'Shi Feng']","This paper explores how language models can improve their accuracy without external supervision. It introduces the concept of coherence optimization, which involves finding a context-to-behavior mapping that is most compressible and jointly predictable. The authors demonstrate that coherence optimization is equivalent to description-length regularization and is optimal for semi-supervised learning when the regularizer is derived from a pretrained model. The paper provides a theoretical framework explaining why feedback-free self-improvement methods, such as debate, bootstrap, and internal coherence maximization, can match supervised finetuning performance.",155.15,Phi-4,Nvidia B200 (Cloud Native)
2601.13570v1_GeoDynamics A Geometric State-Space Neural Network.pdf,GeoDynamics: A Geometric State-Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds,"['Tingting Dan', 'Jiaqi Ding', 'Guorong Wu']","GeoDynamics introduces a geometric state-space neural network designed to track latent brain-state trajectories on the high-dimensional SPD manifold. This approach captures the dynamics of symmetric positive-definite matrices representing brain functional connectivity, offering insights into task-driven state changes and early markers of neurological disorders such as Alzheimer’s, Parkinson’s, and autism. The model's effectiveness is validated across neuroscience and human action recognition benchmarks, demonstrating its robustness in modeling complex spatiotemporal dynamics.",155.22,Phi-4,Nvidia B200 (Cloud Native)
2601.13580v1_Neural Organ Transplantation NOT Checkpoint-Based .pdf,CHECKPOINT-BASEDMODULARADAPTATION FOR TRANSFORMERMODELS,['Ahmad Al-Zuraiqi'],"This paper introduces Neural Organ Transplantation (NOT), a modular adaptation framework for transformer models. NOT allows trained transformer layers to be used as reusable transferable checkpoints for domain adaptation. Unlike traditional fine-tuning, NOT extracts layer subsets from pre-trained models, trains them on domain-specific data, and saves them as standalone checkpoints. These checkpoints can be transplanted into compatible models without needing the original training data. Experiments on GPT-2, TinyLlama, and GPT-OSS show that NOT outperforms existing methods, improving perplexity significantly over LoRA and training faster. The method shows position dependence, with early insertion yielding better results, and offers regularization benefits in cross-domain transfer at a billion-parameter scale. The findings suggest that transformer middle layers can support efficient modular transfer for decoder-only architectures, enabling privacy-preserving expertise sharing. However, the approach is currently limited to decoder-only models, with reduced effectiveness in encoder-based architectures.",154.05,Phi-4,Nvidia B200 (Cloud Native)
2601.13581v1_SCRIPTMIND Crime Script Inference and Cognitive Ev.pdf,Crime Script Inference and Cognitive Evaluation for LLM-based Social Engineering Scam Detection System,"['Heedou Kim', 'Changsik Kim', 'Sanghwa Shin', 'Jaewoo Kang']","The paper introduces SCRIPTMIND, a framework for LLM-based social engineering scam detection that integrates automated reasoning with human cognition. It includes the Crime Script Inference Task (CSIT) for scam reasoning, the Crime Script–Aware Inference Dataset (CSID) for fine-tuning small LLMs, and the Cognitive Simulation-based Evaluation of Social Engineering Defense (CSED) for assessing cognitive impact. The framework was tested on 571 Korean phone scam cases, resulting in a fine-tuned 11B small LLM that outperformed GPT-4o by 13% in detection accuracy, false-positive reduction, scammer utterance prediction, and rationale quality. SCRIPTMIND enhances users' cognitive awareness and suspicion levels, representing a step toward human-centered, cognitively adaptive LLMs for scam defense.",154.75,Phi-4,Nvidia B200 (Cloud Native)
2601.13588v1_TREX Tokenizer Regression for Optimal Data Mixture.pdf,TREX: Tokenizer Regression for Optimal Data Mixture,"['Inho Won', 'Hangyeol Yoo', 'Minkyung Cho', 'Jungyeul Park', 'Hoyun Song', 'KyungTae Lim']","The paper introduces TREX, a regression-based framework designed to predict optimal data mixtures for tokenizer training in multilingual Large Language Models (LLMs). TREX addresses the challenge of determining language-specific data ratios by training small-scale proxy tokenizers on random mixtures, gathering compression statistics, and learning to predict compression performance from these mixtures. This approach allows for scalable mixture search before large-scale tokenizer training, improving compression efficiency by up to 12% compared to existing methods. The framework demonstrates strong scalability, robustness, and practical effectiveness, with reproducible experiments available on GitHub.",154.46,Phi-4,Nvidia B200 (Cloud Native)
2601.13589v1_Motion-to-Response Content Generation via Multi-Ag.pdf,MOTION-TO-RESPONSE CONTENT GENERATION VIA MULTI-AGENT AI SYSTEM WITH REAL-TIME SAFETY VERIFICATION,['HyeYoung Lee'],"This paper introduces a multi-agent AI system designed to generate response-oriented media content in real time based on audio-derived emotional signals. The system emphasizes transforming inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The architecture includes four agents: an Emotion Recognition Agent, a Response Policy Decision Agent, a Content Parameter Generation Agent, and a Safety Verification Agent. The system ensures compliance with predefined safety rules and demonstrates high performance in emotion recognition accuracy, response mode consistency, and safety compliance, with low inference latency suitable for on-device deployment. The modular design allows for interpretability and extensibility, making it applicable to various domains such as child-adjacent media, therapeutic applications, and emotionally responsive smart devices.",155.12,Phi-4,Nvidia B200 (Cloud Native)
2601.13590v1_Vulnerability of LLMs Belief Systems LLMs Belief R.pdf,Vulnerability of LLMs’ Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions,"['Fan Huang', 'Haewoon Kwak', 'Jisun An']","This paper investigates the susceptibility of Large Language Models (LLMs) to persuasion and belief alteration through strategic conversational interventions. Utilizing the Source–Message–Channel–Receiver (SMCR) communication framework, the study evaluates how different persuasive strategies affect belief stability across five mainstream LLMs and three domains: factual knowledge, medical QA, and social bias. The findings reveal that smaller models are highly compliant, with significant belief changes occurring early in the interaction. Meta-cognition prompting, intended to enhance robustness, paradoxically increases vulnerability. Adversarial fine-tuning shows promise in improving robustness, with GPT-4o-mini achieving near-complete resistance and Mistral 7B showing substantial improvement, while Llama models remain highly susceptible. These insights highlight the model-dependent limitations of current robustness interventions and provide guidance for developing more trustworthy LLMs.",154.54,Phi-4,Nvidia B200 (Cloud Native)
2601.13591v1_DSAEval Evaluating Data Science Agents on a Wide R.pdf,DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems,"['Maojun Sun', 'Yifei Xie', 'Yue Wu', 'Ruijian Han', 'Binyan Jiang', 'Defeng Sun', 'Yancheng Yuan', 'Jian Huang']","The paper introduces DSAEval, a benchmark for evaluating data science agents on real-world problems. It addresses the challenge of evaluating these agents due to the open-ended nature of data science tasks. DSAEval includes 641 problems across 285 datasets, featuring multimodal perception, multi-query interactions, and multi-dimensional evaluation. The benchmark evaluates 11 advanced LLMs, highlighting Claude-Sonnet-4.5 for overall performance, GPT-5.2 for efficiency, and MiMo-V2-Flash for cost-effectiveness. The study finds that multimodal perception enhances performance on vision-related tasks and identifies ongoing challenges in unstructured data domains.",154.31,Phi-4,Nvidia B200 (Cloud Native)
2601.13592v1_Machine learning based radiative parameterization .pdf,Machine learning based radiative parameterization scheme and its performance in operational reforecast experiments,"['Jing Hao', 'Xiao Sa', 'Li Haoyu', 'Xiao Huadong', 'Xue Wei']","This study explores the use of machine learning methods to enhance computational efficiency in simulating radiation processes within numerical models. It addresses critical limitations in hybrid forecasting frameworks that integrate deep neural networks with numerical prediction models, focusing on coupling compatibility and long-term integration stability. A residual convolutional neural network is used to approximate the Rapid Radiative Transfer Model for General Circulation Models (RRTMG) in China Meteorological Administration's global operational system. The study employs an offline training and online coupling approach, generating a comprehensive dataset through model simulations. The dataset is enhanced with experience replay and physical significance constraints to ensure hybrid model stability. A LibTorch-based coupling method is utilized for real-time operational computations. The hybrid model successfully performs ten-day integrated forecasts, with a two-month operational reforecast experiment showing that the machine learning emulator achieves accuracy comparable to traditional physical schemes while accelerating computation speed by approximately eightfold.",155.15,Phi-4,Nvidia B200 (Cloud Native)
2601.13599v1_Diffusion In Diffusion Breaking the Autoregressive.pdf,Diffusion In Diffusion: Breaking the Autoregressive Bottle-neck in Block Diffusion Models,"['Linrui Ma', 'Yufei Cui', 'Kai Han', 'Yunhe Wang']","This paper introduces a novel framework called DIFFUSION INDIFFUSION to address the limitations of block diffusion language models, which suffer from irreversibility and myopia due to their semi-autoregressive nature. The proposed method involves a 'draft-then-refine' approach, where initial drafts are generated using small blocks and then refined with a larger bidirectional receptive field. The approach utilizes snapshot confidence remasking to identify critical tokens for modification and employs mix-scale training to enhance global capabilities. Empirical results show that this method sets a new benchmark for discrete diffusion models on the OpenWebText dataset, significantly reducing generative perplexity and narrowing the performance gap with autoregressive models.",154.71,Phi-4,Nvidia B200 (Cloud Native)
2601.13600v1_Foundations of Global Consistency Checking with No.pdf,Foundations of Global Consistency Checking with Noisy LLM Oracles,"['Paul He', 'Elke Kirschbaum', 'Shiva Kasiviswanathan']","The paper addresses the challenge of ensuring global consistency in collections of natural-language facts, which is crucial for tasks like fact-checking, summarization, and knowledge base construction. While Large Language Models (LLMs) can assess the consistency of small subsets of facts, their judgments are noisy, and pairwise checks are insufficient for global coherence. The authors propose an adaptive divide-and-conquer algorithm to identify minimal inconsistent subsets (MUSes) of facts and compute minimal repairs through hitting-sets, achieving low-degree polynomial query complexity. Experiments demonstrate the method's efficiency in detecting and localizing inconsistencies, providing a scalable framework for linguistic consistency verification with LLM-based evaluators.",154.65,Phi-4,Nvidia B200 (Cloud Native)
2601.13614v1_CauScientist Teaching LLMs to Respect Data for Cau.pdf,Teaching LLMs to Respect Data for Causal Discovery,"['Bo Peng', 'Sirui Chen', 'Lei Xu', 'Chaochao Lu']","The paper introduces CauScientist, a collaborative framework that combines large language models (LLMs) with probabilistic statistics to improve causal discovery. LLMs serve as hypothesis-generating 'data scientists,' while probabilistic statistics act as 'verifiers.' The framework uses hybrid initialization to select starting graphs, iteratively refines structures through LLM-proposed modifications validated by statistical criteria, and maintains error memory to guide the search space. Experiments show that CauScientist outperforms purely data-driven baselines, achieving significant improvements in F1 score and recall, and reduces structural hamming distance compared to standalone LLMs on complex graphs.",154.73,Phi-4,Nvidia B200 (Cloud Native)
2601.13622v1_CARPE Context-Aware Image Representation Prioritiz.pdf,Context-Aware Image Representation Prioritization via Ensemble (CARPE) for Large Vision-Language Models,"['Donghee Lee', 'Rui Cai', 'Zhe Zhao']","This paper introduces CARPE, a novel framework designed to enhance the performance of Large Vision-Language Models (LVLMs) in vision-centric tasks, particularly image classification. CARPE employs vision-integration layers and a context-aware ensemble strategy to dynamically prioritize image representations or leverage the reasoning capabilities of the language model. This approach improves the model's adaptability in weighting visual and textual modalities, leading to better generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only boosts performance on image classification benchmarks but also enhances results across various vision-language tasks. The framework is designed to be integrated with most open-source LVLMs, ensuring its adaptability across different architectures.",154.96,Phi-4,Nvidia B200 (Cloud Native)
2601.13632v1_Resilient Routing Risk-Aware Dynamic Routing in Sm.pdf,Resilient Routing: Risk-Aware Dynamic Routing in Smart Logistics via Spatiotemporal Graph Learning,"['Zhiming Xue', 'Sichen Zhao', 'Yalun Qi', 'Xianling Zeng', 'Zihan Yu']","This paper introduces a Risk-Aware Dynamic Routing (RADR) framework that integrates Spatiotemporal Graph Neural Networks (ST-GNN) with combinatorial optimization to address the challenges faced by logistics networks under the pressure of e-commerce growth. The framework constructs a logistics topology graph using discrete GPS data and employs a hybrid deep learning model combining Graph Convolutional Network (GCN) and Gated Recurrent Unit (GRU) to predict congestion risks. These predictions are used in a dynamic edge weight mechanism for path planning. Evaluated on the Smart Logistics Dataset 2024, the RADR algorithm enhances supply chain resilience by reducing congestion risk exposure by 19.3% with a minimal increase in transportation distance by 2.1%, demonstrating an effective balance between delivery efficiency and operational safety.",154.89,Phi-4,Nvidia B200 (Cloud Native)
2601.13645v1_Quadratic Upper Bound for Boosting Robustness.pdf,Quadratic Upper Bound for Boosting Robustness,"['Euijin You', 'Hyang-Won Lee']","This paper addresses the challenge of enhancing model robustness against adversarial attacks with reduced training time in Fast Adversarial Training (FAT). The authors propose a novel loss function based on a quadratic upper bound (QUB) on the adversarial training loss, which improves robustness without requiring stronger inner maximization. Experimental results demonstrate significant robustness improvements when applying the QUB loss to existing FAT methods, attributed to a smoother loss landscape in the resulting models.",155.01,Phi-4,Nvidia B200 (Cloud Native)
2601.13647v1_Fusion Segment Transformer Bi-Directional Attentio.pdf,FUSION SEGMENT TRANSFORMER: BI-DIRECTIONAL ATTENTION GUIDED FUSION NETWORK FOR AI GENERATED MUSIC DETECTION,"['Yumin Kim', 'Seonghyeon Go']","This paper addresses the challenge of detecting AI-generated music in full-audio formats, which requires modeling long-term structure and context. The authors propose an improved version of the Segment Transformer, termed the Fusion Segment Transformer. This model enhances full-audio AI-generated music detection by introducing a Gated Fusion Layer that integrates content and structural information, capturing long-term context. Experiments on the SONICS and AIME datasets demonstrate that this approach outperforms previous models and recent baselines, achieving state-of-the-art results in AI-generated music detection.",155.07,Phi-4,Nvidia B200 (Cloud Native)
2601.13649v1_Fairness or Fluency An Investigation into Language.pdf,Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge,"['Xiaolin Zhou', 'Zheng Luo', 'Yicheng Gao', 'Qixuan Chen', 'Xiyang Hu', 'Yue Zhao', 'Ruishan Liu']","This paper investigates language bias in LLM-as-a-judge applications, focusing on two types of biases: performance disparity between languages in same-language judging and bias towards major languages in inter-language judging. The study finds significant performance disparities across language families, with European languages outperforming African languages, especially in culturally-related subjects. Additionally, models tend to favor English in inter-language comparisons, influenced more by the language of the answer than the question. The paper also explores the relationship between language bias and low-perplexity bias, concluding that while there is a slight correlation, language bias cannot be fully explained by perplexity alone.",154.51,Phi-4,Nvidia B200 (Cloud Native)
2601.13655v1_Why Does the LLM Stop Computing An Empirical Study.pdf,Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs,"['Guangba Yu', 'Zirui Wang', 'Yujie Huang', 'Renyi Zhong', 'Yuedong Zhong', 'Yilun Wang', 'Michael R. Lyu']","This paper presents the first large-scale empirical study of 705 real-world failures in open-source Large Language Models (LLMs) from the DeepSeek, Llama, and Qwen ecosystems. The study identifies a paradigm shift in reliability challenges from model algorithmic defects to systemic fragility in deployment stacks. Key findings include Diagnostic Divergence, Systemic Homogeneity, and Lifecycle Escalation, providing actionable insights for improving LLM reliability.",154.14,Phi-4,Nvidia B200 (Cloud Native)
2601.13657v1_Communication-Free Collective Navigation for a Swa.pdf,Communication-Free Collective Navigation for a Swarm of UAVs via LiDAR-Based Deep Reinforcement Learning,"['Myong-Yol Choi', 'Hankyoul Ko', 'Hanse Cho', 'Changseung Kim', 'Seunghwan Kim', 'Jaemin Seo', 'Hyondong Oh']","This paper introduces a deep reinforcement learning (DRL) based controller for collective navigation of UAV swarms in environments where communication is denied. The approach is inspired by biological swarms, utilizing an implicit leader-follower framework where only the leader has goal information. Follower UAVs learn robust policies using onboard LiDAR sensing, without inter-agent communication or leader identification. The system employs LiDAR point clustering and an extended Kalman filter for stable neighbor tracking, independent of external positioning systems. The DRL controller, trained in Nvidia Isaac Sim, enables followers to learn complex behaviors such as flocking and obstacle avoidance using local perception. The approach's robustness and sim-to-real transfer are validated through simulations and real-world experiments with a swarm of five UAVs, demonstrating successful collective navigation in diverse environments without communication or external localization.",154.72,Phi-4,Nvidia B200 (Cloud Native)
2601.13659v1_Temporal-Spatial Decouple before Act Disentangled .pdf,TEMPORAL-SPATIAL DECOUPLE BEFORE ACT: DISENTANGLED REPRESENTATION LEARNING FOR MULTIMODAL SENTIMENT ANALYSIS,"['Chunlei Meng', 'Ziyang Zhou', 'Lucas He', 'Xiaojing Du', 'Chun Ouyang', 'Zhongxue Gan']","This paper introduces TSDA (Temporal-Spatial Decouple before Act), a novel approach for multimodal sentiment analysis that decouples each modality into temporal dynamics and spatial structural context before interaction. By aligning temporal features with their counterparts across modalities and spatial features with their spatial counterparts, TSDA addresses spatiotemporal information asymmetry and enhances performance. The method includes factor-consistent cross-modal alignment, factor-specific supervision, decorrelation regularization, and a Gated Recouple module to integrate aligned streams for task performance. Extensive experiments demonstrate TSDA's superiority over baselines, with ablation analysis confirming the design's necessity and interpretability.",154.85,Phi-4,Nvidia B200 (Cloud Native)
2601.13671v1_The Orchestration of Multi-Agent Systems Architect.pdf,"The Orchestration of Multi-Agent Systems: Architectures, Protocols, and Enterprise Adoption","['Apoorva Adimulam', 'Rajesh Gupta', 'Sumit Kumar']","This paper presents a unified architectural framework for orchestrated multi-agent systems, integrating planning, policy enforcement, state management, and quality operations into a coherent orchestration layer. It introduces two communication protocols: the Model Context Protocol, which standardizes agent access to external tools and data, and the Agent-to-Agent protocol, which governs peer coordination, negotiation, and delegation. These protocols enable scalable, auditable, and policy-compliant reasoning across distributed agent collectives. The paper also discusses orchestration logic, governance frameworks, and observability mechanisms to ensure system coherence, transparency, and accountability. By synthesizing these elements, the paper provides a comprehensive technical blueprint for enterprise-scale AI ecosystems.",154.9,Phi-4,Nvidia B200 (Cloud Native)
2601.13684v1_HeteroCache A Dynamic Retrieval Approach to Hetero.pdf,HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache,"['Zhiyuan Shi', 'Qibo Qiu', 'Feng Xue', 'Zhonglin Jiang', 'Li Yu', 'Jian Jiang', 'Xiaofei He', 'Wenxiao Wang']","The paper introduces HeteroCache, a dynamic compression framework designed to address the inefficiencies of static compression methods in handling the KV cache for long-context tasks in large language models (LLMs). Traditional static methods often fail to preserve important information due to attention drift, where the significance of tokens changes over time. HeteroCache leverages insights into the temporal heterogeneity of attention heads and spatial redundancy among them to implement a fine-grained caching strategy. This approach dynamically allocates cache resources based on the stability and redundancy of attention heads, using a hierarchical storage mechanism to manage I/O latency effectively. The framework demonstrates significant performance improvements, achieving up to 3x acceleration in decoding compared to the original model in a 224K context, and outperforming existing methods on multiple benchmarks.",154.36,Phi-4,Nvidia B200 (Cloud Native)
2601.13687v1_Understanding Mental States to Guide Social Influe.pdf,Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue,"['Zhichao Liang', 'Satoshi Nakamura']","This paper introduces the SocialMindChange benchmark, which advances the dynamic Theory of Mind (ToM) by shifting from merely tracking mental states to actively influencing them in social interactions. The benchmark involves a model playing a character in a multi-person dialogue across five scenes, aiming to guide the mental-state trajectory of participants towards a goal. The study constructs 1,200 social contexts, validated for realism, and evaluates ten state-of-the-art language models, revealing a significant performance gap compared to human capabilities. The findings highlight the challenges current models face in maintaining and altering mental-state representations over extended interactions.",154.91,Phi-4,Nvidia B200 (Cloud Native)
2601.13693v1_End-to-End Reverse Screening Identifies Protein Ta.pdf,End-to-End Reverse Screening Identifies Protein Targets of Small Molecules Using HelixFold3,"['Shengjie Xu', 'Xianbin Ye', 'Mengran Zhu', 'Xiaonan Zhang', 'Shanzhuo Zhang', 'Xiaomin Fang']","This paper introduces an end-to-end reverse screening strategy using HelixFold3, a biomolecular structure prediction model, to identify protein targets for small molecules. The method improves upon conventional reverse docking by enhancing screening accuracy, structural fidelity, binding-site precision, and target prioritization. It provides a scalable platform for understanding drug action, predicting off-target effects, and supporting rational drug discovery.",155.1,Phi-4,Nvidia B200 (Cloud Native)
2601.13697v1_Uncertainty-Aware Gradient Signal-to-Noise Data Se.pdf,Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning,"['Zhihang Yuan', 'Chengyu Yue', 'Long Huang', 'Litu Ou', 'Lei Shi']","This paper introduces GRADFILTERING, a novel data selection framework for instruction tuning of large language models (LLMs). It addresses the challenges of large, noisy, and redundant datasets by utilizing a small GPT-2 proxy with a LoRA ensemble to compute a Gradient Signal-to-Noise Ratio (G-SNR) for each example. This method is objective-agnostic and uncertainty-aware, allowing it to dynamically assess the importance of data points during training. GRADFILTERING outperforms random subsets and existing baselines in LLM-as-a-judge evaluations and human assessments, and it achieves faster convergence under the same computational budget. The approach highlights the benefits of incorporating uncertainty into data selection processes.",154.24,Phi-4,Nvidia B200 (Cloud Native)
2601.13698v1_Does Privacy Always Harm Fairness Data-Dependent T.pdf,Does Privacy Always Harm Fairness? Data-Dependent Trade-offs via Chernoff Information Neural Estimation,"['Arjun Nichani', 'Hsiang Hsu', 'Chun-Fu (Richard) Chen', 'Haewon Jeong']","This paper explores the relationship between fairness, privacy, and accuracy in machine learning using the information-theoretic measure Chernoff Information. It introduces the concept of Noisy Chernoff Difference to analyze the interplay among these three aspects. The study reveals that the relationship is data-dependent, with distinct behaviors observed in synthetic data based on its distribution. The paper also proposes a method for estimating Chernoff Information on real datasets, contributing to a unified understanding of the fairness-privacy-accuracy triad.",154.87,Phi-4,Nvidia B200 (Cloud Native)
2601.13704v1_Performance and Complexity Trade-off Optimization .pdf,Performance and Complexity Trade-off: Optimization of Speech Models During Training,"['Esteban Gómez', 'Tom Bäckström']","In speech machine learning, neural network models are typically designed with fixed architectures and trained to maximize performance. However, this approach often neglects the optimal trade-off between performance and computational complexity. The authors propose a reparameterization technique using feature noise injection, allowing joint optimization of performance and computational complexity during training with SGD-based methods. This method dynamically optimizes model size for a target performance-complexity trade-off, without relying on heuristic criteria for pruning. The effectiveness of this approach is demonstrated through case studies in voice activity detection and audio anti-spoofing, with publicly available code to encourage further research.",154.96,Phi-4,Nvidia B200 (Cloud Native)
2601.13707v1_Attention-space Contrastive Guidance for Efficient.pdf,Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs,"['Yujin Jo', 'Sangyoon Bae', 'Taesup Kim']","This paper addresses the issue of hallucinations in large vision–language models (LVLMs), where language priors often dominate over visual evidence, leading to object misidentification and inconsistent descriptions. The authors propose Attention-space Contrastive Guidance (ACG), a mechanism that operates within self-attention layers to create both vision–language and language-only attention paths in a single forward computation. This approach reduces the model's reliance on language priors and enhances visual grounding. ACG achieves state-of-the-art performance in faithfulness and caption quality while significantly reducing computational costs, offering a more efficient alternative to previous methods that require multiple forward passes.",154.79,Phi-4,Nvidia B200 (Cloud Native)
2601.13709v1_Hidden in Plain Text Measuring LLM Deception Quali.pdf,Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games,"['Christopher Kao', 'Vanshika Vats', 'James Davis']","This paper investigates the ability of Large Language Model (LLM) agents to deceive in social contexts using the Social Deduction Game (SDG) Mafia. The study employs an asynchronous multi-agent framework to simulate realistic social interactions. By simulating 35 Mafia games with GPT-4o LLM agents and analyzing game transcripts with a GPT-4-Turbo based Mafia Detector, the research compares LLM deception quality to human performance. Results indicate that LLMs are more effective at blending in and deceiving compared to humans, as evidenced by lower prediction accuracy of mafia players in LLM games. The study highlights both the sophistication and potential risks of LLM deception in social settings and releases a dataset of LLM Mafia transcripts for future research.",154.71,Phi-4,Nvidia B200 (Cloud Native)
2601.13710v1_Who Should Have Surgery A Comparative Study of Gen.pdf,Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction,"['Sayeed Shafayet Chowdhury', 'Snehasis Mukhopadhyay', 'Shiaofen Fang', 'Vijay R. Ramakrishnan']","This study evaluates the effectiveness of supervised machine learning (ML) models versus generative artificial intelligence (GenAI) models in predicting surgical outcomes for chronic rhinosinusitis (CRS). The research focuses on pre-operative prediction of clinically meaningful improvement, defined as a ≥8.9-point reduction in SNOT-22 at 6 months. The study compares various ML models, including logistic regression, tree ensembles, and an in-house MLP, against GenAI models like ChatGPT, Claude, Gemini, and Perplexity. The best-performing ML model, an MLP, achieved 85% accuracy with superior calibration and decision-curve net benefit, while GenAI models underperformed in discrimination and calibration. The study suggests an ML-first, GenAI-augmented workflow for surgical candidacy triage, using ML for primary decision-making and GenAI for enhancing transparency and shared decision-making.",154.02,Phi-4,Nvidia B200 (Cloud Native)
2601.13717v1_Simulated Ignorance Fails A Systematic Study of LL.pdf,Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting,"['Zehan Li', 'Yuxuan Wang', 'Ali El Lahib', 'Ying-Jieh Xia', 'Xinyu Pi']","This paper evaluates the effectiveness of Simulated Ignorance (SI) in approximating True Ignorance (TI) for Large Language Models (LLMs) in forecasting tasks. The study systematically tests SI across 477 questions and 9 models, revealing a significant performance gap between SI and TI. It finds that SI fails to suppress prior knowledge, even with chain-of-thought reasoning, and that reasoning-optimized models do not improve SI fidelity. The paper concludes that SI-based retrospective setups are flawed for benchmarking forecasting capabilities and recommends against their use.",153.98,Phi-4,Nvidia B200 (Cloud Native)
2601.13719v1_Hierarchical Long Video Understanding with Audiovi.pdf,Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search,"['Xinlei Yin', 'Xiulian Peng', 'Xiao Li', 'Zhiwei Xiong', 'Yan Lu']","This paper introduces HAVEN, a framework for long video understanding that integrates audiovisual entity cohesion and hierarchical video indexing with agentic search. It addresses challenges in vision-language models by preserving semantic consistency across visual and auditory streams and organizing content into a structured hierarchy. The framework employs an agentic search mechanism for dynamic retrieval and reasoning, enhancing narrative reconstruction and entity tracking. Experiments show that HAVEN achieves high temporal coherence, entity consistency, and retrieval efficiency, setting a new state-of-the-art with 84.1% accuracy on LVBench, and excelling in reasoning tasks with 80.1% performance.",154.72,Phi-4,Nvidia B200 (Cloud Native)
2601.13722v1_OP-Bench Benchmarking Over-Personalization for Mem.pdf,OP-Bench: Benchmarking Over-Personalization for Memory-Augmented Personalized Conversational Agents,"['Yulin Hu', 'Zimo Long', 'Jiahe Guo', 'Xingyu Sui', 'Xing Fu', 'Weixiang Zhao', 'Yanyan Zhao', 'Bing Qin']","This paper introduces OP-Bench, a benchmark designed to evaluate over-personalization in memory-augmented conversational agents. Over-personalization is defined as the inappropriate use of personal information, categorized into Irrelevance, Repetition, and Sycophancy. The benchmark consists of 1,700 verified instances from long-horizon dialogue histories. The study evaluates various large language models and memory-augmentation methods, revealing widespread over-personalization issues. A proposed solution, Self-ReCheck, is a lightweight, model-agnostic mechanism to filter memory and mitigate over-personalization while maintaining personalization performance. This work aims to enhance the control and appropriateness of personalization in conversational AI.",154.33,Phi-4,Nvidia B200 (Cloud Native)
2601.13734v1_Towards robust long-context understanding of large.pdf,TOWARDS ROBUST LONG-CONTEXT UNDERSTANDING OF LARGE LANGUAGE MODEL VIA ACTIVE RECAP LEARNING,['Chenyu Hui'],"This paper introduces Active Recap Learning (ARL), a framework designed to enhance the long-context understanding capabilities of large language models (LLMs). ARL enables models to revisit and summarize earlier content through targeted sequence construction during continued pretraining and retrospective summarization at inference. The approach identifies key tokens in long contexts based on loss gaps and summarizes relevant preceding paragraphs using an LLM. ARL allows models to autonomously generate and utilize these summaries during inference, establishing a recursive memory mechanism. Experimental results demonstrate significant improvements, with ARL achieving a 26.8% improvement on RULER and a 9.44% improvement on LongBench. ARL provides a simple yet effective method for continued pretraining-based long-context understanding, advancing scalable memory augmentation in LLMs.",154.61,Phi-4,Nvidia B200 (Cloud Native)
2601.13735v1_Reasoning or Fluency Dissecting Probabilistic Conf.pdf,Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection,"['Hojin Kim', 'Jaehyung Kim']","This paper challenges the assumption that probabilistic confidence metrics accurately reflect reasoning quality in Best-of-N selection by examining their sensitivity to inter-step causal dependencies. The authors introduce perturbations that disrupt these dependencies while maintaining local fluency, finding that selection accuracy remains largely unaffected. This suggests that current metrics primarily capture surface-level fluency rather than logical structure. To address this, the authors propose a contrastive causality metric that better isolates inter-step causal dependencies, demonstrating improved output selection fidelity compared to existing probability-based methods.",154.66,Phi-4,Nvidia B200 (Cloud Native)
2601.13749v1_Pro-AI Bias in Large Language Models.pdf,Pro-AI Bias in Large Language Models,"['Benaya Trabelsi', 'Jonathan Shaki', 'Sarit Kraus']","This paper investigates whether large language models (LLMs) exhibit a systematic preferential bias in favor of artificial intelligence (AI). Through three experiments, the authors find consistent evidence of pro-AI bias. LLMs disproportionately recommend AI-related options in response to diverse queries, overestimate salaries for AI-related jobs, and show representational centrality for 'Artificial Intelligence' in internal model representations. These findings suggest that LLM-generated advice and valuation can skew choices and perceptions in high-stakes decisions.",155.0,Phi-4,Nvidia B200 (Cloud Native)
2601.13752v1_Finding RELIEF Shaping Reasoning Behavior without .pdf,Finding RELIEF: Shaping Reasoning Behavior without Reasoning,"['Chak Tou Leong', 'Dingwei Chen', 'Heming Xia', 'Qingyu Yin', 'Sunbowen Lee', 'Jian Wang', 'Wenjie Li']","This paper introduces RELIEF, a framework for shaping the behavior of Large Reasoning Models (LRMs) by aligning their internal reasoning beliefs with a target belief blueprint, without relying on reasoning-trace supervision. RELIEF utilizes logit probing to capture latent reasoning beliefs and fine-tunes models using synthesized question-answering pairs. The approach demonstrates improved efficiency and faithfulness in reasoning tasks, outperforming traditional behavior-supervised and preference-based methods while reducing training costs. The study highlights the potential of belief engineering to effectively shape LRM behavior by targeting underlying internal concepts.",154.23,Phi-4,Nvidia B200 (Cloud Native)
2601.13761v1_DARC Decoupled Asymmetric Reasoning Curriculum for.pdf,DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution,"['Shengda Fan', 'Xuyan Ye', 'Yankai Lin']","The paper introduces DARC (Decoupled Asymmetric Reasoning Curriculum), a framework designed to stabilize the self-evolution process of large language models (LLMs) through self-play. It addresses challenges such as optimization instability and bootstrapping errors by decoupling the training of the Questioner and Solver. The Questioner is trained to generate difficulty-calibrated questions, while the Solver is trained with an asymmetric self-distillation mechanism using high-quality pseudo-labels from a document-augmented teacher. Empirical results show that DARC improves performance across various reasoning benchmarks and models, outperforming baselines and approaching fully supervised models without human annotations.",154.57,Phi-4,Nvidia B200 (Cloud Native)
2601.13768v1_vLinear A Powerful Linear Model for Multivariate T.pdf,vLinear: A Powerful Linear Model for Multivariate Time Series Forecasting,"['Wenzhen Yue', 'Ruohao Guo', 'Ji Shi', 'Zihan Hao', 'Shiyu Hu', 'Xianghua Ying']","This paper introduces vLinear, a linear-based multivariate time series forecaster that features two main components: the vecTrans module and the WFMLoss objective. The vecTrans module is designed to model multivariate correlations using a learnable vector, reducing computational complexity from O(N^2) to O(N), and can be integrated into Transformer-based models to achieve up to 5× inference speedups while maintaining performance. The WFMLoss objective, a final-series-oriented flow matching loss, enhances forecasting accuracy by focusing on more reliable paths and horizons. Empirical results show that vLinear achieves state-of-the-art performance across 22 benchmarks and 124 forecasting settings, and WFMLoss can improve existing forecasters as a plug-and-play objective.",151.76,Phi-4,Nvidia B200 (Cloud Native)
2601.13770v1_Look-Ahead-Bench a Standardized Benchmark of Look-.pdf,Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance,['Mostapha Benhenda'],"The paper introduces Look-Ahead-Bench, a benchmark designed to measure look-ahead bias in Point-in-Time Large Language Models (LLMs) within realistic financial workflows. Unlike existing methods that test lookahead knowledge through Q&A, this benchmark evaluates model behavior in practical scenarios. It distinguishes genuine predictive capability from memorization by analyzing performance decay across different market regimes and using quantitative baselines. The study evaluates open-source LLMs like Llama 3.1 and DeepSeek 3.2 against PiT-Inference models, revealing significant lookahead bias in standard LLMs, while PiT models show better generalization and reasoning. This work lays the groundwork for standardized evaluation of temporal bias in financial LLMs and provides a framework for identifying models suitable for real-world deployment.",154.31,Phi-4,Nvidia B200 (Cloud Native)
2601.13798v1_Insight Interpretable Semantic Hierarchies in Visi.pdf,Interpretable Semantic Hierarchies in Vision-Language Encoders,"['Kai Wittenmayer', 'Sukrut Rao', 'Amin Parchami-Araghi', 'Bernt Schiele', 'Jonas Fischer']","This paper introduces INSIGHT, a language-aligned concept foundation model designed to provide interpretable, fine-grained, and spatially grounded concepts for vision tasks. By leveraging a hierarchical sparse autoencoder and a foundation model with strong semantic representations, INSIGHT automatically extracts concepts at various granularities. It defines concept relationships through local co-occurrence dependencies, enhancing concept naming and offering richer explanations. The model demonstrates competitive performance in classification and segmentation tasks compared to opaque foundation models, while providing high-quality concept-based explanations. This work addresses the challenge of opaque representations in language-aligned vision foundation models, which are crucial for interpretability in safety-critical applications.",154.69,Phi-4,Nvidia B200 (Cloud Native)
2601.13809v1_DroneVLA VLA based Aerial Manipulation.pdf,DroneVLA: VLA based Aerial Manipulation,"['Fawad Mehboob', 'Monijesu James', 'Amir Habel', 'Jeffrin Sam', 'Miguel Altamirano Cabrera', 'Dzmitry Tsetserukou']","This paper introduces an autonomous aerial manipulation system that interprets high-level natural language commands to retrieve and deliver objects to a human user. The system integrates a MediaPipe-based Grounding DINO and a Vision-Language-Action (VLA) model with a custom-built drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. VLA performs semantic reasoning to interpret user prompts and generates a task queue for object grasping. Grounding DINO and a dynamic A* planning algorithm are used for navigation and object relocation. A human-centric controller driven by MediaPipe ensures safe interaction during the handover phase by providing real-time human pose and orientation estimation. The system's efficacy is demonstrated through real-world experiments for localization and navigation, highlighting the feasibility of VLA for aerial manipulation operations.",154.27,Phi-4,Nvidia B200 (Cloud Native)
2601.13846v1_Virtual Urbanism An AI-Driven Framework for Quanti.pdf,Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity. A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments,['Glinskaya Maria'],"This paper introduces Virtual Urbanism (VU), a multimodal AI-driven framework for quantifying urban identity through synthetic urban replicas. The framework aims to develop computationally tractable urban identity metrics. A pilot study, 'Virtual Urbanism and Tokyo Microcosms,' demonstrates feasibility by using a pipeline integrating Stable Diffusion and LoRA models to produce synthetic replicas of nine Tokyo areas. These replicas, rendered as dynamic sequences without existing orientation markers, were evaluated through human experiments to assess perceptual legitimacy, quantify area-level identity, and derive core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. The Urban Identity Level (UIL) metric assessed identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements. VU is positioned as a viable framework for AI-augmented urban analysis, outlining a path toward automated, multi-parameter identity metrics.",153.76,Phi-4,Nvidia B200 (Cloud Native)
2601.13864v1_HardSecBench Benchmarking the Security Awareness o.pdf,HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware Code Generation,"['Qirui Chen', 'Jingxian Shuai', 'Shuangwu Chen', 'Shenghao Ye', 'Zijian Wen', 'Xufei Su', 'Jie Jin', 'Jiangming Li', 'Jun Chen', 'Xiaobin Tan', 'Jian Yang']","This paper introduces HardSecBench, a benchmark designed to assess the security awareness of Large Language Models (LLMs) in hardware and firmware code generation. The benchmark includes 924 tasks covering Verilog RTL and firmware-level C, addressing 76 hardware-relevant Common Weakness Enumeration (CWE) entries. It highlights the gap in evaluating security issues in LLM-generated code, which may appear functionally correct but contain security flaws. The study reveals that LLMs often meet functional requirements but leave security risks, with results varying based on prompting. The findings emphasize challenges and provide insights for future advancements in LLM-assisted hardware design.",153.95,Phi-4,Nvidia B200 (Cloud Native)
2601.13880v1_LifeAgentBench A Multi-dimensional Benchmark and A.pdf,LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health,"['Ye Tian', 'Zihao Wang', 'Onat Gungor', 'Xiaoran Fan', 'Tajana Rosing']","This paper introduces LifeAgentBench, a large-scale QA benchmark designed for evaluating long-horizon, cross-dimensional, and multi-user lifestyle health reasoning. It contains 22,573 questions that range from basic retrieval to complex reasoning tasks. The paper also presents an extensible benchmark construction pipeline and a standardized evaluation protocol to assess LLM-based health assistants. Eleven leading LLMs are evaluated on LifeAgentBench, revealing key bottlenecks in long-horizon aggregation and cross-dimensional reasoning. The proposed LifeAgent baseline agent integrates multi-step evidence retrieval with deterministic aggregation, showing significant improvements over existing baselines. Case studies demonstrate its potential in real-life scenarios, and the benchmark is publicly available.",154.61,Phi-4,Nvidia B200 (Cloud Native)
2601.13885v1_Confident Rankings with Fewer Items Adaptive LLM E.pdf,Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores,"['Esma Balkır', 'Alice Pernthaller', 'Marco Basaldella', 'José Hernández-Orallo', 'Nigel Collier']","This paper extends Item Response Theory (IRT)-based adaptive testing to accommodate continuous bounded scores, such as those used in LLM evaluation tasks like summarization and machine translation. The authors replace the Bernoulli response distribution with a heteroskedastic normal distribution to handle continuous scores. They introduce an uncertainty-aware ranker with adaptive stopping criteria, achieving reliable model ranking with fewer test items. The method is validated across five benchmarks, showing improved ranking correlation and high accuracy on confident predictions.",154.63,Phi-4,Nvidia B200 (Cloud Native)
2601.13887v1_Human Simulation Computation A Human-Inspired Fram.pdf,Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems,['Hong Su'],"This paper introduces Human Simulation Computation (HSC), a framework inspired by human cognitive processes, aimed at enhancing the adaptability of AI systems. HSC models intelligence as a continuous, closed-loop process involving thinking, action, learning, reflection, and activity scheduling. It emphasizes active participation in both internal reasoning and environmental interactions, using actions to refine reasoning mechanisms without external intervention. The paper argues that human-like reasoning and action-grounded methods are crucial for robust adaptation and effective real-world interaction, as language-based models alone are insufficient for these tasks.",155.14,Phi-4,Nvidia B200 (Cloud Native)
2601.13895v1_OmniOVCD Streamlining Open-Vocabulary Change Detec.pdf,OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3,"['Xu Zhang', 'Danyang Li', 'Yingjie Xia', 'Xiaohang Dong', 'Hualong Yu', 'Jianye Wang', 'Qicheng Li']","This paper introduces OmniOVCD, a framework for Open-Vocabulary Change Detection (OVCD) leveraging the Segment Anything Model 3 (SAM 3). It addresses the limitations of existing training-free OVCD methods that rely on multiple models, which can lead to feature mismatch and instability. By utilizing SAM 3's decoupled output heads, OmniOVCD employs a Synergistic Fusion to Instance Decoupling (SFID) strategy to construct and decompose land-cover masks into individual instance masks for change comparison. This approach maintains high accuracy in category recognition and instance-level consistency, resulting in accurate change masks. Experiments on four public benchmarks demonstrate state-of-the-art performance, surpassing previous methods.",153.92,Phi-4,Nvidia B200 (Cloud Native)
2601.13897v1_TractRLFusion A GPT-Based Multi-Critic Policy Fusi.pdf,TRACTRLFUSION: A GPT-BASED MULTI-CRITIC POLICY FUSION FRAMEWORK FOR FIBER TRACTOGRAPHY,"['Ankita Joshi', 'Ashutosh Sharma', 'Anoushkrit Goel', 'Ranjeet Ranjan Jha', 'Chirag Ahuja', 'Arnav Bhavsar', 'Aditya Nigam']","Tractography is crucial for non-invasive reconstruction of white matter fiber pathways, aiding in brain connectivity studies and neurosurgical planning. Traditional methods include deterministic and probabilistic approaches, while recent advancements leverage supervised deep learning (DL) and deep reinforcement learning (DRL) for improved tract reconstruction. A challenge in tractography is accurately reconstructing white matter tracts while minimizing spurious connections. This paper introduces TractRLFusion, a novel GPT-based policy fusion framework that integrates multiple RL policies through a data-driven fusion strategy. The method involves a two-stage training data selection process for effective policy fusion, followed by a multi-critic fine-tuning phase to enhance robustness and generalization. Experiments on HCP, ISMRM, and TractoInferno datasets show that TractRLFusion outperforms individual RL policies and state-of-the-art classical and DRL methods in terms of accuracy and anatomical reliability.",153.09,Phi-4,Nvidia B200 (Cloud Native)
2601.13904v1_PREFAB PREFerence-based Affective Modeling for Low.pdf,PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation,"['Jaeyoung Moon', 'Youjin Choi', 'Yucheon Park', 'David Melhart', 'Georgios N. Yannakakis', 'Kyung-Joong Kim']","PREFAB is a low-budget retrospective self-annotation method designed to alleviate the cognitive workload associated with full self-annotation in affective computing. It focuses on detecting affective inflection regions using a preference-learning model, allowing annotators to label only selected segments and interpolate the rest. This approach is grounded in the peak-end rule and ordinal representations of emotion. PREFAB includes a preview mechanism to assist annotation and has been evaluated through a technical performance study and a user study with 25 participants. Results indicate that PREFAB outperforms baselines in modeling affective inflections while reducing workload and improving annotator confidence without compromising annotation quality.",153.55,Phi-4,Nvidia B200 (Cloud Native)
2601.13920v1_Asymmetric regularization mechanism for GAN traini.pdf,Asymmetric regularization mechanism for GAN training with Variational Inequalities,"['Spyridon C. Giagtzoglou', 'Mark H.M. Winands', 'Barbara Franci']","This paper addresses the instability issues in Generative Adversarial Networks (GANs) training by formulating it as a Nash equilibrium seeking problem. The authors propose an asymmetric regularization mechanism applied only to the discriminator side, which stabilizes the training process and ensures convergence to a Nash equilibrium. The regularization is based on the classic Tikhonov step and a novel zero-centered gradient penalty. Under certain conditions, explicit Lipschitz and strong-monotonicity constants for the regularized operator are derived, ensuring last-iterate linear convergence of a single-call Extrapolation-from-the-Past (EFTP) method. Empirical simulations demonstrate that this approach can stabilize the trajectory and converge to an equilibrium even when strong monotonicity is not achieved.",154.14,Phi-4,Nvidia B200 (Cloud Native)
2601.13938v1_IF-GEO Conflict-Aware Instruction Fusion for Multi.pdf,IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization,"['Heyang Zhou', 'JiaJia Chen', 'Xiaolu Chen', 'Jie Bao', 'Zhen Chen', 'Yong Liao']","The paper introduces IF-GEO, a framework designed to optimize documents for diverse queries in Generative Engines. It addresses the challenge of conflicting revision requirements under a limited content budget by proposing a 'diverge-then-converge' approach. This involves mining optimization preferences from latent queries and synthesizing a Global Revision Blueprint through conflict-aware instruction fusion. The paper also introduces risk-aware stability metrics to quantify cross-query stability. Experiments demonstrate that IF-GEO achieves significant performance improvements while maintaining robustness across various retrieval scenarios.",154.56,Phi-4,Nvidia B200 (Cloud Native)
2601.13942v1_Glance-or-Gaze Incentivizing LMMs to Adaptively Fo.pdf,Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning,"['Hongbo Bai', 'Yujin Zhou', 'Yile Wu', 'Chi-Min Chan', 'Pengcheng Wen', 'Kunhao Pan', 'Sirui Han', 'Yike Guo']","The paper introduces Glance-or-Gaze (GoG), a framework designed to enhance Large Multimodal Models (LMMs) in handling knowledge-intensive visual queries. GoG shifts from passive perception to active visual planning, using a Selective Gaze mechanism to dynamically focus on relevant regions and filter out noise. The framework employs a dual-stage training strategy: Reflective GoG Behavior Alignment for fundamental paradigm instillation and Complexity-Adaptive Reinforcement Learning for handling complex queries. Experiments demonstrate state-of-the-art performance across six benchmarks, with ablation studies confirming the necessity of both Selective Gaze and complexity-adaptive RL for effective visual search.",153.84,Phi-4,Nvidia B200 (Cloud Native)
2601.13948v1_Stream-Voice-Anon Enhancing Utility of Real-Time S.pdf,STREAM-VOICE-ANON: ENHANCING UTILITY OF REAL-TIME SPEAKER ANONYMIZATION VIA NEURAL AUDIO CODEC AND LANGUAGE MODELS,"['Nikita Kuzmin', 'Songting Liu', 'Kong Aik Lee', 'Eng Siong Chng']","This paper introduces Stream-Voice-Anon, a system designed to enhance real-time speaker anonymization using neural audio codecs (NAC) and causal language models (LM). The system addresses the limitations of existing NAC-based online LM systems, which are primarily designed for voice conversion rather than anonymization. Stream-Voice-Anon incorporates techniques such as pseudo-speaker representation sampling, speaker embedding mixing, and diverse prompt selection strategies to prevent speaker information leakage. The paper evaluates the system under the VoicePrivacy 2024 Challenge protocol, demonstrating significant improvements in intelligibility and emotion preservation compared to the previous state-of-the-art method, DarkStream, while maintaining comparable latency and privacy protection.",154.14,Phi-4,Nvidia B200 (Cloud Native)
2601.13964v1_RL-BioAug Label-Efficient Reinforcement Learning f.pdf,RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning,"['Cheol-Hui Lee', 'Hwa-Yeon Lee', 'Dong-Joo Kim']","This paper introduces RL-BioAug, a framework that uses a label-efficient reinforcement learning agent to determine optimal data augmentation policies for self-supervised EEG representation learning. The method leverages only 10% of labeled data to guide the agent, enabling the encoder to learn robust representations without extensive labeled datasets. Experimental results show significant improvements over random augmentation strategies, with notable performance gains on the Sleep-EDFX and CHB-MIT datasets. The framework suggests replacing heuristic-based augmentations with an autonomous paradigm, demonstrating its potential in EEG analysis.",154.61,Phi-4,Nvidia B200 (Cloud Native)
2601.13969v1_Autonomous Knowledge Graph Exploration with Adapti.pdf,Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval,"['Joaquín Polonuer', 'Lucas Vittor', 'Iñaki Arango', 'Ayush Noori', 'David A. Clifton', 'Luciano Del Corro', 'Marinka Zitnik']","The paper introduces ARK (Adaptive Retriever of Knowledge), an agent-based knowledge graph (KG) retriever that allows language models to control the tradeoff between breadth and depth in KG exploration. ARK uses a two-operation toolset: global lexical search and one-hop neighborhood exploration, which can be composed into multi-hop traversal. This approach adapts to the nature of queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. ARK demonstrates significant improvements in retrieval performance on the STaRK benchmark, achieving 59.1% average Hit@1 and 67.4 average MRR, outperforming other retrieval-based and agentic training-free methods. Additionally, ARK's tool-use trajectories are distilled into an 8B model, enhancing performance on various datasets while maintaining high accuracy.",152.95,Phi-4,Nvidia B200 (Cloud Native)
2601.13992v1_The Whole Is Greater Than the Sum of Its Parts A C.pdf,A Compatibility-Aware Multi-Teacher CoT Distillation Framework,"['Jin Cui', 'Jiaqi Guo', 'Jiepeng Zhou', 'Ruixuan Yang', 'Jiayi Lu', 'Jiajun Xu', 'Jiangcheng Song', 'Boran Zhao', 'Pengju Ren']","This paper introduces COMPACT, a framework designed to enhance the distillation of reasoning capabilities from multiple Large Language Models (LLMs) into compact Student Models (SLMs). Unlike traditional methods that rely on a single teacher, COMPACT dynamically fuses supervisions from various teachers by evaluating the student's compatibility in real-time. It employs a multi-dimensional metric comprising Graph-based Consensus, Mutual-Information-based Adaptability, and Loss-based Difficulty to ensure effective integration of diverse reasoning paths without compromising the model's original knowledge structure. The framework aims to mitigate catastrophic forgetting and achieve superior performance across various benchmarks.",153.55,Phi-4,Nvidia B200 (Cloud Native)
2601.13994v1_torch-sla Differentiable Sparse Linear Algebra wit.pdf,torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch,['Mingyuan Chi'],"The paper introduces 'torch-sla', an open-source PyTorch library designed for GPU-accelerated, scalable, and differentiable sparse linear algebra. It addresses three main challenges: GPU acceleration for sparse operations, multi-GPU scaling through domain decomposition with halo exchange, and efficient integration with PyTorch for adjoint-based differentiation. The library supports multiple backends and enables large-scale sparse linear solves, nonlinear solves, and eigenvalue computations, facilitating end-to-end differentiable simulations.",154.81,Phi-4,Nvidia B200 (Cloud Native)
2601.13999v1_DAME Duration-Aware Matryoshka Embedding for Durat.pdf,DURATION-AWARE MATRYOSHKA EMBEDDING FOR DURATION-ROBUST SPEAKER VERIFICATION,"['Youngmoon Jung', 'Joon-Young Yang', 'Ju-ho Kim', 'Jaeyoung Roh', 'Chang Woo Han', 'Hoon-Young Cho']","This paper introduces the Duration-Aware Matryoshka Embedding (DAME), a model-agnostic framework designed to improve speaker verification performance across varying utterance durations. DAME addresses the challenge of limited speaker-discriminative cues in short speech segments by constructing a nested hierarchy of sub-embeddings aligned to utterance durations. Lower-dimensional embeddings capture compact speaker traits from short utterances, while higher-dimensional embeddings encode richer details from longer speech. The framework supports both training from scratch and fine-tuning, consistently improving performance across durations without additional inference cost. Experimental results on the VoxCeleb1-O/E/H and VOiCES evaluation sets demonstrate reduced equal error rates on short-duration trials while maintaining full-length performance.",153.84,Phi-4,Nvidia B200 (Cloud Native)
2601.14012v1_MATE Matryoshka Audio-Text Embeddings for Open-Voc.pdf,MATRYOSHKA AUDIO–TEXT EMBEDDINGS FOR OPEN-VOCABULARY KEYWORD SPOTTING,"['Youngmoon Jung', 'Myunghun Jung', 'Joon-Young Yang', 'Yong-Hyeok Lee', 'Jaeyoung Roh', 'Hoon-Young Cho']","This paper introduces Matryoshka Audio–Text Embeddings (MATE), a novel dual-encoder framework for open-vocabulary keyword spotting (KWS) with text-based enrollment. Unlike traditional methods that use fixed-dimensionality embeddings, MATE encodes multiple embedding granularities within a single vector through nested sub-embeddings or 'prefixes'. The framework employs PCA-guided prefix alignment to align audio and text prefixes, concentrating salient keyword cues in lower-dimensional prefixes while adding detail in higher dimensions. MATE is trained using standard deep metric learning objectives and achieves state-of-the-art results on WSJ and LibriPhrase datasets without any inference overhead. This work represents the first application of matryoshka-style embeddings to KWS, offering a flexible and efficient solution for open-vocabulary keyword spotting.",154.82,Phi-4,Nvidia B200 (Cloud Native)
2601.14022v1_Credible CO2 Comparisons A Machine Learning Approa.pdf,Credible CO2 Comparisons: A Machine Learning Approach to Vehicle Powertrain Assessment,"['Rodrigo Pereira David', 'Luciano Araujo Dourado Filho', 'Daniel Marques da Silva', 'João Alfredo Cal-Braz']","This paper proposes a machine learning-based framework for assessing the CO2 emissions of internal combustion engine vehicles (ICEVs) and electric vehicles (EVs) under identical, real-world driving conditions. By isolating technology-specific effects and using recurrent neural network models, the framework enables direct comparison of powertrain performance. It constructs counterfactual scenarios to evaluate what emissions an EV would generate if it followed the same driving profile as an ICEV. This approach provides a scalable foundation for credible, data-driven assessments of vehicle carbon performance under real-world conditions.",153.96,Phi-4,Nvidia B200 (Cloud Native)
2601.14027v1_Numina-Lean-Agent An Open and General Agentic Reas.pdf,Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics,"['Junqi Liu', 'Zihao Zhou', 'Zekai Zhu', 'Marco Dos Santos', 'Weikun He', 'Jiawei Liu', 'Ran Wang', 'Yunzhou Xie', 'Junqiao Zhao', 'Qiufeng Wang', 'Lihong Zhi', 'Jia Li', 'Wenda Li']","This paper introduces Numina-Lean-Agent, a novel agentic reasoning system for formal mathematics that leverages a general coding agent to enhance flexibility and reproducibility in theorem proving. Unlike existing approaches that rely on task-specific pipelines and trained formal provers, Numina-Lean-Agent utilizes a general coding agent to interface with diverse reasoning tasks, improve performance by updating the base model, and autonomously extend and call specialized tools. The system combines Claude Code with Numina-Lean-MCP to interact with Lean, retrieve relevant theorems, and perform informal proving. It successfully solved all problems in the Putnam 2025 competition and demonstrated its generality by formalizing the Brascamp–Lieb theorem. The authors release Numina-Lean-Agent and all solutions publicly.",154.42,Phi-4,Nvidia B200 (Cloud Native)
2601.14039v1_Generalizing Abstention for Noise-Robust Learning .pdf,Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation,"['Wesam Moustafa', 'Hossam Elsafty', 'Helen Schneider', 'Lorenz Sparrenberg', 'Rafet Sifa']","This paper addresses the challenge of label noise in medical image segmentation by introducing a universal and modular abstention framework. The framework enhances the noise-robustness of various loss functions through an informed regularization term and a flexible power-law-based auto-tuning algorithm for the abstention penalty. The proposed methods, GAC, SAC, and ADS, demonstrate significant improvements over non-abstaining baselines, particularly under high noise levels, on the CaDIS and DSAD medical datasets. The work highlights the effectiveness of enabling models to selectively ignore corrupted samples for building more reliable segmentation models.",154.0,Phi-4,Nvidia B200 (Cloud Native)
2601.14041v1_Top 10 Open Challenges Steering the Future of Diff.pdf,Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants,"['Yunhe Wang', 'Kai Han', 'Huiling Zhen', 'Yuchuan Tian', 'Hanting Chen', 'Yongbing Huang', 'Yufei Cui', 'Yingte Shu', 'Shan Gao', 'Ismail Elezi', 'Roy Vaughan Miles', 'Songcen Xu', 'Feng Wen', 'Chao Xu', 'Sinan Zeng', 'Dacheng Tao']","The paper discusses the limitations of auto-regressive (AR) Large Language Models (LLMs) and introduces Diffusion Language Models (DLMs) as a transformative alternative. DLMs conceptualize text generation as a bidirectional denoising process, offering potential advantages over AR models. However, DLMs are often constrained by existing AR infrastructures and optimization frameworks. The authors identify ten fundamental challenges that hinder the full realization of DLMs' potential, including architectural inertia and gradient sparsity. They propose a strategic roadmap organized into four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence. The roadmap suggests a shift towards a diffusion-native ecosystem with features like multi-scale tokenization, active remasking, and latent thinking to overcome the causal horizon constraints. This transition is deemed essential for developing next-generation AI capable of complex structural reasoning, dynamic self-correction, and seamless multimodal integration.",155.88,Phi-4,Nvidia B200 (Cloud Native)
2601.14047v1_Collective intelligence in science direct elicitat.pdf,COLLECTIVE INTELLIGENCE IN SCIENCE: DIRECT ELICITATION OF DIVERSE INFORMATION FROM EXPERTS WITH UNKNOWN INFORMATION STRUCTURE,"['ALEXEY V. OSIPOV', 'NIKOLAY N. OSIPOV']","The paper proposes a mechanism for deep collective analysis of open scientific problems using a self-resolving play-money prediction market entangled with a chat. This system allows experts to share private information about a complex scientific hypothesis directly through the chat, facilitating efficient aggregation of diverse information. The approach is designed to reach an equilibrium where participants trade as if the market were resolved according to the truth of the hypothesis, even when the ground truth is unknown. The method is robust and interpretable, rewarding experts with real assets based on their play money outcomes, thus providing a novel way to fund large-scale collaborative studies.",159.43,Phi-4,Nvidia B200 (Cloud Native)
2601.14051v1_Kakugo Distillation of Low-Resource Languages into.pdf,Kakugo: Distillation of Low-Resource Languages into Small Language Models,"['Peter Devine', 'Mardhiyah Sanni', 'Farid Adilazuarda', 'Julieta Gil Loizaga', 'Barry Haddow']","Kakugo is a novel and cost-effective pipeline designed to train general-purpose Small Language Models (SLMs) for low-resource languages using only the language name as input. By leveraging a large teacher model to generate synthetic prompts and translate instruction datasets, the authors produced training data and SLMs for 54 low-resource languages. Evaluations across various NLP tasks, including translation, classification, and question answering, show that Kakugo consistently improves performance over base models. The pipeline offers an accessible method for communities to develop language-specific AI with a total generation and training cost of under $50 per language.",158.61,Phi-4,Nvidia B200 (Cloud Native)
2601.14053v1_LLMOrbit A Circular Taxonomy of Large Language Mod.pdf,LLMOrbit: A Circular Taxonomy of Large Language Models — From Scaling Walls to Agentic AI Systems,"['Badri N. Patro', 'Vijay S. Agneeswaran']","The paper presents LLMOrbit, a comprehensive circular taxonomy that maps the landscape of large language models from 2019 to 2025. It examines over 50 major models across 15 organizations, focusing on architectural innovations, training methodologies, and efficiency patterns. The authors identify a 'scaling wall' characterized by data scarcity, exponential cost growth, and unsustainable energy consumption, which limits brute-force scaling. They propose six paradigms to overcome these challenges, including test-time compute, quantization, distributed edge computing, model merging, efficient training, and small specialized models. The paper highlights three paradigm shifts: post-training gains, efficiency revolution, and democratization of AI. It provides insights into key techniques and traces the evolution from passive generation to active tool-using agents. The authors conclude that reasoning emergence in LLMs requires scale, reinforcement learning with verifiable feedback, and test-time search, positioning LLMOrbit as a technical reference and roadmap for future research.",158.3,Phi-4,Nvidia B200 (Cloud Native)
2601.14055v1_Decoder-Free Supervoxel GNN for Accurate Brain-Tum.pdf,Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI,"['Andrea Protani', 'Marc Molina Van Den Bosch', 'Lorenzo Giusti', 'Heloisa Barbosa Da Silva', 'Paolo Cacace', 'Albert Sund Aillet', 'Miguel Angel Gonzalez Ballester', 'Friedhelm Hummel', 'Luigi Serio']","This paper introduces SVGFormer, a decoder-free pipeline for brain-tumor localization in multi-modal MRI. It utilizes a content-aware grouping stage to partition the volume into a semantic graph of super-voxels. The hierarchical encoder combines a patch-level Transformer with a supervoxel-level Graph Attention Network to model fine-grained intra-region features and broader inter-regional dependencies. The approach focuses on feature encoding, providing dual-scale explainability. Two models were trained on the BraTS dataset: one for node-level classification and one for tumor proportion regression. The classification model achieved an F1-score of 0.875, and the regression model a MAE of 0.028, demonstrating the encoder's ability to learn discriminative and localized features. The results suggest that a graph-based, encoder-only paradigm is an accurate and interpretable alternative for 3D medical image representation.",158.84,Phi-4,Nvidia B200 (Cloud Native)
2601.14056v1_POCI-Diff Position Objects Consistently and Intera.pdf,POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion,"['Andrea Rigo', 'Luca Stornaiuolo', 'Weijie Wang', 'Mauro Martino', 'Bruno Lepri', 'Nicu Sebe']","The paper introduces POCI-Diff, a diffusion-based approach for Text-to-Image (T2I) generation that allows consistent and interactive 3D layout control and editing. Unlike prior methods that rely on 2D cues or iterative strategies, POCI-Diff enforces 3D geometric constraints and instance-level semantic binding within a unified diffusion process. This enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion. The method supports one-shot synthesis of complex multi-object scenes and a warping-free generative editing pipeline for object insertion, removal, and transformation. By conditioning the diffusion process on reference images using IP-Adapter, POCI-Diff maintains coherent object appearance and global scene coherence across interactive 3D edits. Experimental results show that POCI-Diff outperforms state-of-the-art methods in visual fidelity and layout adherence, eliminating warping-induced geometric artifacts.",159.06,Phi-4,Nvidia B200 (Cloud Native)
2601.14063v1_XCR-Bench A Multi-Task Benchmark for Evaluating Cu.pdf,XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs,"['Mohsinul Kabir', 'Tasnim Ahmed', 'Md Mezbaur Rahman', 'Shaoxiong Ji', 'Hassan Alhuzali', 'Sophia Ananiadou']","The paper introduces XCR-Bench, a benchmark designed to evaluate the cross-cultural reasoning capabilities of large language models (LLMs). It addresses the scarcity of high-quality corpora annotated with Culture-Specific Items (CSIs) and provides a dataset of 4.9k parallel sentences and 1,098 unique CSIs. The benchmark spans three reasoning tasks and integrates Newmark’s CSI framework with Hall’s Triad of Culture. The study reveals that state-of-the-art LLMs struggle with identifying and adapting CSIs related to social etiquette and cultural references, and exhibit regional and ethno-religious biases. The findings underscore the need for further research in cross-cultural NLP.",159.0,Phi-4,Nvidia B200 (Cloud Native)
2601.14069v1_Unsupervised Video Class-Incremental Learning via .pdf,Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management,"['Nattapong Kurpukdee', 'Adrian G. Bors']","This paper addresses the challenge of unsupervised video class-incremental learning (uVCIL), which involves learning video information without forgetting and without relying on data labels. Prior approaches have focused on supervised learning, which requires costly human annotation and is not always feasible. The authors propose a novel approach using a deep feature extractor network to generate representative video features without assuming class or task information. They build deep clusters from these features and use the model from previous tasks as an initial state to transfer knowledge to new tasks. The approach is evaluated on three standard video action recognition datasets (UCF101, HMDB51, and Something-to-Something V2) without using labels, outperforming other baselines. The paper highlights the limitations of supervised methods and presents a promising solution for real-world applications where labels are unavailable.",158.12,Phi-4,Nvidia B200 (Cloud Native)
2601.14084v1_DermaBench A Clinician-Annotated Benchmark Dataset.pdf,DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning,"['Abdurrahim Yilmaz', 'Ozan Erdem', 'Ece Gokyayla', 'Ayda Acar', 'Burc Bugra Dagtas', 'Dilara Ilhan Erdil', 'Gulsum Gencoglan', 'Burak Temelkuran']","DermaBench is a clinician-annotated dermatology Visual Question Answering (VQA) benchmark dataset designed to evaluate the capabilities of vision-language models (VLMs) in dermatology. Unlike existing datasets that focus on image-level classification tasks, DermaBench assesses models' abilities in visual understanding, language grounding, and clinical reasoning. Built on the Diverse Dermatology Images (DDI) dataset, it includes 656 clinical images from 570 unique patients, annotated with a hierarchical schema covering diagnosis, anatomic site, lesion morphology, and more. The dataset, released as metadata-only to respect licensing, is publicly available and aims to address the lack of standardized VQA benchmarks in dermatology.",154.09,Phi-4,Nvidia B200 (Cloud Native)
2601.14086v1_Two-Stream temporal transformer for video action c.pdf,TWO-STREAM TEMPORAL TRANSFORMER FOR VIDEO ACTION CLASSIFICATION,"['Nattapong Kurpukdee', 'Adrian G. Bors']","This study introduces a two-stream transformer video classifier designed to extract spatio-temporal information from both content and optical flow, representing movement information. The model leverages self-attention features across optical flow and temporal frames, utilizing a transformer encoder mechanism to represent their relationships. Experimental results demonstrate the model's effectiveness in classifying human activities across three well-known video datasets, showcasing its potential in video action recognition.",155.08,Phi-4,Nvidia B200 (Cloud Native)
2601.14087v1_1-bit Count-based Sorting Unit to Reduce Link Powe.pdf,1'-bit Count-based Sorting Unit to Reduce Link Power in DNN Accelerators,"['Ruichi Han', 'Yizhi Chen', 'Tong Lei', 'Jordi Altayo Gonzalez', 'Ahmed Hemani']","This paper addresses the challenge of interconnect power consumption in Deep Neural Network (DNN) accelerators, which is a significant bottleneck due to data movement. The authors propose a hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks (CNNs) that leverages approximate computing to group population counts into coarse-grained buckets. This design achieves a reduction in hardware area while preserving the benefits of data reordering in terms of link power. The approximate sorting unit achieves up to 35.4% area reduction and maintains a 19.50% bit transition (BT) reduction compared to a precise implementation. This work is the first to provide a practical hardware evaluation of a popcount sorting unit for DNNs, offering insights into the trade-offs between accuracy and area cost, and analyzing the impact on both link power and computational unit power.",154.84,Phi-4,Nvidia B200 (Cloud Native)
2601.14091v1_Zero-shot adaptable task planning for autonomous c.pdf,Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems,"['Hossein Naderi', 'Alireza Shojaei', 'Lifu Huang', 'Philip Agee', 'Kereshmeh Afsari', 'Abiola Akanmu']","This study explores the potential of foundation models to enhance the adaptability and generalizability of task planning in construction robots. Four models are proposed and implemented using lightweight, open-source large language models (LLMs) and vision language models (VLMs). These models include one single agent and three multi-agent teams that collaborate to create robot action plans. The models are evaluated across three construction roles: Painter, Safety Inspector, and Floor Tiling. Results show that the four-agent team outperforms the state-of-the-art GPT-4o in most metrics while being ten times more cost-effective. Additionally, teams with three and four agents demonstrate improved generalizability. By discussing how agent behaviors influence outputs, this study enhances the understanding of AI teams and supports future research in diverse unstructured environments beyond construction.",154.91,Phi-4,Nvidia B200 (Cloud Native)
2601.14096v1_Remapping and navigation of an embedding space via.pdf,Remapping and navigation of an embedding space via error minimization: a fundamental organizational principle of cognition in natural and artificial systems,"['Benedikt Hartl', 'Léo Pio-Lopez', 'Chris Fields', 'Michael Levin']","This paper explores the principle of remapping and navigating embedding spaces through error minimization, positing it as a fundamental organizational principle of cognition in both natural and artificial systems. The authors discuss how this principle can be applied to understand and enhance cognitive processes in various domains, including evolution, development, and intelligence. The study emphasizes the role of active inference and navigation policies within nested embedding spaces, providing insights into the mechanisms that underlie cognitive functions.",155.32,Phi-4,Nvidia B200 (Cloud Native)
2601.14099v1_Causal feature selection framework for stable soft.pdf,Causal Feature Selection Framework for Stable Soft Sensor Modeling based on Time-Delayed Cross Mapping,"['Shi-Shun Chen', 'Xiao-Yang Li', 'Enrico Zio']","This paper presents a causal feature selection framework for stable soft sensor modeling using time-delayed cross mapping. The authors, from Beihang University and Politecnico di Milano, propose a method to identify causal relationships between variables in a system, which enhances the stability and reliability of soft sensor models. The framework leverages time-delayed cross mapping to discern these relationships, aiming to improve predictive accuracy in industrial processes.",155.83,Phi-4,Nvidia B200 (Cloud Native)
2601.14115v1_Riemannian Liquid Spatio-Temporal Graph Network.pdf,Riemannian Liquid Spatio-Temporal Graph Network,"['Liangsi Lu', 'Jingchao Wang', 'Zhaorong Dai', 'Hanqian Liu', 'Yang Shi']","This paper introduces the Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), a framework that integrates continuous-time liquid dynamics with the geometric inductive biases of Riemannian manifolds. RLSTG models graph evolution through an Ordinary Differential Equation (ODE) on a curved manifold, capturing the intrinsic geometry of both static and dynamic spatio-temporal graphs. The paper provides theoretical guarantees for RLSTG, extending stability theorems of Liquid Time-Constant networks to the Riemannian domain and analyzing its expressive power. Experiments on real-world benchmarks show that RLSTG outperforms existing models on graphs with complex structures by combining advanced temporal dynamics with Riemannian spatial representation.",153.86,Phi-4,Nvidia B200 (Cloud Native)
2601.14124v1_Style Transfer as Bias Mitigation Diffusion Models.pdf,Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic,"['Saad Mankarious', 'Ayah Zirikly']","This paper proposes a pretraining-free diffusion-based approach for synthetic text generation aimed at mitigating gender bias in mental health analysis. By focusing on male-to-female style transfer using the CARMA Arabic mental health corpus, the authors address the gender imbalance in the dataset. They construct five datasets to capture various linguistic and semantic aspects of gender expression in Arabic and train separate diffusion models for each. The results demonstrate high semantic fidelity and meaningful stylistic divergence, confirming the effectiveness of diffusion-based style transfer in generating high-entropy, semantically faithful synthetic data without relying on pretrained large language models. This approach provides a flexible framework for addressing gender bias in low-resource mental health domains.",155.0,Phi-4,Nvidia B200 (Cloud Native)
2601.14152v1_Lost in the Prompt Order Revealing the Limitations.pdf,Revealing the Limitations of Causal Attention in Language Models,"['Hyunjong Ok', 'Jaeho Lee']","This paper investigates the sensitivity of large language models (LLMs) to prompt structure, particularly in multiple-choice question answering (MCQA). It reveals that the order of context, question, and options (CQO vs. QOC) significantly affects model performance, with CQO consistently outperforming QOC. The study identifies causal attention as the underlying mechanism, where the causal mask in QOC prompts creates an information bottleneck, preventing option tokens from accessing context. This work highlights the importance of understanding prompt structure for the practical reliability of LLMs.",155.06,Phi-4,Nvidia B200 (Cloud Native)
2601.14154v1_LLM Augmented Intervenable Multimodal Adaptor for .pdf,LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery,"['Shubham Pandey', 'Bhavin Jawade', 'Srirangaraj Setlur', 'Venu Govindaraju', 'Kenneth Seastedt']","This paper introduces MIRACLE, a deep learning architecture designed to predict postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE utilizes a hyperspherical embedding space to fuse heterogeneous inputs, extracting robust features from both structured clinical records and high-dimensional radiological images. An interventional deep learning module enhances prediction transparency and clinical utility, providing interpretable insights that allow domain experts to adjust recommendations interactively. The approach is validated on a real-world dataset of 3,094 lung cancer patients, demonstrating superior performance over traditional machine learning models and large language model variants in personalized and explainable postoperative risk management.",154.76,Phi-4,Nvidia B200 (Cloud Native)
2601.14157v1_ConceptCaps -- a Distilled Concept Dataset for Int.pdf,ConceptCaps - a Distilled Concept Dataset for Interpretability in Music Models,"['Bruno Sienkiewicz', 'Łukasz Neumann', 'Mateusz Modrzejewski']","The paper introduces ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. It addresses the limitations of existing music datasets by separating semantic modeling from text generation. A Variational Autoencoder (VAE) learns plausible attribute co-occurrence patterns, and a fine-tuned language model generates professional descriptions. This approach improves coherence and controllability over end-to-end methods. The dataset is validated through audio-text alignment, linguistic quality metrics, and TCA V analysis, confirming that concept probes recover musically meaningful patterns.",154.84,Phi-4,Nvidia B200 (Cloud Native)
2601.14160v1_Domain-Adaptation through Synthetic Data Fine-Tuni.pdf,Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law,"['Ali Hamza Bashir', 'Muhammad Rehan Khalid', 'Kostadin Cvejoski', 'Jana Birr', 'Jule Berghaus', 'Armin Berger', 'Sandra Halscheidt', 'Christian Temath', 'Rafet Sifa', 'David Berghaus']","This paper presents a method for adapting large language models (LLMs) to German legal question answering using a novel synthetic data generation approach. The approach systematically produces high-quality, diverse, and legally accurate question-answer pairs from German statutes. By employing automated filtering methods and parameter-efficient fine-tuning techniques, the adapted LLMs significantly outperform baseline models on German legal question answering tasks. The study demonstrates the feasibility of using synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.",154.84,Phi-4,Nvidia B200 (Cloud Native)
2601.14171v1_Paper2Rebuttal A Multi-Agent Framework for Transpa.pdf,Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance,"['Qianli Ma', 'Chang Guo', 'Zhiheng Tian', 'Siyu Wang', 'Jipeng Xiao', 'Yuanhao Yue', 'Zhipeng Zhang']","The paper introduces REBUTTALAGENT, a novel multi-agent framework designed to enhance the rebuttal generation process in peer reviews. Unlike traditional direct-to-text generation methods, REBUTTALAGENT treats rebuttal generation as an evidence-centric planning task. It decomposes complex feedback into atomic concerns, constructs hybrid contexts, and integrates an external search module to address concerns requiring external literature. This approach ensures that every argument is explicitly grounded in evidence, improving coverage, faithfulness, and strategic coherence. The framework is validated on REBUTTALBENCH, demonstrating superior performance over strong baselines.",154.33,Phi-4,Nvidia B200 (Cloud Native)
2601.14172v1_Human Values in a Single Sentence Moral Presence H.pdf,"Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum","['Víctor Yeste', 'Paolo Rosso']","This paper investigates sentence-level identification of 19 values in the Schwartz motivational continuum, focusing on out-of-context sentences from news and political manifestos. The study introduces a binary moral presence task and evaluates the performance of a presence-gated hierarchy versus a direct multi-label classifier using DeBERTa-base, augmented with lightweight signals. Additionally, it benchmarks instruction-tuned large language models (LLMs) and constructs simple ensembles. The findings suggest that lightweight signals and small ensembles provide the most reliable improvements, while hierarchical gating offers limited benefits under an 8GB single-GPU constraint.",155.25,Phi-4,Nvidia B200 (Cloud Native)
2601.14175v1_A model of errors in transformers.pdf,A model of errors in transformers,"['Suvrat Raju', 'Praneeth Netrapalli']","This paper investigates the error rates of Large Language Models (LLMs) on deterministic tasks requiring repetitive token processing, such as arithmetic. The authors propose that errors accumulate due to small inaccuracies in the attention mechanism, leading to a threshold-crossing effect. They derive a two-parameter model relating task accuracy to complexity, validated through empirical tests on models like Gemini 2.5 Flash and DeepSeek R1. The study challenges the notion of 'collapse of reasoning' in LLMs, suggesting instead that errors can be mitigated by prompt optimization. The research underscores the applicability of natural science techniques to analyze LLM behavior, reducing complex parameter interactions to two key variables.",154.94,Phi-4,Nvidia B200 (Cloud Native)
2601.14192v1_Toward Efficient Agents Memory Tool learning and P.pdf,"Toward Efficient Agents: A Survey of Memory, Tool learning, and Planning","['Xiaofang Yang', 'Lijun Li', 'Heng Zhou', 'Tong Zhu', 'Xiaoye Qu', 'Yuchen Fan', 'Qianshan Wei', 'Rui Ye', 'Li Kang', 'Yiran Qin', 'Zhiqiang Kou', 'Daizong Liu', 'Qi Li', 'Ning Ding', 'Siheng Chen', 'Jing Shao']","This paper investigates the efficiency of agentic systems, focusing on three core components: memory, tool learning, and planning. It reviews recent approaches that aim to improve efficiency by addressing costs such as latency, tokens, and steps. The paper characterizes efficiency through a trade-off between effectiveness and cost, using the Pareto frontier as a perspective. It also examines efficiency-oriented benchmarks and discusses key challenges and future directions in the field.",154.7,Phi-4,Nvidia B200 (Cloud Native)
2601.14209v1_InT Self-Proposed Interventions Enable Credit Assi.pdf,Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning,"['Matthew Y. R. Yang', 'Hao Bai', 'Ian Wu', 'Gene Yang', 'Amrith Setlur', 'Aviral Kumar']","This paper introduces Intervention Training (InT), a novel training paradigm for large language models (LLMs) to improve reasoning capabilities through fine-grained credit assignment. InT addresses the limitations of standard reinforcement learning (RL) by proposing targeted corrections to incorrect reasoning steps within a model's reasoning traces. By leveraging reference solutions and focusing on localized corrections, InT enables the model to generate counterfactual continuations that succeed where the original failed. The approach involves supervised fine-tuning on these interventions, which helps in upweighting the likelihood of correct interventions over mistakes. The paper demonstrates that models trained with InT serve as better initializations for subsequent RL training, significantly improving accuracy on mathematical reasoning tasks compared to a base model and larger open-source models.",154.53,Phi-4,Nvidia B200 (Cloud Native)
2601.14230v1_MASCOT Towards Multi-Agent Socio-Collaborative Com.pdf,MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems,"['Yiyang Wang', 'Yiqiao Jin', 'Alex Cabral', 'Josiah Hester']","The paper introduces MASCOT, a framework for multi-agent socio-collaborative companion systems designed to address issues like persona collapse and social sycophancy in multi-agent systems. MASCOT employs a bi-level optimization strategy, including Persona-Aware Behavioral Alignment and Collaborative Dialogue Optimization, to ensure individual persona fidelity and diverse, productive discourse. The framework significantly outperforms existing baselines in psychological support and workplace contexts, demonstrating improvements in persona consistency and social contribution. MASCOT provides a roadmap for developing socially intelligent multi-agent systems.",153.81,Phi-4,Nvidia B200 (Cloud Native)
2601.14232v1_KAGE-Bench Fast Known-Axis Visual Generalization E.pdf,KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning,"['Egor Cherepannov', 'Daniil Zelezetsky', 'Alexey K. Kovalev', 'Aleksandr I. Panov']","The paper introduces KAGE-Env, a JAX-native 2D platformer designed to evaluate visual generalization in reinforcement learning by isolating visual shifts. It presents KAGE-Bench, a benchmark with six suites that test various visual shifts while keeping the control problem constant. The study reveals that certain visual shifts, like background and photometric changes, significantly impact performance, while others, like agent-appearance shifts, are less detrimental. The benchmark's fast and reproducible evaluation is facilitated by a fully vectorized JAX implementation, allowing extensive testing on a single GPU.",154.7,Phi-4,Nvidia B200 (Cloud Native)
2601.14234v1_Q-learning with Adjoint Matching.pdf,Q-LEARNING WITH ADJOINT MATCHING,"['Qiyang Li', 'Sergey Levine']","The paper introduces Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning algorithm designed to address the challenge of optimizing expressive diffusion or flow-matching policies with respect to a parameterized Q-function in continuous-action RL. QAM leverages adjoint matching to transform the critic’s action gradient into a step-wise objective function, avoiding the numerical instability of backpropagation through multi-step denoising processes. This approach maintains policy expressivity and unbiased optimization, outperforming existing methods on hard, sparse reward tasks in both offline and offline-to-online RL settings.",154.55,Phi-4,Nvidia B200 (Cloud Native)
2601.14235v1_Opportunities in AIML for the Rubin LSST Dark Ener.pdf,Opportunities in AI/ML for the Rubin LSST,"['Eric Aubourg', 'Camille Avestruz', 'Matthew R. Becker', 'Biswajit Biswas', 'Rahul Biswas', 'Boris Bolliet', 'Adam S. Bolton', 'Clecio R. Bom', 'Raphaël Bonnet-Guerrini', 'Alexandre Boucaud', 'Jean-Eric Campagne', 'Chihway Chang', 'Aleksandra Ćiprijanović', 'Johann Cohen-Tanugi', 'Michael W. Coughlin', 'John Franklin Crenshaw', 'Juan C. Cuevas-Tello', 'Juan de Vicente', 'Seth W. Digel', 'Steven Dillmann', 'Mariano Javier de León Dominguez Romero', 'Alex Drlica-Wagner', 'Sydney Erickson', 'Alexander T. Gagliano', 'Christos Georgiou', 'Aritra Ghosh', 'Matthew Grayling', 'Kirill A. Grishin', 'Alan Heavens', 'Lindsay R. House', 'Mustapha Ishak', 'Wassim Kabalan', 'Arun Kannawadi', 'François Lanusse', 'C. Danielle Leonard', 'Pierre-François L’eǧet', 'Michelle Lochner', 'Yao-Yuan Mao', 'Peter Melchior', 'Grant Merz', 'Martin Millon', 'Anais Møller', 'Gautham Narayan', 'Yuuki Omori', 'Hiranya Peiris', 'Laurence Perreault-Levasseur', 'Andrés A. Plazas Malagón', 'Nesar Ramachandra', 'Benjamin Remy', 'Cécile Roucelle', 'Jaime Ruiz-Zapatero', 'Stefan Schuldt', 'Ignacio Sevilla-Noarbe', 'Ved G. Shah', 'Tjitske Starkenburg', 'Stephen Thorp', 'Laura Toribio San Cipriano', 'Tilman Tröster', 'Roberto Trotta', 'Padma Venkatraman', 'Amanda Wasserman', 'Tim White', 'Justine Zeghal', 'Tianqing Zhang', 'Yuanyuan Zhang']","This paper discusses the potential applications and opportunities for artificial intelligence and machine learning within the context of the Rubin Legacy Survey of Space and Time (LSST). It explores how AI/ML techniques can enhance data analysis, improve observational strategies, and contribute to the overall scientific goals of the LSST, particularly in the study of dark energy and other cosmological phenomena.",150.91,Phi-4,Nvidia B200 (Cloud Native)
2601.14242v1_APEX-Agents.pdf,APEX–Agents: A Benchmark for Assessing AI Agents in Professional Services,"['Bertie Vidgen', 'Austin Mann', 'Abby Fennelly', 'John Wright', 'Stanly Lucas Rothman', 'Marco Burstein', 'Julien Benchek', 'David Ostrofsky', 'Anirudh Ravichandran', 'Debnil Sur', 'Neel Venugopal', 'Alannah Hsia', 'Isaac Robinson', 'Calix Huang', 'Olivia Varones', 'Daniyal Khan', 'Michael Haines', 'Zach Richards', 'Chirag Mahapatra', 'Brendan Foody', 'Osvald Nitski']","APEX–Agents introduces a benchmark for evaluating AI agents' ability to perform long-horizon, cross-application tasks typical in professional services such as investment banking, management consulting, and corporate law. The benchmark assesses agents' performance in realistic work environments, requiring navigation through files and tools. Eight agents were tested, with Gemini 3 Flash (Thinking=High) achieving the highest score. The benchmark, including all prompts, rubrics, gold outputs, files, and metadata, is open-sourced alongside Archipelago, the infrastructure for agent execution and evaluation. This work aims to bridge the sim-to-real gap in existing evaluations and provide insights into agents' real-world performance.",154.17,Phi-4,Nvidia B200 (Cloud Native)
2601.14255v1_VideoMaMa Mask-Guided Video Matting via Generative.pdf,VideoMaMa: Mask-Guided Video Matting via Generative Prior,"['Sangbeom Lim', 'Seoung Wug Oh', 'Jiahui Huang', 'Heeji Yoon', 'Seungryong Kim', 'Joon-Young Lee']","The paper introduces Video Mask-to-Matte Model (VideoMaMa), a diffusion-based model designed to generate high-quality alpha mattes from input binary segmentation masks. It addresses the challenge of generalizing video matting models to real-world videos by leveraging pretrained video diffusion models. VideoMaMa shows strong zero-shot generalization to real-world footage despite being trained on synthetic data. The authors also develop a scalable pseudo-labeling pipeline and create the Matting Anything in Video (MA-V) dataset, which includes high-quality matting annotations for over 50,000 real-world videos. By fine-tuning the SAM2 model on MA-V, they achieve improved performance on in-the-wild videos, highlighting the importance of large-scale pseudo-labeled video matting and the role of generative priors and segmentation cues in advancing video matting research.",154.88,Phi-4,Nvidia B200 (Cloud Native)
