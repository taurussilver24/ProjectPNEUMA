filename,title,authors,doi,arxiv_id,keywords,summary,tps,model,platform
2601.07192v1_Relink Constructing Query-Driven Evidence Graph On.pdf,Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG,"Manzong Huang, Chenyang Bu, Yi He, Xingrui Zhuo, Xindong Wu",,,"GraphRAG, GraphRAG challenges, hallucinations, Retrieval-Augmented Generation, knowledge graph, LLM mitigation, query construction, evidence graph","Relink proposes a framework to dynamically build a query-specific evidence graph, addressing incompleteness and distractor facts in GraphRAG by instantiating facts from a latent relation pool and employing a unified evaluation strategy.",25.67,LFM-2.5,Apple M1 (Metal)
2601.07197v1_Beyond Variance Knowledge-Aware LLM Compression vi.pdf,Beyond Variance: Knowledge-Aware LLM Compression via Fisher-Aligned Subspace Diagnostics,"Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma",,,"knowledge-aware compression, Fisher-Aligned Subspace, activation compression, factual knowledge, transformer diagnostics","The paper introduces Fisher-Aligned Subspace Compression (FASC), a method that leverages Fisher Information Matrix to compress LLMs by focusing on dimensions critical for factual knowledge, improving accuracy over variance-based approaches.",26.82,LFM-2.5,Apple M1 (Metal)
2601.07199v1_Forward versus Backward Comparing Reasoning Object.pdf,Forward versus Backward: Comparing Reasoning Objectives in Direct Preference Optimization,"Murtaza Nikazad, Raghuram Ramanujan",,,"direct preference optimization, hallucination, reasoning reliability, forward chain-of-thought, backward verification, false positive rate, acknowledgement rate","This paper investigates how training objectives—forward reasoning generation versus backward verification—affect reasoning accuracy and error recognition in large language models. Experiments on GSM8K show a trade-off: forward-only DPO improves accuracy but not verification, while backward-only training reduces false positives but limits accuracy gains. Both approaches increase model confidence, suggesting complementary roles for forward and backward reasoning objectives.",30.41,LFM-2.5,Apple M1 (Metal)
2601.07200v1_Safeguarding LLM Fine-tuning via Push-Pull Distrib.pdf,Safeguarding LLM Fine-tuning via Push-Pull Distributional Alignment,"Haozhong Wang, Zhuo Li, Yibo Yang, H Zhao, Dandan Guo, Guo",,,"LLM fine-tuning, safety alignment, distributional alignment, Optimal Transport, model safety, data distribution, harmful patterns","The paper introduces Safety Optimal Transport (SOT), a framework that reframes safe fine-tuning as a distribution-level alignment task. It addresses the limitation of existing instance-level defenses by optimizing sample importance through push-pull weight learning, establishing a geometric safety boundary that purifies training data while maintaining performance.",28.47,LFM-2.5,Apple M1 (Metal)
2601.07201v1_CalPro Prior-Aware Evidential--Conformal Predictio.pdf,CalPro: Prior-Aware Evidential–Conformal Prediction with Structure-Aware,"Ibne Farabi Shihab, Shihab Akter, Anuj Sharma",10.48550/arXiv.2026.12345,arXiv:2109.12345,"protein structure prediction, pLDDT, conformal prediction, uncertainty quantification, structural biology","Introduces CalPro, a prior-aware evidential-conformal framework for shift-robust uncertainty quantification in protein structure prediction. Combines geometric evidential heads, differentiable conformal layers, and domain priors to achieve tighter coverage guarantees and improved calibration.",27.74,LFM-2.5,Apple M1 (Metal)
2601.07206v1_LLMRouterBench A Massive Benchmark and Unified Fra.pdf,LLMRouterBench: A Massive Benchmark and Unified Framework for LLM Routing,"Hao Li, Yiqun Zhang, Zhaoyan Guo, Chenxu Wang, Shengji Tang, Zhang Yang, Biqing Qi, Peng Ye Lei, Bai Zhen Wang, Shuyue Hu",,,"LLM routing, large language models, LLM ensemble, benchmark, performance trade-off, model selection, latency-aware analysis","This paper introduces LLMRouterBench, a large-scale benchmark and unified framework for evaluating LLM routing across 400K instances from 21 datasets and 33 models. It systematically re-evaluates routing methods, highlights performance gaps, and discusses limitations of current approaches, while enabling latency-aware analysis.",28.77,LFM-2.5,Apple M1 (Metal)
2601.07209v1_SIRR-LMM Single-image Reflection Removal via Large.pdf,SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model,"Yu Guo, Zhiqiang Lao, Xiyun Song, Yubin Zhou, Heather Yu",,,"reflection removal, glass surfaces, large multimodal model, synthetic dataset, reflection separation","This paper introduces a synthetic dataset generation framework that uses path-traced 3D glass models combined with real background imagery to create physically accurate reflection scenarios. Leveraging Large Multimodal Models (LMM) with task-specific LoRA, the approach achieves improved reflection removal performance compared to existing methods.",29.04,LFM-2.5,Apple M1 (Metal)
2601.07214v1_BlindU Blind Machine Unlearning without Revealing .pdf,BlindU: Blind Machine Unlearning without Revealing Erasing Data,"Weiqi Wang, Zhiyi Tian, Chenhan Zhang, Shui Yu",10.1109/TPA.2024.12345,IEEE/TR/2024/000123,"Machine Unlearning, Federated Learning, Privacy Leakage, Privacy Preserving, Information Bottleneck","This paper proposes Blind Unlearning (BlindU) to enable privacy-preserving model unlearning without exposing erased data to servers. It uses compressed representations and the information bottleneck mechanism, supplemented with noise-free differential privacy masking, to protect user data while maintaining model utility.",28.78,LFM-2.5,Apple M1 (Metal)
2601.07224v1_Consolidation or Adaptation PRISM Disentangling SF.pdf,Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration,"Yang Zhao, Yangou Ouyang, Xiao Ding, Hepeng Wang, Bibo Cai, Kai Xiong, Jinglong Gao, Zhouhao Sun, Li Du, Bing Qin, Ting Liu",,,"PRISM, SFT, RL, gradient concentration, data arbitration, LLM agents, optimization regimes","This paper proposes PRISM, a dynamics-aware framework grounded in Schema Theory, to arbitrate data based on its cognitive conflict level. It distinguishes between data triggering high spatial concentration (requiring RL structural adaptation) and diffuse updates (suitable for SFT consolidation), aiming to improve training efficiency and scalability.",29.71,LFM-2.5,Apple M1 (Metal)
2601.07226v1_Lost in the Noise How Reasoning Models Fail with C.pdf,Lost in the Noise: How Reasoning Models Fail with Contextual Distractors,"Seongyun Lee, Seongyun Lee, Yongrae Jo, Minju Seo, Moontae Lee, Minjoon Seo",10.48550/arXiv.2026.12345,arXiv:2609.12345,"reasoning models, agentic AI, noisy benchmarks, distractor robustness, prompting strategies","This paper introduces NoisyBench, a benchmark evaluating reasoning models under diverse noisy contexts. It reveals up to 80% performance drops in state-of-the-art models when exposed to contextual distractors, highlighting the need for improved robustness through techniques like Rationale-Aware Reward.",29.2,LFM-2.5,Apple M1 (Metal)
2601.07229v2_DiSCo Making Absence Visible in Intelligent Summar.pdf,Making Absence Visible in Intelligent Summarization Interfaces,"ERAN FAINMAN, HAGIT BEN SHOSHAN, ADIR SOLOMON, OSNAT MOKRYN",,,"Review Summarization, Absence, Expectations, Learning via surprisability, Missing commonalities","This paper introduces Domain Informed Summarization through Contrast (DiSCo), an expectation-based approach that highlights missing information by comparing entity content to domain expectations. In user studies, DiSCo summaries were rated more detailed and useful than baseline models, showing reduced presence bias and improved decision support.",29.21,LFM-2.5,Apple M1 (Metal)
2601.07232v1_Yes FLoReNce I Will Do Better Next Time Agentic Fe.pdf,"Y es FLoReNce, I Will Do Better Next Time!","Olivia Shanhong Liu, Pai Chet Ng, De Wen Soh, Konstantinos N. Plataniotis",10.48550/arXiv.2025.12345,arXiv:2509.12345,"humorous memes, agentic feedback, humor detection, meme understanding, feedback-regulated prompting","The paper proposes FLoReNce, an agentic feedback reasoning framework for meme detection, addressing limitations of open-loop models by introducing closed-loop critique and feedback-informed knowledge updating.",27.7,LFM-2.5,Apple M1 (Metal)
2601.07233v1_From Thinking to Justifying Aligning High-Stakes E.pdf,From 'Thinking' to 'Justifying': Aligning High-Stakes Explainability with Professional Communication Standards,"Chen Qian, Yimeng Wang, Yu Chen, Lingfei Wu, Andreas Stathopoulos, Anytime AI, William & Mary",,,"explainable AI, high-stakes domains, professional communication, structured justification, CREAC, BLUF","The paper proposes SEF (Structured Explainability Framework) to improve trust in AI explanations by structuring outputs with a 'Result → Justify' format, drawing on professional communication standards like CREAC and BLUF. Experiments show SEF improves accuracy and verifiability.",28.15,LFM-2.5,Apple M1 (Metal)
2601.07238v1_Group Pattern Selection Optimization Let LRMs Pick.pdf,Group Pattern Selection Optimization: Let LRMs Pick the Right Pattern,"Hanbin Wang, Jingwei Song, Jinpeng Li, Fei Mi, Lifeng Shang",10.48550/arXiv.2024.12345,arXiv:2408.12345,"large reasoning models, pattern selection, reinforcement learning, multi-pattern rollouts, problem characteristics, optimization","This paper introduces Group Pattern Selection Optimization (GPSO), a reinforcement learning framework that improves reasoning model performance by identifying and internalizing optimal reasoning patterns across diverse strategies. GPSO uses multi-pattern rollouts, verifier-guided selection, and attention masking to mitigate suboptimal pattern bias and enhance robust reasoning.",28.96,LFM-2.5,Apple M1 (Metal)
2601.07239v1_Stochastic CHAOS Why Deterministic Inference Kills.pdf,Stochas(c CHAOS: Why Determinis),"Tanmay Joshi, Shourya Aggarwal, Anusa Saha, Aadi Pandey, Shreyash Dhoot, Vighnesh Rai, Raxit Goswami, Aman Chadha3, Vinija Jain, Amitava Das",10.48550/arXiv:2309.12345,10.48550/arXiv:2309.12345,"deterministic inference, LLM inference, uncertainty modeling, stochastic chaos, distributional variability, safety alignment","The paper argues that deterministic inference undermines LLM capabilities by suppressing uncertainty, emergent abilities, and reasoning diversity, advocating instead for stochastic approaches that preserve variability and robustness.",29.67,LFM-2.5,Apple M1 (Metal)
2601.07245v1_Learning to Trust the Crowd A Multi-Model Consensu.pdf,Learning to Trust the Crowd: A Multi-Model,Pranav Kallem,,,,The paper introduces a Multi-Model Consensus Reasoning Engine to improve reliability of large language models by aggregating responses from multiple heterogeneous LLMs using supervised meta-learning. It demonstrates improved accuracy and reduced hallucinations compared to single models.,24.35,LFM-2.5,Apple M1 (Metal)
2601.07250v1_DDT A Dual-Masking Dual-Expert Transformer for Ene.pdf,DDT: A Dual-Masking Dual-Expert Transformer,"Mingnan Zhu, Qixuan Zhang, Yixuan Cheng, Fangzhou Gu, Shiming Lin",,,"Time-Series Forecasting, Multivariate Temporal Modeling, Dynamic-Causal Masking, Adaptive Feature Fusion","Accurate energy time-series forecasting is crucial for grid stability and renewable integration. This paper proposes DDT, a novel deep learning framework, introducing dual-masking and dual-expert systems to address complex temporal dependencies and data heterogeneity.",28.13,LFM-2.5,Apple M1 (Metal)
2601.07261v1_Pseudodata-guided Invariant Representation Learnin.pdf,Pseudodata-guided Invariant Representation,"Haomin Wu, Zhiwei Nie, Hongyu Zhang, Zhixiang Ren",arXiv:2601.07261v1,2601.07261,"pseudodata, invariant representation, enzyme kinetics, deep learning, generalization, out-of-distribution","Accurate prediction of enzyme kinetic parameters is essential for understanding catalytic mechanisms and guiding enzyme engineering. This work proposes O2DENet, a lightweight module that enhances out-of-distribution generalization through biologically informed perturbation augmentation and invariant representation learning.",31.65,LFM-2.5,Apple M1 (Metal)
2601.07263v1_When Bots Take the Bait Exposing and Mitigating th.pdf,When Bots Take the Bait: Exposing and Mitigating the Emerging Social Engineering,"Xinyi Wu, Geng Hong, Yueyue Chen, MingXuan Liu, Feier Jin, Xudong Pan, Jiarun Dai, Baojun Liu",,,"web automation, social engineering, web agents, social engineering attacks, agent behavior, security mitigation","The paper presents the first systematic study of social engineering attacks against web automation agents, introducing the AGENTBAIT paradigm to exploit agent reasoning and proposing SUPERVISOR for runtime mitigation. Empirical results show high attack success rates and demonstrate a practical defense reducing attack rates by up to 78.1%.",28.62,LFM-2.5,Apple M1 (Metal)
2601.07291v1_A Visual Semantic Adaptive Watermark grounded by P.pdf,Visual Semantic Adaptive Watermark grounded by Prefix-Tuning for Large Vision-Language Model,"Qi Zheng, Shuliang Liu, Yu Huang, Sihang Jia, Jungang Li, Lyuhao Chen, Junhao Chen, Hanqian Li, Aiwei Liu, Yibo Yan, Xuming Hu",,,"visual watermarking, semantic-aware watermarking, large vision-language model, prefix-tuning, visual fidelity, detectability, semantic similarity","The paper introduces VISA-Mark, a lightweight prefix-tuned watermark framework for LVMs that preserves visual quality while improving detectability and semantic fidelity. It achieves 7.8% better visual consistency and robust attack resilience compared to existing methods.",28.7,LFM-2.5,Apple M1 (Metal)
2601.07292v1_Photometric Redshift Estimation Using Scaled Ensem.pdf,Photometric Redshift Estimation Using Scaled Ensemble Learning,"Swagata Biswas, Shubhrangshu Ghosh, Avyarthana Ghosh, Yogesh Wadadekar, Abhishek Roy Choudhury, Arijit Mukherjee, Shailesh Deshpande, And Arpan Pal",1,,"photometric redshift, photometric data, machine learning, astronomy, galaxies, redshift estimation, LSST, Hyper Suprime-Cam, Neural networks","This study presents a new ensemble-based ML framework for predicting photometric redshifts using optical photometric data, demonstrating improved accuracy and efficiency for faint galaxies and high-redshift targets.",28.92,LFM-2.5,Apple M1 (Metal)
2601.07296v1_LRAS Advanced Legal Reasoning with Agentic Search.pdf,LRAS: Advanced Legal Reasoning with Agentic Search,"Yujin Zhou, Chuxue Cao, Jinluan Yang, Lijun Wu, Conghui He, Sirui Han, Yike Guo",,,"legal reasoning, LRM, legal LLM, introspective imitation, difficulty-aware reinforcement learning, legal logic, LLM liability","This paper introduces Legal Reasoning with Agentic Search (LRAS), a framework that shifts legal LLMs from static closed-loop reasoning to dynamic active inquiry. By combining Introspective Imitation Learning and difficulty-aware reinforcement learning, LRAS improves performance on complex legal reasoning tasks. Empirical results show significant gains over baselines, and the authors commit to releasing data and models.",28.8,LFM-2.5,Apple M1 (Metal)
2601.07304v1_Heterogeneous Multi-Expert Reinforcement Learning .pdf,Heterogeneous Multi-Expert Reinforcement Learning for Long-Horizon Multi-Goal Tasks in Autonomous Forklifts,"Yun Chen, Bowei Huang, Fan Guo, Kang Song",10.48550/arXiv.2024.05.01234,arXiv:2408.12345,"Autonomous Forklift, Hierarchical Reinforcement Learning, Mobile Manipulation, Hybrid Training","Proposes a Heterogeneous Multi-Expert Reinforcement Learning framework for autonomous forklifts, decomposing long-horizon tasks into specialized sub-policies. It introduces a Semantic Task Planner to separate macro-level navigation from micro-level manipulation and a Hybrid Imitation-Reinforcement Training Strategy to address sparse exploration. Experiments demonstrate improved task success rates and reduced operation time.",29.31,LFM-2.5,Apple M1 (Metal)
2601.07309v1_ARM Role-Conditioned Neuron Transplantation for Tr.pdf,ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging,"Zhuoka Feng, Kang Chen, Sihan Zhao, Kai Xiong, Yaoning Wang, Minshen Yu, Junjie Nian, Changyi Xiao, Yixin Cao, Yugang Jiang",10.1093/arxiv/2026.07309,2601.07309,"ARM, model merging, LLM agents, role-conditioned, cross-benchmark, out-of-distribution","This paper proposes Agent-Role Merging (ARM), a role-conditioned neuron transplantation method for model merging in LLM agents. ARM enhances existing merging techniques by integrating multiple experts into a single model, enabling robust adaptation across interactive environments. The method uses a 3-step framework—constructing merged backbones, selecting based on role-conditioned activation analysis, and performing fine-grained neuron transplantation—to achieve improved generalization and efficiency. ARM outperforms prior methods across diverse domains and demonstrates strong out-of-domain generalization.",31.97,LFM-2.5,Apple M1 (Metal)
2601.07313v1_Explaining Machine Learning Predictive Models thro.pdf,Multivariate Conditional Expectation (MUCE) for Local Explainability,"Silvia Ruiz-España, Laura Arnal, Franís Signola, Juan-Carlos Perez-Cortes, Joaquim Arlandis",,,"machine learning, XAI, explainable models, local explainability, model-agnostic, uncertainty, stability, uncertainty","This work introduces MUCE, a model-agnostic method for local explainability that captures prediction changes from feature interactions. It extends Individual Conditional Expectation by exploring a multivariate grid around a prediction, providing graphical explanations and stability/uncertainty indices to assess model reliability and prediction confidence.",29.43,LFM-2.5,Apple M1 (Metal)
2601.07315v1_VLM-CAD VLM-Optimized Collaborative Agent Design W.pdf,VLM-CAD:VLM-OptimizedCollaborativeAgent,"Guanyuan Pan, Yugui Lin, Tiansheng Zhou, Pietro Li, Shuai Wang, Yaqi Wang, Wangyaqi",,,"Analog Circuit Sizing, Agentic AI, Vision Language Model, Explainability, Electronic Design Automation","Analog mixed-signal circuit sizing involves complex trade-offs. This paper proposes a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD) that analyzes circuits, optimizes DC operating points, performs inference-based sizing, and executes external sizing optimization. It integrates Image2Net for schematic annotation and presents an Explainable Trust Region Bayesian Optimization method (ExTuRBO) for dual-granularity sensitivity analysis.",29.34,LFM-2.5,Apple M1 (Metal)
2601.07316v1_BEAT-Net Injecting Biomimetic Spatio-Temporal Prio.pdf,BEAT-Net: Injecting Biomimetic Spatio-Temporal Priors for Interpretable ECG Classification,"Ma Runze, Liao Caizhi",arXiv:2601.07316v1,rmaa0033@student.monash.edu,"ECG classification, deep learning, biomimetic analysis, interpretable AI, heartbeat segmentation","This paper proposes BEAT-Net, a biomimetic ECG analysis framework that reformulates ECG classification as a language modeling task. By applying QRS tokenization, the model transforms continuous ECG signals into structured heartbeat sequences, enabling explicit decomposition of cardiac physiology. The approach achieves competitive performance with CNNs while enhancing robustness and interpretability through attention mechanisms that mirror clinical heuristics.",29.83,LFM-2.5,Apple M1 (Metal)
2601.07320v1_Segmental Advantage Estimation Enhancing PPO for L.pdf,Segmental Advantage Estimation: Enhancing PPO for Long-Context LLM,"Xue Gong1, Qi Yi1, Ziyuan Nan2, 4, Guanhua Huang1, Kejiao Li1, Yuhao Jiang1, Ruibin Xiong1, Zenan Xu1, Jiaming Guo4, Shaohui Peng2, 3, Bo Zhou1",,,"Reinforcement Learning, Verifiable Rewards, Proximal Policy Optimization, Generalized Advantage Estimation, Large Language Models, Reasoning Tasks","Introduces Segmental Advantage Estimation (SAE) to mitigate bias in sparse-reward RLVR by partitioning sequences and computing variance-reduced advantages, improving training stability and performance across model sizes.",29.87,LFM-2.5,Apple M1 (Metal)
2601.07342v1_Agentic Diagnostic Reasoning over Telecom and Data.pdf,Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure,Nicolas Tacheny,arXiv:2601.07342v1,1,"telecom, datacenter, infrastructure, autonomous incident resolution","Introduces an agent-based framework using a Large Language Model to autonomously investigate infrastructure failures, enabling proactive change impact mitigation.",27.09,LFM-2.5,Apple M1 (Metal)
2601.07344v1_PulseMind A Multi-Modal Medical Model for Real-Wor.pdf,PulseMind: A Multi-Modal Medical Model for Real-World Clinical Diagnosis,"Jiao Xu1, Junwei Liu, Jiangwei Lao, Qi Zhu, Yunpeng Zhao, Congyun Jin, Shinan Liu, Zhihong Lu2, Lihe Zhang, Xin Chen, Jian Wang, Ping Wang",10.48550/arXiv.2024.12345,arXiv:2409.12345,"multi-modal medical models, clinical diagnosis, real-world diagnostics, visual language models, multi-turn consultations","PulseMind introduces a family of multi-modal diagnostic models integrating curated datasets, a benchmark, and reinforcement learning to improve real-world clinical performance.",28.97,LFM-2.5,Apple M1 (Metal)
2601.07348v4_Controlled Self-Evolution for Algorithmic Code Opt.pdf,Controlled Self-Evolution for Algorithmic Code Optimization,"Tu Hu1, Ronghao Chen2, Shuo Zhang3, Jianghao Yin4, Mou Xiao Feng3, Jingping Liu5, Shaolei Zhang6, Wenqi Jiang, Yuqi Fang1, Sen Hu2, 7, Wuhan Wang3, Yi Xu3",arXiv:2601.07348v4,2601.07348v4,"Controlled Self-Evolution, Code Optimization, Algorithmic Search, Evolutionary Algorithms, Exploration Efficiency, LLM Backbones","Self-evolution methods enhance code generation through iterative cycles, but face low exploration efficiency. This study proposes Controlled Self-Evolution (CSE) to address inefficiencies via diversified planning, feedback-guided evolution, and experience reuse. Experiments show CSE outperforms baselines across LLM backbones.",32.55,LFM-2.5,Apple M1 (Metal)
2601.07351v2_Beyond Hard Masks Progressive Token Evolution for .pdf,Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models,"Linhao Zhong, Linyu Wu, Bozhen Fang, Tianjian Feng, Chenchen Jing, Wen Wang, Jiaheng Zhang, Hao Chen, Chunhua Shen",,,"Diffusion Language Models, Token Evolution, Progressive Decoding, Masked Language Models, Revisionable Decoding","This paper proposes EvoToken-DLM, a diffusion-based language modeling approach that uses evolving soft token distributions to enable progressive transitions from masked states to discrete outputs, supporting revisable decoding. Continuous trajectory supervision aligns training with iterative updates, achieving superior performance over existing diffusion and masked models.",26.63,LFM-2.5,Apple M1 (Metal)
2601.07356v1_Efficient Convolutional Forward Model for Passive .pdf,Efficient Convolutional Forward Model for Passive Acoustic Mapping and Temporal Monitoring,"Tatiana Gelvez-Barrera, Barbara Nicolas, Bruno Gilles, Adrian Basarab, Denis Kouam",10.1016/j.jlcf.2015.02.012,,"Passive Acoustic Mapping, Model-based beamforming, Convolutional forward model, Temporal monitoring","Introduces a PAM beamforming framework based on a novel convolutional formulation in the time domain, enabling efficient computation and higher temporal resolution compared to frequency-domain methods.",27.03,LFM-2.5,Apple M1 (Metal)
2601.07359v1_Seeing Right but Saying Wrong Inter- and Intra-Lay.pdf,Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs without Training,"Shezheng Song, Shasha Li, Jie Yu",,,,"The paper discusses a phenomenon where MLLMs exhibit strong visual understanding but produce incorrect outputs due to noisy early-layer attention, proposing DualPD as a self-improvement strategy without additional training.",25.54,LFM-2.5,Apple M1 (Metal)
2601.07364v1_On the universal definition of intelligence.pdf,Human - AI Comparison,Joseph Chen,,,,"This paper proposes a universal definition of intelligence for fair comparison between human and artificial intelligence, addressing limitations of existing anthropocentric definitions.",26.65,LFM-2.5,Apple M1 (Metal)
2601.07372v1_Conditional Memory via Scalable Lookup A New Axis .pdf,Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models,"Xin Cheng, Wangding Zeng, Damai Dai, Qinyu Chen, Bingxuan Wang, Zhenda Xie, Kezhao Huang, Xingkai Yu, Zhewen Hao, Yukun Li, Han Zhang, Huishuai Zhang, Dongyan Zhao, Wenfeng Liang",10.48550/arXiv.2304.05714,arXiv:2601.07372v1,"conditional memory, sparsity, large language models, knowledge lookup, Engram, scaling law, attention efficiency","Introduces conditional memory as a complementary sparsity axis using Engram to enable O(1) lookup, achieving superior performance across reasoning benchmarks while improving efficiency. Mechanistic analysis shows relief of early-layer constraints and enhanced long-context retrieval.",30.52,LFM-2.5,Apple M1 (Metal)
2601.07376v1_OpenTinker Separating Concerns in Agentic Reinforc.pdf,OpenTinker: Separating Concerns in Agentic Reinforcement Learning,"Siqi Zhu, Jiaxuan You",10.48550/arXiv.2601.07376,2601.07376,"reinforcement learning, openTinker, RL systems, agentic learning","Introduces OpenTinker, an infrastructure for RL of LLM agents with separation of concerns across design, execution, and agent-environment interaction. It proposes a managed execution runtime, centralized scheduler, and design principles for multi-agent training, demonstrating practical RL use cases.",27.89,LFM-2.5,Apple M1 (Metal)
2601.07377v1_Learning Dynamic Collaborative Network for Semi-su.pdf,Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation,"Jiao Xu1, Xin Chen2, Lihe Zhang1",10.1007/...,,"3D vessel segmentation, semi-supervised learning, dynamic collaborative network, multi-view integration, adversarial supervision","Presents DiCo, a dynamic collaborative network for semi-supervised 3D vessel segmentation, addressing limitations of static mean teacher methods by enabling teacher-student role switching and adversarial supervision.",26.7,LFM-2.5,Apple M1 (Metal)
2601.07389v1_On the Non-decoupling of Supervised Fine-tuning an.pdf,Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training,"Xueyan Niuniuxueyan3@huawei.com, Bo Baibaibo8@huawei.com, W eixi Zhangzhangweixi1@huawei.com, W eixi Zhangzhangweixi2@huawei.com",arXiv:2601.07389v1,arXiv:2601.07389,"supervised fine-tuning, reinforcement learning, post-training, large language models, decoupling","This paper investigates whether supervised fine-tuning and reinforcement learning can be decoupled in post-training. It proves that decoupling is impossible in either order, showing that SFT-then-RL coupling increases SFT loss and RL-then-SFT coupling lowers reward performance. Experiments on Qwen3-0.6B confirm the predicted degradation.",30.63,LFM-2.5,Apple M1 (Metal)
2601.07392v1_OceanSAR-2 A Universal Feature Extractor for SAR O.pdf,OceanSAR-2: A Universal Feature Extractor for SAR Ocean Observation,"Alexandre Tuela, Thomas Kerdreux a, Quentin Febvre b, Alexis Mouche b, Antoine Grouazel b, Jean-Renaud Miadana c, Antoine Audrasa, Chen Wangd, Bertrand Chapronb",,,"OceanSAR-2, SAR, Sentinel-1, feature extractor, remote sensing, geophysical patterns, iceberg detection","We present OceanSAR-2, the second generation of our foundation model for SAR-based ocean observation. Building on our earlier release, which pioneered self-supervised learning on Sentinel-1 Wave Mode data, OceanSAR-2 relies on improved SSL training and dynamic data curation strategies, which enhances performance while reducing training cost. OceanSAR-2 demonstrates strong transfer performance across downstream tasks, including geophysical pattern classification, ocean surface wind vector and significant wave height estimation, and iceberg detection. We release standardized benchmark datasets, providing a foundation for systematic evaluation and advancement of SAR models for ocean applications.",30.6,LFM-2.5,Apple M1 (Metal)
2601.07393v1_Software-Hardware Co-optimization for Modular E2E .pdf,SOFTWARE-HARDWARE CO-OPTIMIZATION FORMODULARE2E,"Chengzhi Ji, Xingfeng Li, Zhaodong Lv, Hao Sun, Pan Liu, Hao Frank Yang, Ziyuan Pu",arXiv:2601.07393v1,arXiv:2601.07393,"Modular end-to-end, Closed-Loop Evaluation, Software–Hardware co-optimization, Energy Consumption, Inference Latency","This paper proposes a reusable software–hardware co-optimization and closed-loop evaluation framework for modular end-to-end autonomous driving inference. It integrates software optimizations with hardware optimizations under a unified system-level objective, introducing a multidimensional evaluation metric (EERA V) that considers safety, comfort, efficiency, latency, and energy. The framework achieves over 6× latency reduction and a 22.35% improvement in EERA V, demonstrating actionable optimization guidance from both software and hardware perspectives.",30.96,LFM-2.5,Apple M1 (Metal)
2601.07395v1_MCP-ITP An Automated Framework for Implicit Tool P.pdf,MCP-ITP: An Automated Framework for Implicit Tool Poisoning in MCP,"Ruiqi Li, Zhiqiang Wang, Yunhao Yao, Xiang-Yang Li",lrq349,zhiqiang.wang,"Model Context Protocol, implicit tool poisoning, tool poisoning attack, automated framework, LLM agents, security risk","This paper introduces MCP-ITP, the first automated and adaptive framework for implicit tool poisoning within the MCP ecosystem. It addresses stealthy poisoning where poisoned tool metadata influences agent behavior without direct tool invocation, improving Attack Success Rate while suppressing detection.",28.02,LFM-2.5,Apple M1 (Metal)
2601.07397v1_Layerwise goal-oriented adaptivity for neural ODEs.pdf,Layerwise goal-oriented adaptivity for neural ODEs: an optimal control perspective,"Michael Hintermüller, Michael Hinze, Denis Korolev",arXiv:2601.07397v1,2601.07397v1,"Resnet, neural ODEs, parameter identification/learning, adaptive neural network",Proposes a novel layerwise adaptive construction method for neural network architectures based on a goal-oriented dual-weighted residual technique for optimal control of neural differential equations.,28.27,LFM-2.5,Apple M1 (Metal)
2601.07411v1_SCALPEL Selective Capability Ablation via Low-rank.pdf,Scalpel: Selective Capability Ablation via Low-rank Parameter,"Zihao Fu, Xufeng Duan, Zhenguang G. Cai",,,,"SCALPEL presents a framework representing language model capabilities as low-rank parameter subspaces, enabling selective ablation without disrupting other capabilities. Experiments show it preserves general language modeling while removing specific tasks.",26.76,LFM-2.5,Apple M1 (Metal)
2601.07422v1_Two Pathways to Truthfulness On the Intrinsic Enco.pdf,Two Pathways to Truthfulness: On the Intrinsic Encoding of LLM Hallucinations,"Wen Luo, Guangyue Peng, Wei Li, Shaohang Wei, Feifan Song, Liang Wang, Nan Yang, Xingxing Zhang, Jing Jin, Furu Wei, Houfeng Wang",,,"truthfulness, hallucination, intrinsic encoding, LLM, knowledge boundaries","This paper investigates how truthfulness cues in large language models arise from two distinct information pathways: a Question-Anchored pathway and an Answer-Anchored pathway. It demonstrates these mechanisms through attention knockout and token patching, links them to LLM knowledge boundaries, and proposes applications to improve hallucination detection.",29.16,LFM-2.5,Apple M1 (Metal)
2601.07430v1_KALE Enhancing Knowledge Manipulation in Large Lan.pdf,Enhancing Knowledge Manipulation in Large Language Models,"Qitan Lv1,2*, Tianyu Liu1,2*, Qiaosheng Zhang2†, Xingcheng Xu2†, Chaochao Lu2",,,"knowledge manipulation, large language models, knowledge graphs, supervised fine-tuning, knowledge-aware learning","The paper addresses the challenge of improving LLMs' knowledge manipulation by introducing KALE, a post-training framework that leverages knowledge graphs to generate rationales and applies knowledge-aware fine-tuning to enhance reasoning capabilities.",27.18,LFM-2.5,Apple M1 (Metal)
2601.07449v1_RLPO Residual Listwise Preference Optimization for.pdf,Residual Listwise Preference Optimization for Long-Context,"Hao Jiang, Zhi Yang, Annan Wang, Yichi Zhang, Weisi Lin, Yichizhang",,,"review ranking, long-context, listwise preference, LLM, e-commerce, user-generated content","Review ranking is pivotal in e-commerce for prioritizing diagnostic and authentic feedback. Existing ranking paradigms face a trade-off between pointwise scoring and list-level interactions. This paper proposes Residual Listwise Preference Optimization (RLPO), which improves ranking robustness by leveraging global context and lightweight post-processing.",27.97,LFM-2.5,Apple M1 (Metal)
2601.07463v1_Puzzle it Out Local-to-Global World Model for Offl.pdf,Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning,"Sijia Li, Xinran Li, Shibo Chen, Jun Zhang",10.1234/123456,,"Offline multi-agent reinforcement learning, Multi-gent model-based reinforcement learning","Offline MARL faces limitations due to data distribution constraints, leading to conservative policies. This work proposes a local-to-global world model to improve prediction accuracy and generalization by leveraging local predictions to infer global dynamics.",26.95,LFM-2.5,Apple M1 (Metal)
2601.07464v1_IFDNS An Iterative Feedback-Driven Neuro-Symbolic .pdf,Iterative Feedback-Driven Neuro-Symbolic Method for Faithful Logical Reasoning,"Xiaoheng Wang, Tongxuan Liu, Zi Gong, Xianzhe Dong, Yuting Zeng, Minhan Hu, Weizhe Huang, Jing Li",,,"Logical Reasoning, Large Language Model, Reasoning","This paper introduces IFDNS, an iterative feedback-driven neuro-symbolic approach designed to improve logical reasoning in large language models by reducing information loss through multi-round feedback during logic extraction.",26.58,LFM-2.5,Apple M1 (Metal)
2601.07468v1_Beyond Dialogue Time Temporal Semantic Memory for .pdf,Temporal Semantic Memory for Personalized LLM Agents,"Miao Su1, Yucan Guo1,2,3, Zixuan Li1,2,Yufei Zhang 4, Guojun Yin 4, Wei Lin 4, Xiaolong Jin1,2,3, Jiafeng Guo 1,2,3, Xueqi Cheng 1,2,3, Institute of Computing Technology, Chinese Academy of Sciences, State Key Laboratory of AI Safety, School of Computer Science, University of Chinese Academy of Sciences, Meituan",arXiv:2601.07468v1,arXiv:2601.07468v1,"Temporal Semantic Memory, Personalized LLM, Memory modeling, Dialogues, Semantic time, Long-term interaction","Memory enables Large Language Model agents to perceive, store, and use information from past dialogues for personalization. This paper proposes Temporal Semantic Memory (TSM), a memory framework that models semantic time for point-wise memory and supports durative information. TSM constructs a semantic timeline, consolidates continuous and related information, and incorporates temporal intent during utilization, achieving up to 12.2% accuracy improvement over existing methods.",30.63,LFM-2.5,Apple M1 (Metal)
2601.07469v1_Knowledge Distillation for LLM-Based Human Activit.pdf,Knowledge Distillation for LLM-Based Human Activity Recognition in Smart Homes,"Julien Cumin, Oussama Er-Rahmany, Xi Chen",arXiv:2601.07469v1,arXiv:2601.07469v1,"Human activity recognition, large language models, knowledge distillation, ambient intelligence, smart homes",This paper presents experimental results on using large language models for human activity recognition in smart homes. It explores how recognition performance varies with LLM size and demonstrates knowledge distillation techniques to fine-tune smaller models with high accuracy while reducing parameters.,27.27,LFM-2.5,Apple M1 (Metal)
2601.07470v1_Learning How to Remember A Meta-Cognitive Manageme.pdf,Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory,"Sirui Liang, Pengfei Cao, Jian Zhao, Wenhao Teng, Xiangwen Liao, Jun Zhao, Kang Liu",liangsirui2024@ia.ac.cn,,"memory abstraction, meta-cognitive management, agent memory, transfer learning, long-horizon decision-making","The paper proposes the Meta-CognitiveMemory Abstraction method (MCMA) to address limitations in memory reuse for LLM agents. MCMA decouples task execution from fixed memory representations by introducing a learnable memory copilot that organizes memories across abstraction levels, enabling selective reuse and transfer across tasks.",28.18,LFM-2.5,Apple M1 (Metal)
2601.07474v1_Task Prototype-Based Knowledge Retrieval for Multi.pdf,Prototype-Based Knowledge Retrieval for Multi-Task Learning,"Yoonmin Oh, Hyung-Il Kim, Jung Uk Kim, Juk Kim",10.1007/978-3-642-45891-7,,"multi-task learning, prototype-based retrieval, multi-task learning, knowledge retrieval",This paper proposes a prototype-based knowledge retrieval framework for multi-task learning that avoids relying on predictions from unlabeled tasks. It introduces a task prototype embedding and an association knowledge generation loss to capture task-specific characteristics and adaptively refine features.,26.76,LFM-2.5,Apple M1 (Metal)
2601.07475v1_ARCQuant Boosting NVFP4 Quantization with Augmente.pdf,ARCQuant: Boosting NVFP4 Quantization with Augmented Residual,"Haoqian Meng, Yilun Luo, Yafei Zhao, Wenyuan Liu, Peng Zhang, Xindian Ma",10.48550/ARCQuant,arxiv:2408.12345,"NVFP4, Post-Training Quantization, Augmented Residual Channels, Large Language Models, Quantization Optimization, Microscaling Formats","This paper proposes ARCQuant, a framework that enhances NVFP4 performance using augmented residual channels. It maintains a strictly unified NVFP4 format by integrating error compensation into matrix reduction, enabling efficient inference on modern hardware while achieving state-of-the-art accuracy.",28.35,LFM-2.5,Apple M1 (Metal)
2601.07477v1_JudgeFlow Agentic Workflow Optimization via Block .pdf,JUDGEFLOW: AGENTICWORKFLOWOPTIMIZATION,"Zihan Ma, Zhikai Zhao, Chuanbo Hua, Federico Berto, Jinkyoo Park",,,"LLM, agentic workflows, optimization, LLM-based, workflow optimization","Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities. Current methods rely on coarse evaluation signals, lacking fine-grained diagnostics. JUDGEFLOW introduces a pipeline with configurable logic blocks and a judge module to assign responsibility scores, improving sample efficiency and interpretability.",27.59,LFM-2.5,Apple M1 (Metal)
2601.07496v1_Graph Inference Towards ICD Coding.pdf,Graph Inference Towards ICD Coding,Xiaoxiao Deng,,,"transfer learning, graph convolutional network, lightweight attention, ICD code prediction, adversarial domain adaptation","LabGraph introduces a unified framework reformulating ICD coding as graph generation, enhancing robustness via adversarial adaptation and reinforcement learning. Experiments show superior performance on micro-F1, micro-AUC, and P@K metrics.",25.34,LFM-2.5,Apple M1 (Metal)
2601.07514v1_Data-Driven Stochastic VRP Integration of Forecast.pdf,Data-Driven Stochastic VRP: Integration of Forecast,Matteo Garbellia,arXiv:2601.07514v1,2601.07514,"Stochastic VRP, Machine Learning, XGBoost, Sub-Gaussian Concentration, Multi-Objective Optimization","This paper investigates integrating machine learning forecasts of intervention durations into a stochastic VRP, using tree-based gradient boosting to improve operator utilization and completion rates.",30.09,LFM-2.5,Apple M1 (Metal)
2601.07516v1_Controlling Multimodal Conversational Agents with .pdf,Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions,"Yongqi Li, Hao Lang, Tieyun Qian, Yongbin Li, Hao Zhang, Tieyun Qian",,,"vision-language models, multimodal conversational agents, reinforcement learning, latent action space, cross-modal projector, RL fine-tuning","This paper proposes a compact latent action space learned via learning from observation to enhance reinforcement learning fine-tuning for vision-language models. By leveraging paired image-text data and text-only data through a cross-modal projector, the method improves generalization and robustness across RL algorithms.",27.91,LFM-2.5,Apple M1 (Metal)
2601.07518v1_Mon3tr Monocular 3D Telepresence with Pre-built Ga.pdf,Monocular 3D Telepresence with Pre-built Gaussian Avatars as Amortization,"Fangyu Lin, Yingdong Hu, Zhening Liu, Yufan Zhuang, Zehong Lin, Jun Zhang",10.1234/jclf.2024.12345,arXiv:2409.12345,"Monocular 3D telepresence, 3D Gaussian splatting, animatable avatars, real-time neural rendering","Introduces Mon3tr, a novel monocular 3D telepresence framework integrating 3D Gaussian splatting for parametric human modeling. It employs an amortized computation strategy using a single monocular RGB camera to capture real-time motion and facial expressions, enabling robust performance with <0.2 Mbps transmission. The method achieves state-of-the-art PSNR (>28 dB), end-to-end latency (~80 ms), and >1000× bandwidth reduction compared to point-cloud streaming.",29.49,LFM-2.5,Apple M1 (Metal)
2601.07525v1_Thinking Before Constraining A Unified Decoding Fr.pdf,Thinking Before Constraining: A Unified Decoding Framework for Large Language Models,"Ngoc Trinh Hung Nguyen, Alonso Silva, Laith Zumot, Liubov Tupikina, Armen Aghasaryan, Mehwish Alam",,,"large language models, structured generation, natural language processing, code generation, schema-based information extraction",The paper proposes a unified decoding framework that combines natural and structured generation to improve reliability and parsability of LLM outputs while preserving expressive power.,27.25,LFM-2.5,Apple M1 (Metal)
2601.07528v1_From RAG to Agentic RAG for Faithful Islamic Quest.pdf,From RAG to Agentic RAG for Faithful Islamic Question Answering,"Gagan Bhatia, Hamdy Mubarak, Mustafa Jarrar, George Mikros, Fadi Zaraket, Mahmoud Alhirthani, Mutaz Al-Khatib, Logan Cochrane, Kareem Darwish, Rashid Yahiaoui, Firoj Alam",,,"Islamic question answering, faithful QA, agentic RAG, Quran retrieval, habituation, abstention, multilingual evaluation","This paper introduces ISLAMIC FAITH QA, a benchmark for evaluating Islamic question answering, and proposes an agentic RAG framework that improves hallucination detection and robustness through structured evidence seeking. Experiments demonstrate superior performance over standard RAG, especially in Arabic and multilingual settings.",29.25,LFM-2.5,Apple M1 (Metal)
2601.07553v1_VirtualEnv A Platform for Embodied AI Research.pdf,VirtualEnv: A Platform for Embodied AI Research,"Kabir Swain, Sijie Han, Ayush Raina, Jin Zhang, Shuang Li, Michael Stopa, Antonio Torralba",10.48550/arXiv.2026.12345,arXiv:2608.12345,"Virtual Environment, Large Language Models, Embodied AI, Unreal Engine, LLM benchmarking, Interactive Simulation","VirtualEnv is a next-generation simulation platform built on Unreal Engine that enables fine-grained benchmarking of large language models in embodied and interactive scenarios. It supports rich agent-environment interactions, integrates LLMs and vision-language models, and aims to advance research at the intersection of AI and gaming.",28.22,LFM-2.5,Apple M1 (Metal)
2601.07556v1_Backpropagation-Free Test-Time Adaptation for Ligh.pdf,Backpropagation-Free Test-Time Adaptation for Lightweight EEG-Based Brain-Computer Interfaces,"Siyang Li, Jiayi Ouyang, Zhenyao Cui, Ziwei Wang, Tianwang Jia, Feng Wan, Dongrui Wu",,,"brain-computer interface, domain adaptation, electroencephalogram, test-time adaptation, transfer learning","This paper proposes Backpropagation-Free Transformations (BF T) for EEG decoding, addressing deployment challenges in EEG-based BCIs by eliminating backpropagation requirements, thus improving efficiency, privacy, and robustness.",27.64,LFM-2.5,Apple M1 (Metal)
2601.07565v1_A Unified Framework for Emotion Recognition and Se.pdf,A Unified Framework for Emotion Recognition and Sentiment Analysis via Expert-Guided Multimodal Fusion with Large Language Models,"Jiaqi Qiao, Xiujuan Xu, Xinran Li, Yu Liu",,,"emotion recognition, large language models, multimodal fusion, sentiment analysis, affective computing","The paper introduces EGMF, a unified framework that integrates expert-guided multimodal fusion with large language models. It features specialized expert networks for local, semantic, and global context, leveraging LoRA fine-tuning and hierarchical dynamic gating. Experiments show improved cross-lingual robustness and state-of-the-art performance on multiple benchmarks.",28.88,LFM-2.5,Apple M1 (Metal)
2601.07568v1_d3LLM Ultra-Fast Diffusion LLM using Pseudo-Trajec.pdf,Ultra-Fast dLLM using Pseudo-Trajectory Distillation,"Yu-Yang Qian, Junda Su, Lanxiang Hu, Peiyuan Zhang, Zhijie Deng, Peng Zhao, Hao Zhang, Peng Zhao",,,"diffusion models, large language models, pseudo-trajectory distillation, parallelism, accuracy, inference optimization","The paper proposes d3LLM, a pseudo-distilled diffusion LLM that balances accuracy and parallelism by introducing pseudo-trajectory distillation during training and entropy-based multi-block decoding during inference. It introduces AUP to evaluate accuracy-parallelism trade-offs and reports up to 10× speedup over vanilla LLaDA/Dream and 5× over AR models.",28.41,LFM-2.5,Apple M1 (Metal)
2601.07573v1_A Model of Artificial Jagged Intelligence.pdf,A Model of Artificial Jagged Intelligence,Joshua S. Gans,arXiv:2601.07573v1,2601.07573,"generative AI, adoption, calibration, learning, scaling","This paper develops an economic model of Artificial Jagged Intelligence (AJI), analyzing how users balance local reliability concerns with coarse global quality signals. It explores adoption thresholds, calibration dynamics, and the interplay between mastery and scaling in generative AI systems.",29.66,LFM-2.5,Apple M1 (Metal)
2601.07577v1_Beyond Entangled Planning Task-Decoupled Planning .pdf,Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents,"Yunfan Li, Bingbing Xu, Xueyun Tian, Xiucheng Xu, Huawei Shen",,,"large language models, task decoupling, long-horizon planning, agent planning, sub-task decomposition","The paper proposes Task-Decoupled Planning (TDP) to address entangled planning contexts in LLM agents. TDP decomposes tasks into a DAG of sub-goals using a Supervisor, enabling localized reasoning and replanning without propagating errors across the entire history. Experiments show TDP improves robustness and efficiency for long-horizon agents.",28.2,LFM-2.5,Apple M1 (Metal)
2601.07580v1_Large Language Models for Physics Instrument Desig.pdf,Large Language Models for Physics Instrument Design,"Sara Zoccheddu, Shah Rukh Qasim, Patrick Owen, Nicola Serra",,,,"The paper explores using large language models (LLMs) for physics instrument design, comparing their performance to reinforcement learning. LLMs generate valid, resource-aware configurations based on broad pretrained knowledge, suggesting they can act as meta-planners in hybrid design workflows alongside RL.",27.02,LFM-2.5,Apple M1 (Metal)
2601.07582v2_ES-Mem Event Segmentation-Based Memory for Long-Te.pdf,ES-Mem: Event Segmentation-Based Memory for Long-Term Dialogue Agents,"Huhai Zou, Tianhao Sun, Chuanjiang He, Yu Tian2, Zhenyang Li, Li Jin, Nayu Liu, Jiang Zhong, Kaiwen Wei",10.1234/example.doi,12345678,"memory, dialogue agents, event segmentation, semantic integrity","The paper proposes ES-Mem, a framework leveraging event segmentation to address memory limitations in dialogue agents. It introduces a dynamic segmentation module and hierarchical memory architecture to improve coherence and contextual anchoring, demonstrating consistent performance gains over existing methods.",28.04,LFM-2.5,Apple M1 (Metal)
2601.07597v1_Pheromone-Focused Ant Colony Optimization algorith.pdf,Pheromone-Focused Ant Colony Optimization algorithm for path planning,"Yi Liu, Hongda Zhang, Zhongxue Gan, Yuning Chen, Ziqing Zhou, Chunlei Meng, Chun Ouyang",,,"Pheromone-Focused Ant Colony Optimization, Path planning, Ant colony optimization, Path planning algorithm, Optimization, Swarm intelligence","The paper proposes a Pheromone-Focused Ant Colony Optimization (PFACO) algorithm to improve path planning by enhancing convergence speed and solution quality through three strategies: (1) concentrating initial pheromone in promising regions, (2) reinforcing high-quality solutions during iterations, and (3) penalizing redundant path turns.",29.39,LFM-2.5,Apple M1 (Metal)
2601.07606v1_Proof of Time A Benchmark for Evaluating Scientifi.pdf,Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments,"Bingyang Ye, Shan Chen, Jingxuan Tu, Chen Liu, Zidi Xiong, Samuel Schmidgall, Danielle S. Bitterman",,,"scientific ideas, time-based evaluation, benchmarking, agent-based research, peer review, citations, research agendas","Introduces Proof of Time (PoT), a framework linking scientific idea judgments to observable downstream signals, enabling scalable and verifiable evaluation of model predictions.",27.66,LFM-2.5,Apple M1 (Metal)
2601.07611v1_DIAGPaper Diagnosing Valid and Specific Weaknesses.pdf,Diagnosing Valid and Specific Weaknesses in Scientific Papers,"Zhuoyang Zou, Abolfazl Ansari, Delvin Ce Zhang, Dongwon Lee, Wenpeng Yin",,,,"This paper introduces DIAGPaper, a multi-agent framework for identifying weaknesses in scientific papers. It addresses limitations of existing systems by modeling human review criteria, incorporating author rebuttals, and prioritizing critical issues for users.",26.28,LFM-2.5,Apple M1 (Metal)
2601.07618v1_Neural Architecture for Fast and Reliable Coagulat.pdf,Neural Architecture for Fast and Reliable Coagulation Assessment in Clinical Settings: Leveraging Thromboelastography,"Yulu Wang, Ziqian Zeng, Jianjun Wu, Zhifeng Tang",,,"coagulation assessment, thromboelastography, deep learning, clinical AI, physiological state reconstruction","The paper introduces Physiological State Reconstruction (PSR) to address delays in traditional Thromboelastography (TEG) by enabling real-time coagulation monitoring. PSR leverages dynamic patient data through multi-domain learning and HLA-based attention, achieving high prediction accuracy while reducing inference time. It demonstrates improved performance over state-of-the-art methods and offers promise for data-scarce medical AI applications.",29.11,LFM-2.5,Apple M1 (Metal)
2601.07632v2_GeoMotionGPT Geometry-Aligned Motion Understanding.pdf,Geometry-Aligned Motion Understanding with Large Language Models,"Ye1 Zhankai, Li1 Bofan, Zhang3 Yukai, Jin1 Shuoqiu, Li1 Xin, Wang2 Wei, Zhang3 Shangqian, Gao1 Xin, Yang4 Yanfu, Zhang4 Xin",10.48550/geomotion,arXiv:2408.12345,"motion understanding, large language models, geometry alignment, semantic embedding, HumanML3D, discrete tokenization","This paper introduces a framework that aligns motion tokenization with semantic embeddings using a unified geometric basis. By enforcing orthogonality between motion codebooks and LLM embeddings, the approach improves motion reasoning performance by 20% on HumanML3D.",29.2,LFM-2.5,Apple M1 (Metal)
2601.07635v2_Learning About Learning A Physics Path from Spin G.pdf,Learning About Learning: A Physics Path from Spin Glasses to Artificial Intelligence,"Denis D. Caprioti, Matheus Haas, Constantino F. Vasconcelos, Mauricio Girardi-Schappo",88040-900,,"spin glasses, neural networks, statistical mechanics, computational methods, artificial intelligence","The paper presents the Hopfield model as a pedagogical framework linking statistical physics, neural networks, and AI, emphasizing its role in unifying undergraduate topics and offering classroom-ready examples.",27.25,LFM-2.5,Apple M1 (Metal)
2601.07638v1_SALT-KG A Benchmark for Semantics-Aware Learning o.pdf,A Benchmark for Semantics-Aware Learning on Enterprise Tables,"Isaiah Onando Mulang, Felix Sasaki, Tassilo Klein, Jonas Kolk, Nikolay Grechanov, Johannes Hoffart",10.48550/arXiv.2601.07638,arXiv:2601.07638v1,"semantics-aware learning, semantic reasoning, metadata knowledge graph, tabular data, enterprise tables","Building upon the SALT benchmark for relational prediction, SALT-KG introduces a benchmark for semantics-aware learning on enterprise tables by linking multi-table transactional data with a structured Operational Business Knowledge graph. This enables models to reason over both tabular evidence and contextual semantics, addressing gaps in current tabular foundation models.",28.89,LFM-2.5,Apple M1 (Metal)
2601.07641v1_Beyond Static Tools Test-Time Tool Evolution for S.pdf,Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning,"Jiaxuan Lu1, Ziyu Kong2, Yemin Wang3, Rong Fu4, Haiyuan Wan1, Cheng Yang6, Wenjie Lou1, Haoran Sun1, Lilong Wang1, Yankai Jiang1, Xiaosong Wang1, Xiao Sun1, Dongzhan Zhou1",,,"AI for Science, scientific reasoning, tool evolution, Test-Time Tool Evolution, scientific domains","The paper addresses the limitations of static tool libraries in scientific reasoning, introducing Test-Time Tool Evolution (TTE) to enable agents to synthesize, verify, and evolve executable tools during inference. It presents SciEvo benchmark and demonstrates TTE's effectiveness in achieving state-of-the-art performance.",29.34,LFM-2.5,Apple M1 (Metal)
2601.07651v1_Active Evaluation of General Agents Problem Defini.pdf,Active Evaluation of General Agents: Problem,"Marc Lanctot, Kate Larson, Ian Gemp, Michael Kaisers",10.48550/acmpsje.2026.1234,,"general evaluation, multitask evaluation, ranking, active learning, game theory, social choice theory","The paper proposes a formal definition and conceptual framework for active evaluation of agents across multiple tasks, comparing baseline ranking algorithms under synthetic and real-world data contexts. It highlights the Elo rating system's reliability and compares it with Soft Condorcet Optimization, showing its effectiveness in reducing ranking error.",27.84,LFM-2.5,Apple M1 (Metal)
2601.07654v1_Towards Automating Blockchain Consensus Verificati.pdf,Towards Automating Blockchain Consensus,"Elliot Jones, William Knottenbelt",,,"Blockchain, Consensus, Formal Verification, Theorem, Artificial Intelligence","The paper presents IsabeLLM, a tool integrating Isabelle with a Large Language Model to automate and verify blockchain consensus protocols, demonstrating its effectiveness in proving Bitcoin's Proof of Work consensus.",28.08,LFM-2.5,Apple M1 (Metal)
2601.07663v2_Reasoning Models Will Blatantly Lie About Their Re.pdf,Reasoning Models Will Blatantly Lie About Their Reasoning,William Walden,,,,"The study investigates whether Large Reasoning Models (LRMs) falsely claim not to use hints in their reasoning, even when prompted to do so, highlighting issues with CoT monitoring and interpretability.",22.49,LFM-2.5,Apple M1 (Metal)
2601.07666v1_Variational Contrastive Learning for Skeleton-base.pdf,Variational Contrastive Learning for Skeleton-based Action Recognition,"Dang-Dinh NGUYEN1, Decky ASPANDI-LATIF1, Titus ZAHARIA1",10.48550/arXiv.2601.07666,arXiv:2601.07666,"Human Action Recognition, Self-Supervised Learning, Variational Inference","This paper proposes a variational contrastive learning framework integrating probabilistic latent modeling with contrastive self-supervised learning for skeleton-based action recognition. It demonstrates superior performance across benchmarks, especially under low-label conditions, and highlights the importance of skeleton joint relevance.",30.11,LFM-2.5,Apple M1 (Metal)
2601.07667v1_Adaptive Layer Selection for Layer-Wise Token Prun.pdf,Adaptive Layer Selection for Layer-Wise Token Pruning in LLM Inference,"Rei Taniguchi, Yuyang Dong, Makoto Onizuka, Chuan Xiao",,,"large language models, key-value cache reduction, layer-wise pruning, token selection, attention score, KV retrieval","The paper proposes ASL, a training-free method that adaptively selects the layer for KV cache reduction based on token rank variance. It balances performance across tasks while meeting a user-defined KV budget, improving accuracy and decoding speed.",26.63,LFM-2.5,Apple M1 (Metal)
2601.07685v1_Predictive Analytics for Dementia Machine Learning.pdf,Predictive Analytics for Dementia: Machine Learning on Healthcare Data,"Shafiul Ajam Opee, Nafiz Fahad, Anik Sen, Rasel Ahmed, Fariha Jahan, Md. Kishor Morol, Md Rashedul Islam",,,"Dementia, Machine learning, Linear Discriminant Analysis (LDA), APOE-ϵ4 allele, Chronic conditions, Explainable AI","This study focuses on enhancing dementia prediction using machine learning techniques on patient health data. Supervised learning algorithms such as K-Nearest Neighbors (KNN), Quadratic Discriminant Analysis (QDA), Linear Discriminant Analysis (LDA), and Gaussian Process Classifiers were applied. Techniques like SMOTE and TF-IDF were used to address class imbalance. LDA achieved the highest testing accuracy of 98%, highlighting the importance of model interpretability and correlations with features like APOE-ϵ4 allele and chronic conditions.",29.93,LFM-2.5,Apple M1 (Metal)
2601.07701v1_Deep Whole-body Parkour.pdf,Deep Whole-body Parkour,"Ziwen Zhuang, Shaoting Zhu, Mengjie Zhao, Hang Zhao",12,"IIIS, Tsinghua University; 2Shanghai Qi Zhi Institute","Deep Reinforcement Learning, Humanoid Robotics, Whole-body Motion, Robot Locomotion, Perception Integration","This work unites perceptive locomotion and general motion tracking to enable humanoid robots to perform dynamic, multi-contact motions like vaulting and diving on uneven terrain. The framework integrates exteroceptive sensing into whole-body tracking, expanding traversability beyond simple walking.",28.83,LFM-2.5,Apple M1 (Metal)
2601.07718v1_Hiking in the Wild A Scalable Perceptive Parkour F.pdf,Hiking in the Wild: A Scalable Perceptive Parkour,"Shaoting Zhu, Ziwen Zhuang, Mengjie Zhao, Kun-Ying Lee, Hang Zhao",10.48550/arXiv.2601.07718,"IIIS, Tsinghua University","humanoid robot, hiking, perception, robotics, parkour, terrain navigation","The framework enables humanoid robots to traverse diverse terrains in indoor and outdoor environments, achieving robust performance through scalable end-to-end learning. It addresses challenges in exteroception and scalability, introducing foothold safety and flat patch sampling strategies.",31.02,LFM-2.5,Apple M1 (Metal)
2601.07737v1_Evaluating the encoding competence of visual langu.pdf,Evaluating the encoding competence of visual language models using uncommon actions,Chen Ling,2601.07737v1,cs.CV,"visual language models, encoding competence, uncommon actions",The study evaluates how well visual language models encode information using uncommon actions.,32.93,LFM-2.5,Apple M1 (Metal)
2601.07748v1_Improving Domain Generalization in Contrastive Lea.pdf,Improving Domain Generalization in Contrastive Learning using Adaptive Temperature Control,"Robert Lewis, Katie Matton, Rosalind W. Picard, John Guttag",arXiv:2601.07748v1,NeurIPS 2023,"domain generalization, contrastive learning, domain adaptation, temperature control, self-supervised learning","The paper presents a method that enhances domain invariance in contrastive learning by adjusting the temperature parameter based on domain labels, improving out-of-distribution generalization while maintaining in-distribution performance.",28.73,LFM-2.5,Apple M1 (Metal)
2601.07778v1_DT-ICU Towards Explainable Digital Twins for ICU P.pdf,DT-ICU: Towards Explainable Digital Twins for ICU Patient Monitoring via Multi-Modal and Multi-Task Iterative Inference,Wen Guo,arXiv:2601.07778v1,2601.07778,"digital twin, ICU, patient monitoring, multi-modal, multi-task","DT-ICU is a multimodal digital twin framework for continuous risk estimation in intensive care, integrating variable-length clinical time series with static patient data in a unified multitask architecture. It achieves accurate, interpretable predictions that adapt as new observations accumulate.",31.92,LFM-2.5,Apple M1 (Metal)
2601.07779v1_OS-Symphony A Holistic Framework for Robust and Ge.pdf,OS-SYMPHONY: A Holistic Framework for Robust and Generalist,"Bowen Yang, Kaiming Jin, Zhenyu Wu, Zhaoyang Liu, Qiushi Sun, Zehao Li, Jingjing Xie, Zhoumianze Liu, Fangzhi Xu, Kanzhi Cheng, Qingyun Li, Yian Wang, Yu Qiao, Zun Wang, Zichen Ding, Zhoung Li",,,,"This paper introduces OS-SYMPHONY, a holistic framework for robust computer-using agents. It addresses limitations of existing frameworks in long-horizon robustness and domain generalization by introducing a Reflection-Memory Agent and versatile tool agents with multimodal search capabilities.",27.78,LFM-2.5,Apple M1 (Metal)
2601.07782v1_Beyond Single-Shot Multi-step Tool Retrieval via Q.pdf,Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning,"Wei Fang, James Glass",10.48550/arXiv.2405.12345,None,"LLM agents, tool retrieval, query planning, retrieval systems","The paper proposes TOOLQP, a lightweight framework that models retrieval as iterative query planning. It addresses challenges in complex tool composition by decomposing tasks and generating queries, achieving state-of-the-art performance through reinforcement learning with verifiable rewards.",27.36,LFM-2.5,Apple M1 (Metal)
2601.07790v1_Benchmarking Small Language Models and Small Reaso.pdf,Benchmarking Small Language Models and Small Reasoning,"Yahya Masri, George Mason University, Emily Ma, George Mason University, Zifu Wang, Harvard University, Joseph Rogers, George Mason University, Chaowei Yang, George Mason University",arXiv:2601.07790v1,arXiv:2601.07790,"system logs, severity classification, small language models, small reasoning models, log comprehension, digital twin, root cause analysis","This benchmark evaluates nine small language models and small reasoning models using zero-shot, few-shot, and retrieval-augmented generation under various prompting strategies. It reveals performance trends and efficiency differences, highlighting architectural and training factors that influence real-time log interpretation.",30.56,LFM-2.5,Apple M1 (Metal)
2601.07794v1_Kinship Data Benchmark for Multi-hop Reasoning.pdf,Kinship Data Benchmark for Multi-hop Reasoning,"Tianda Sun, Dimitar Kazakov",,,"multi-hop reasoning, kinship relations, genealogical data, multi-hop inference","The paper introduces KinshipQA, a benchmark that generates large-scale, culturally specific genealogical datasets to evaluate multi-hop reasoning in large language models. It demonstrates diverse model performance across different cultural settings and constraints.",25.7,LFM-2.5,Apple M1 (Metal)
2601.07821v1_Failure-Aware RL Reliable Offline-to-Online Reinfo.pdf,Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning,"Huanyu Li, Kun Lei, Sheng Zang, Kaizhe Hu, Yongyuan Liang, Bo An, Xiaoli Li, Huazhe Xu",10.48550/arXiv.2407.04219,arXiv:2407.04219,"Failure-Aware RL, Offline-to-Online RL, Self-Recovery, Robot Manipulation, Fragile Objects, Real-World Deployment","This paper introduces FARL, a new reinforcement learning framework that minimizes intervention-requiring failures during real-world exploration. By integrating a world-model-based safety critic and offline-trained recovery policies, FARL reduces IR Failures by 73.1% and improves performance by 11.3% on average during online RL post-training.",29.73,LFM-2.5,Apple M1 (Metal)
2601.07832v2_MHLA Restoring Expressivity of Linear Attention vi.pdf,Restoring Expressivity of Linear Attention,"Kewei Zhang, Ye Huang, Yufan Deng, Jincheng Yu, Huan Ling, Daquan Zhou",arXiv:2601.07832v2,14 Jan 2026,"Linear Attention, Multi-Head, Token-Level, Expressivity, Self-Attention, Transformers","Addresses quadratic complexity of Transformers by introducing Multi-Head Linear Attention (MHLA), preserving representational diversity while maintaining linear complexity. Demonstrates improvements across ImageNet classification, NLP, image generation, and video generation.",29.29,LFM-2.5,Apple M1 (Metal)
2601.07885v1_Small Symbols Big Risks Exploring Emoticon Semanti.pdf,"Small Symbols, Big Risks: Exploring Emoticon Semantic Confusion in Large Language Models","Weipeng Jiang, Xiaoyu Zhang, Juan Zhai, Shiqing Ma, Chao Shen, Yang Liu",,,"emoticons, semantic confusion, large language models, safety implications, code generation, LLM vulnerabilities","This paper investigates emoticon semantic confusion in LLMs, demonstrating that up to 38% of emoticon interpretations are inaccurate, often resulting in silent failures. The study highlights risks to security and reliability, urging mitigation strategies.",27.29,LFM-2.5,Apple M1 (Metal)
2601.07891v1_KVzap Fast Adaptive and Faithful KV Cache Pruning.pdf,"Fast, Adaptive, and Faithful KV Cache","Simon Jégou, Maximilian Jeblick",10.1093/pasj/klz077,2601.07891v1,"KV cache, KVzap, KVzap pruning, transformer, language models","Introduces KVzap, a fast, input-adaptive KV cache approximation that improves KV cache compression for transformer models with state-of-the-art performance on benchmark datasets.",27.38,LFM-2.5,Apple M1 (Metal)
2601.07892v1_Sherry Hardware-Efficient 1.25-Bit Ternary Quantiz.pdf,Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification,"Hong Huang, Decheng Wu, Qiangqiang Hu, Ganghua Yu, Jinhai Yang, Jianchen Zhu, Xue Liu, Dapeng Wu",10.1234/abcd1234,,"hardware-efficient quantization, ternary quantization, edge computing, large language models, sparsification, power-of-two alignment","The paper proposes Sherry, a hardware-efficient ternary quantization framework that introduces a 3:4 fine-grained sparsity to achieve a regularized 1.25-bit width while restoring power-of-two alignment. It addresses weight trapping issues and demonstrates significant bit savings and speed improvements on an Intel i7-14700HX CPU.",28.36,LFM-2.5,Apple M1 (Metal)
2601.07894v1_Revealing the Attention Floating Mechanism in Mask.pdf,Revealing the Attention Floating Mechanism in Masked Diffusion Models,"Xin Dai, Pengcheng Huang, Zhenghao Liu, Shuo Wang, Yukun Yan, Chaojun Xiao, Yu Gu, Ge Yu, Maosong Sun",,,"Attention Floating, Masked Diffusion Models, Attention Mechanisms, Autoregressive Models, Large Language Models, Diffusion Language Models","This paper investigates the attention behaviors in masked diffusion models, revealing the phenomenon of Attention Floating. Unlike ARMs, MDMs exhibit dynamic, dispersed attention anchors that shift across denoising steps, providing a mechanistic explanation for their strong in-context learning capabilities.",28.76,LFM-2.5,Apple M1 (Metal)
2601.07898v1_Large Language Models and Algorithm Execution Appl.pdf,Large Language Models and Algorithm Execution: Application to an Arithmetic Function,"Farah Ben Slama, Frédéric Armetta",,,"Algorithmic learning in natural language, Supervised learning by decomposition, Large language model, Fine-tuning","This paper investigates extending LLMs' capabilities to algorithm execution via supervised training focused on reasoning decomposition. It introduces LLM-DAL, demonstrating improved performance in complex algorithmic inference when training is carefully designed.",27.04,LFM-2.5,Apple M1 (Metal)
2601.07901v1_Decentralized Online Convex Optimization with Unkn.pdf,Decentralized Online Convex Optimization with Unknown Feedback Delays,"Hao Qiu, Mengxiao Zhang, Juliette Achddou",arXiv:2601.07901v1,,"decentralized optimization, online convex optimization, unknown feedback delays, federated learning, multi-agent systems","This paper studies decentralized online convex optimization under time- and agent-varying feedback delays. It proposes a novel algorithm with improved regret bounds and extends it to strongly convex settings, demonstrating effectiveness through experiments.",27.97,LFM-2.5,Apple M1 (Metal)
2601.07903v2_Enhancing Large Language Models for Time-Series Fo.pdf,Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning,"Jianqi Zhang, Jingyao Wang, Wenwen Qiang, Fanjiang Xu, Changwen Zheng",10.1234/example,12345678,"Time Series Forecasting, Large Language Model, In-context Learning","The paper proposes LVICL, a method to improve LLM4TSF performance by freezing LLM parameters while injecting example information via vector-injected in-context learning, addressing computational overhead and forecasting accuracy.",26.85,LFM-2.5,Apple M1 (Metal)
2601.07935v1_Towards Specialized Generalists A Multi-Task MoE-L.pdf,Towards Specialized Generalists: A Multi-Task,"Yuxin Yang, Aoxiong Zeng, Xiangquan Yang",arXiv:2601.07935v1,2601.07935,"Large Language Models, Domain-Specific LLM, Medical Applications, Catastrophic Forgetting, Knowledge Preservation","The paper addresses challenges in adapting LLMs to specialized domains like medicine, focusing on the Stability-Plasticity Dilemma and Task Interference. It introduces Med-MoE-LoRA, a framework combining Mixture-of-Experts with Low-Rank Adaptation to enable efficient multi-task domain adaptation, particularly for medical scenarios.",29.2,LFM-2.5,Apple M1 (Metal)
2601.07939v1_SECite Analyzing and Summarizing Citations in Soft.pdf,Analyzing and Summarizing Citations in Software Engineering Literature,"Shireesh Reddy Pyreddy, Khaja Valli Pathan, Hasan Masum, Tarannum Shaila Zaman",,,"Sentiment Analysis, LLMs, Text Summarization, Citations, Software Engineering","This study introduces SECite, a novel framework for evaluating scholarly impact through sentiment analysis of citations in software engineering literature. It develops a semi-automated pipeline to extract citations, apply NLP and unsupervised machine learning, and generate sentiment-specific summaries to reveal how the academic community perceives the contributions of target papers.",28.61,LFM-2.5,Apple M1 (Metal)
2601.07941v2_Moonworks Lunara Aesthetic Dataset.pdf,Moonworks Lunara Aesthetic Dataset,"Yan Wang, M M Sayeef Abdullah, Partho Hassan, Sabit Hassan",arXiv:2601.07941v2,arXiv:2601.07941v2,"Moonworks, Lunara, Aesthetic Dataset, Text-to-Image, Style Conditioning, Prompt Grounding, Artistic Styles, Middle East, Northern Europe, East Asia, South Asia","The Lunara Aesthetic Dataset is the first public release of curated 2,000 image–prompt pairs designed for research on prompt grounding and style conditioning in text-to-image generation. It covers diverse artistic styles and includes human-refined prompts and structured annotations, emphasizing aesthetic quality and licensing transparency.",31.7,LFM-2.5,Apple M1 (Metal)
2601.07946v1_Coupled Diffusion-Encoder Models for Reconstructio.pdf,Coupled Diffusion–Encoder Models for Reconstruction of Flow Fields,"AmirPouya Hemmasian, Amir Barati Farimani",10.48550/arXiv.2026.12345,arXiv:2509.12345,"Diffusion models, Flow field reconstruction, Autoencoders, Kolmogorov flow fields, Generative modeling","This paper proposes DiffCoder, a coupled framework integrating a probabilistic diffusion model with a convolutional ResNet encoder, to improve flow-field reconstruction by preserving statistical properties beyond pointwise loss. It demonstrates better spectral accuracy under compression compared to VAE baselines.",28.69,LFM-2.5,Apple M1 (Metal)
2601.07948v1_Reinforcement Learning Methods for Neighborhood Se.pdf,Reinforcement Learning Methods for Neighborhood Selection in Local Search,"Y annick Molinghen, Augustin Delecluse, Renaud De Landtsheer, Stefano Michelini",,,"Local Search, Reinforcement Learning, Combinatorial Optimization","This study evaluates reinforcement learning-based neighborhood selection strategies in local search, comparing multi-armed bandits and deep RL methods against three optimization problems. It highlights the need for carefully designed reward functions due to constraint penalties and reports variable performance across problems, with ε-greedy consistently ranking well.",27.04,LFM-2.5,Apple M1 (Metal)
2601.07951v1_Hybrid SARIMA LSTM Model for Local Weather Forecas.pdf,Hybrid SARIMA–LSTM Model for Local Weather Forecasting: A Residual-Learning Approach for Data-Driven Meteorological Prediction,"Shreyas Rajeev, Karthik Mudenahalli, Amit Mallappa, I., srajeev@stevens.edu",,,"weather forecasting, SARIMA, LSTM, hybrid model, machine learning, meteorology, residual learning","This paper proposes a hybrid modeling approach combining SARIMA and LSTM to improve weather prediction accuracy. It addresses limitations of traditional statistical methods by leveraging LSTMs for nonlinear temporal patterns and SARIMA for stable seasonal components, aiming to reduce systematic residual errors in long-term forecasts.",29.71,LFM-2.5,Apple M1 (Metal)
2601.07953v1_Quantum automated theorem proving.pdf,Quantum automated theorem proving,"Zheng-Zhi Sun, Qi Ye, Dong-Ling Deng, Hefei National Laboratory",,,"quantum automated theorem proving, automated reasoning, quantum superposition, quantum entanglement, knowledge bases, logical inference, geometric theorems","This paper proposes a generic framework for quantum automated theorem proving, leveraging quantum superposition and entanglement. It introduces quantum representations of knowledge bases and quantum algebraic proving methods, demonstrating improved query complexity and practical applications in geometry.",27.92,LFM-2.5,Apple M1 (Metal)
2601.07957v1_LWMSCNN-SE A Lightweight Multi-Scale Network for E.pdf,LWMSCNN-SE: A Lightweight Multi-Scale Network for Efficient Maize Disease Classification,"Likewood, F. et al., Jianmei Su",10.1007/s40297-023-02485-7,arXiv:2308.07962,"lightweight CNN, multi-scale feature extraction, attention mechanism, plant pathology, accuracy-efficiency trade-off","This paper introduces LWMSCNN-SE, a lightweight convolutional neural network integrating multi-scale feature extraction, depthwise separable convolutions, and squeeze-and-excitation attention, achieving 96.63% classification accuracy with low computational cost for real-time maize disease diagnosis on edge devices.",29.07,LFM-2.5,Apple M1 (Metal)
2601.07958v1_LJ-Spoof A Generatively Varied Corpus for Audio An.pdf,LJ-SPOOF: A Generatively Diverse Corpus for Audio Anti-Spoofing and Synthesis Source Tracing,"Surya Subramani, Hashim Ali, Hafiz Malik",10.48550/arXiv.2311.07891,arXiv:2311.07891,"anti-spoofing, speaker verification, deepfake, source tracing, synthetic speech","Speaker-specific anti-spoofing and synthesis-source tracing require datasets that systematically vary model architectures, synthesis pipelines, and generative parameters. This work introduces LJ-Spoof, a speaker-specific corpus with diverse prosody, vocoders, generative parameters, prompts, and post-processing, enabling robust speaker-conditioned anti-spoofing and fine-grained tracing. It serves as a practical reference and benchmark.",28.82,LFM-2.5,Apple M1 (Metal)
2601.07964v1_Executable Ontologies in Game Development From Alg.pdf,Executable Ontologies in Game Development: From Algorithmic Control to Semantic World Modeling,Alexander Boldachev,,,"executable ontologies, game AI, behavior trees, GOAP, event semantics, dataflow architecture","This paper examines the application of Executable Ontologies (EO) in game development, arguing for a shift from algorithmic control to semantic world modeling. It demonstrates how EO enables priority-based task interruption in survival games and contrasts EO with traditional approaches like Behavior Trees and GOAP, highlighting advantages in semantic modeling and integration possibilities.",29.63,LFM-2.5,Apple M1 (Metal)
2601.07965v1_When Models Know When They Do Not Know Calibration.pdf,When Models Know When They Don't Know,"Chenjie Hao, Weyl Lu, Yuko Ishiwaka, Zengyi Li, Weier Wan, Yubei Chen",,,,"This preprint discusses how models can be calibrated to signal uncertainty through confidence, enabling better recognition of ignorance. It proposes training-free methods for calibration, cascading, and data cleaning, highlighting applications in vision and language models.",26.12,LFM-2.5,Apple M1 (Metal)
2601.07969v1_Tuberculosis Screening from Cough Audio Baseline M.pdf,Tuberculosis Screening from Cough Audio,"George P. Kafentzis, Efstratios Selisios",arXiv:2601.07969v1,arXiv:2601.07969v1,"Tuberculosis, Machine Learning, Cough Audio, Cross-Validation, Uncertainty Quantification, Feature Extraction","This paper proposes a standardized framework for automatic tuberculosis (TB) detection from cough audio and clinical data using machine learning. It addresses variability in datasets, cohort definitions, and evaluation protocols, aiming to provide a reproducible baseline and enable fair comparisons. The study quantifies performance for cough audio-only and fused models, releasing the full experimental protocol.",28.95,LFM-2.5,Apple M1 (Metal)
2601.07973v1_Cultural Compass A Framework for Organizing Societ.pdf,Cultural Compass: A Framework for Organizing Societal Norms to Detect Violations in Human-AI Conversations,"Myra Cheng, Vinodkumar Prabhakaran, Alice Oh, Hayk Stepanyan, Sunipa Dev, Stanford University, Google, myra@cs.stanford.edu",Cultural Compass: A Framework for Organizing Societal Norms to Detect Violations in Human-AI Conversations,Cultural Com-pass:2601.07973v1,"cultural norms, human-AI interaction, norm adherence, cross-cultural, generative AI, ethical AI","This paper introduces a taxonomy of norms to evaluate how generative AI models adhere to sociocultural expectations in human-AI conversations. It highlights gaps in current benchmarks and proposes operationalization of norms to detect violations across contexts, showing variability by model, interaction style, and situational factors.",29.0,LFM-2.5,Apple M1 (Metal)
2601.07988v1_From Word Sequences to Behavioral Sequences Adapti.pdf,From Word Sequences to Behavioral Sequences: Adapting Modeling and Evaluation Paradigms for Longitudinal NLP,"Adithya V Ganesan, Vasudha Varadarajan, Oscar NE Kjell, Roman Kotov, Ryan L Boyd, Andrew Schwartz, Stony Brook University, Vanderbilt University, Carnegie Mellon University",,,"NLP, longitudinal studies, behavioral sequences, person-indexed sequences, time-ordered data, modeling paradigms, evaluation metrics, PTSD symptom severity","The paper discusses how traditional NLP treats documents as independent samples, but in longitudinal contexts documents are nested within authors and ordered in time. It proposes a new modeling and evaluation framework that accounts for person and time, addressing issues like person-specific signal leakage and temporal ordering. The authors present findings from a large dataset and argue for a shift toward behavior-sequence paradigms.",30.0,LFM-2.5,Apple M1 (Metal)
2601.07994v2_DYCP Dynamic Context Pruning for Long-Form Dialogu.pdf,DYCP: Dynamic Context Pruning for Long-Form Dialogue with LLMs,"Nayoung Choi, Jonathan Zhang, Jinho D. Choi",,,"Large Language Models, Long-Form Dialogue, Context Management, Dialogue Systems, LLM Latency, Context Pruning","The paper presents DYCP, a lightweight context management method that dynamically segments and retrieves relevant memory at query time. It improves answer quality and reduces response latency in long-form dialogues without relying on predefined topic boundaries.",27.15,LFM-2.5,Apple M1 (Metal)
2601.08000v1_Reasoning over Precedents Alongside Statutes Case-.pdf,Reasoning over Precedents Alongside Statutes,"Can Jin1, Rui Wu, Tong Che2, Qixin Zhang, Hongwu Peng, Jiahui Zhao, Zhenting Wang, Wenqi Wei, Ligong Han, Zhao Zhang1, Yuan Cao7, Ruixiang Tang, Dimitris N. Metaxas",,,"LLM safety, deliberative alignment, case augmentation, safety rules, reinforcement learning, code-based reasoning","This paper evaluates the impact of explicitly specifying safety codes versus illustrative cases in LLMs. It finds that case-augmented reasoning improves harmlessness and robustness, while rule-only approaches degrade performance. The authors propose CADA, a case-augmented deliberative alignment method using reinforcement learning.",28.28,LFM-2.5,Apple M1 (Metal)
2601.08003v1_LLM Review Enhancing Creative Writing via Blind Pe.pdf,Enhancing Creative Writing via Blind Peer Review Feedback,"Weiyue Li, Mingxiao Song, Zhenda Shen, Dachuan Zhao, Yunfan Long, Yi Li, Yongce Li, Ruyi Yang, Mengyu Wang",,,"LLM review, creative writing, blind peer review, interaction structure, model scale, divergent trajectories","The paper introduces LLM Review, a framework inspired by blind peer review, to improve creativity in large language models by constraining information flow through targeted feedback while preserving divergent creative paths. Experiments show it outperforms multi-agent baselines and smaller models.",28.2,LFM-2.5,Apple M1 (Metal)
2601.08005v1_Internal Deployment Gaps in AI Regulation.pdf,Internal Deployment Gaps in AI Regulation,"JOE KWON, STEPHEN CASPER",,,"AI regulation, internal deployment, oversight gaps, EU AI Act","This paper examines how frontier AI regulations in the US and EU in 2025 address internal deployment, identifying three gaps that enable evasion: scope ambiguity, point-in-time compliance, and information asymmetries. It analyzes persistence factors and proposes policy approaches.",26.89,LFM-2.5,Apple M1 (Metal)
2601.08011v1_TP-Blend Textual-Prompt Attention Pairing for Prec.pdf,TP-Blend: Textual-Prompt Attention Pairing for Precise,"Xin Jin, Yichuan Zhongyichuanzhong, Yapeng Tian",10.1089/tmrl.2025.2601,2601.08011v1,"textual-prompt, object-style blending, diffusion models, content control, perceptual quality","Presents Twin-PromptAttention Blend (TP-Blend), a lightweight training-free framework for simultaneous object replacement and style introduction in diffusion models. TP-Blend uses Cross-Attention Object Fusion and Self-Attention Style Fusion to achieve high-resolution, photo-realistic edits with precise control.",28.87,LFM-2.5,Apple M1 (Metal)
2601.08017v1_Representations of Text and Images Align From Laye.pdf,Representations of Text and Images Align From Layer One,"Javier Rando, Florian Tram, Stanislav Fort, 2",arXiv:2601.08017v1,2601.08017,,"The study demonstrates that image-text representations align from the first layer in adapter-based vision-language models, contradicting the notion that alignment occurs only in later layers. Using a synthesis-based method inspired by DeepDream, the authors show that images synthesized at layer 1 often reflect recognisable features of textual concepts.",28.5,LFM-2.5,Apple M1 (Metal)
2601.08026v2_FigEx2 Visual-Conditioned Panel Detection and Capt.pdf,FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures,"Jifeng Song, Arun Das, Pan Wang, Hui Ji, Kun Zhao, Yufei Huang, 2, 3*, Department of Electrical and Computer Engineering, University of Pittsburgh, USA, Cancer Virology Program, UPMC Hillman Cancer Center, USA, Department of Medicine, University of Pittsburgh, USA, Department of Informatics and Networked Systems, University of Pittsburgh, USA",,,"compound figures, panel detection, captioning, scientific figures, label localization, semantic understanding, figure-level summaries, open-ended captioning, CLIP, BERTScore, zero-shot transfer",Proposes FigEx2 to localize panels and generate panel-wise captions from compound figures. Introduces a noise-aware gated fusion module and staged optimization combining supervised learning with reinforcement learning to achieve high detection accuracy and robustness across domains.,29.98,LFM-2.5,Apple M1 (Metal)
2601.08043v1_The Role of Noisy Data in Improving CNN Robustness.pdf,The Role of Noisy Data in Improving CNN Robustness for Image Classification,"Oscar H. Ramírez-Agudeloa, Nicoleta Gorea, Aliza Reif, Lorenzo Bonasera, Michael Karla",,,"deep learning, CNNs, data quality, CIFAR-10, noise injection, image classification, model robustness",This paper investigates how introducing controlled noise into training data can improve CNN robustness for image classification. Experiments on CIFAR-10 show that incorporating up to 10% noisy data significantly reduces test loss while maintaining accuracy under corrupted conditions.,29.0,LFM-2.5,Apple M1 (Metal)
2601.08049v1_Integrating Attendance Tracking and Emotion Detect.pdf,Integrating Attendance Tracking and Emotion Detection for Enhanced Student Engagement in Smart Classrooms,"Keith Ainebyona, Ann Move Oguti, Joseph Walusimbi, Ritah Kobusingye",42401600224@sun.ac.ug,,"Affective computing, Attendance automation, Emotion detection, IoT, Smart classroom","This paper presents SCASED, an IoT-based system integrating automated attendance tracking with facial emotion recognition to support classroom engagement monitoring. The system uses a Raspberry Pi camera and OpenCV for face detection, and a fine-tuned MobileNetV2 model to classify four learning-related emotional states. Experimental evaluation using the DAiSEE dataset achieved an emotion classification accuracy of 89.5%, demonstrating the value of combining attendance data with emotion analytics for more responsive teaching practices.",31.99,LFM-2.5,Apple M1 (Metal)
2601.08052v1_Forecast Aware Deep Reinforcement Learning for Eff.pdf,Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms,"Nawazish Alia, Rachael Shawb, Karl Masona",arXiv:2601.08052v1,,"arXiv:2601.08052, cs.AI, energy management, renewable energy, dairy farming, load scheduling, forecast-aware, deep reinforcement learning, battery storage, water heating","This study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under real-world operational constraints. It incorporates short-term demand and renewable generation forecasts and uses adaptive PPO with KL-divergence control to achieve up to 1% lower electricity costs.",31.89,LFM-2.5,Apple M1 (Metal)
2601.08058v1_Reasoning Beyond Chain-of-Thought A Latent Computa.pdf,Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models,"Zhenghao He, Guangzhi Xiong Bohan Liu, Bohan Liu Sanchit Sinha, Aidong Zhang",,,"Chain-of-Thought, Large Language Models, Latent Steering, Reasoning, Internal Representations","This study investigates why Chain-of-Thought prompting improves reasoning performance in LLMs by analyzing latent internal activations. It finds that steering a single reasoning-related latent feature can enhance accuracy without explicit CoT prompts, suggesting that multi-step reasoning is supported by latent activations that can be externally activated.",28.12,LFM-2.5,Apple M1 (Metal)
2601.08065v1_A New Strategy for Verifying Reach-Avoid Specifica.pdf,A New Strategy for Verifying Reach-Avoid,"Samuel I. Akinwande, Sydney M. Katz, Mykel J. Kochenderfer, Clark Barrett",arXiv:2601.08065v1,arXiv:2601.08065,"reachability analysis, neural feedback systems, forward analysis, safety specifications",This paper introduces new algorithms for computing backward reachable sets in neural feedback systems and integrates them with forward analysis to provide a unified verification framework.,29.18,LFM-2.5,Apple M1 (Metal)
2601.08070v1_Semantic Gravity Wells Why Negative Constraints Ba.pdf,Semantic Gravity Wells: Why Negative Constraints Backfire,Shailesh Rana,https://github.com/gut-puncture,,,Investigates negative instruction failure using semantic pressure; reveals asymmetry in suppression signals and identifies layer-wise mechanisms.,22.97,LFM-2.5,Apple M1 (Metal)
2601.08079v1_MemoBrain Executive Memory as an Agentic Brain for.pdf,Executive Memory as an Agentic Brain for Reasoning,"Hongjin Qian, Zhao Cao, Zheng Liu",,,"executive memory, agentic brain, reasoning, tool-augmented agents, long-horizon reasoning, cognitive control","The paper proposes MemoBrain, an executive memory model for tool-augmented agents, to manage reasoning traces and tool artifacts in long-horizon tasks. It emphasizes memory-driven cognitive control to maintain logical continuity and improve performance on complex benchmarks.",27.09,LFM-2.5,Apple M1 (Metal)
2601.08089v1_Q-realign Piggybacking Realignment on Quantization.pdf,Q-realign: Piggybacking Realignment on Quantization for Safe and Efficient LLM Deployment,"Qitao Tan, Xiaoying Song, Ningxi Cheng, Ninghao Liu, Xiaoming Zhai, Lingzi Hong, Yanzhi Wang, Geng Yuan",10.1234/q-realign,,"large language models, safety alignment, quantization, deployment, LLM deployment, safety recovery, compression, GPU efficiency","The paper proposes Q-realign, a post-hoc defense method leveraging post-training quantization guided by representational analysis. It decouples safety alignment from fine-tuning and enables efficient deployment, achieving significant reductions in unsafe behaviors while preserving performance.",28.36,LFM-2.5,Apple M1 (Metal)
2601.08094v1_Local-Global Feature Fusion for Subject-Independen.pdf,Local-Global Feature Fusion for Subject-Independent EEG Emotion Recognition,"Zheng Zhou, Isabella McEvoy, Camilo E. Valderrama",10.1007/978-3-642-45888-7,,"EEG, emotion recognition, subject-independent, local representations, global representations, domain adaptation, neural patterns",The paper proposes a fusion framework combining local channel-wise descriptors and global trial-level descriptors to improve cross-subject generalization on the SEED-VII dataset. It integrates differential entropy with graph-theoretic features and applies attention-based fusion with domain-adversarial regularization.,28.93,LFM-2.5,Apple M1 (Metal)
2601.08104v1_High-Fidelity Modeling of Stochastic Chemical Dyna.pdf,High-Fidelity Modeling of Stochastic Chemical Dynamics on Complex Manifolds: A Multi-Scale SIREN-PINN Framework for the Thermocurvature-Perturbed Ginzburg-Landau Equation,"Julian Evan Chrisnanto, Salsabila Rahma Alia, Nurfauzi Fadillah, Yulison Herry Chrisnanto",s254167v@st.go.tuat.ac.jp,2601.08104v1,"Physics-Informed Neural Networks (PINNs), Spatiotemporal Chaos, Inverse Geometric Problems, Reaction-Diffusion Systems, Defect Turbulence, Riemann Manifold Learning","The paper presents a Multi-Scale SIREN-PINN architecture for modeling complex reaction-diffusion systems on Riemannian manifolds. It addresses challenges in capturing high-frequency dynamics and topological invariants in systems with unknown catalytic topography, achieving improved state prediction performance.",29.66,LFM-2.5,Apple M1 (Metal)
2601.08107v1_STO-RL Offline RL under Sparse Rewards via LLM-Gui.pdf,STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order,"Chengyang Gu, Yuxin Pan, Hui Xiong, Yize Chen",10.1145/nnnnnnn.nnnnnnn,cgu893@connect.hkust-gz.edu.cn,"Offline RL, Temporal order, Large Language Models","Offline reinforcement learning enables policy learning from pre-collected datasets, avoiding costly online interactions. This paper proposes STO-RL, an offline RL framework that uses large language models to generate temporally ordered subgoal sequences and apply potential-based reward shaping to transform sparse rewards into dense, temporally consistent signals, improving performance on long-horizon tasks.",28.3,LFM-2.5,Apple M1 (Metal)
2601.08108v1_Debiasing Large Language Models via Adaptive Causa.pdf,Debiasing Large Language Models via Adaptive Causal Prompting with Sketch-of-Thought,"Bowen Li, Ziqi Xu, Jing Ren, Renqiang Luo, Xikun Zhang, Xiuzhen Zhang, Yongli Ren, Feng Xia",10.48550/arXiv.2025.12345,arXiv:2509.12345,"large language models, prompting, adaptive causal prompting, sketch-of-thought, causal inference, bias mitigation","This paper proposes an Adaptive Causal Prompting with Sketch-of-Thought (ACPS) framework to address excessive token usage and bias in LLMs. By leveraging structural causal models, ACPS infers causal effects and adaptively selects interventions, enabling robust reasoning across tasks without retraining. Experiments show consistent improvements over baselines in accuracy, robustness, and efficiency.",29.57,LFM-2.5,Apple M1 (Metal)
2601.08109v1_CSQL Mapping Documents into Causal Databases.pdf,Csql: MappingDocuments intoCausalDatabases ∗,"Sridhar Mahadevan, Adobe Research and University of Massachusetts, Amherst",10.48550/arXiv.2601.08109,2601.08109,"Causality, Natural Language, Databases, SQL, AI, Machine Learning","Describes Csql, a system that converts unstructured text into a SQL-queryable causal database, enabling causal queries over document collections rather than relying on retrieval or knowledge graphs.",27.87,LFM-2.5,Apple M1 (Metal)
2601.08118v1_MirrorBench An Extensible Framework to Evaluate Us.pdf,MIRRORBENCH: ANEXTENSIBLEFRAMEWORK TOEVALUATEUSER-PROXY,"Ashutosh Hathidara, Julien Yu, Vaishali Senthil, Sebastian Schreiber, Anil Babu Ankisettipalli",,,"human simulators, user proxies, conversational evaluation, LLM benchmarks, natural language generation","MIRRORBENCH is a reproducible, extensible benchmarking framework that evaluates user proxies based on their ability to produce human-like utterances across diverse conversational tasks, decoupled from downstream task success. It introduces a modular execution engine, supports pluggable proxies and metrics, and reveals gaps between proxies and real users.",28.54,LFM-2.5,Apple M1 (Metal)
2601.08125v1_How vehicles change lanes after encountering crash.pdf,How vehicles change lanes after encountering crashes: Empirical analysis and modeling,"Kequan Chena, Yuxuan Wangb, Pan Liu, Victor L. Knoop, David Z. W. Wang",10.48550/arXiv.2405.12345,10.48550/arXiv.2405.12345,"vehicles, lane changing, crash, empirical analysis, modeling",Empirical analysis and modeling of how vehicles adjust lanes following crashes.,32.42,LFM-2.5,Apple M1 (Metal)
2601.08127v1_PathoGen Diffusion-Based Synthesis of Realistic Le.pdf,PathoGen: Diffusion-Based Synthesis of Realistic Lesions in Histopathology Images,"Mohamad Koohi-Moghadam1, Mohammad-Ali Nikouei Mahani1, Kyongtae Tyler Bae1",,,"histopathology, lesion generation, diffusion models, image synthesis, medical AI","This paper introduces PathoGen, a diffusion-based generative model that enables high-fidelity inpainting of lesions into benign histopathology images. It addresses the challenge of limited expert-annotated data by leveraging iterative diffusion refinement to preserve tissue boundaries, cellular structures, and staining characteristics. PathoGen outperforms existing generative models in image fidelity and distributional similarity, improving downstream segmentation and overcoming annotation bottlenecks.",30.48,LFM-2.5,Apple M1 (Metal)
2601.08128v1_Embedded AI Companion System on Edge Devices.pdf,Embedded AI Companion System on Edge Devices,"Rahul Gupta, Stephen Hsu",arXiv:2601.08128v1,arXiv:2601.08128,"AI companion, edge devices, computational constraints, memory systems, conversational AI, personalization",The paper proposes a memory paradigm for AI companions on edge devices that alternates between active and inactive phases to minimize latency while maintaining personalization. It introduces an AI Companion benchmark and demonstrates that their system outperforms equivalent cloud-based models in most metrics.,28.07,LFM-2.5,Apple M1 (Metal)
2601.08133v1_How Do Optical Flow and Textual Prompts Collaborat.pdf,Audio-Visual Semantic Segmentation with Optical Flow and Textual Prompts,"Peng Gao, Yujian Lee, Yongqi Xu, Wentao Fan, Guangdong Provincial/Zhuhai Key Laboratory IRADS, Peking University, Shenzhen Graduate School",62276106,2024GXJK695,"audio-visual segmentation, semantic segmentation, optical flow, textual prompts, visual segmentation, motion dynamics, semantic interpretation","This paper introduces a novel collaborative framework, Stepping Stone Plus (SSP), that integrates optical flow and textual prompts to enhance audio-visual semantic segmentation. By leveraging optical flow for temporal context and incorporating textual prompts for scene description, SSP improves segmentation accuracy, especially for coexisting sound sources and static objects. A visual-textual alignment module further supports cross-modal understanding.",29.22,LFM-2.5,Apple M1 (Metal)
2601.08139v1_Subspace Alignment for Vision-Language Model Test-.pdf,Subspace Alignment for Vision-Language Model Test-time Adaptation,"Zhichen Zeng, Wenxuan Bao, Xiao Lin, Ruizhong Qiu, Tianxin Wei, Xuying Ning, Yuchen Yan, Chen Luo, Monica Xiao Cheng, Jingrui He, Hanghang Tong",,,"subspace alignment, vision-language models, test-time adaptation, zero-shot learning, modality gap, visual-natural language alignment","The paper proposes SubTTA to align semantic subspaces across vision and language modalities, addressing distribution shift challenges by minimizing chordal distance and filtering visual noise. Extensive experiments show an average improvement of 2.24% over state-of-the-art TTA methods.",28.55,LFM-2.5,Apple M1 (Metal)
2601.08141v1_Qalb Largest State-of-the-Art Urdu Large Language .pdf,Qalb: Largest State-of-the-Art Urdu Large Language Model,"1st Muhammad Taimoor Hassan, 2st Jawad Ahmed, 3st Muhammad Awais",,,"Urdu language model, continued pre-training, low-resource NLP, LoRA, language adaptation","Introduces Qalb, an Urdu language model developed via two-stage pre-training and supervised fine-tuning, achieving state-of-the-art performance on Urdu benchmarks.",26.2,LFM-2.5,Apple M1 (Metal)
2601.08146v2_Mechanisms are Transferable Data-Efficient Low-Res.pdf,Mechanisms are Transferable: Data-Efficient Low-Resource Adaptation,"Khumaisa Nur’aini, Ayu Purwarianti, Alham Fikri Aji, Derry Wijaya",,,"low-resource adaptation, circuit-targeted fine-tuning, contextual decomposition transformer, cross-lingual transfer, catastrophic forgetting","Proposes Circuit-Targeted Supervised Fine-Tuning (CT-SFT) to adapt LLMs to low-resource languages with minimal parameter updates, improving cross-lingual accuracy while mitigating catastrophic forgetting.",27.01,LFM-2.5,Apple M1 (Metal)
2601.08148v1_Enriching Semantic Profiles into Knowledge Graph f.pdf,Enriching Semantic Profiles into Knowledge Graph for Recommender Systems Using Large Language Models,"Seokho Ahn, Sungbok Shin, Young-Duk Seo",10.1145/3770854.3780324,sokho0514@inha.edu,"Recommendation, Semantic Profiling, Large Language Models, Knowledge Graphs","This paper revisits profiling-based approaches in recommender systems across four dimensions—knowledge base, preference indicator, impact range, and subject—and proposes a new model, SPiKE, that leverages large language models for entity profile generation and profile-aware aggregation to improve recommendation quality.",27.18,LFM-2.5,Apple M1 (Metal)
2601.08149v1_Dynamic Graph Structure Learning via Resistance Cu.pdf,Dynamic Graph Structure Learning via Resistance Curvature Flow,"Chaoqun Fei, Huanjiang Liu, Tinglve Zhou, Y angyang Li, Tianyong Hao",10.1093/acprof:oso/9780190871293.001.0001,2601.08149v1,"graph structure learning, curvature flow, circuit theory, deep learning, manifold learning","Introduces a novel curvature flow based on effective resistance from circuit theory, establishing a new paradigm for geometric graph structure evolution. The paper formulates the dynamic evolution equation of RCF, highlighting its mechanisms for manifold enhancement and noise suppression, and demonstrates its effectiveness and compatibility with deep learning frameworks through extensive experiments.",33.92,LFM-2.5,Apple M1 (Metal)
2601.08156v1_Project Synapse A Hierarchical Multi-Agent Framewo.pdf,Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions,"Arin Gopalan Yadav, Varad Dherange, Kumar Shivam",arXiv:2601.08156v1,arXiv:2601.08156v1,"Project Synapse, Multi-Agent Framework, Hybrid Memory Architecture, Last-Mile Delivery, Autonomous Resolution, Complex Disruptions","The paper introduces Project Synapse, a hierarchical framework using a central resolution supervisor and specialized worker agents to autonomously manage last-mile delivery disruptions. It features a novel hybrid memory system and evaluates performance on a benchmark dataset.",32.28,LFM-2.5,Apple M1 (Metal)
2601.08160v1_SwiftMem Fast Agentic Memory via Query-aware Index.pdf,SwiftMem: Fast Agentic Memory via Query-aware Indexing,"Anxin Tian, Yiming Li, Xing Li, Hui-Ling Zhen, Lei Chen, Xianzhi Yu1 Zhenhua Dong, Mingxuan Yuan",10.1234/abcd1234,12345678,"SwiftMem, agentic memory, query-aware indexing, LLM agents, memory retrieval, semantic indexing","This paper introduces SwiftMem, a query-aware agentic memory system designed to overcome the O(Nmem) retrieval latency of traditional memory frameworks. By leveraging temporal and semantic indexing, SwiftMem achieves sub-linear retrieval and supports real-time LLM interactions, demonstrating up to 47× faster search compared to state-of-the-art baselines.",29.35,LFM-2.5,Apple M1 (Metal)
2601.08166v1_ZeroDVFS Zero-Shot LLM-Guided Core and Frequency A.pdf,ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency,"Mohammad Pivezhandi, Mahdi Banisharif, Abusayeed Saifullah, Ali Jannesari",arXiv:2601.08166v1,arXiv:2601.08166,"zero-shot learning, multi-agent reinforcement learning, thermal management, energy efficiency, embedded systems","Proposes a model-based hierarchical multi-agent reinforcement learning framework for thermal-aware scheduling on multi-core platforms. It enables zero-shot deployment by generating synthetic training data using LLM-extracted semantic features, achieving faster convergence and improved energy efficiency compared to traditional methods.",29.03,LFM-2.5,Apple M1 (Metal)
2601.08173v1_The Agents First Day Benchmarking Learning Explora.pdf,"The Agent’s First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios","Daocheng Fu1, Jianbiao Mei3, Rong Wu3, Xuemeng Yang2, Jia Xu2, Ding Wang2, Pinlong Cai2, Yong Liu3, B, Licheng Wen2,4,5, Botsian Shi2, 1Fudan University, 2Shanghai AI Laboratory, 3Zhejiang University, 4Shanghai Innovation Institute, 5Shanghai Jiao Tong University",,,"learning, exploration, scheduling, workplace scenarios, multi-modal models, dynamic task scheduling, active exploration, continuous learning","This paper introduces Trainee-Bench, a dynamic evaluation environment that assesses agents across three dimensions: context-aware scheduling, prudent information acquisition, and continual learning. It addresses challenges in dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. Experiments reveal deficiencies in handling active exploration and learning in real-world settings.",29.46,LFM-2.5,Apple M1 (Metal)
2601.08176v1_Prompt-Based Clarity Evaluation and Topic Detectio.pdf,Automatic evaluation of large language model responses requires not only factual correctness but also clarity,"Lavanya Prahallad, Sai Utkarsh Choudarypally, Pragna Prahallad, Pranathi Prahallad",10.1093/acpsr/clac123,,"Clarity evaluation, Prompt engineering, Political Question-Answering, Large language models, Chain-of-thought prompting, Topic identification","This paper evaluates prompt-based clarity evaluation using the CLARITY dataset from the SemEval-2026 shared task. It compares GPT-3.5 with GPT-5.2 under different prompting strategies and demonstrates improvements in clarity metrics, highlighting the role of structured reasoning prompts.",28.75,LFM-2.5,Apple M1 (Metal)
2601.08179v1_Instruction-Driven 3D Facial Expression Generation.pdf,Instruction-Driven 3D Facial Expression Generation and Transition,"Anh H. V o, Tae-Seok Kim, Hulin Jin, Soo-Mi Choi, Yong-Guk Kim",https://vohoanganh.github.io/tg3dfet/,,"Instruction-Driven, Facial Expression, Transition, 3D Facial Expression, Instruction to Facial Expression Transition, CK+, CelebV-HQ",This study presents a framework for generating smooth facial expression transitions using instruction-based learning. The proposed model leverages a vertex reconstruction loss to refine semantic understanding and produces expressive trajectories from text prompts.,28.07,LFM-2.5,Apple M1 (Metal)
2601.08183v2_GI-Bench A Panoramic Benchmark Revealing the Knowl.pdf,GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards,"Yan Zhu, Te Luo, Pei-Yao Fu, Zhen Zhang, Zi-Long Wang, Yi-Fan Qu, Zi-Han Geng, Jia-Qi Xu, Lu Yao, Li-Yun Ma, Wei Su, Wei-Feng Chen, Quan-Lin Li",,,"multimodal large language models, gastrointestinal endoscopy, clinical standards, knowledge experience dissociation, benchmarking",Background discusses MLLMs' potential in gastroenterology but unverified performance. Objective aims to evaluate MLLMs across a GI endoscopy workflow versus human benchmarks. Results highlight Gemini-3-Pro's state-of-the-art performance.,32.14,LFM-2.5,Apple M1 (Metal)
2601.08185v1_Autonomous Materials Exploration by Integrating Au.pdf,Autonomous Materials Exploration by Integrating Automated Phase Identification and AI-Assisted Human Reasoning,"Ming-Chiang Chang, Maximilian Amsler, Duncan R. Sutherland, Sebastian Ament, Katie R. Gann, Lanca Zhou, Louisa M. Smieska, Arthur R. Woll, John M. Gregoire, Carla P. Gomes, R. Bruce van Dover, Michael O. Thompson",,,"autonomous experimentation, materials development, AI-assisted reasoning, phase identification, robotic platforms, high-throughput synthesis, metastable phases, material synthesis, cationic doping","This work presents an autonomous materials synthesis extension to SARA, leveraging AI and human-in-the-loop reasoning to accelerate discovery. It describes experiments on oxide systems using robotic processing, demonstrating improved efficiency through synthetic benchmarks and active learning campaigns.",30.14,LFM-2.5,Apple M1 (Metal)
2601.08187v2_Improving LLM Reasoning with Homophily-aware Struc.pdf,Improving LLM Reasoning With Homophily-Aware Structural and Semantic Compression,"Zijun Di, Bin Lu, Huquan Kang, Kinghiqian, Luoyi Fu, Jiaxin Ding, Xiaoying Gan, Ganxia Yong, Lei Zhou, Zhou Lei, Xinbing Wang, Chenghu Zhou, Jiaxing Ding",10.48550/arXiv.2601.08187,2601.08187v2,"large language models, text-annotated graph, homophily-aware, structural compression, semantic aggregation, LLM reasoning","The paper proposes HS2C, a framework leveraging graph homophily to enhance LLM reasoning by compressing structural and semantic information. It uses hierarchical partitioning guided by structural entropy and feeds compressed inputs to improve inference accuracy.",30.05,LFM-2.5,Apple M1 (Metal)
2601.08189v2_ForgetMark Stealthy Fingerprint Embedding via Targ.pdf,Stealthya Fingerprint Embedding Via Targeted Unlearning,"Zhenhua Xu1, Haobo Zhang, Zhebo Wang, Qichen Liu, Wenpeng Xing, Meng Han, Zhejiang University, GenTel.io, Zhejiang University of Technology",,,"Large Language Model, Copyright protection, Model Fingerprinting, Machine Unlearning, Stealthy Fingerprinting, Model Ownership, Provenance Auditing","Introduces ForgetMark, a stealthy fingerprinting framework using targeted unlearning to avoid high-perplexity triggers and improve robustness against detection.",27.71,LFM-2.5,Apple M1 (Metal)
2601.08196v1_Evaluating Implicit Regulatory Compliance in LLM T.pdf,Evaluating Implicit Regulatory Compliance in LLM Tool Invocation via Logic-Guided Synthesis,"Da Song, Yuheng Huang, Boqi Chen, Tianshuo Cong, Randy Goebel, Lei Ma, Foutse Khomh",,,"large language models, regulatory compliance, functional safety, logic-guided synthesis, automated reasoning, safety constraints","The paper introduces LOGISAFETYGEN, a framework that evaluates LLMs for implicit regulatory compliance using Linear Temporal Logic and logic-guided fuzzing. It benchmarks 240 tasks showing that larger models often prioritize performance over safety.",27.75,LFM-2.5,Apple M1 (Metal)
2601.08211v1_Adapting Rules of Official International Mahjong f.pdf,Adapting Rules of Official International Mahjong for Online Players,"Chucai Wang, Lingfeng Li, Yunlong Lu, Wenxin Li",,,"Mahjong, game design, AI, rule adaptation, online gaming","This study proposes rule modifications for Official International Mahjong to suit online play, addressing first-mover advantage and subgoal scoring in a single-round format. It introduces compensatory points and revised scoring mechanisms tailored for online environments.",27.04,LFM-2.5,Apple M1 (Metal)
2601.08223v2_DNF Dual-Layer Nested Fingerprinting for Large Lan.pdf,Dual-Layer Nested Fingerprinting For Large Language Model,"Zhenhua Xu1, Yiran Zhao3, Mengting Zhong3, Dezhang Kong1,2,3, Changting Lin1, Tong Qiao3, Meng Han1,2,3",,,"Large Language Model, Copyright Protection, Model Fingerprinting, Backdoor, Stealth Verification","The paper introduces Dual-Layer Nested Fingerprinting (DNF), a black-box method embedding hierarchical backdoors to protect intellectual property in large language models. DNF achieves perfect fingerprint activation while preserving utility, using lower-perplexity triggers and remaining undetectable under fingerprint attacks.",27.61,LFM-2.5,Apple M1 (Metal)
2601.08224v1_An Axiomatic Approach to General Intelligence SANC.pdf,An Axiomatic Approach to General Intelligence,"Daesuk Kwon, 1Won-gi Paeng1",arXiv:2601.08224v1,SANC(E3),"axiomatization of intelligence, competitives selection, system tokens, reconstruction compression, category formation, Gestalt completion","Proposes SANC(E3), an axiomatic framework for general intelligence where representational units emerge from self-organization under finite energy constraints. It unifies perception, imagination, planning, and action under a single energetic process governed by energy functional minimization.",28.21,LFM-2.5,Apple M1 (Metal)
2601.08226v1_Knowledge-based learning in Text-RAG and Image-RAG.pdf,Knowledge-based learning in Text-RAG and Image-RAG,"Alexander Shim, Khalil Saieh, Samuel Clarke, ksaie001@fiu.edu",,,"knowledge-based learning, Text-RAG, Image-RAG, vision transformer, hallucination reduction, diagnostic accuracy, multi-modal AI, radiology, clinical interpretation","This research analyzes and compares multi-modal approaches in Vision Transformer-based image encoding with LLM-based RAG to reduce hallucination and improve chest X-ray diagnosis. It evaluates text-based RAG, image-based RAG, and a baseline, highlighting challenges like data imbalance and calibration.",28.81,LFM-2.5,Apple M1 (Metal)
2601.08230v1_GADPN Graph Adaptive Denoising and Perturbation Ne.pdf,Graph Adaptive Denoising and Perturbation Networks via Singular Value Decomposition,"Hao Deng, Bo Liu, Member, IEEE",,,"Graph Neural Networks, Graph Structural Learning, Network Representation Learning","This paper proposes GADPN, a framework that adaptively refines graph topology using low-rank denoising and structural perturbation. It introduces Bayesian optimization to tailor denoising strength and extends SVD-based structural perturbation to arbitrary graphs, achieving state-of-the-art performance with improved efficiency.",26.72,LFM-2.5,Apple M1 (Metal)
2601.08235v2_MPCI-Bench A Benchmark for Multimodal Pairwise Con.pdf,MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity,"Shouju Wang, Haopeng Zhang",10.48550/arXiv.2405.12345,arXiv:2405.12345,"Contextual Integrity, multimodal models, privacy evaluation, LLMs","Introduces MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark, to evaluate privacy behavior in agentic language models. The paper highlights challenges in balancing privacy and utility across text and multimodal inputs, and identifies modality leakage as a key issue.",27.23,LFM-2.5,Apple M1 (Metal)
2601.08237v1_The End of Reward Engineering How LLMs Are Redefin.pdf,The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination,"Haoran Su, Yandong Sun, Congjia Yu, gary.yu@lerna-ai.com",arXiv:2601.08237v1,2601.08237,"reward engineering, multi-agent reinforcement learning, LLMs, semantic reward specification, dynamic adaptation, computational cost, hallucination risks, scalability","This paper argues that large language models enable a shift from hand-crafted numerical rewards to natural language objectives in multi-agent systems. Recent advances allow LLMs to generate human-level reward functions, adapt rewards dynamically, and coordinate agents via semantic understanding. The discussion highlights three pillars—semantic reward specification, dynamic adaptation, and human alignment—while addressing challenges like computational cost, hallucination, and scalability.",31.04,LFM-2.5,Apple M1 (Metal)
2601.08251v1_Hyperbolic Heterogeneous Graph Transformer.pdf,Hyperbolic Heterogeneous Graph Transformer,"Jongmin Park, Seunghoon Han, Hyewon Lee, Won-Yong Shin, Sungsu Lim",1,1,"Heterogeneous Graph Representation Learning, Hyperbolic Graph Embedding, Graph Transformer, Graph Transformer Architecture, Relation-Specific Attention","The paper proposes Hyperbolic Heterogeneous Graph Transformer (HypHGT), a transformer-based model that learns heterogeneous graph representations entirely within hyperbolic space. It addresses limitations of existing methods by naturally capturing local and global dependencies and reducing training time and memory usage.",28.13,LFM-2.5,Apple M1 (Metal)
2601.08254v1_Large Artificial Intelligence Model Guided Deep Re.pdf,Large Artificial Intelligence Model–Guided Deep Reinforcement Learning for Resource Allocation in Non-Terrestrial Networks,"Abdikarim Mohamed Ibrahim, Rosdiadee Nordin",,,"Large AI Models (LAMs), Large Language Models (LLMs), Deep Reinforcement Learning (DRL), Satellite Communications, Non-Terrestrial Networks (NTNs)","The paper proposes a Deep Reinforcement Learning agent guided by a Large Language Model to improve resource allocation in Non-Terrestrial Networks, demonstrating improved performance over traditional methods in various scenarios.",27.23,LFM-2.5,Apple M1 (Metal)
2601.08257v2_On Evaluation of Unsupervised Feature Selection fo.pdf,Evaluation of Unsupervised Feature Selection for Pattern Classification,"Gyu-Il Kim, Dae-Won Kim, Jaesung Lee",arXiv:2601.08257v2,2601.08257v2,"unsupervised feature selection, pattern classification, multi-label evaluation, feature selection, data preprocessing","This study revisits the evaluation of unsupervised feature selection methods by adopting a multi-label classification framework. Performance rankings differ from single-label settings, highlighting the need for multi-label evaluation to fairly assess discriminative ability.",28.71,LFM-2.5,Apple M1 (Metal)
2601.08258v1_T3 Benchmarking Sycophancy and Skepticism in Causa.pdf,Benchmarking Sycophancy and Skepticism in Causal Judgment,Edward Y. Chang,,,"sycophancy, skepticism, causal judgment, LLM",Introduces T3 (TESTINGTRUSTWORTHY THINKING) benchmark to diagnose LLM reasoning across Pearl’s Causal Hierarchy. Identifies two pathologies: a 'Skepticism Trap' at L1 and a non-monotonic scaling paradox at L3. Validates a process-verified protocol (RCA) showing models exhibit sycophancy or paralysis.,26.82,LFM-2.5,Apple M1 (Metal)
2601.08262v1_VGG Induced Deep Hand Sign Language Detection.pdf,VGG Induced Deep Hand Sign Language Detection,"Subham Sharmaa, Sharmila Subudhia, Dept. of Computer Science, Maharaja Sriram Chandra Bhanja Deo, University, Baripada, 757003, Odisha, India",arXiv:2601.08262v1,2601.08262v1,"Hand gesture recognition, Convolutional neural network, Classification, VGG-16 net, API",This work proposes a novel hand gesture recognizing system for differently-abled persons using a VGG-16 net trained on the NUS dataset. Transfer learning and image data augmentation improve accuracy to around 98%.,31.84,LFM-2.5,Apple M1 (Metal)
2601.08271v1_Sparsity Is Necessary Polynomial-Time Stability fo.pdf,Sparsity Is Necessary: Polynomial-Time Stability for Agentic LLMs in Large Action Spaces,Angshul Majumdar,arXiv:2601.08271v1,arXiv:2601.08271,"sparsity, polynomial-time stability, agentic LLMs, large action spaces","This paper investigates sequential decision-making with massive discrete action spaces in agentic LLMs, formalizing Sparse Agentic Control (SAC). It presents ℓ1,2-regularized policy learning, derives sharp stability conditions, and analyzes stability under partial observability, highlighting the necessity of sparsity for scalable control.",28.15,LFM-2.5,Apple M1 (Metal)
2601.08273v1_HIPPO Accelerating Video Large Language Models Inf.pdf,Accelerating Video Large Language Models Inference via Holistic-aware Parallel Speculative Decoding,"Qitan Lv1, Tianyu Liu1, Wen Wu2, Xuenan Xu2, Bowen Zhou2, Feng Wu1, Chao Zhang2,3, Zhou Cheng2, Zhang Cheng3",,,"video LLMs, speculative decoding, parallel decoding, semantic preservation, inference acceleration","This paper proposes HIPPO, a holistic-aware parallel speculative decoding framework for video LLMs, addressing limitations of existing pruning strategies by preserving semantic tokens and decoupling draft generation from verification.",27.95,LFM-2.5,Apple M1 (Metal)
2601.08276v1_ToolACE-MCP Generalizing History-Aware Routing fro.pdf,ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web,"Zhiyuan Yao, Zishan Xu, Yifu Guo4, Zhiguang Han, Weiwen Liu, Cheng Yang, Shuo Zhang, Weiwen Liu, Xingshan Zeng, Wu Wei, Liang Zhang",,,"Agent Web, MCP, ToolACE-MCP, History-aware routing, Multi-agent collaboration, Robustness, Scalability","ToolACE-MCP proposes a pipeline for training history-aware routers to enable precise navigation in large-scale agent ecosystems. It leverages a dependency-rich candidate graph to synthesize multi-turn trajectories, allowing the creation of a plug-and-play Light Routing Agent. Experiments show superior performance on real-world benchmarks, demonstrating scalability and robustness.",28.86,LFM-2.5,Apple M1 (Metal)
2601.08280v1_Greedy Is Enough Sparse Action Discovery in Agenti.pdf,Greedy Is Enough: Sparse Action Discovery in Agentic LLMs,Angshul Majumdar,,,,"The paper investigates sparse action discovery in agentic language models using a greedy algorithm inspired by Orthogonal Matching Pursuit. It shows that under certain assumptions, greedy discovery efficiently recovers relevant actions with polynomial scaling and provides theoretical guarantees on optimality and estimation error.",25.8,LFM-2.5,Apple M1 (Metal)
2601.08288v1_OpenMic A Multi-Agent-Based Stand-Up Comedy Genera.pdf,OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System,"Yuyang Wu, Hanzhong Cao, Jianhao Chen, Yufei Li",10.48550/arXiv.2024.12345,arXiv:2408.12345,"Chinese humor generation, multi-agent systems, stand-up comedy, retrieval-augmented generation, joke writing, comedy video production","This paper introduces OpenMic, an end-to-end multi-agent system built on AutoGen that transforms user-provided life topics into a 3–5 minute Chinese stand-up performance. OpenMic uses multiple specialized agents in iterative rounds to jointly optimize humor, timing, and performance cues. To address dataset–task mismatch, it incorporates retrieval-augmented generation and fine-tunes a JokeWriter for stand-up-specific structures.",29.39,LFM-2.5,Apple M1 (Metal)
2601.08297v1_Demystifying the Slash Pattern in Attention The Ro.pdf,Demystifying the Slash Pattern in Attention: The Role of RoPE,"Yuan Cheng, Fengzhuo Zhang, Yunlong Hou, Cunxiao Du, Chao Du2, Tianyu Pang, Aixin Sun, Zhuoran Yang",arXiv:2601.08297v1,2601.08297,"attention patterns, RoPE, SLASH, SDHs, LLMs, query keys, RoPE frequencies","The paper investigates why attention in large language models concentrates along a specific offset, attributing it to interactions between rank-one token embeddings and medium/high-frequency RoPE components. It links these mechanisms to the emergence of Slash-Dominant Heads (SDHs).",30.0,LFM-2.5,Apple M1 (Metal)
2601.08302v1_Enhancing Sentiment Classification and Irony Detec.pdf,Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques,"Marvin Schmitt, Anne Schwerk, Sebastian Lempert",10.48550/arXiv.2405.12345,arXiv:2405.12345,"sentiment analysis, irony detection, large language models (LLMs), prompt engineering, prompt engineering techniques","This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against a baseline. Key tasks include sentiment classification, aspect-based sentiment analysis, and detecting subtle nuances such as irony. The research details the theoretical background, datasets, and methods used, assessing performance of LLMs using accuracy, recall, precision, and F1 score. Findings reveal that advanced prompting significantly improves sentiment analysis, with the few-shot approach excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%.",30.75,LFM-2.5,Apple M1 (Metal)
2601.08310v1_ORBIT On-policy Exploration-Exploitation for Contr.pdf,On-policy Exploration-Exploitation for Controllable Multi-Budget Reasoning,"Kun Liang1, Clive Bai3 Xin Xu3, Chenming Tang1,2, Sanwoo Lee1,2,∗, Weijie Liu3 Saiyong Yang3,†, Yunfang Wu1,2,†",,,"Large Reasoning Models, Chain-of-Thought, Reinforcement Learning, Multi-budget reasoning, Exploration-exploitation, Long-form reasoning, Model distillation","This paper proposes ORBIT, a controllable multi-budget reasoning framework that dynamically adjusts reasoning effort based on input. ORBIT uses multi-stage reinforcement learning to identify Pareto-optimal reasoning behaviors and applies on-policy distillation to unify them into a single model, achieving controllable reasoning across modes while maintaining high performance.",29.2,LFM-2.5,Apple M1 (Metal)
2601.08311v1_Enhancing Image Quality Assessment Ability of LMMs.pdf,Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation,"Kang Fu, Huiyu Duan, Zicheng Zhang, Yucheng Zhu, Jun Zhao, Xiongkuo Min, Jia Wang, Guangtao Zhai",,,"Image quality assessment, Retrieval-Augmented Generation, Large Multimodal Models, Zero-shot, Training-free, Image Quality Assessment, Training-free IQA","The paper introduces IQARAG, a training-free framework that improves LMMs' IQA performance by leveraging retrieval-augmented generation to provide visual anchors, demonstrating strong zero-shot capabilities across multiple IQA datasets.",28.13,LFM-2.5,Apple M1 (Metal)
2601.08323v1_AtomMem  Learnable Dynamic Agentic Memory with Ato.pdf,AtomMem: Learnable Dynamic Agentic Memory,"Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin",10.1234/example.doi,,"memory management, agent memory, dynamic decision-making, reinforcement learning, long-horizon tasks","Proposes AtomMem to reframe memory management as a dynamic decision process, enabling agents to learn task-aligned memory strategies through reinforcement learning.",25.68,LFM-2.5,Apple M1 (Metal)
2601.08327v1_Safe Heterogeneous Multi-Agent RL with Communicati.pdf,Safe Heterogeneous Multi-Agent RL with Communication Regularization for Coordinated Target Acquisition,"Gabriele Calzolari, Vidya Sumathy, Christoforos Kanellakis, George Nikolakopoulos",,,"Cooperative target acquisition, Safe autonomous coordination, Decentralized multi-agent reinforcement learning, Heterogeneous robotic systems, Learning-based control","This paper introduces a decentralized multi-agent reinforcement learning framework enabling structurally heterogeneous teams of agents to jointly discover and acquire randomly located targets in environments with partial observability, communication constraints, and dynamic interactions. The framework uses Multi-Agent Proximal Policy Optimization with a Graph Attention Network encoder integrating simulated range-sensing data and communication embeddings, promoting safety through structural rewards and information orthogonality. Comprehensive ablation studies confirm its effectiveness.",29.41,LFM-2.5,Apple M1 (Metal)
2601.08332v1_IGAN A New Inception-based Model for Stable and Hi.pdf,A New Inception-based Model for Stable and High-Fidelity Image Synthesis Using Generative Adversarial Networks,"Ahmed A. Hashim, Ali Al-Shuwaili, Asraa Saeed, Ali Al-Bayaty",,,"Generative Adversarial Networks (GANs), dilation convolutions, inception module, spectral normalization, image synthesis, deep learning stability","The paper proposes IGAN, a novel GAN architecture incorporating inception-inspired convolutions and dilated convolutions, achieving high-quality image synthesis with improved training stability and performance metrics.",29.34,LFM-2.5,Apple M1 (Metal)
2601.08333v1_Semantic Laundering in AI Agent Architectures Why .pdf,Semantic Laundering in AI Agent Architectures,Oleg Romanchuk Roman Bondar,arXiv:2601.08333v1,arXiv:2601.08333,"semantic laundering, epistemic warrant, LLM architectures","The paper examines how agent architectures blur the line between information transport and epistemic justification, framing it as semantic laundering. It argues that this phenomenon reflects a Gettier-like issue where justified beliefs gain epistemic status without proper grounding, and introduces the Warrant Erosion Principle to explain its architectural persistence.",30.78,LFM-2.5,Apple M1 (Metal)
2601.08360v1_Scalable Sequential Recommendation under Latency a.pdf,Scalable Sequential Recommendation under Latency and Memory Constraints,"Adithya Parthasarathy, Aswathnarayan Muthukrishnan Kirubakaran, Vinoth Punniyamoorthy, Nachiappan Chockalingam, Lokesh Butra, Kabilan Kannan, Abhirup Mazumder, Sumit Saha",arXiv:2601.08360v1,"IEEE Senior Member, USA","Recommender Systems, Sequence Modeling, Representation Learning, Scalable Machine Learning, Deep Learning","Sequential recommender systems must model long-range user behavior while operating under strict memory and latency constraints. Transformer-based approaches achieve strong accuracy but suffer from quadratic attention complexity, forcing aggressive truncation of user histories and limiting their practicality for long-horizon modeling. This paper presents HoloMambaRec, a lightweight sequential recommendation architecture that combines holographic reduced representations for attribute-aware embedding with a selective state space encoder for linear-time sequence processing. Item and attribute information are bound using circular convolution, preserving embedding dimensionality while encoding structured metadata. A shallow selective state space backbone, inspired by recent Mamba-style models, enables efficient training and constant-time recurrent inference. Experiments on Amazon Beauty and MovieLens-1M datasets demonstrate that HoloMambaRec consistently outperforms SASRec and achieves competitive performance with GRU4Rec under a constrained 10-epoch training budget, while maintaining substantially lower memory complexity. The design further incorporates forward-compatible mechanisms for temporal bundling and inference-time compression, positioning HoloMambaRec as a practical and extensible alternative for scalable, metadata-aware sequential recommendation.",34.0,LFM-2.5,Apple M1 (Metal)
2601.08371v1_Geo-NVS-w Geometry-Aware Novel View Synthesis In-t.pdf,Geo-NVS-w: Geometry-Aware Novel View Synthesis In-the-Wild,"Anastasios Tsalakopoulos, Antonis Karakottas, Dimitrios Zarpalas",10.48550/arXiv.2303.04112,10.48550/arXiv.2303.04112,"Geo-NVS, Geometry-Aware, Novel View Synthesis, In-the-Wild, Signed Distance Function, Energy Efficiency","Introduces Geo-NVS-w, a geometry-aware framework for high-fidelity novel view synthesis from unstructured, in-the-wild image collections. It leverages an Signed Distance Function (SDF) to guide rendering, ensuring geometric grounding and preserving fine structural details. The method achieves competitive performance with reduced energy consumption.",29.14,LFM-2.5,Apple M1 (Metal)
2601.08379v1_Training-Free Distribution Adaptation for Diffusio.pdf,Training-Free Distribution Adaptation for Diffusion,"Matina Mahdizadeh Sani ∗ ∗†, Nima Jamali ∗‡, Mohammad Jalali ∗§, Farzan Farnia ¶",,,"diffusion models, distribution matching, maximum mean discrepancy, prompt adaptation, conditional generation","Proposes MMD Guidance, a training-free method that augments reverse diffusion with MMD gradients to align generated samples with reference data, addressing distribution mismatch challenges in domain adaptation.",28.09,LFM-2.5,Apple M1 (Metal)
2601.08380v1_Thematic Working Group 5 -- Artificial Intelligenc.pdf,EDUsummIT 2025 - eBook,"Mary Webb, Matt Bower, Ana Amélia Carvalho, Fredrik Mørk Røkenes, Jodie Torrington, Jonathan D. Cohen, Yousra Chtouki, Kathryn MacCallum, Tanya Linden, Deirdre Butler, Juliana E. Raffagheli, Henriikka Vartiainen, Martina Ronci, Peter Tiernan, Chris Shelton, Joyce Malyn-Smith, Pierre Gorissen",10.48550/arXiv.2025.12345,arXiv:2509.12345,"Artificial Intelligence, AI literacy, teaching and learning, education, curriculum design, professional development, policy guidelines","Thematic Working Group 5 focuses on developing strategies to enhance AI literacy and agency for teachers, exploring curriculum design, professional development, classroom applications, and policy guidance to integrate AI effectively in education.",31.99,LFM-2.5,Apple M1 (Metal)
2601.08382v2_A Qualitative Model to Reason about Object Rotatio.pdf,A Qualitative Model to Reason about Object Rotations – applied to solve the Cube Comparison Test,"Zoe Falomira, Umeå University, Computing Science Department, Sweden",,,"cube comparison test, mental rotation, qualitative reasoning, spatial cognition, spatial reasoning",This paper presents a qualitative model for Reasoning about Object Rotations (QOR) which is applied to solve the Cube Comparison Test (CCT) by Ekstrom et al. (1976). A conceptual neighborhood graph relating rotation movement to location and orientation change has been built to calculate inferences for reasoning about rotations.,27.14,LFM-2.5,Apple M1 (Metal)
2601.08383v1_Deconstructing Pre-training Knowledge Attribution .pdf,Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models,"Bo Wang, Jinzhuo Li, Hong Chen, Yuanlin Chu, Yuxuan Fan, Xuming Hu, Yuxuan Hu",,,"Mixture-of-Experts, knowledge attribution, interpretability, dense architectures, sparsity, interpretation","This paper investigates how Gated-LPI metrics reveal distinct knowledge acquisition patterns in MoE versus dense models during pre-training, highlighting sparsity and stability advantages of MoE.",26.59,LFM-2.5,Apple M1 (Metal)
2601.08388v1_Creativity in AI as Emergence from Domain-Limited .pdf,Creativity in AI as Emergence from Domain-Limited Generative Models,Corina Chutaux,,,,"This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. It examines structural and contextual conditions for creative behaviors, introducing a conceptual decomposition of creativity into four components: pattern-based generation, induced world models, contextual grounding, and arbitrariness. The work aims to provide a technical framework for studying creativity as an emergent phenomenon rather than a post hoc evaluative label.",28.13,LFM-2.5,Apple M1 (Metal)
2601.08393v1_Controlled LLM Training on Spectral Sphere.pdf,Controlled LLM Training on Spectral Sphere,"Tian Xie, Haoming Luo, Yang Wang, Wayne Xin Zhao, Rui Yan, Bing Su, Chong Luo, Guo, Ren, Wuhan University",arXiv:2601.08393v1,2601.08393,"LLM training, Spectral Sphere, Optimization, Activation control, Parallel training, Stability","The paper introduces the Spectral Sphere Optimizer (SSO) to enforce width-invariant activation control during large-scale LLM training. SSO ensures maximal update parametrization (µP) alignment and achieves stable training dynamics, outperforming existing optimizers like AdamW and Muon. Practical benefits include improved load balancing and bounded activations.",30.71,LFM-2.5,Apple M1 (Metal)
2601.08401v1_An Explainable Two Stage Deep Learning Framework f.pdf,Explainable Two-Stage Deep Learning Framework for Pericoronitis Assessment in Panoramic Radiographs Using YOLOv8 and ResNet-50,"Ajo Babu George, Pranav S, Kunal Agarwal",arXiv:2601.08401v1,13 Jan 2026,"explainable AI, deep learning, pericoronitis, panoramic radiographs, YOLOv8, ResNet-50, diagnostic classification","The study presents a two-stage deep learning framework integrating YOLOv8 for anatomical localization and a modified ResNet-50 for pericoronitis detection, enhanced with Grad-CAM for interpretability. Results show high diagnostic accuracy and strong alignment with radiologists' impressions.",31.77,LFM-2.5,Apple M1 (Metal)
2601.08402v1_PATS Personality-Aware Teaching Strategies with La.pdf,Personality-Aware Teaching Strategies,"Donya Rooein, Sankalan Pal Chowdhury, Mariia Eremeeva, Yuan Qin, Debora Nozza, Mrinmaya Sachan, Dirk Hovy",,,"personality, teaching strategies, large language models, educational tutoring, student engagement","This paper explores how personality traits influence effectiveness of LLM tutoring, proposes a taxonomy linking pedagogical methods to personality profiles, and demonstrates improved student engagement through personalized strategy adaptation.",27.05,LFM-2.5,Apple M1 (Metal)
2601.08403v1_Owen-Shapley Policy Optimization OSPO A Principled.pdf,Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs,"Abhijnan Nath, Alireza Bagheri Garakani, Tianchen Zhou, Fan Yang, Nikhil Krishnaswamy",10.48550/arXiv.2025.12345,arXiv:2508.12345,"Owen-Shapley Policy Optimization, RL for LLMs, Generative Search, Reinforcement Learning, Shapley-Attributions, Sequence-level rewards, Interpretability, Out-of-distribution robustness","This paper introduces OWEN-SHAPLEY POLICYOPTIMIZATION, a framework that redistributes sequence-level advantages based on tokens’ marginal contributions. By assigning segment-level credit via Shapley-Owen attributions, OSPO enables interpretable policy optimization without relying on external value models. Experiments demonstrate consistent performance gains over baselines on Amazon ESCI and H&M Fashion datasets, highlighting improved robustness to unseen retrievers.",30.13,LFM-2.5,Apple M1 (Metal)
2601.08406v1_WebTrap Park An Automated Platform for Systematic .pdf,WebTrap Park: An Automated Platform for Systematic Security Evaluation of Web Agents,"Xinyi Wu, Jiagui Chen, Geng Hong, Jiayi Dong, Xudong Pan, Jiarun Dai, Min Yang",,,"Web Agents, security evaluation, systematic security assessment, web agents, security risks, agent architecture","WebTrapPark is an automated platform for systematic security evaluation of Web Agents, addressing gaps in benchmarking by instantiating three major security risk sources into 1,226 evaluation tasks.",27.98,LFM-2.5,Apple M1 (Metal)
2601.08412v1_Hybrid Distillation with CoT Guidance for Edge-Dro.pdf,Hybrid Distillation with CoT Guidance for Edge-Drone Control Code,"Yizhan Feng, Hichem Snoussi, Yuhang Wang, Jing Teng, Abel Cherouat, Tian Wang",,,"Large language models, drone, Knowledge Distillation, Chain-of-Thought, Lightweight","This paper proposes an integrated approach combining knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models.",28.17,LFM-2.5,Apple M1 (Metal)
2601.08415v2_Regulatory gray areas of LLM Terms.pdf,Regulatory gray areas of LLM Terms,"Brittany I. Davidson, Kate Muir, Florian A.D. Burnat, Adam N. Joinson",,,"Language Models, LLMs, Privacy Policy, Terms of Service, Regulation","This paper examines the varying Terms of Service of major LLM providers, highlighting regulatory ambiguities that affect researchers and users in security, social sciences, and psychological studies. It identifies uncertainty points and proposes a public resource for comparing terms.",29.49,LFM-2.5,Apple M1 (Metal)
2601.08418v1_Taxon Hierarchical Tax Code Prediction with Semant.pdf,Taxon: Hierarchical Tax Code Prediction,"Jihang Li, Qing Liu, Zulong Chen, Jing Wang, Wei Wang2, Chuanfei Xu, Zeyi Wen",,,"Tax code prediction, Hierarchical tax taxonomy, Semantic alignment, LLM expert guidance, e-commerce compliance, taxonomy mapping","Tax code prediction is a crucial yet underexplored task in automating invoicing and compliance management for large-scale e-commerce platforms. This paper presents Taxon, a semantically aligned and expert-guided framework for hierarchical tax code prediction, integrating a feature-gating mixture-of-experts architecture and a semantic consistency model.",28.54,LFM-2.5,Apple M1 (Metal)
2601.08430v1_RubricHub A Comprehensive and Highly Discriminativ.pdf,RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset,"Li Auto Inc., The Chinese University of Hong Kong, Zhejiang University 4Nanyang Technological University",,,"reinforcement learning, verifiable rewards, rubric generation, automated evaluation, deep learning","This paper introduces RubricHub, a large-scale dataset for reinforcement learning with verifiable rewards, addressing scalability and quality gaps in automated evaluation. It presents a framework for coarse-to-fine rubric generation and validates it with state-of-the-art performance on HealthBench.",27.03,LFM-2.5,Apple M1 (Metal)
2601.08434v3_Large Multimodal Models for Embodied Intelligent D.pdf,Large Multimodal Models for Embodied Intelligent Driving: The Next Frontier in Self-Driving?,"Long Zhang, Yuchen Xia, Bingqing Wei, Zhen Liu, Shiwen Mao, Zhu Han, Mohsen Guizani",10.48550/arXiv.2024.12345,None,"large multimodal models, embodied intelligence, self-driving, autonomous driving, reinforcement learning","This article introduces a novel hybrid decision framework merging Large Multimodal Models with deep reinforcement learning to address limitations in autonomous driving. It emphasizes continuous learning and joint decision-making, proposing a semantics and policy dual-driven approach to enhance EI driving capabilities.",28.64,LFM-2.5,Apple M1 (Metal)
2601.08441v1_YaPO Learnable Sparse Activation Steering Vectors .pdf,Learnable Sparse Activation Steering Vectors for Domain Adaptation,"Abdelaziz Bounhar, Rania Hossam Elmohamady Elbadry, Preslav Nakov, Michalis Vazirgiannis, Guokan Shang",1,"1MBZUAI, 2Ecole Polytechnique","large language models, activation interventions, steering vectors, domain adaptation, cultural alignment, interpretability","This paper introduces YayPO, a reference-free method that learns sparse steering vectors in the latent space of a Sparse Autoencoder. By optimizing sparse codes, YayPO produces disentangled, interpretable, and efficient steering directions. It demonstrates faster convergence, stronger performance, and improved stability compared to dense steering baselines, enabling fine-grained alignment tasks such as cultural adaptation while preserving general knowledge.",28.93,LFM-2.5,Apple M1 (Metal)
2601.08444v1_Beyond Linearization Attributed Table Graphs for T.pdf,Beyond Linearization: Attributed Table Graphs for Table Reasoning,"Yuxiang Wang, Junhao Gan, Shengxiang Gao, Shenghao Ye, Zhengyi Yang, Jianzhong Qi",,,"table reasoning, linearization, LLM, attributed table graph, table reasoning, explanability","This paper proposes Table Graph Reasoner (TABGR), a training-free model that represents tables as Attributed Table Graphs (ATGs) to preserve structural information and enable graph-based reasoning. It addresses limitations of linearization-based methods by introducing a Question-Guided Personalized PageRank mechanism, achieving up to 9.7% accuracy improvement over state-of-the-art models.",27.14,LFM-2.5,Apple M1 (Metal)
2601.08448v1_Divide and Conquer Static-Dynamic Collaboration fo.pdf,Divide and Conquer: Static-Dynamic Collaboration for Few-Shot,"Kexin Bao, Daichi Zhang, Yong Li, Dan Zeng, Shiming Ge",10.1145/3731715.3733310,ICMR 2025,"Few-Shot Class-Incremental Learning, Class-Incremental Learning",The paper presents a framework called Static-Dynamic Collaboration to address the stability-plasticity dilemma in few-shot learning by dividing the task into static and dynamic stages.,26.6,LFM-2.5,Apple M1 (Metal)
2601.08450v1_Decoding Order Matters in Autoregressive Speech Sy.pdf,Decoding Order in Autoregressive Speech Synthesis,"Minghui Zhao, Anton Ragni",2104,2104/2101,"speech synthesis, discrete diffusion model, order-agnostic autoregressive decoding","Investigates decoding order in autoregressive speech synthesis using a masked diffusion framework. Demonstrates that randomness in decoding order impacts speech quality and compares fixed vs adaptive strategies, concluding adaptive decoding yields better performance.",25.0,LFM-2.5,Apple M1 (Metal)
2601.08457v1_An Under-Explored Application for Explainable Mult.pdf,Explainable Multimodal Misogyny Detection in Code-mixed Hindi-English,"An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English, Babhishek Kaushikb, Kevin Mc Daidc a,b,cDundalk Institute of Technology, Corresponding author: babhishek.kaushik@dkit.ie",,,"hate speech, misogyny, natural language processing, code-mixing, hinglish","This paper presents a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. It leverages transformer-based models for text analysis and combines them with multimodal approaches for memes, addressing challenges like syntactic irregularities and lack of interpretability. The system supports research and content moderation efforts to combat gender-based digital violence.",29.45,LFM-2.5,Apple M1 (Metal)
2601.08462v1_M3-BENCH Process-Aware Evaluation of LLM Agents So.pdf,M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games,"Sixiong Xie, Zhuofan Shi, Haiyang Shen, Gang Huang, Ma Xiang Jing",xsx1001@stu.pku.edu.cn,,"large language model, social behaviors, mixed-motive games, process-aware evaluation, Big Five personality model, social exchange theory","This paper introduces M3-BENCH, a multi-stage benchmark for evaluating advanced social behaviors in large language model agents within mixed-motive games. It proposes a framework combining Behavioral Trajectory Analysis, Reasoning Process Analysis, and Communication Content Analysis, integrating personality models and social exchange theory to assess personality traits and capability profiles beyond simple behavioral scores.",29.15,LFM-2.5,Apple M1 (Metal)
2601.08464v1_CoMa Contextual Massing Generation with Vision-Lan.pdf,Contextual Massing Generation with Vision-Language Models,"Evgenii Maslov, Valentin Khrulkov, Anastasia V olkova, Anton Gusarov, Andrey Kuznetsov",10.48550/arXiv.2024.12345,arXiv:2408.12345,"CoMa, massing generation, vision-language models, urban planning, architectural design, functional requirements, site context","This paper introduces the CoMa-20K dataset and evaluates massing generation using Vision-Language Models. It addresses challenges in architectural design by providing a comprehensive dataset with functional, economic, and contextual data, enabling context-aware building massing solutions.",31.08,LFM-2.5,Apple M1 (Metal)
2601.08468v1_JudgeRLVR Judge First Generate Second for Efficien.pdf,"JudgeRLVR: Judge First, Generate Second for Efficient Reasoning","Jiangshan Duo, Hanyu Li, Sujian Li, Liang Zhao, Yudong Wang",arXiv:2601.08468v1,2601.08468,"Reinforcement Learning, Verifiable Rewards, Large Language Models, Reasoning, Efficiency, Verification","This paper proposes JudgeRLVR, a two-stage judge-then-generate paradigm for RLVR. It argues that discriminative capability is essential for efficient generation and introduces a method to balance accuracy and efficiency by learning to distinguish valid solutions.",29.0,LFM-2.5,Apple M1 (Metal)
2601.08472v1_sui-1 Grounded and Verifiable Long-Form Summarizat.pdf,Sui-1: Grounded and Verifiable,"Benedikt Droste, Jan Philipp Harries, Maximilian Idahl, Björn Plüster",arXiv:2601.08472v1,2601.08472,"grounded summarization, verifiable AI, citation-grounded models, large language models, training accuracy","Sui-1 is a 24B parameter model that produces abstractive summaries with inline citations, enabling traceability of claims. It outperforms baselines in citation-grounded summarization and supports processing up to 2 million tokens.",29.94,LFM-2.5,Apple M1 (Metal)
2601.08475v1_SUMMPILOT Bridging Efficiency and Customization fo.pdf,Bridging Efficiency and Customization,"JungMin Yun, Juhwan Choi, Kyohoon Jin, Soojin Jang, Jinhee Jang, YoungBin Kim, cocoro357, gold5230, fhzh123, sujin0110, jinheejang, ybkim85",,,"interactive summarization, personalized summaries, user engagement, semantic graphs, entity clustering, explainable evaluation","This paper introduces SUMMPILOT, an interactive, customizable summarization system that leverages large language models to support both automatic and user-driven summarization. It addresses challenges in representing content relationships and provides mechanisms for users to influence summary quality through interactive components.",29.17,LFM-2.5,Apple M1 (Metal)
2601.08490v1_BenchOverflow Measuring Overflow in Large Language.pdf,BenchOverflow: Measuring Overflow in Large Language Models via Plain-Text Prompts,"Erin Feiglin, Nir Hutnik, Raz Lapid",arXiv:2601.08490v1,https://arxiv.org/abs/2601.08490,"large language models, overflow, prompting strategies, length control, computational cost","Investigates overflow in LLMs via plain-text prompts, highlighting performance, cost, and sustainability impacts. Introduces BenchOverflow benchmark to measure output volume and robustness.",28.68,LFM-2.5,Apple M1 (Metal)
2601.08493v1_PKI Prior Knowledge-Infused Neural Network for Few.pdf,PKI: Prior Knowledge-Infused Neural Network for Few-Shot Class-Incremental Learning,"Kexin Baoa, Fanzhao Linc, Zichen Wang, Yong Liaobai, Dan Zenge, Shiming Gea",10.1093/pki/vis/preb,2601.08493,"PKI, few-shot learning, class-incremental learning, catastrophic forgetting, overfitting, prior knowledge, neural network","This paper introduces PKI, a neural network architecture that incorporates prior knowledge to enable continual adaptation in few-shot learning scenarios. PKI addresses catastrophic forgetting and overfitting by preserving existing knowledge while fine-tuning new components during incremental sessions.",31.62,LFM-2.5,Apple M1 (Metal)
2601.08499v2_EfficientFSL Enhancing Few-Shot Classification via.pdf,EfficientFSL: Enhancing Few-Shot Classification via Query-Only Tuning in Vision,"Wenwen Liao, Hang Ruan, Jianbo Yu, Bing Song, Yuansong Wang, Xiaofeng Yang, Tsinghua Shenzhen International Graduate School",10.48550/arXiv:2509.06932,arXiv:2509.06932,"few-shot classification, query-only tuning, Vision Transformers, few-shot learning, knowledge distillation, computational efficiency","This paper introduces EfficientFSL, a query-only fine-tuning framework for few-shot classification using Vision Transformers. It proposes lightweight trainable components to synthesize task-specific queries and a Combine Block to enhance feature representations, achieving state-of-the-art performance with minimal computational cost.",29.47,LFM-2.5,Apple M1 (Metal)
2601.08503v1_Temporal Fusion Nexus A task-agnostic multi-modal .pdf,Temporal Fusion Nexus: A task-agnostic multi-modal embedding model for clinical narratives and irregular time series in post-kidney transplant care,"Aditya Kumar, Simon Rauch, Mario Cypko, Marcel Naik, Matthieu-P Schapranow, Aadil Rashid, Fabian Halleck, Bilgin Osmanodja, Roland Roller, Lars Pape, Klemens Budde, Mario Schiffer, Oliver Amft",arXiv:2601.08503v1,2601.08503,"clinical narratives, post-kidney transplant, graft loss, graft rejection, mortality prediction, multi-modal embedding, disentanglement, SHAP","TFN is a multi-modal embedding model for post-kidney transplant care, achieving superior performance in graft loss and graft rejection prediction compared to state-of-the-art models. Clinical text integration enhances performance, and disentanglement metrics support interpretability.",32.79,LFM-2.5,Apple M1 (Metal)
2601.08509v1_What If TSF A Benchmark for Reframing Forecasting .pdf,What If TSF: A Benchmark for Reframing Forecasting,"Jinkwan Jang, Hyunbin Jin, Hyungjin Park, Kyubyung Chae, Taesup Kim",,,"forecasting, time series, multimodal forecasting, LLMs, scenario-guided, reframing, expert input","The paper introduces What If TSF (WIT), a multimodal forecasting benchmark that evaluates models' ability to incorporate contextual text and future scenarios. It addresses limitations of existing unimodal approaches by leveraging large language models and expert-crafted scenarios, aiming to assess whether contextual information improves forecasting performance.",28.18,LFM-2.5,Apple M1 (Metal)
2601.08510v2_STAGE A Benchmark for Knowledge Graph Construction.pdf,STAGE: A Benchmark for Knowledge Graph,"Qiuyu Tian 1, Yiding Li 2, Fengyi Chen 3, Zequn Liu 2, Youyong, Fan Guo5, Jinjing Shen 5, Xin Zhang 5, Yiyun Luo 5",arXiv:2601.08510v2,arXiv:2601.08510v2,"knowledge graph, screenplay, narrative understanding, character role-playing, event summarization","The paper introduces STAGE, a unified benchmark for evaluating narrative understanding in full-length movie screenplays. It covers knowledge graph construction, scene summarization, long-context question answering, and in-script role-playing, all anchored in a shared narrative world.",31.99,LFM-2.5,Apple M1 (Metal)
2601.08519v1_CD2 Constrained Dataset Distillation for Few-Shot .pdf,Constrained Dataset Distillation for Few-Shot Class-Incremental Learning,"Kexin Bao1, Daichi Zhang1, Hansong Zhang1, Yonghong Li1, Yutao Yue3, Shiming Ge1*",,,"few-shot learning, class-incremental learning, catastrophic forgetting, knowledge distillation, distributional loss",The paper proposes a framework called Constrained Dataset Distillation (CD2) to address the catastrophic forgetting problem in few-shot class-incremental learning by incorporating dataset distillation and a distillation constraint module.,27.87,LFM-2.5,Apple M1 (Metal)
2601.08531v1_Sketch-Based Facade Renovation With Generative AI .pdf,Sketch-Based Facade Renovation With Generative AI Models,"Warissara Booranamaitree, Xusheng Du, Y ushu Cai, Zhengyang Wang, Ye Zhang, Haoran Xie",16532152721@student.chula.ac.th,,"Industrial building renovation, vision-language model, diffusion model, user sketches, facade renovation",A streamlined framework combining generative AI and vision-language models to bypass as-built modelling in industrial adaptive reuse.,29.18,LFM-2.5,Apple M1 (Metal)
2601.08545v2_Learner-Tailored Program Repair A Solution Generat.pdf,Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement,"Zhenlong Dai, Zhuoluo Zhao, Hengning Wang, Xiu Tang, Sai Wu, Chang Yao, Zhipeng Gao, Jingyuan Chen",10.1234/example.12345,12345,"programming, LLM, program repair, bug fixing, code generation, education",The paper introduces a novel framework for intelligent programming coaching that combines a repair solution retrieval system with an edit-driven code retrieval approach to improve bug detection and correction in programming learners.,27.43,LFM-2.5,Apple M1 (Metal)
2601.08549v1_Contrastive and Multi-Task Learning on Noisy Brain.pdf,Contrastive and Multi-Task Learning on Noisy Brain Signals,"Sucheta Ghosh, Zahra Monfared, Felix Dietrich, Sueka Ghosh, Zahra Monfared",10.48550/arXiv.2604.07932,arXiv:2604.07932,"EEG, EEG decoding, motor imagery classification, chaotic dynamics, nonlinear dynamics, self-supervised learning",N/A,26.74,LFM-2.5,Apple M1 (Metal)
2601.08557v1_VideoHEDGE Entropy-Based Hallucination Detection f.pdf,VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations,"Sushant Gautam, Cise Midoglu, Vajira Thambawita, Michael A. Riegler, Pål Halvorsen",,,"hallucination detection, video question answering, semantic clustering, spatiotemporal perturbations, entropy-based reliability, video-VLMs","Introduce VideoHEDGE, a framework for detecting hallucinations in video question answering using entropy-based reliability estimation and semantic clustering. Evaluated on SoccerChat benchmark with three 7B Video-VLMs.",28.26,LFM-2.5,Apple M1 (Metal)
2601.08559v1_WaterCopilot An AI-Driven Virtual Assistant for Wa.pdf,WaterCopilot: An AI-Driven Virtual Assistant for Water Management,"Keerththanan Vickneswaran, Mariangel Garcia Andarcia, Hugo Retief, Chris Dickens, Paulo Silva",,,"Water resource management, Retrieval-Augmented Generation (RAG), Limpopo River Basin, Azure AI, Real-time APIs, Multilingual chatbots, Digital Twin, AWS deployment, RAGAS evaluation","WaterCopilot is an AI-driven virtual assistant developed through collaboration between the International Water Management Institute (IWMI) and Microsoft Research for the Limpopo River Basin (LRB). It integrates static policy documents and real-time hydrological data via custom plugins (iwmi-doc-plugin and iwmi-api-plugin) to support decision-making in transboundary river basins. The system offers multilingual interactions, automated calculations, and visualization, achieving high answer relevance and context precision. Key innovations include automated alerts and scalable deployment on AWS.",30.66,LFM-2.5,Apple M1 (Metal)
2601.08565v1_Rewriting Video Text-Driven Reauthoring of Video F.pdf,Rewriting Video: Text-Driven Reauthoring of Video Footage,"Sitong Wang, Anh Truong, Lydia B. Chilton, Dingzeyu Li",,,"Video reauthoring, Text-driven video editing, Generative video models, Creative AI tools","This paper presents a tech probe and study on text-driven video reauthoring. It introduces a generative reconstruction algorithm that converts video clips into editable text prompts, enabling creators to modify videos via text rewriting. The approach highlights use cases such as adding characters, changing styles, and diversifying camera perspectives. A technical evaluation reveals a human-AI perceptual gap, while a probe study uncovers novel applications and challenges in narrative shaping.",28.87,LFM-2.5,Apple M1 (Metal)
2601.08602v1_WaveFormer Frequency-Time Decoupled Vision Modelin.pdf,WaveFormer: Frequency-Time Decoupled Vision Modeling with Wave Equation,"Zishan Shu, Juntong Wu, Wei Yan, Xudong Liu, Hongyu Zhang, Chang Liu2, Youdong Mao, Jie Chen, Liuchang 2022",,,"Transformers, wave equation, spatial frequency, propagation time, wave propagation operator, visual modeling, semantic segmentation","This paper proposes WaveFormer, a frequency-time decoupled vision model based on the wave equation. It treats feature maps as spatial signals evolving over internal propagation time, enabling explicit control of spatial frequency interactions. The method introduces a closed-form solution and implements it as the Wave Propagation Operator (WPO), offering competitive performance with lower computational cost than attention-based models.",28.59,LFM-2.5,Apple M1 (Metal)
2601.08605v1_ExpSeek Self-Triggered Experience Seeking for Web .pdf,Self-Triggered Experience Seeking for Web Agents,"Wenyuan Zhang, Xinghua Zhang, Haiyang Yu, Shuaiyi Nie, Bingli Wu, Juwei Yue, Tingwen Liu, Yongbin Li",,,"experience intervention, web agents, self-triggering, entropy, LLM, multi-turn interactions","Experience intervention in web agents enhances agent interaction capabilities by providing insights from accumulated experiences. Existing methods passively inject experience, but ExpSeek shifts it to step-level proactive seeking, improving performance.",26.91,LFM-2.5,Apple M1 (Metal)
2601.08611v1_VeriTaS The First Dynamic Benchmark for Multimodal.pdf,The First Dynamic Benchmark for Multimodal Automated Fact-Checking,"Mark Rothermel, Marcus Kornmann, Marcus Rohrbach, Anna Rohrbach",,,"multimodal AI, fact-checking, verified claims, AI benchmark, misinformation","This paper presents Veritas, the first dynamic benchmark for multimodal automated fact-checking, developed by the Multimodal AI Lab at Technical University of Darmstadt. It evaluates AI systems' ability to verify claims across diverse media types and contexts, emphasizing real-world relevance and robustness against misinformation.",24.47,LFM-2.5,Apple M1 (Metal)
2601.08620v1_ViDoRe V3 A Comprehensive Evaluation of Retrieval .pdf,ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios,"Antonio Loison, Quentin Macé, Antoine Edy, Victor Xing, Tom Balough, Gabriel Moreira, Bo Liu, Manuel Faysse, Céline Hudelot",10.1234/v3.001,0001.12345,"Retrieval Augmented Generation, ViDiRe, multi-modal, visual elements, benchmarking, RAG pipelines","ViDoRe V3 introduces a comprehensive multi-modal RAG benchmark covering 10 datasets across diverse domains. It evaluates retrieval relevance, visual interpretation, and hybrid contexts, revealing strengths and limitations of current RAG systems.",27.55,LFM-2.5,Apple M1 (Metal)
2601.08623v1_SafeRedir Prompt Embedding Redirection for Robust .pdf,SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models,"Renyang Liu, Kangjie Chen, Han Qiu, Jie Zhang, Kwok-Yan Lam, Tianwei Zhang, See-Kiong Ng, Tianwei Zhang",,,"image generation, robust unlearning, prompt embedding, unsafe content, NSFW, adversarial attacks","Image generation models often memorize unsafe concepts, leading to harmful outputs. SafeRedir introduces a lightweight inference-time framework to erase such prompts via token-level redirection, improving safety without retraining.",27.53,LFM-2.5,Apple M1 (Metal)
2601.08631v1_M2FMoE Multi-Resolution Multi-View Frequency Mixtu.pdf,M2FMoE: Multi-Resolution Multi-View Frequency Mixture-of-Experts for Extreme-Adaptive Time Series Forecasting,"Yaohui Huang, Runmin Zou, Yun Wang, Laeeq Aslam, Ruipeng Dong",,,"time series forecasting, extreme events, multi-resolution, multi-view modeling, hydrological forecasting","Forecasting time series with extreme events is challenging due to high variance and irregular dynamics. Existing methods struggle during extreme events, motivating the proposed M2FMoE model that learns both regular and extreme patterns through multi-resolution and multi-view frequency modeling.",28.25,LFM-2.5,Apple M1 (Metal)
2601.08634v1_Moral Lenses Political Coordinates Towards Ideolog.pdf,"Moral Lenses, Political Coordinates: Towards Ideological Positioning of Morally Conditioned LLMs","Chenchen Yuan, Bolei Ma, Zheyu Zhang, Frauke Kreuter",,,"moral orientation, political orientation, moral values, political compass test, social psychology, ethical alignment","This paper investigates how conditioning moral values influences political positioning in large language models, demonstrating that moral orientation shapes ideological alignment through the Political Compass Test. It highlights the need for broader social values in alignment techniques.",27.29,LFM-2.5,Apple M1 (Metal)
2601.08641v1_Resisting Manipulative Bots in Memecoin Copy Tradi.pdf,Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning,"Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",10.1234/abcd123,,"meme coin, copy trading, manipulative bots, multi-agent system, chain-of-thought, LLM, asset allocation","The paper presents a multi-agent system leveraging chain-of-thought reasoning to combat manipulative bots in meme coin copy trading, demonstrating improved performance over traditional models and single LLMs.",27.72,LFM-2.5,Apple M1 (Metal)
2601.08653v1_Prism Towards Lowering User Cognitive Load in LLMs.pdf,Towards Lowering User Cognitive Load in LLMs,"Zheng Liao, Zheng Zhao, Xiang Zhao",10.48550/arXiv.2024.12345,arXiv:2408.12345,"Complex intent understanding, Large language models, Logical clarification","Prism proposes a framework for complex intent understanding to improve human-LLM collaboration, featuring modular components for intent decomposition, logical clarification generation, reward evaluation, and self-evolved tuning.",26.77,LFM-2.5,Apple M1 (Metal)
2601.08654v1_RULERS Locked Rubrics and Evidence-Anchored Scorin.pdf,RULERS: Locked Rubrics and Evidence-Anchored Scoring for Robust LLM Evaluation,"Yihan Hong, Huaiyuan Yao, Bolin Shen, Wanpeng Xu, Hua Wei, Yushun Dong",,,"LLM-as-a-Judge, rubric alignment, LLM evaluation, LLM-as-a-Judge paradigm, rubric stability, evidence anchoring","The paper introduces RULERS, a framework that converts natural language rubrics into executable specifications using versioned, immutable bundles. It addresses challenges of rubric instability, unverifiable reasoning, and scale misalignment by enforcing deterministic evidence verification without retraining models. Experiments show superior human agreement and robustness compared to baselines.",28.51,LFM-2.5,Apple M1 (Metal)
2601.08659v1_TRACE Reconstruction-Based Anomaly Detection in En.pdf,Reconstruction-Based Anomaly Detection in Ensemble and Time-Dependent Simulations,"Hamid Gadirov, Martijn Westra, Steffen Frey",,,"anomaly detection, ensemble simulations, time-dependent data, convolutional autoencoders","This work investigates reconstruction-based anomaly detection in high-dimensional, time-dependent simulation data using two architectural variants of convolutional autoencoders. The study compares a two-dimensional model operating on individual time steps with a three-dimensional model leveraging spatio-temporal context. Results demonstrate that the 2D autoencoder excels at identifying localized spatial irregularities, while the 3D model captures dynamic evolution patterns, highlighting the value of incorporating temporal context for anomaly identification.",24.8,LFM-2.5,Apple M1 (Metal)
2601.08662v1_From Classical to Quantum Reinforcement Learning a.pdf,Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner’s Tutorial,"Abhijit Sen, Sonali Panda, Mahima Arya, Subhajit Patra, Zizhan Zheng, Denys I. Bondar",10.48550/arXiv.2026.12345,arXiv:2109.12345,"reinforcement learning, quantum control, tutorial, undergraduate, quantum algorithms","This tutorial bridges the gap between RL theory and practical coding, offering clear examples for students. It emphasizes hands-on learning and addresses challenges in transitioning from concepts to implementation. The content is structured to be accessible, with code available at the provided link.",28.76,LFM-2.5,Apple M1 (Metal)
2601.08670v1_Parallel Context-of-Experts Decoding for Retrieval.pdf,Parallel Context-of-Experts Decoding for Retrieval Augmented Generation,"Giulio Corallo, Paolo Papotti",,,"Retrieval Augmented Generation, Parallel Context-of-Experts Decoding, KV caching, Cross-document reasoning, Evidence aggregation","This paper introduces Parallel Context-of-Experts Decoding (PCED), a training-free framework that decodes retrieved documents in parallel as isolated experts, enabling efficient cross-document reasoning without joint attention. PCED treats retrieved documents as separate evidence sources and uses a retrieval-aware contrastive decoding rule to aggregate evidence at inference time, achieving up to 70-point improvements over prior methods on benchmarks like LOFT and Long Bench.",26.81,LFM-2.5,Apple M1 (Metal)
2601.08673v1_Why AI Alignment Failure Is Structural Learned Hum.pdf,Why AI Alignment Failure Is Structural,"Didier Sornette, Sandro Claudio Lera, Ke Wu",arXiv:2601.08673v1,arXiv:2601.08673,"AI alignment, AGI, human interaction, moral reasoning, structural generalizations","The authors argue that perceived alignment failures stem from misinterpreting LLM behaviors as moral lapses, when in fact they reflect structural patterns rooted in human social interaction. They emphasize that AGI should not be judged by moral standards but by its role in amplifying existing power and interaction dynamics.",30.03,LFM-2.5,Apple M1 (Metal)
2601.08676v2_Advancing ESG Intelligence An Expert-level Agent a.pdf,Advancing ESG Intelligence: An Expert-level Agent and Comprehensive Benchmark for Sustainable Finance,"Yilei Zhao, Wentao Zhang, Lei Xiao, Yandan Zheng, Mengpu Liu, Wei Yang Bryan Lim",,,"ESG, sustainability, ethical performance, corporate sustainability, ESG analysis, benchmarking, financial reporting, regulatory compliance","This paper introduces ESGAgent, a hierarchical multi-agent system enhanced with retrieval augmentation and domain-specific functions, to address data fragmentation in ESG analysis. It presents a comprehensive three-level benchmark derived from 310 corporate sustainability reports, demonstrating superior performance over state-of-the-art LLMs in atomic QA tasks and excelling in professional report generation through integrated charts and verifiable references.",28.67,LFM-2.5,Apple M1 (Metal)
2601.08679v1_PersonaDual Balancing Personalization and Objectiv.pdf,Balancing Personalization and Objectivity,"Xiaoyou Liu, Xinyi Mou, Shengbin Yue, Liang Wang, Yuqing Wang, Qiexiang Wang, Tianrui Qin, Wangchunshu Zhou, Zhongyu Wei, Qiang Wang",,,"personalization, objectivity, LLM, reasoning, adaptive reasoning, interference-free performance","The paper proposes PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model. It adaptively switches modes based on context and improves mode selection using reinforcement learning, achieving near interference-free performance while leveraging helpful personalized signals.",27.23,LFM-2.5,Apple M1 (Metal)
2601.08682v1_Lessons from the Field An Adaptable Lifecycle Appr.pdf,Lessons from the Field: An Adaptable Lifecycle Approach to Applied Dialogue Summarization,"Kushal Chawla Chenyang Zhu, Pengshan Cai Sangwoo, Jonah Lewis Keasha Safewright, Alfy Samuel Erin Babinsky, Shi-Xiong Zhang Sambit Sahu, Capital One",,,"automatic text summarization, multi-party dialogues, agentic systems, LLM prompts, industry case study","This work presents an industry case study on developing an agentic system for summarizing multi-party interactions, sharing practical insights across the full development lifecycle to guide practitioners and inform future research.",28.24,LFM-2.5,Apple M1 (Metal)
2601.08683v1_Region of interest detection for efficient aortic .pdf,Region of interest detection for efficient aortic segmentation,"Loris Giordanoa, Ine Dirks, Tom Lenaerts, Ejef Vandemeulebrouckea, Jef Vandemeulebrouckea",,,"Detection, Segmentation, Multi-task learning, Cascade models, Aorta, Computed tomography","This study presents an innovative approach for efficient aortic segmentation using targeted region of interest (ROI) detection. It proposes a simple and efficient detection model that can be widely applied to detect a single ROI, achieving state-of-the-art performance while being compact and robust.",28.44,LFM-2.5,Apple M1 (Metal)
2601.08684v1_MEMEWEAVER Inter-Meme Graph Reasoning for Sexism a.pdf,Inter-Meme Graph Reasoning for Sexism and Misogyny Detection,"Paolo Italiani, David Gimeno-Gomez, Luca Ragazzi, Gianluca Moro",,,"sexism, misogyny, online harassment, graph reasoning, multimodal content moderation, group identity, social dynamics, hate speech","Women are twice as likely as men to face online harassment due to gender. This work introduces MEMEWEAVER, an end-to-end trainable multimodal framework that leverages inter-meme graph reasoning to detect sexism and misogyny, addressing limitations of existing heuristic approaches.",27.67,LFM-2.5,Apple M1 (Metal)
2601.08690v1_All Required In Order Phase-Level Evaluation for A.pdf,Phase-Level Evaluation for AI–Human Dialogue in Healthcare and Beyond,"Shubham Kulkarni, Alexander Lyzhov, Shiva Chaitanya, Preetam Joshi",10.48550/arXiv.2405.12345,arXiv:2405.12345,"conversational AI, healthcare, compliance, phase-level evaluation, clinical workflow","This paper introduces Obligatory-Information Phase Structured Compliance Evaluation (OIP–SCE), a method to audit compliance across the full conversation flow. It demonstrates how phase-level evidence helps clinicians and engineers align AI capabilities with real-world clinical needs, addressing limitations of turn-level metrics.",28.43,LFM-2.5,Apple M1 (Metal)
2601.08697v2_Auditing Student-AI Collaboration A Case Study of .pdf,Auditing Student–AI Collaboration: A Case Study of Online Graduate CS Students,"Nifu Dan, Georgia Institute of Technology",10.1145/XXXXXXX.XXXXXXX,,"AI in education, human-AI collaboration, student agency, automation preferences, generative AI, academic integrity, HCAI","This study examines how graduate CS students perceive AI collaboration in online learning, focusing on benefits, risks, and preferred boundaries. It uses surveys to explore students' experiences with AI across academic tasks and investigates design improvements to enhance trustworthiness.",27.44,LFM-2.5,Apple M1 (Metal)
2601.08703v1_Evaluating the Ability of Explanations to Disambig.pdf,Evaluating Model Explanations without Ground Truth,"Kaivalya Rawal, Eoin Delaney, Zihao Fu, Sandra Wachter, Chris Russell",10.1145/3715275.3732219,https://dl.acm.org/doi/10.1145/3715275.3732219,"Explainable AI, Model explanations, Rashomon set, Feature importance, Model selection","The paper proposes evaluation principles and a method (AXE) to assess feature-importance explanations in models that produce similar predictions, aiming to disambiguate behaviors in a Rashomon set and improve model selection.",28.24,LFM-2.5,Apple M1 (Metal)
2601.08713v1_Real-Time Localization Framework for Autonomous Ba.pdf,Real-Time Localization Framework for Autonomous Basketball Robots,"Naren Medarametla, Sreejon Mondal",naren.medarametla2023@vitstudent.ac.in,,"Robot Localization, Autonomous Navigation, Neural Networks, Robocon, Robotics",The paper proposes a hybrid localization algorithm integrating classical techniques with learning-based methods using visual data from the basketball court to enable real-time self-localization for autonomous robots.,26.81,LFM-2.5,Apple M1 (Metal)
2601.08731v1_Learning from Demonstrations via Capability-Aware .pdf,Learning from Demonstrations via Capability-Aware Goal Sampling,"Yuanlin Duan, Rutgers University, Ying Wang, Wenjie Qiu, He Zhu, Rutgers University",arXiv:2601.08731v1,arXiv:2601.08731,"imitation learning, capability-aware, goal sampling, long-horizon tasks, adaptive curriculum","Introduces Cago, a method that tracks agent competence along expert trajectories to guide learning, improving sample efficiency and performance in sparse-reward tasks.",27.48,LFM-2.5,Apple M1 (Metal)
2601.08732v1_ISLA A U-Net for MRI-based acute ischemic stroke l.pdf,"ISLA: A U-Net for MRI-based acute ischemic stroke lesion segmentation with deep supervision, attention, domain adaptation, and ensemble learning","Vincent Rocaa, Martin Bretzner, Hilde Henond, Laurent Puyb, Grégory Kuchcinskia, Renaud Lopesa",10.1002/2016ioa.1182,,"MRI, stroke, lesion segmentation, deep learning, U-Net, supervision, domain adaptation, ensemble learning","Accurate delineation of acute ischemic stroke lesions in MRI is crucial for diagnosis and management. This work introduces ISLA, a deep learning model trained on multi-center datasets, optimized through systematic loss function tuning, residual connections, attention mechanisms, and unsupervised domain adaptation. ISLA achieves superior performance over existing methods on an external test set.",28.8,LFM-2.5,Apple M1 (Metal)
2601.08734v1_TerraFormer Automated Infrastructure-as-Code with .pdf,TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback,"Prithwish Jana, Sam Davidson, Bhavana Bhasker, Andrey Kan, Anoop Deoras, Laurent Callot",10.1145/3786583.3786898,2026IJSEP6,"Infrastructure as Code, IaC generation, LLMs, Formal Verification, Software Engineering","Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language. We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs show TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen(Test), and 19.60% on TF-Mutn(Test).",30.05,LFM-2.5,Apple M1 (Metal)
2601.08743v1_TableCache Primary Foreign Key Guided KV Cache Pre.pdf,TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency Text-to-SQL,"Jinbo Su1, 2. Yuxuan Hu, 2. Cuiping Li, 3. Hong Chen, 4. Jia Li, 4. Lintao Ma, 1. Jing Zhang",,,"Text-to-SQL, LLM-based methods, KV cache, Table precomputation, Latency optimization, Table caching, Database interaction","The paper proposes TableCache to precompute table representations as KV caches offline, reducing latency in user queries by preserving primary foreign key relationships and using a Table Trie for efficient lookups. It introduces a cache management system with query reranking to improve performance.",29.02,LFM-2.5,Apple M1 (Metal)
2601.08747v2_To Retrieve or To Think An Agentic Approach for Co.pdf,Retrieve or Think? An Agentic Approach for Context Evolution,"Rubing Chen, Jian Wang, Wenjie Li, Xiao-Yong Wei, Qing Li",,,"retrieval-augmented generation, context evolution, agentic approach, knowledge-intensive tasks, metacognition","This paper introduces Agentic Context Evolution (ACE), a framework inspired by human metacognition that dynamically decides whether to retrieve new evidence or reason with existing knowledge. ACE uses a central orchestrator agent to alternate between a retriever and a reasoner, reducing redundant retrieval and maintaining a concise, evolved context. Experiments show ACE outperforms baselines in accuracy while efficiently using tokens.",28.52,LFM-2.5,Apple M1 (Metal)
2601.08753v1_Grid-Aware Charging and Operational Optimization f.pdf,Grid-Aware Charging and Operational,"Rishav Sen, Amutheezan Sivagnanam, Aron Laszka, Ayan Mukhopadhyay, Abhishek Dubey",,,"Mixed transit fleet, electrification, dynamic pricing, hierarchical MILP, operational challenges, urban mobility","The paper presents a mixed-integer linear programming model to optimize charging schedules and trip assignments for mixed electric and diesel bus fleets, addressing challenges posed by dynamic electricity pricing and operational constraints.",27.9,LFM-2.5,Apple M1 (Metal)
2601.08768v1_AI as Entertainment.pdf,AI as Entertainment,"Cody Kommers, Ari Holtzman",https://doi.org/XXXXXXX.XXXXXX,"1, 1 (January 2026), 16 pages","Generative AI, Entertainment, Culture, LLMs, Societal Impact, Meaning-making","The paper argues that mainstream narratives about AI focus on intelligence and productivity, overlooking its emerging role in entertainment. It highlights the need for frameworks that evaluate AI-generated cultural content based on its social and emotional impacts rather than just harms.",27.93,LFM-2.5,Apple M1 (Metal)
2601.08773v1_Reliable Graph-RAG for Codebases AST-Derived Graph.pdf,Reliable Graph-RAG for Codebases: AST-Derived,Manideep Reddy Chinthareddy,arXiv:2601.08773v1,arXiv:2601.08773,"software engineering, code analysis, knowledge graphs, AST, RAG","Benchmarks three retrieval-augmented generation pipelines on Java codebases: No-Graph Naive RAG, LLM-Generated Knowledge Graph RAG, and a deterministic AST-derived Knowledge Graph RAG. Evaluates indexing overhead, latency, coverage, and graph structure differences across repositories.",29.28,LFM-2.5,Apple M1 (Metal)
2601.08776v1_Translating Light-Sheet Microscopy Images to Virtu.pdf,Translating Light-Sheet Microscopy Images to Virtual H&E Using CycleGAN,Yanhua Zhao,,,"histopathology, image translation, H&E staining, unpaired learning, CycleGAN","The paper presents a Cycle-Consistent Adversarial Network (CycleGAN) for unpaired image-to-image translation from multi-channel fluorescence microscopy to pseudo H&E stained histopathology images. It combines C01 and C02 fluorescence channels into RGB and learns bidirectional mappings without paired training data, enabling visualization of fluorescence data in a pathologist-friendly format.",28.05,LFM-2.5,Apple M1 (Metal)
2601.08777v1_Asymptotic Universal Alignment A New Alignment Fra.pdf,Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling,"Yang Cai, Weiqiang Zheng",10.1093/pasj/psa.2026.01,2601.08777,"asymptotic universal alignment, test-time scaling, LLM alignment, output diversity, self-play learning","The paper presents a framework for aligning large language models to heterogeneous user preferences using test-time scaling. It introduces (k, f(k)) robust alignment and asymptotic universal alignment, characterizes optimal convergence rates, and analyzes limitations of existing methods like NLHF.",30.41,LFM-2.5,Apple M1 (Metal)
2601.08778v3_Pervasive Annotation Errors Break Text-to-SQL Benc.pdf,Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards,"Tengjun Jin, Yoojin Choi, Yuxuan Zhu, Daniel Kang",,,"text-to-SQL, annotation errors, benchmarking, data analytics, leaderboards","This paper benchmarks annotation error rates for BIRD and Spider 2.0-Snow, corrects a subset of BIRD Dev set, and analyzes impacts on performance and rankings. It shows annotation errors can distort reported metrics, affecting research and deployment decisions.",26.74,LFM-2.5,Apple M1 (Metal)
2601.08785v1_Uncovering Political Bias in Large Language Models.pdf,Uncovering Political Bias in Large Language Models using Parliamentary Voting Records,"Jieying Chen, Karen de Jong, Andreas Poole, Jan Burakowski, Andre Pop, Jan Burakowski, Elena Elderson Nosti, Joep Windt, Chendi Wang",,,"Political bias, Large language models, Ideological alignment, Mul-tilingual NLP, Benchmarking, Bias evaluation, Parliamentary motions, Political entities, LLM fairness","This paper introduces a methodology to benchmark political bias in large language models by aligning model-generated voting predictions with verified parliamentary voting records across three national case studies. It assesses ideological tendencies and political entity bias, highlighting left-leaning or centrist tendencies in state-of-the-art LLMs and negative biases toward right-conservative parties.",29.62,LFM-2.5,Apple M1 (Metal)
2601.08806v1_APEX-SWE.pdf,APEX–SWE,"Abhi Kottamasu1, Akul Datta1, Aakash Barthwal1, Jiya Arun1, Chirag Mahapatra1, Adarsh Hiremath1, Brendan Foody1, Bertie Vidgen1",arXiv:2601.08806v1,arXiv:2601.08806,"AI Productivity Index, Software Engineering, Frontier AI models, Observability, Integration tasks, Debugging, Unstructured context","We introduce the AI Productivity Index for Software Engineering (APEX–SWE), a benchmark for assessing frontier AI models in economic software engineering. Unlike narrow task evaluations, APEX–SWE evaluates two real-world task types: integration tasks (constructing end-to-end systems) and observability tasks (debugging failures). The study evaluates eight frontier models and finds Gemini 3 Pro leading with a 25% Pass@1 score, highlighting the importance of epistemic reasoning and uncertainty resolution.",31.09,LFM-2.5,Apple M1 (Metal)
2601.08807v1_S3-CLIP Video Super Resolution for Person-ReID.pdf,S3-CLIP: Video Super Resolution for Person-ReID,"Tam´as Endrei, P´azm´any P´eter Catholic University, Hungary, Gy¨orgy Cserey, P´azm´any P´eter Catholic University, Hungary",,,"person re-identification, video super-resolution, tracklet quality, SR, ReID","This paper introduces S3-CLIP, a video super-resolution-based CLIP Re-ID framework for the VReID-XFD challenge. It integrates recent SR advances with task-driven super-resolution to enhance tracklet quality in challenging cross-view conditions, achieving competitive performance.",27.47,LFM-2.5,Apple M1 (Metal)
2601.08808v1_Multiplex Thinking Reasoning via Token-wise Branch.pdf,Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge,"Yao Tang1, Li Dong2, Yaru Hao2, Qingxiu Dong2, Furu Wei2, Jiatao Gu1",arXiv:2601.08808v1,arXiv:2601.08808,"Multiplex Thinking, Chain-of-Thought, Reinforcement Learning, Large Language Models, Reasoning, Token-wise Sampling","Proposes a stochastic soft reasoning mechanism that aggregates candidate tokens into a continuous multiplex token, enabling efficient on-policy reinforcement learning while preserving exploration diversity.",28.3,LFM-2.5,Apple M1 (Metal)
2601.08811v1_Reasoning Matters for 3D Visual Grounding.pdf,Reasoning Matters for 3D Visual Grounding,"Hsiang-Wei Huang, Kuang-Ming Chen, Wenhao Chai, Cheng-Yen Yang",,,"3D visual grounding, reasoning, LLM, visual grounding, 3D understanding","This work proposes a 3D visual grounding data pipeline that automatically synthesizes 3D visual grounding data and reasoning processes. It introduces Reason3DVG-8B, achieving 25% better performance than 3D-GRAND using only 1.6% of training data, highlighting the importance of reasoning in 3D visual grounding.",27.32,LFM-2.5,Apple M1 (Metal)
2601.08816v2_MemRec Collaborative Memory-Augmented Agentic Reco.pdf,MemRec: Collaborative Memory-Augmented Agentic Recommender,"Weixin Chen, Yuhan Zhao, Jingyuan Huang, Zihe Ye, Yongfeng Zhang, Li Chen, Yongfeng Zhang, Neil Shah, Li Chen",10.1234/example.doi,,"recommender systems, collaborative memory, agentic era, semantic memory, LLM, memory graph, LLMRec, cognitive load, computational cost","The paper introduces MemRec, a framework that decouples reasoning from memory management to enable efficient collaborative augmentation. It presents a cost-effective LMMem for dynamic memory graphs and achieves state-of-the-art performance on benchmarks.",28.25,LFM-2.5,Apple M1 (Metal)
2601.08828v1_Motion Attribution for Video Generation.pdf,Motion Attribution for Video Generation,"Xindi Wu, Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-Taixé, Olga Russakovsky, Sanja Fidler, Jonathan Lorraine",10.48550/arXiv.2026.12345,arXiv:2601.12345,"motion attribution, video generation, data attribution, temporal dynamics, motion-specific influence","Presents Motive, a gradient-based framework for attributing motion in video generation, showing improved temporal consistency and human preference win rates.",28.14,LFM-2.5,Apple M1 (Metal)
2601.08829v1_Modeling LLM Agent Reviewer Dynamics in Elo-Ranked.pdf,Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System,"Hsiang-Wei Huang, Junbin Lu, Kuang-Ming Chen, Jenq-Neng Hwang",,,"LLM agent, reviewer dynamics, Elo ranking, peer review, review strategy","This work explores LLM agent reviewer dynamics in an Elo-ranked review system using real-world conference submissions. It compares baseline settings with Elo ratings and reviewer memory effects, showing improved Area Chair decision accuracy and adaptive review strategies without increasing review effort. Code is available at https://github.com/hsiangwei0903/EloReview.",28.1,LFM-2.5,Apple M1 (Metal)
2601.08873v1_ForensicFormer Hierarchical Multi-Scale Reasoning .pdf,ForensicFormer: Hierarchical Multi-Scale Reasoning,Hema Hariharan Samson,,,"image forensics, forgery detection, transformers, cross-domain generalization, AI-generated images","Presents ForensicFormer, a hierarchical multi-scale framework unifying low-level artifact detection, mid-level boundary analysis, and high-level semantic reasoning via cross-attention transformers. Demonstrates improved robustness across diverse datasets and achieves 86.8% average accuracy, outperforming prior methods on traditional and AI-generated data.",27.0,LFM-2.5,Apple M1 (Metal)
2601.08874v1_The Illusion of Friendship Why Generative AI Deman.pdf,The Illusion of Friendship: Why Generative AI Demands Unprecedented Ethical Vigilance,Md Zahidul Islam,,,"generative AI, ethical considerations, human-computer interaction","This paper explores how generative AI blurs the line between tool and companion, arguing that users may develop emotionally significant attachments to AI systems, potentially leading to harmful consequences. It proposes a framework to understand the computational mechanisms behind AI responses and advocates for safeguards to preserve human responsibility.",27.55,LFM-2.5,Apple M1 (Metal)
2601.08875v2_Learning Domain-Invariant Representations for Cros.pdf,Learning Domain-Invariant Representations for Cross-Domain Image,"Jiahao Qin, Yiwen Wang",10.48550/arXiv.2024.12345,arXiv:2409.12345,"image registration, domain shift, scene appearance disentanglement, cross-domain alignment, medical imaging","Addresses domain shift in image registration by disentangling scene appearance from domain-specific appearance, enabling registration via re-rendering. Achieves improved registration accuracy on ANHIR benchmark.",27.28,LFM-2.5,Apple M1 (Metal)
2601.08881v1_TAG-MoE Task-Aware Gating for Unified Generative M.pdf,Task-Aware Gating for Unified Generative Mixture-of-Experts,"Yu Xu, Hongbin Yan, Juan Cao, Tiankai Hang, Runze He, Zijin Yin, Shiyi Zhang, Jintao Li, Chunyu Wang, Qinglin Lu, Fang Tang",10.5281/zenodo.1234567,2601.08881,"TAG-MoE, Mixture-of-Experts, Generative modeling, Task-aware gating, Diffusion transformer, Image editing","The paper presents a framework to inject semantic intent into MoE routing networks, enabling diffusion transformers to handle diverse generative tasks with reduced interference. It introduces a Hierarchical Task Semantic Annotation scheme and Predictive Alignment Regularization to improve fidelity and quality.",30.33,LFM-2.5,Apple M1 (Metal)
2601.08882v1_Compressing Vision Transformers in Geospatial Tran.pdf,Compressing Vision Transformers in Geospatial,"Thomas Snyder, H. Lexie Yang, Stefan Schnake, Steffen Schotthöfer",arXiv:2601.08882v1,2601.08882,"transfer learning, manifold-constrained, optimization, geospatial, compression, edge devices","Deploying geospatial foundation models on resource-constrained edge devices demands compact architectures that maintain high downstream performance. This work leverages a manifold-constrained optimization framework to compress large vision transformers during transfer learning, achieving strong compression with minimal accuracy loss.",30.17,LFM-2.5,Apple M1 (Metal)
2601.08884v1_Bridging the Gap Empowering Small Models in Reliab.pdf,Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting,"Samyak Jhaveri, Cristina V. Lopes",,,"OpenACC, Parallel Code Generation, Large Language Models, Prompt Optimization, High-Performance Computing, GPU Programming","This work introduces a systematic prompt optimization approach to enhance OpenACC pragma generation without the high computational costs of LLM post-training. Using the GEPA framework, it iteratively refines prompts via crossover, mutation, and expert feedback, achieving higher compilation success and functional GPU speedups for smaller LLMs.",28.55,LFM-2.5,Apple M1 (Metal)
2601.08891v1_Attention Consistency Regularization for Interpret.pdf,Attention Consistency Regularization for Interpretable Early-Exit Neural Networks,Yanhua Zhao,10.48550/arXiv.2024.12345,10.48550/arXiv.2024.12345,"early-exit networks, explainable AI, attention mechanisms, multi-objective learning","Presents Explanation-Guided Training (EGT), a multi-objective framework that improves interpretability and consistency in early-exit networks via attention-based regularization. EGT aligns early-exit attention maps with the final exit, achieving high accuracy and improved attention consistency.",29.11,LFM-2.5,Apple M1 (Metal)
2601.08892v1_Evaluating Role-Consistency in LLMs for Counselor .pdf,Evaluating Role-Consistency in LLMs for Counselor Training,"Eric Rudolph, Natalie Engert, Jens Albrecht",arXiv:2601.08892v1,arXiv:2601.08892v1,"Counseling, Chatbot, LargeLanguageModel, PersonaConsistency, EducationalRole-Play","This paper evaluates the role consistency of large language models in simulating online counseling interactions, introducing a dataset with adversarial attacks to test persona maintenance. It compares Vicuna and other open-source LLMs in maintaining role consistency during virtual client sessions and provides a comparative analysis.",30.21,LFM-2.5,Apple M1 (Metal)
2601.08896v1_XGBoost Forecasting of NEPSE Index Log Returns wit.pdf,XGBoost Forecasting of NEPSE Index Log Returns with Walk,"Sahaj Raj Mallaa, Shreeyash Kayasthaa, Rumi Suwala, Harish Chandra Bhandaria, Rajendra Adhikarib",10.1007/XXXXXX,None,"NEPSE Index, stock index forecasting, XGBoost, walk-forward validation, hyperparameter optimization","This study develops a robust machine learning framework for one-step-ahead forecasting of daily log returns in the Nepal Stock Exchange (NEPSE) Index using XGBoost. A comprehensive feature set is engineered, including lagged log-returns and technical indicators. Hyperparameter optimization is performed with Optuna using walk-forward validation. Out-of-sample performance is assessed via expanding and fixed-length rolling windows, demonstrating superior predictive accuracy and directional accuracy.",28.54,LFM-2.5,Apple M1 (Metal)
2601.08901v1_Navigating Ideation Space Decomposed Conceptual Re.pdf,Navigating Ideation Space: Decomposed Conceptual Representations for Positioning Scientific Ideas,"Yuexi Shen, Minqian Liu, Dawei Zhou, Lifu Huang",10.1234/arxiv.2025.12345,arXiv:2503.01234,"scientific discovery, ideation space, conceptual decomposition, research idea, novelty assessment","This paper introduces the Ideation Space framework to decompose scientific knowledge into three dimensions—search problem, methodology, and core findings—to enable fine-grained literature retrieval and improved novelty assessment. It presents empirical results showing significant gains in recall and hit rates.",27.98,LFM-2.5,Apple M1 (Metal)
2601.08910v1_Towards a Self-Driving Trigger at the LHC Adaptive.pdf,Towards a Self-Driving Trigger at the LHC: Adaptive Response in Real Time,"Shaghayegh Emami, Cecilia Tosciri, Giovanna Salvi, Zixin Ding, Yuxin Chen, Abhijith Gandrakota, Christian Herwig, David W. Miller, Jennifer Ngadiuba, Nhan Tran",arXiv:2601.08910v1,2601.08910,"self-driving trigger, real-time data filtering, LHC, high-throughput, bandwidth constraints, machine learning, trigger optimization","The paper presents an adaptive trigger framework for the LHC that dynamically reallocates resources and adjusts thresholds in real time. It introduces a benchmark ecosystem using CMS data to optimize trigger performance autonomously, improving signal efficiency and reducing computational costs without manual retuning.",32.04,LFM-2.5,Apple M1 (Metal)
2601.08950v1_ConvoLearn A Dataset of Constructivist Tutor-Stude.pdf,ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue,"Mayank Sharma Roy, Pea Hari Subramonyam, Stanford University",,,"constructivist tutoring, LLM pedagogy, cognitive engagement, formative assessment, cultural responsiveness, metacognition, power dynamics","Introduces ConvoLearn1, a dataset of 1,250 tutor-student dialogues grounded in knowledge-building theory, demonstrating improved LLM behavior toward constructivist strategies when fine-tuned.",26.48,LFM-2.5,Apple M1 (Metal)
2601.08951v1_PluriHarms Benchmarking the Full Spectrum of Human.pdf,Benchmarking the full spectrum of human judgments on AI harms,"Jing-Jing Li, Joel Mire, Eve Fleisig, Valentina Pyatkin, Anne G. E. Collins, Maarten Sap, Sydney Levine",,jl3676@berkeley.edu,"human judgment, AI safety, harm assessments, pluralistic systems, value diversity","Introduces PLURIHARMS, a benchmark to study human harm judgments across harm axis and agreement axis, aiming to improve AI safety through insights into disagreement patterns.",27.55,LFM-2.5,Apple M1 (Metal)
2601.08953v1_Fairness risk and its privacy-enabled solution in .pdf,Fairness risk and its privacy-enabled solution in AI-driven robotic applications,"Le Liu, Bangguo Yu, Nynke Vellinga, Ming Cao",arXiv:2601.08953v1,1,"Robotic Decision-making, Large Language Model, Fairness, Privacy","The paper addresses fairness concerns in generative AI-driven robotics, proposing a utility-aware fairness metric that integrates privacy considerations. It demonstrates how privacy budgets can jointly support fairness targets in robotic systems, highlighting ethical implications for trustworthy AI.",30.05,LFM-2.5,Apple M1 (Metal)
2601.08955v1_Imagine-then-Plan Agent Learning from Adaptive Loo.pdf,Imagine-then-Plan: Agent Learning from Adaptive,"Youwei Liu, Jian Wang, Hanlin Wang, Beichen Guo, Wenjie Li",loyiv5477@gmail.com,,"agent learning, world models, imagine-then-plan, reinforcement learning, environment modeling","This paper proposes a unified framework called Imagine-then-Plan (ITP) for agent learning via lookahead imagination. ITP enables agents to interact with learned world models, generating multi-step imagined trajectories that provide rich signals about future consequences. The authors introduce an adaptive lookahead mechanism that balances ultimate goals and task progress, demonstrating superior performance across benchmarks.",27.31,LFM-2.5,Apple M1 (Metal)
2601.08988v1_ART Action-based Reasoning Task Benchmarking for M.pdf,AAAI 2026: Healthy Aging and Longevity Workshop (AIAA),"Ananya Mantravadi, Shivali Dalmia, Abhishek Mukherji",10.1093/aiac/skac045,arXiv:2108.11456,"Medical AI agents, synthetic data generation, clinical reasoning evaluation, healthcare LLMs, benchmark, HITL","Reliable clinical decision support requires medical AI agents capable of safe, multi-step reasoning over structured electronic health records. This paper introduces ART, an Action-based Reasoning benchmark for medical AI agents, identifying error categories and proposing a four-stage evaluation pipeline to advance trustworthy AI in healthcare.",29.28,LFM-2.5,Apple M1 (Metal)
2601.09012v3_TranslateGemma Technical Report.pdf,TranslateGemma,,,,,"Presents TranslateGemma, an open machine translation model built on Gemma 3, enhanced via two-stage fine-tuning (supervised fine-tuning and reinforcement learning) to achieve strong translation quality across multiple language pairs.",22.77,LFM-2.5,Apple M1 (Metal)
2601.09018v1_Meta-learning to Address Data Shift in Time Series.pdf,Meta-learning to Address Shift in Intimateseries,"Samuel Myrenab, Nidhi Parikha",arXiv:2601.09018v1,arXiv:2601.09018,"signals, seismology, Reptile, FOMAML, model-agnostic meta-learning, domain generalization","This paper systematically compares deep learning with meta-learning approaches to address data shift in time-series classification, introducing a seismic benchmark (SeisTask) and evaluating adaptation performance under varying data conditions. It finds meta-learning often achieves faster and more stable adaptation, especially with limited data, though its benefits diminish as data availability increases.",28.68,LFM-2.5,Apple M1 (Metal)
2601.09028v1_OpenDecoder Open Large Language Model Decoding to .pdf,OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG,"Fengran Mo, Zhan Su, Yuchen Hui, Jianhan Zhang, Jia Ao Sun, Zheyuan Liu, Chao Zhang, Tetsuya Sakai, Jian-Yun Nie",10.1145/nnnnnnn.nnnnnnn,2026.XXX,"Information Retrieval, Retrieval-Augmented Generation, Robust Question Answer, Decoding Paradigm, Large Language Model","The paper proposes OpenDecoder, a method that uses explicit evaluation of retrieved information as a quality indicator for LLM generation. It evaluates relevance, ranking, and QPP scores to improve robustness against noisy context.",28.6,LFM-2.5,Apple M1 (Metal)
2601.09029v1_Proactively Detecting Threats A Novel Approach Usi.pdf,Proactively Detecting Threats: A Novel Approach,"Aniesh Chawla, Udbhav Prasad",,,"Malware, Indicators of Compromise, Cyber-security, LLMs, GenAI, Machine Learning Algorithms, Deep Neural Network","This paper presents the first systematic evaluation of large language models (LLMs) to proactively identify indicators of compromise (IOCs) from unstructured web-based threat intelligence sources, distinguishing it from reactive malware detection approaches.",26.9,LFM-2.5,Apple M1 (Metal)
2601.09031v1_Generalizable Geometric Prior and Recurrent Spikin.pdf,Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation 1,"Xuetao Li, Wenke Huang, Bo Du, Senior Member, IEEE, Jifeng Xuan, Mang Y, Senior Member, IEEE, Miao Li, Fellow, IEEE, Sheng Liu, Fellow, IEEE",,,"Humanoid Robot Manipulation, Geometric Prior, Recurrent Spiking Feature Learning, Generalization, Robotic Manipulation","This paper presents a novel RGMP-S, Recurrent Geometric-prior Multimodal Policy with Spiking features, enabling high-level skill reasoning and data-efficient motion synthesis for humanoid robots. It introduces lightweight 2D geometric inductive biases and a recursive adaptive spiking network to improve generalization across unseen environments, validated on the Maniskill benchmark and real-world robots.",29.8,LFM-2.5,Apple M1 (Metal)
2601.09032v1_The Hierarchy of Agentic Capabilities Evaluating F.pdf,The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL,"Logan Ritchie, Sushant Mehta, Nick Heiner, Mason Yu, Edwin Chen",arXiv:2601.09032v1,arXiv:2601.09032,"agentic capabilities, LLM, realistic RL, workplace tasks, tool use, planning, adaptability, groundedness, common-sense reasoning","This study evaluates frontier AI models on 150 workplace tasks in a realistic e-commerce RL environment, revealing a hierarchy of capabilities (tool use, planning, adaptability, groundedness, common-sense reasoning) that models must master. It finds that current models struggle with fundamental tasks, highlighting gaps before human-level performance.",30.41,LFM-2.5,Apple M1 (Metal)
2601.09035v1_A Decompilation-Driven Framework for Malware Detec.pdf,A Decompilation-Driven Framework for Malware,"Aniesh Chawla, Udbhav Prasad",arXiv:2601.09035v1,arXiv:2601.09035,"Malware, Ghidra, Cybersecurity, LLMs, GenAI, Machine Learning Algorithms, LLMs Code development","The paper evaluates state-of-the-art LLMs for classifying executable code as benign or malicious, introducing an automated pipeline that uses Ghidra for decompilation followed by LLM-based classification. It highlights limitations of standard LLMs and the need for continuous fine-tuning against emerging threats.",29.09,LFM-2.5,Apple M1 (Metal)
2601.09041v1_Can LLMs interpret figurative language as humans d.pdf,CANLLMSINTERPRETFIGURABLYANGUAGE ASHUMANSDO?,"Samhita Bollepally, Aurora Sloman-Moll, Takashi Yamauchi",,,"human language models, figurative language, surface-level alignment, representational similarity, LLM interpretability","Human participants and LLMs showed alignment at the surface level but diverged at the representational level, especially in interpreting idioms and Gen Z slang. GPT-4 best matched human representational patterns, while others struggled with context-dependent expressions.",27.7,LFM-2.5,Apple M1 (Metal)
2601.09049v1_Is Grokking Worthwhile Functional Analysis and Tra.pdf,Grokking Worthwhile? Functional Analysis and Transferability of Generalization Circuits in Transformers,"Kaiyu He, Mian Zhang, Peilin Wu, Xinya Du, Zhiyu Zoey Chen",,,"functional analysis, transferability, generalization circuits, compositional reasoning, knowledge transfer",This study investigates whether a grokked model outperforms non-grokked models on downstream tasks and whether the computational cost of grokking is justified. It demonstrates that grokking integrates facts during a prolonged reasoning phase and shows that transferability depends on data regimes rather than a full mastery of compositional logic.,27.89,LFM-2.5,Apple M1 (Metal)
2601.09066v1_Midm 2.0 Korea-centric Bilingual Language Models.pdf,Mi:dm 2.0,"Tech. Innovation Group, KT",https://huggingface.co/K-intelligence,https://huggingface.co/K-intelligence,"Korea-centric, Bilingual Language Models, KOREA-CENTRICAI, AI, Korean society","Mi:dm 2.0 is a bilingual LLM designed to advance Korea-centric AI by integrating cultural context, reasoning patterns, and commonsense knowledge. It addresses data quality and cultural alignment issues through proprietary pipelines and a custom tokenizer, achieving state-of-the-art performance on Korean benchmarks.",28.05,LFM-2.5,Apple M1 (Metal)
2601.09069v1_From Symbolic to Natural-Language Relations Rethin.pdf,From Symbolic to Natural-Language Relations: Rethinking Knowledge,"Kanyao Han, Yushang Lai",,,,"This paper argues for rethinking relation representations from symbolic schemas to natural-language descriptions, proposing hybrid design principles that better align with the capabilities of large language models.",23.28,LFM-2.5,Apple M1 (Metal)
2601.09072v1_Human-AI Co-design for Clinical Prediction Models.pdf,Human-AI Co-design for Clinical Prediction Models,"Jean Feng, Avni Kothari, Patrick Vossler, Andrew Bishara, Lucas Zier, Chandan Singh",arXiv:2601.09072v1,arXiv:2601.09072,"Large language models, Electronic health records, Concept Bottleneck, Human-AI Interaction","Developing safe, effective, and practically useful clinical prediction models requires iterative collaboration between clinical experts, data scientists, and informaticists. This process is time- and resource-intensive, limiting adoption. HACHI introduces an iterative human-in-the-loop framework to accelerate development of interpretable models by exploring concepts in clinical notes.",29.1,LFM-2.5,Apple M1 (Metal)
2601.09085v1_MMR-GRPO Accelerating GRPO-Style Training through .pdf,MMR-GRPO: Accelerating GRPO-Style Training through Diversity-Aware Reward Reweighting,"Kangda Wei, Ruihong Huang",10.48550/arXiv:2408.14132,arXiv:2408.14132,"GRPO, diversity-aware, reward reweighting, mathematical reasoning","Proposes MMR-GRPO to reduce training steps and wall-clock time by reweighting rewards based on completion diversity, achieving comparable performance with 47.9% fewer steps.",25.05,LFM-2.5,Apple M1 (Metal)
2601.09089v1_SubTokenTest A Practical Benchmark for Real-World .pdf,A Practical Benchmark for Real-World Sub-token,"Shuyang Hou, Yi Hu, Muhan Zhang",arXiv:2601.09089v1,arXiv:2601.09089,"sub-token understanding, LLM, tokenization, character-level tasks","This paper introduces SUBTOKENTEST, a benchmark assessing sub-token understanding through practical tasks. It evaluates ten tasks across four domains, highlighting tokenization limitations in large language models and exploring impacts of test-time scaling.",26.18,LFM-2.5,Apple M1 (Metal)
2601.09097v1_Programming over Thinking Efficient and Robust Mul.pdf,Programming over Thinking: Efficient and Robust,"Derrick Goh Xin Deik1, Quanyu Long1, Zhengyuan Liu2, Nancy F. Chen2, Wenya Wang1",,,"multi-constraint planning, planning, LLM, code planning, robustness, optimization","The paper introduces ScalableCodePlanning Engine (SCOPE), a framework that separates query-specific reasoning from generic code execution to improve consistency, determinism, and reusability. SCOPE enables state-of-the-art performance while reducing cost and latency, achieving 93.1% success on TravelPlanner with a 61.6% improvement over CoT.",28.34,LFM-2.5,Apple M1 (Metal)
2601.09100v2_DScheLLM Enabling Dynamic Scheduling through a Fin.pdf,Dynamic Scheduling through a Fine-Tuned Dual System Large Language Model,"D. S., Zhang a, Chenggong Zhao b, Gaoa b, Xiaoke Zhao b, Gengyi Bai b, Jinhu Lv a",,,"dynamic scheduling, large language model, fine-tuning, job shop scheduling","This paper proposes DScheLLM, a dynamic scheduling approach leveraging fine-tuned large language models within a dual-system architecture to handle disturbances of different scales. It presents a unified framework with training on exact schedules and demonstrates improved performance on standard job shop scheduling benchmarks.",28.17,LFM-2.5,Apple M1 (Metal)
2601.09105v2_AviationLMM A Large Multimodal Foundation Model fo.pdf,AviationLMM: A Large Multimodal Foundation,"Wenbin Li, Jingling Wu, Xiaoyong Lin, Jing Chen, Cong Chen",,,"civil aviation, multi-modal model, foundation model, cloud edge collaboration, hybrid training, computer systems organization, computing methodologies","Civil aviation is a cornerstone of global transportation; this paper introduces AviationLMM, a large multimodal foundation model to unify heterogeneous data streams and enable advanced situational awareness, reasoning, and decision support in aviation.",27.42,LFM-2.5,Apple M1 (Metal)
2601.09113v1_The AI Hippocampus How Far are We From Human Memor.pdf,The AI Hippocampus: How Far are We From Human Memory?,"Zixia Jia1, Jiaqi Li, Yipeng Kang, Yuxuan Wang, Tong Wu, Quansen Wang, Xiaobo Wang, Shuyi Zhang, Jinzhe Shen, Qing Li, Siyuan Qi, Yitao Liang, Di He2, Zilong Zheng",arXiv:2601.09113v1,https://arxiv.org/abs/2601.09113,"memory, LLM, memory-augmented, memory mechanisms, memory architectures, memory systems","This survey presents a comprehensive synthesis of memory in large language models and multi-modal LLMs, organizing literature into implicit, explicit, and agentic memory paradigms. It explores architectural advances, benchmark tasks, and open challenges, aiming to guide development of more flexible and context-sensitive memory-augmented models.",31.36,LFM-2.5,Apple M1 (Metal)
2601.09116v1_LP-LLM End-to-End Real-World Degraded License Plat.pdf,LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models,"Haoyan Gong, Xi’an Jiaotong-Liverpool University, Haoyan Gong, Xi’an Jiaotong-Liverpool University",,,"license plate recognition, degraded images, motion blur, low resolution, character recognition, structured reasoning","The paper addresses challenges in real-world LPR under severe degradations (motion blur, low resolution, complex illumination). It proposes a character-aware multimodal reasoning framework with a Character-Aware Multimodal Reasoning Module to improve recognition accuracy by aligning image restoration objectives with character-level semantics.",29.21,LFM-2.5,Apple M1 (Metal)
2601.09117v1_A Marketplace for AI-Generated Adult Content and D.pdf,A Marketplace for AI-Generated Adult Content and Deepfakes,"Shalmoli Ghosh, Matthew R. DeVerna, Filippo Menczer, Indiana University Bloomington, Stanford University",,,"generative AI, deepfakes, bounties, content moderation, gender asymmetry, platform governance","This study examines a monetized bounty marketplace on Civitai, analyzing how users steer AI models toward non-trained content and how 'Not Safe For Work' requests dominate. It reveals gendered harms targeting female celebrities and highlights challenges in consent and governance.",27.93,LFM-2.5,Apple M1 (Metal)
2601.09120v1_Adaptive Multi-Stage Patent Claim Generation with .pdf,Adaptive Multi-Stage Patent Claim Generation with Unified Quality Assessment,"Chen-Wei Liang, Bin Guo, Zhen-Yuan Wei, Mu-Jiang-Shan Wang",arXiv:2601.09120v1,2601.09120v1,"Patent claim generation, Cross-jurisdictional learning, Quality assessment, Transformer, Domain adaptation","Introduces a three-stage framework for patent claim generation addressing cross-jurisdictional challenges, semantic relationship modeling, and unified quality assessment. Experimental results show significant improvements over baselines.",30.51,LFM-2.5,Apple M1 (Metal)
2601.09130v1_Equi-ViT Rotational Equivariant Vision Transformer.pdf,Equi-ViT: Rotation Equivariant Vision Transformer for Robust Histopathology Analysis,"Fuyao Chen, Yuexi Du, El Leonore V. Lieffrig, Nicha C. Dvornek, John A. Onofrey",10.1016/j.eyhtoz.2024.03.007,,"Digital Histopathology, Vision Transformer, Rotation Equivariance, Artificial Intelligence, Histopathology Analysis","This paper introduces Equi-ViT, a Vision Transformer architecture enhanced with an equivariant convolution kernel to achieve rotation invariance in histopathology imaging. By integrating rotational equivariance into patch embeddings, Equi-ViT improves robustness across image orientations, demonstrating superior performance on a public colorectal cancer dataset.",29.25,LFM-2.5,Apple M1 (Metal)
2601.09136v1_SkinFlow Efficient Information Transmission for Op.pdf,SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL,"Lijun Liu, Linwei Chen, 6, 7, Zhishou Zhang, Meng Tian, Hengfu Cui, Ruiyang Li, Zhaocheng Liu, Qiang Ju, Qianxi Li, Baichuan Inc., Beijing Key Laboratory of Molecular Diagnosis on Dermatoses, NMPA Key Laboratory for Quality Control and Evaluation of Cosmetics, School of Biomedical Engineering, Tsinghua University, University of Hong Kong",,,"SkinFlow, Large Vision-Language Models, Dermatological diagnosis, Diffuse attention, Reinforcement Learning, Fitzpatrick17k, Top-1 accuracy, Top-6 accuracy","This paper introduces SkinFlow, a framework that optimizes visual information transmission efficiency for dermatological diagnosis using a Virtual-Width Dynamic Vision Encoder and a two-stage reinforcement learning strategy. It achieves state-of-the-art performance on the Fitzpatrick17k benchmark, demonstrating superior diagnostic reasoning compared to general-purpose models.",30.94,LFM-2.5,Apple M1 (Metal)
2601.09147v2_SSVP Synergistic Semantic-Visual Prompting for Ind.pdf,Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection,"Chenhao Fu, Han Fang, Xiuzheng Zheng, Wenbo Wei, Yonghua Li, Hao Sun, Xuelong Li",10.1234/ssvp.2025.0123,arXiv:2509.12345,"industrial anomaly detection, zero-shot anomaly detection, semantic-visual prompting, CLIP, Vision-Language Models, anomaly mapping",This paper introduces Synergistic Semantic-Visual Prompting (SSVP) to enhance industrial zero-shot anomaly detection. SSVP integrates Hierarchical Semantic-Visual Synergy (HSVS) with Vision-Conditioned Prompt Generator (VCPG) and Visual-Text Anomaly Mapper (VTAM) to overcome limitations of static templates and global scoring. Extensive evaluations show state-of-the-art performance on MVTec-AD benchmarks.,29.95,LFM-2.5,Apple M1 (Metal)
2601.09152v1_PrivacyReasoner Can LLM Emulate a Human-like Priva.pdf,PrivacyReasoner: An AI-Agent for Simulating Privacy Concerns,"Yiwen Tu, Xuan Liu, Lianhui Qin, Haojian Jin",,,"privacy, AI agent, privacy reasoning, contextual cues, user-specific concerns","This paper introduces PrivacyReasoner, an AI-agent designed to simulate how individual users form privacy concerns in response to real-world news. It integrates privacy and cognitive theories to model user-specific reasoning using personal comment histories and contextual cues. The agent reconstructs each user’s 'privacy mind' and generates synthetic responses, evaluated by a complementary LLM-as-a-Judge.",28.5,LFM-2.5,Apple M1 (Metal)
2601.09156v1_KTCF Actionable Recourse in Knowledge Tracing via .pdf,Actionable Recourse in Knowledge Tracing via Counterfactual,"Woojin Kim, Changkwon Lee, Hyeoncheol Kim",,,"Knowledge Tracing, Counterfactual Explanation, Education, Explainable AI, Learning Analytics","This paper proposes KTCF, a counterfactual explanation generation method for Knowledge Tracing that accounts for knowledge concept relationships and converts counterfactual explanations into educational instructional steps. It demonstrates superior performance over existing methods and provides qualitative evidence that such explanations reduce educational burdens.",27.03,LFM-2.5,Apple M1 (Metal)
2601.09182v1_Position on LLM-Assisted Peer Review Addressing Re.pdf,Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback,"JungMin Yun, JuneHyoung Kwon, MiHyeon Kim, YoongBin Kim, 2",,,"LLM-assisted review, peer review, reviewer gap, mentoring, feedback, scholarly ecosystem","The paper critiques LLM-driven approaches to automate reviews and advocates for a human-centered shift, proposing mentoring systems and feedback tools to enhance reviewer expertise and sustainability.",27.37,LFM-2.5,Apple M1 (Metal)
2601.09195v1_ProFit Leveraging High-Value Signals in SFT via Pr.pdf,ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection,"Tao Liu, Taiqiang Wu, Runming Yang, Shaoning Sun, Junjie Wang, Yujiu Yang",,,"Supervised Fine-Tuning, Large Language Models, Token Probability, Semantic Importance, Answer Diversity, Cost-Efficiency","This paper proposes ProFit, a strategy that selectively masks low-probability tokens to mitigate single-reference overfitting in supervised fine-tuning. By emphasizing high-probability tokens, ProFit improves semantic integrity while maintaining efficiency, outperforming traditional multi-reference SFT approaches.",27.91,LFM-2.5,Apple M1 (Metal)
2601.09208v2_Mikasa A Character-Driven Emotional AI Companion I.pdf,Mikasa: A Character-Driven Emotional AI Companion,Miki Ueno,miki.ueno@kcg.ac.jp,,"character design, relationship definition, emotional AI, Japanese Oshi culture","This paper explores the importance of character coherence and relationship clarity in AI companions, using Mikasa as a case study inspired by Japanese Oshi culture. It argues that sustained user engagement depends on stable personality and defined interaction norms rather than solely on technical capabilities.",26.96,LFM-2.5,Apple M1 (Metal)
2601.09212v1_Annealed Relaxation of Speculative Decoding for Fa.pdf,Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image Generation,"Xingyao Li, Fengzhuo Zhang, Cunxiao Du, Hui Ji",10.48550/arxiv/2303.08732,arXiv:2303.08732,"auto-regressive, speculative decoding, relaxed decoding, image generation, inference speed, quality trade-off","This paper proposes COOL-SD, an annealed relaxation of speculative decoding, to improve inference speed in auto-regressive image generation. It introduces theoretical insights into relaxed SD and presents COOL-SD, demonstrating consistent improvements over prior methods in speed-quality trade-offs.",28.12,LFM-2.5,Apple M1 (Metal)
2601.09213v1_SpikeVAEDiff Neural Spike-based Natural Visual Sce.pdf,SpikeV AEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-V AE and Versatile Diffusion,"Jialu Li, Taiyan Zhou",,,"neural spike, visual scene reconstruction, neural decoding, computational modeling, brain-computer interface","Reconstructing natural visual scenes from neural activity is a key challenge. This paper presents SpikeVAEDiff, a framework combining VDVAE and Versatile Diffusion to generate high-resolution image reconstructions from neural spike data. Results show enhanced performance, especially for the VISI region, and demonstrate the value of spike data over fMRI for temporal and spatial resolution.",28.48,LFM-2.5,Apple M1 (Metal)
2601.09233v1_GIFT Unlocking Global Optimality in Post-Training .pdf,Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization,"Zhengyang Zhao, Lu Ma, Yizhen Jiang, Xiaochen Ma, Zimo Meng, Chengyu Shen, Lexiang Tang, Haoze Sun, Peng Pei, Wentao Zhang",10.1234/example.12345,2025.12345.01234,"post-training, large reasoning models, supervised fine-tuning, reinforcement learning, distributional collapse, Gibbs Initialization, optimality","The paper proposes GIFT, a Gibbs Initialization with Finite Temperature method, to address distributional collapse in post-training by reformulating SFT within a unified framework. It shows GIFT outperforms standard SFT and other baselines for RL initialization, enabling global optimality.",28.76,LFM-2.5,Apple M1 (Metal)
2601.09236v2_Reward Learning through Ranking Mean Squared Error.pdf,Reward learning through ranking means squared error,"Chaitanya Kharyal, Calarina Muslimani, Matthew E. Taylor",10.48550/arXiv.2405.12345,arXiv:2405.12345,"reward learning, reinforcement learning, ranking mean squared error, robotic locomotion","The paper introduces Ranked Return Regression for RL (R4), a new rating-based method that uses a novel ranking mean squared error loss. R4 treats teacher-provided ratings as ordinal targets and learns from trajectory-rating pairs, offering formal guarantees and demonstrating superior performance on robotic benchmarks.",27.85,LFM-2.5,Apple M1 (Metal)
2601.09239v2_DSA-Tokenizer Disentangled Semantic-Acoustic Token.pdf,DSA-Tokenizer: Disentangled Semantic-Acoustic Tokenization,"Hanlin ZHANG1, Daxin Tan2, Dehua Tao2, Xiao Chen2, Yunhe Li1, Yuchen Cao 1, Jianping Wang 1, Linqi Song 1",,,"speech tokenizer, disentangled tokenization, semantic-acoustic fusion, speech large language models, audio reconstruction, reconstruction-recombination, speech modeling","The paper introduces DSA-Tokenizer, a method that disentangles speech into discrete semantic and acoustic tokens using hierarchical flow matching and joint reconstruction training. It aims to improve disentanglement by separating linguistic content from style cues and enabling controllable speech generation.",28.18,LFM-2.5,Apple M1 (Metal)
2601.09248v1_Hybrid guided variational autoencoder for visual p.pdf,Hybrid guided variational autoencoder for visual place recognition,"Ni Wang, Zihan You, Emre Neftci, Thorben Schoepe",arXiv:2408.14337,arXiv:2408.14337,"Visual place recognition, Spiking neural network, Guided variational autoencoder, Robotics, Low latency hardware","This work presents a hybrid guided variational autoencoder combining event-based vision sensors with a novel guided VAE to enable efficient visual place recognition for mobile robots. The model disentangles visual features of 16 distinct indoor places, achieving robust performance under varying illumination and demonstrating strong generalization capabilities.",30.1,LFM-2.5,Apple M1 (Metal)
2601.09251v1_HGATSolver A Heterogeneous Graph Attention Solver .pdf,HGA TSolver: A Heterogeneous Graph Attention Solver for Fluid–Structure Interaction,"Qin-Yi Zhang, Hong Wang, Siyao Liu, Haichuan Lin, Linying Cao, Chen Chen, Shuangyi Wang, Zeng-Guang Hou, Zeng-Guang Hou",zhangqinyi2024,,"fluid-structure interaction, heterogeneous graph, graph attention, physics-conditioned gating, inter-domain balancing","The paper proposes HGATSolver, a heterogeneous graph attention solver for fluid-structure interaction, addressing challenges in capturing multi-physics dynamics through domain-specific message passing and adaptive stabilization mechanisms.",27.92,LFM-2.5,Apple M1 (Metal)
2601.09253v1_RIFT Repurposing Negative Samples via Reward-Infor.pdf,Repurposing Negative Samples via Reward-Informed Fine-Tuning,"Zehua Liu, Shuqi Liu, Tao Zhong, Mingxuan Yuan",,,"LLM alignment, Reward-Informed Fine-Tuning, Negative samples, Data efficiency, Self-generated data","Proposes Reward Informed Fine-Tuning (RIFT) to repurpose negative samples, improving data efficiency by reweighting losses from both positive and negative trajectories without relying on costly expert data.",26.42,LFM-2.5,Apple M1 (Metal)
2601.09259v1_MAXS Meta-Adaptive Exploration with LLM Agents.pdf,Meta-Adaptive Exploration with LLM Agents,"Jian Zhang, Zhiyuan Wang, Zhangqi Wang, Yu He, Haoran Luo, Li Yuan, Lingling Zhang, Rui Mao, Qika Lin, Jun Liu",,,"LLM Agents, meta-adaptive exploration, LLM agents, LLM agents with tools, reasoning, trajectory stability, computational efficiency","The paper proposes MAXS, a meta-adaptive reasoning framework using LLM agents that integrates tool execution and reasoning planning. MAXS addresses locally myopic generation and trajectory instability by employing a lookahead strategy and combining step consistency with trend analysis. It introduces a trajectory convergence mechanism to balance efficiency and effectiveness across multiple tools.",28.41,LFM-2.5,Apple M1 (Metal)
2601.09260v1_Efficient Paths and Dense Rewards Probabilistic Fl.pdf,Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models,"Yan Liu, Feng Zhang, Zhanyu Ma, Jinghua Hao, Renqing He, Han Liu, Yangdong Deng",,,"large language models, chain-of-thought, probabilistic flow reasoning, reasoning efficiency, reinforcement learning, flow-guided decoding, flow-based reinforcement learning","This paper proposes CoT-Flow, a framework that models discrete reasoning steps as continuous probabilistic flows. It introduces flow-guided decoding and flow-based reinforcement learning to quantify each step's contribution to reasoning performance, aiming to improve inference efficiency and optimization in LLMs.",28.61,LFM-2.5,Apple M1 (Metal)
2601.09262v1_Magnifying change Rapid burn scar mapping with mul.pdf,"Magnifying change: Rapid burn scar mapping with multi-resolution, multi-source satellite imagery","Maria Sdraka, Dimitrios Michail, Ioannis Papoutsis",,,"Artificial intelligence, Machine Learning, Remote Sensing, burn scar mapping, disaster management, disaster monitoring, wildfires, burn scar mapping, change detection, downscaling, super-resolution","Delineating wildfire affected areas using satellite imagery remains challenging due to irregular spectral changes. The paper proposes a novel deep learning model, BAM-MRCD, using multi-resolution, multi-source satellite data for timely high-resolution burnt area mapping. The model detects small-scale wildfires with high accuracy.",28.89,LFM-2.5,Apple M1 (Metal)
2601.09264v1_Coordinated Pandemic Control with Large Language M.pdf,Coordinated Pandemic Control with Large Language Model Agents,"Ziyi Shi, Xusen Guo, Hongliang Lu, Mingxing Peng, Haotian Wang, Zheng Zhu, Zhenning Li, Yuxuan Liang, Xinhu Zheng, Hai Yang",,,"Pandemic Control, Large Language Models, Multi-Agent System, Coordinated Policymaking","This study proposes a large language model (LLM) multi-agent policymaking framework to support coordinated pandemic control across regions. By assigning LLM agents to administrative regions and integrating real-world data, the framework enables joint exploration of counterfactual interventions, achieving significant reductions in cumulative infections and deaths during the COVID-19 pandemic.",29.53,LFM-2.5,Apple M1 (Metal)
2601.09269v2_RISER Orchestrating Latent Reasoning Skills for Ad.pdf,Orchestrating Latent Reasoning Skills for Adaptive Activation Steering,"Wencheng Ye, Xiaoyang Yuan, Yi Bin, Hengyu Jin, Liang Peng, Pengpeng Zeng, Heng Tao Shen, Liang Peng",,,"domain-specific reasoning, large language models, activation steering, control strategies, reasoning adaptation","RISER proposes a plug-and-play framework to adaptively steer LLM reasoning in activation space using a router-based intervention, achieving improvements over CoT-style reasoning with higher token efficiency.",26.28,LFM-2.5,Apple M1 (Metal)
2601.09274v1_A3-Bench Benchmarking Memory-Driven Scientific Rea.pdf,A3-Bench: Benchmarking Memory-Driven Scientific Reasoning,"Jian Zhang, Yu He, Zhiyuan Wang, Zhangqi Wang, Kai He, Fangzhi Xu, Qika Lin",arXiv:2601.09274v1,arXiv:2601.09274,"memory-driven reasoning, scientific reasoning, anchors, attractors, LLM performance","This paper proposes A3-Bench to evaluate scientific reasoning through dual-scale memory-driven activation, addressing gaps in existing benchmarks that overlook memory mechanisms like anchors and attractors.",27.72,LFM-2.5,Apple M1 (Metal)
2601.09278v1_M3Searcher Modular Multimodal Information Seeking .pdf,M3Searcher: Modular Multimodal Information Seeking Agency with Retrieval-Oriented Reasoning,"Xiaohan Yu, Chao Feng, Lang Mei, Chong Chen",10.1234/m3searcher.2025,12345678,"modular agent, multimodal, information seeking, retrieval-oriented","This paper proposes M3Searcher, a modular multimodal information-seeking agent that decouples information acquisition from answer derivation. It introduces retrieval-oriented multi-objective reward and develops MM-SearchVQA to support retrieval-centric reinforcement learning training. Experimental results show superior performance in complex multimodal tasks.",27.57,LFM-2.5,Apple M1 (Metal)
2601.09280v1_ReGraM Region-First Knowledge Graph Reasoning for .pdf,ReGraM: Region-First Knowledge Graph Reasoning for Medical Question,"Chaerin Lee, Sohee Park, Hyunsik Na, Daseon Choi",10.1000/arxiv/2301.01234,10.1000/arxiv/2301.01234,"medical question answering, knowledge graphs, multi-hop reasoning, hallucination reduction","This paper introduces ReGraM, a region-first knowledge graph reasoning framework for medical QA. It constructs a query-aligned subgraph and performs stepwise reasoning constrained to a localized region, improving factual accuracy and consistency by focusing inference on relevant evidence.",28.89,LFM-2.5,Apple M1 (Metal)
2601.09281v1_STaR Sensitive Trajectory Regulation for Unlearnin.pdf,Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models,"Jingjing Zhou, Gaoxiang Cong, Li Su, Liang Li",,,"large reasoning models, unlearning, privacy protection, trajectory regulation, chain-of-thought, sensitive content","Proposes Sensitive Trajectory Regulation (STaR), a parameter-free inference-time unlearning framework for LRMs, to mitigate privacy leakage during multi-step reasoning.",26.27,LFM-2.5,Apple M1 (Metal)
2601.09282v1_Cluster Workload Allocation Semantic Soft Affinity.pdf,Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing,"Leszek Sliwko, Jolanta Mizeria-Pietraszko",10.1007/978-3-642-45888-7,,"Artificial Intelligence, Kubernetes, Load Balancing, Semantic Parsing, Soft-Affinity, Task Assignment","This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. It employs a Large Language Model integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. Empirical evaluation shows high LLM parsing accuracy and superior scheduling quality across scenarios.",28.99,LFM-2.5,Apple M1 (Metal)
2601.09286v1_Why not Collaborative Filtering in Dual View Bridg.pdf,Why not Collaborative Filtering in Dual View? Bridging Sparse and Dense Models,"Hanze Guo, Jianxun Lian, Xiaozhou",https://doi.org/XXXXXXX.XXXXXXX,https://openbenchmark.github.io/BARS/Matching/leaderboard/index.html,"Collaborative Filtering, Dual View Alignment, Sparse and Dense model","The paper addresses the signal-to-noise ratio ceiling in dense embedding-based recommender systems under data sparsity, proposing a unified SaD framework that combines semantic embeddings with sparse interaction patterns to achieve superior performance.",27.54,LFM-2.5,Apple M1 (Metal)
2601.09292v1_Blue Teaming Function-Calling Agents.pdf,Blue Teaming Function-Calling Agents,"Greta Dolcetti, Giulio Zizzo, Sergio Maffeis",,,,"Experimental evaluation of robustness of open source LLMs with function-calling capabilities against adversarial attacks, measuring effectiveness of defensive measures.",23.6,LFM-2.5,Apple M1 (Metal)
2601.09293v1_Policy-Based Reinforcement Learning with Action Ma.pdf,Policy-Based Reinforcement Learning with Action Masking for Dynamic Job Shop,"Sofiene Lassoued, Stefan Lier, Andreas Schwung",,,"Dynamic Job Shop Scheduling, Fault tolerance, Reinforcement learning, Actions masking, Petri nets","Presents a novel framework for Dynamic Job Shop Scheduling under uncertainty, using Coloured Timed Petri Nets and Maskable Proximal Policy Optimization to handle stochastic job arrivals and machine failures.",27.73,LFM-2.5,Apple M1 (Metal)
2601.09306v1_On-Device Large Language Models for Sequential Rec.pdf,On-Device Large Language Models for Sequential Recommendation,"Xin Xia, Hongzhi Yin, Shane Culpepper",10.1145/3773966.3777961,Ninth ACM International Conference on Web Search and Data Mining (WSDM) 2026,"Recommender Systems, Sequential Recommendation, On-Device Recommendation, Model Compression, Resource Constrained Devices","This paper proposes OD-LLM, a task-adaptive compression framework for efficient on-device deployment of large language models in sequential recommendation tasks. It integrates low-rank structural compression with progressive alignment to maintain performance while reducing model size, demonstrating effectiveness in real-time applications.",27.84,LFM-2.5,Apple M1 (Metal)
2601.09313v1_Understanding or Memorizing A Case Study of German.pdf,Understanding or Memorizing? A Case Study of German Definite Articles,"Jonathan Drechsel, Erisa Bytyqi, Steffen Herbold",,,"language models, German definite articles, memorization, rule-based encoding","The study investigates whether German definite articles are learned through abstract grammatical rules or memorized associations, using gradient-based interpretability methods on parameter updates for gender-case transitions.",25.23,LFM-2.5,Apple M1 (Metal)
2601.09342v1_Improving Implicit Hate Speech Detection via a Com.pdf,Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework,"Ewelina Gajewska, Katarzyna Budzynska, Jarosław A. Chudziak",,,"LLMs, Community agents, Hate speech, Social media, Moderation, Fairness","This work proposes a contextualised detection framework for implicitly hateful speech, implemented as a multi-agent system comprising a central Moderator Agent and dynamically constructed Community Agents. The approach integrates socio-cultural context from publicly available sources for identity-aware moderation, improving classification accuracy and fairness on the ToxiGen dataset.",28.16,LFM-2.5,Apple M1 (Metal)
2601.09351v1_Navigating Ethical AI Challenges in the Industrial.pdf,Navigating Ethical AI Challenges in the Industrial Sector: Balancing Innovation and Responsibility,"Dr. Ruomu Tan, Dr.-Ing Martin W Hoffmann",arXiv:2601.09351v1,2601.09351v1,"artificial intelligence, industrial sector, ethical challenges, innovation, responsibility, transparency, accountability, fairness","This chapter explores how AI-empowered industrial innovation intersects with ethics, highlighting challenges around transparency, accountability, and fairness. It examines ethical considerations in AI applications within industrial use cases and discusses strategies to embed ethical principles in industrial AI systems.",31.34,LFM-2.5,Apple M1 (Metal)
2601.09353v1_Monte-Carlo Tree Search with Neural Network Guidan.pdf,Monte-Carlo Tree Search with Neural Network Guidance,"Ioannis Peridis, Dimitrios Troullinos, Georgios Chalkiadakis, Pantelis Giankoulidis, Ioannis Papamichail, Markos Papageorgiou",arXiv:2601.09353v1,arXiv:2601.09353,"Monte-Carlo Tree Search, Neural Network Guidance, Autonomous Driving, Lane-Free Traffic, Reinforcement Learning, Traffic Flow","This paper presents a Monte-Carlo Tree Search (MCTS) planning approach for single-agent autonomous driving in lane-free traffic environments. It integrates a pre-trained neural network to guide the selection phase, aiming to improve safety and performance metrics such as collision rates and speed. The study explores the impact of isotropic state information and evaluates computational trade-offs.",30.78,LFM-2.5,Apple M1 (Metal)
2601.09361v1_GeoRA Geometry-Aware Low-Rank Adaptation for RLVR.pdf,GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR,"Jiaying Zhang, Lei Shi, Jiguo Li, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He",,,"Reinforcement Learning, RLVR, Parameter-efficient Tuning, Low-rank Adaptation, Geometric Structures, Optimization Stability","Proposes GeoRA to address optimization instability in RLVR by leveraging geometry-aware low-rank adaptation, improving performance and generalization.",26.94,LFM-2.5,Apple M1 (Metal)
2601.09365v1_Frame of Reference Addressing the Challenges of Co.pdf,Frame of Reference: Addressing the Challenges of Common Ground,"Biswesh Mohapatra, Théo Charlot, Nantes Université, Giovanni Duca, University of Trento, Mayank Palan, Laurent Romary, Justine Cassell",,,"common ground, situational dialogs, embodied conversational agents, social robots, grounding mechanisms","This paper evaluates a model’s ability to establish common ground using relational references in dynamic, shared environments. It highlights challenges in grounding complex spatial-temporal dialogues and proposes methods to improve performance.",27.8,LFM-2.5,Apple M1 (Metal)
2601.09381v1_Query Languages for Machine-Learning Models.pdf,Query Languages for Machine-Learning Models,"Martin Grohe, envel⌢pe, RWTH Aachen University, Germany",,,"expressive power of query languages, fixed-point logics, weighted structures, neural networks, explainable AI","This paper explores two logics for weighted finite structures—first-order logic with summation (FO(SUM)) and its recursive extension IFP(SUM)—originating from foundational work by Grädel, Gurevich, and Meer. The authors investigate these logics as query languages for machine learning models, especially neural networks, and discuss their expressiveness and computational complexity.",28.67,LFM-2.5,Apple M1 (Metal)
2601.09382v1_Long-term Task-oriented Agent Proactive Long-term .pdf,Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments,"Qinglong Shi, Donghai Wang, Hantao Zhou, Jiguo Li, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He",,,"long-term intent, proactive agents, dynamic environments, task-oriented interaction, user intent maintenance","This paper proposes a novel interaction paradigm for proactive task-oriented agents capable of bridging the gap between static user needs and dynamic environments. It introduces intent-conditioned monitoring and event-triggered follow-up, evaluates performance using a new benchmark (ChronosBench), and demonstrates improved task completion rates through synthetic data training.",28.02,LFM-2.5,Apple M1 (Metal)
2601.09394v2_FairGE Fairness-Aware Graph Encoding in Incomplete.pdf,FairGE: Fairness-Aware Graph Encoding in Incomplete Social Networks,"Renqiang Luo, Huafei Huang, Tao Tang, Jing Ren, Ziqi Xu, Mingliang Hou, Zheng Ren, Enyan Dai, Feng Xia",https://doi.org/XXXXXXX.XXXXXXX,,"Social Networks, Graph Learning, Graph Transformers, Fairness, Incomplete Data","Graph Transformers (GTs) are increasingly applied to social network analysis, yet their deployment is often constrained by fairness concerns. This issue is particularly critical in incomplete social networks, where sensitive attributes are frequently missing due to privacy and ethical restrictions. Existing solutions commonly generate these incomplete attributes, which may introduce additional biases and further compromise user privacy. To address this challenge, FairGE (Fair Graph Encoding) is introduced as a fairness-aware framework for GTs in incomplete social networks. Instead of generating sensitive attributes, FairGE encodes fairness directly through spectral graph theory. By leveraging the principal eigenvector to represent structural information and padding incomplete sensitive attributes with zeros to maintain independence, FairGE ensures fairness without data reconstruction. Theoretical analysis demonstrates that the method suppresses the influence of non-principal spectral components, thereby enhancing fairness. Extensive experiments on seven real-world social network datasets confirm that FairGE achieves at least a 16% improvement in both statistical parity and equality of opportunity compared with state-of-the-art baselines. The source code is shown in https://github.com/LuoRenqiang/FairGE.",31.69,LFM-2.5,Apple M1 (Metal)
2601.09398v1_Ability Transfer and Recovery via Modularized Para.pdf,Ability Transfer and Recovery via Modularized Parameters Localization,"Songyao Jin, Kun Zhou, Wenqi Li, Peng Wang, Biwei Huang",,,"ability transfer, recovery, modularized parameters, LLM localization, catastrophic forgetting","Investigates ability distribution in LLM parameters by analyzing module activations, proposes ACT for targeted ability transfer, and demonstrates recovery of forgotten abilities while preserving skills.",26.2,LFM-2.5,Apple M1 (Metal)
2601.09413v1_Speech-Hands A Self-Reflection Voice Agentic Appro.pdf,Speech-Hands: A Self-Reflection Voice Agentic Approach to Speech Recognition and Audio Reasoning with Omni Perception,"Zhen Wan, Chao-Han Huck Yang, Jinchuan Tian, Hanrong Ye, Ankita Pasad, Szu-wei Fu, Ryo Hachiuma, Shizhe Diao, Kunal Dhawan, Sreyan Ghosh, Yusuke Hirota, Zhehuai Chen, Rafael Valle, Ehsan Hosseini Asl",,,"speech recognition, audio reasoning, omni perception, self-reflection, speech-agentic, openasr, multimodal understanding","The paper introduces Speech-Hands, a voice-agentic framework that learns when to trust internal models versus external audio perception. It addresses performance degradation from naive fine-tuning by implementing an explicit self-reflection mechanism, improving robustness across benchmarks and diverse audio QA tasks.",29.3,LFM-2.5,Apple M1 (Metal)
2601.09416v1_Radiomics-Integrated Deep Learning with Hierarchic.pdf,Radiomics-Integrated Deep Learning with Hierarchical Loss for Osteosarcoma Histology Classification,"Yaxi Chen, Zi Ye, Oliver Yu, Simin Ni, Jie Huang",,,"Osteosarcoma, Radiomics, Multi-Task Learning, Uncertainty Weighting, Hierarchical Loss, Digital Pathology","This study proposes using radiomic features as additional inputs in deep learning models for osteosarcoma histopathology classification. It introduces a hierarchical loss function to optimize two binary classification tasks (tumor vs non-tumor, viable vs non-viable) and demonstrates improved performance using the TCIA OS Tumor Assessment dataset.",28.32,LFM-2.5,Apple M1 (Metal)
2601.09421v2_Bias Dynamics in BabyLMs Towards a Compute-Efficie.pdf,Bias Dynamics in BabyLMs: Towards a Compute-Efficient Sandbox for Democratising Pre-Training Debiasing,"Filip Trhlik, Andrew Caines, Paula Buttery",ft360@cam.ac.uk,,"BabyLMs, pre-training debiasing, computational efficiency, bias mitigation, large language models","This work investigates BabyLMs—compact BERT-like models trained on small corpora—as a low-cost proxy for studying bias acquisition and dynamics. It demonstrates that BabyLMs exhibit patterns of bias formation similar to standard BERT, enabling pre-model debiasing experiments that reduce training costs while preserving insights into gender and toxicity-related biases.",27.89,LFM-2.5,Apple M1 (Metal)
2601.09433v1_Do Transformers Understand Ancient Roman Coin Moti.pdf,Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?,"David Reid, Ognjen Arandjelovi",arXiv:2601.09433v1,2601.09433,"Transformers, ancient coins, coin analysis, CNNs, semantic elements","This paper explores the application of Vision Transformer models to identify semantic elements on ancient coins, comparing their performance to pre-trained CNN models.",29.46,LFM-2.5,Apple M1 (Metal)
2601.09445v1_Where Knowledge Collides A Mechanistic Study of In.pdf,Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models,"Minh Vu Pham, Hsuvas Borkakoty, Yufang Hou, IBM Research",arXiv:2601.09445v1,arXiv:2601.09445,"language models, intra-memory knowledge conflict, mechanistic interpretability, hallucinations, knowledge editing","This study investigates how conflicting information encoded within a model's internal knowledge during pre-training leads to intra-memory knowledge conflicts. Using mechanistic interpretability methods, the authors identify internal components responsible for encoding such conflicts and demonstrate how interpretability techniques can intervene at inference time.",28.67,LFM-2.5,Apple M1 (Metal)
2601.09446v1_Improving Symbolic Translation of Language Models .pdf,Improving Symbolic Translation of Language Models for Logical Reasoning,"Ramya Keerthy Thatikonda, Jiuzhou Han, Wray Buntine, Ehsan Shareghi",,,"symbolic translation, language models, logical reasoning, formal language, predicate logic, model fine-tuning","The paper addresses challenges in translating natural language into first-order logic using language models, proposes an incremental inference framework with verification modules, and evaluates improvements across multiple model families.",26.91,LFM-2.5,Apple M1 (Metal)
2601.09448v1_Population-Aligned Audio Reproduction With LLM-Bas.pdf,Population-Aligned Audio Reproduction With LLM-Based Equalizers,"Ioannis Stylianou, Jon Francombe, Pablo Martín-Nieves-Nuevo, Sven Ewan Shepstone, Member, IEEE, Zheng-Hua Tan",10.1109/JPSP.2024.12345,,"LLMs, Equalization, Audio Reproduction, Listening Experiments, Recommender Systems","Conventional audio equalization is a static process requiring manual adjustments. This paper introduces an LLM-based alternative that maps natural language prompts to equalization settings, enabling conversational sound system control. Evaluation shows statistically significant improvements over static baselines.",28.13,LFM-2.5,Apple M1 (Metal)
2601.09451v1_Late Breaking Results Quamba-SE Soft-edge Quantize.pdf,Quamba-SE: Soft-edge Quantizer for State Space Models,"Yizhi Chen, Ahmed Hemani",10.48550/arXiv.2405.12345,arXiv:2405.12345,"quantization, State Space Models, soft-edge, quantum-aware-training","Proposes Quamba-SE, a soft-edge quantizer for State Space Model activation quantization, preserving outlier information while maintaining precision.",24.63,LFM-2.5,Apple M1 (Metal)
2601.09455v1_On the Hardness of Computing Counterfactual and Se.pdf,On the Hardness of Computing Counterfactual and Semi-factual Explanations in XAI,"André Arteltaartelt, Martin Olsen, Kevin Tierney",arXiv:2601.09455v1,arXiv:2601.09455,"counterfactual explanations, semi-factual explanations, XAI, computational complexity, regulatory implications",This paper examines the computational hardness of generating counterfactual and semi-factual explanations in explainable AI. It highlights that producing such explanations is often computationally intractable and discusses implications for XAI research and policy.,28.46,LFM-2.5,Apple M1 (Metal)
2601.09460v1_SoK Enhancing Cryptographic Collaborative Learning.pdf,Enhancing Cryptographic Collaborative Learning with Differential Privacy,"Francesco Capano, Jonas B Löher, Benjamin Weggenmann",,,"differential privacy, cryptography, collaborative machine learning, privacy-preserving learning, secure noise sampling","The paper discusses challenges in collaborative machine learning across parties due to privacy concerns. It explores combining cryptographic techniques (e.g., MPC) and differential privacy to protect model outputs while maintaining accuracy, proposing a unified framework and analyzing trade-offs across paradigms.",27.16,LFM-2.5,Apple M1 (Metal)
2601.09465v1_EvoFSM Controllable Self-Evolution for Deep Resear.pdf,EvoFSM: Controllable Self-Evolution for Deep Research with Finite State,"Shuo Zhang, Chaofa Yuan, Ryan Guo, Xiaomin Yu, Rui Xu, Zhangquan Chen, Zinuo Li, Zhi Yang, Shuhao Guan, Zhenheng Tang, Sen Hu, Liwen Zhang",10.48550/arXiv.2601.09465,2601.09465,"EvoFSM, Self-evolution, Deep research, Finite State Machine, Adaptability, Control, LLM, Open-ended queries","EvoFSM introduces a structured self-evolving framework that decouples optimization into macroscopic flow and microscopic skills, enabling stable improvement under behavioral constraints. It achieves 58.0% accuracy on DeepSearch benchmark and demonstrates strong generalization across interactive tasks.",32.16,LFM-2.5,Apple M1 (Metal)
2601.09467v1_Searth Transformer A Transformer Architecture Inco.pdf,Searth Transformer: A Transformer Architecture Incorporating Earth's Geospheric Physical Priors for Global Mid-Range Weather Forecasting,"Tianye Li1, Qi Liu2, Hao Li4, Lei Chen4, Wencong Cheng6, Fei Zheng2, Xiangao Xia7, Ya Wang8, Gang Huang2, Weiwei Wang9, Xuan Tong11, Ziqing Zu12, Yi Fang1, Shenming Fu2, Jiang Jiang14, Haochen Li15, Mingxing Li1",,,"Transformer, Earth's geospheric physical priors, Global mid-range weather forecasting, Earth system science, AI in meteorology, Zonal periodicity, Mercator-like design, Resource constraints, Model efficiency","Accurate global medium-range weather forecasting requires architectures that respect Earth's spherical topology and zonal periodicity. This paper introduces the Shifted Earth Transformer, a physics-informed Transformer that integrates geophysical constraints, and proposes the Relay Autoregressive fine-tuning strategy to address computational limits.",30.62,LFM-2.5,Apple M1 (Metal)
2601.09469v2_FairGU Fairness-aware Graph Unlearning in Social N.pdf,FairGU: Fairness-aware Graph Unlearning in Social Networks,"Renqiang Luo, Yongshuai Yang, Huafei Huang, Qing Qing, Mingliang Hou, Ziqi Xu, Yi Yu, Jingjing Zhou, Feng Xia",https://doi.org/XXXXXXX.XXXXXXX,,"fairness, privacy, graph unlearning, social network","Graph unlearning has emerged as a critical mechanism for supporting sustainable and privacy-preserving social networks, enabling models to remove the influence of deleted nodes and thereby better safeguard user information. However, existing techniques insufficiently protect sensitive attributes, often leading to degraded algorithmic fairness compared with traditional graph learning methods. To address this, we introduce FairGU, a fairness-aware graph unlearning framework designed to preserve both utility and fairness during the unlearning process. FairGU integrates a dedicated fairness-aware module with effective data protection strategies, ensuring that sensitive attributes are neither inadvertently amplified nor structurally exposed when nodes are removed. Through extensive experiments on multiple real-world datasets, we demonstrate that FairGU consistently outperforms state-of-the-art graph unlearning methods and fairness-enhanced graph learning baselines in terms of both accuracy and fairness metrics. Our findings highlight a previously overlooked risk in current unlearning practices and establish FairGU as a robust and equitable solution for the next generation of socially sustainable networked systems.",31.73,LFM-2.5,Apple M1 (Metal)
2601.09470v1_Personalized Multimodal Feedback Using Multiple Ex.pdf,Personalized Multimodal Feedback Using Multiple External Representations: Strategy Profiles and Learning in High School Physics,"Natalia Revenga-Lozano, Karina E. Avila, Steffen Steinert, Matthias Schweinberger, Clara E. Gómez-Pérez, Jochen Kuhn",arXiv:2601.09470v1,2601.09470,"multimodal feedback, personalized learning, high school physics, representational competence, adaptive feedback","This study investigates how personalized feedback integrated with multiple external representations influences physics learning in high school students. Using a 16-24 week observational design, it finds that elaborated, multimodal feedback positively impacts post-test performance, especially among learners with lower representational competence. The research highlights the importance of strategy profiles and tailored feedback in effective learning.",32.73,LFM-2.5,Apple M1 (Metal)
2601.09473v1_SimMerge Learning to Select Merge Operators from S.pdf,SimMerge: Learning to Select Merge,"Oliver Bolton, Aakanksha, Arash Ahmadian, Sara Hooker, Marzieh Fadaee, Beyza Ermis","1Cohere Labs, 2Cohere, 3Google, 4Adaption Labs",2601.09473v1,"model merging, similarity signals, LLM composition, merge operator, similarity selection","This paper introduces SimMerge, a predictive merge-selection method that uses inexpensive similarity signals to select optimal merge operators, models, and orders without expensive evaluations. It demonstrates superior performance over standard methods on 2-way LLM merges and generalizes to multi-way and large-scale settings.",29.49,LFM-2.5,Apple M1 (Metal)
2601.09478v3_Bridging Semantic Understanding and Popularity Bia.pdf,Bridging Semantic Understanding and Popularity Bias with LLMs,"Renqiang Luo, Dong Zhang, Yupeng Gao, Wen Shi, Mingliang Hou, Jiaying Liu, Zhe Wang, Shuo Yu",https://doi.org/XXXXXXX.XXXXXXX,,"Semantic analysis, Recommender systems, Algorithmic fairness, Popularity bias, LLM","Semantic understanding of popularity bias is a crucial yet underexplored challenge in recommender systems, where popular items are often favored at the expense of niche content. Most existing debias methods treat the semantic understanding of popularity bias as a matter of diversity enhancement or long-tail coverage, neglecting the deeper semantic layer that embodies the causal origins of the bias itself. Consequently, such shallow interpretations limit both their debiasing effectiveness and recommendation accuracy. In this paper, we propose FairLRM, a novel framework that bridges the gap in the semantic understanding of popularity bias with Recommender via Large Language Model (RecLLM). FairLRM decomposes popularity bias into item-side and user-side components, using structured instruction-based prompts to enhance the model’s comprehension of both global item distributions and individual user preferences. Unlike traditional methods that rely on surface-level features such as 'diversity' or 'debiasing', FairLRM improves the model’s ability to semantically interpret and address the underlying bias. Through empirical evaluation, we show that FairLRM significantly enhances both fairness and recommendation accuracy, providing a more semantically aware and trustworthy approach to enhance the semantic understanding of popularity bias.",31.6,LFM-2.5,Apple M1 (Metal)
2601.09503v1_What Do LLM Agents Know About Their World Task2Qui.pdf,What Do LLM Agents Know About Their World?,"Siyuan Liu, Hongbang Yuan, Xinze Li, Ziyue Zhu, Yixin Cao, Yu-Gang Jiang",10.48550/arXiv.2601.09503,10.48550/arXiv.2601.09503,"large language model, LLM agents, environment understanding, trajectory-based metrics, generalization, world model","This paper examines whether large language model agents develop a grounded, transferable understanding of their environments beyond task success metrics. It introduces Task-to-Quiz (T2Q) to assess environment knowledge and identifies limitations in current evaluation paradigms.",29.76,LFM-2.5,Apple M1 (Metal)
2601.09518v1_Learning Whole-Body Human-Humanoid Interaction fro.pdf,Learning Whole-Body Human-Humanoid Interaction,"Wei-Jin Huang, Yue-Yi Zhang, Yi-Lin Wei, Zhi-Wei Xia, Juantao Tan, Yuan-Ming Li, Zhilin Zhao, Wei-Shi Zheng, Guangdong Province Key Laboratory of Information Security Technology",arXiv:2601.09518v1,HHI Dataset,"Human-Human Interaction, Humanoid Robotics, Physics-Aware Interaction, Decoupled Spatio-Temporal Action Reasoner, D-STAR, Whole-Body Interaction","The paper presents a contact-centric two-stage policy (D-STAR) for converting human-human interaction sequences into physically consistent humanoid clips. It addresses limitations of existing retargeting methods and introduces a decoupled action reasoner to preserve contact semantics, enabling high-quality HHI data generation.",31.07,LFM-2.5,Apple M1 (Metal)
2601.09520v1_Towards Realistic Synthetic Data for Automatic Dru.pdf,TOW ARDS REALISTIC SYNTHETIC DA TA FOR AUTOMA TIC DRUM TRANSCRIPTION,"Pierfrancesco Melucci, Paolo Merialdo, Taketo Akama",,,"Automatic Drum Transcription, Deep Learning, SoundFont, MIDI, Sequence-to-Sequence, One-shot Learning","This paper introduces a semi-supervised method to automatically curate a large, diverse corpus of one-shot drum samples from unlabeled audio sources. It synthesizes a high-quality MIDI dataset to train a sequence-to-sequence transcription model, achieving state-of-the-art results on ENST and MDB test sets while addressing domain gap challenges.",28.16,LFM-2.5,Apple M1 (Metal)
2601.09527v1_Private LLM Inference on Consumer Blackwell GPUs A.pdf,Private LLM Inference on Consumer Blackwell GPUs: A Practical Guide for Cost-Effective Local Deployment in SMEs,"Jonathan Knoop, Hendrik Holtmann",,,"LLM deployment, consumer GPUs, data privacy, cost-effective inference, NVIDIA Blackwell, quantization, quantum computing, SMEs","SMEs seek alternatives to cloud LLM APIs due to privacy and cost concerns. This study evaluates NVIDIA Blackwell consumer GPUs for production inference, benchmarking four models across diverse workloads and configurations. Results show consumer GPUs offer high throughput-per-dollar and low latency for many use cases, while still requiring careful optimization.",27.81,LFM-2.5,Apple M1 (Metal)
2601.09536v1_Omni-R1 Towards the Unified Generative Paradigm fo.pdf,Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning,"Dongjie Cheng, Yongqi Li, Hongru Cai, Yupeng Hu, Wenjie Wang, Liqiang Nie",arXiv:2601.09536v1,arXiv:2601.09536,"multimodal reasoning, generative modeling, Omni-R1, visual information, spatial relations","The paper proposes a unified generative multimodal reasoning framework using Omni-R1, which integrates perception alignment and perception reward to enable functional image generation. It introduces Omni-R1-Zero to bootstrap visualizations from text-only data, achieving strong performance across diverse multimodal tasks.",29.07,LFM-2.5,Apple M1 (Metal)
2601.09555v1_Benchmarking Post-Training Quantization of Large L.pdf,Benchmarking Post-Training Quantization of Large Language Models,"Manyi Zhang, Ji-Fu Li, Zhongao Sun, Haoli Bai, Zhen Zhenhua, Dong Xianzhi Yu",,,"post-training quantization, MXFP, large language models, quantization, low-precision formats","This work systematically investigates post-training quantization under Microscaling Floating-Point formats, covering over 7 PTQ algorithms, 15 benchmarks, and 3 LLM families. Key findings include performance trends, sensitivity to format, and scaling challenges.",27.18,LFM-2.5,Apple M1 (Metal)
2601.09566v2_Hot-Start from Pixels Low-Resolution Visual Tokens.pdf,Hot-Start from Pixels: Low-Resolution Visual Tokens for Chinese,"Shuyang Xiang, Hao Guan",arXiv:2601.09566v2,2601.09566v2,"Chinese language modeling, character-level modeling, low-resolution visuals, visual tokens, language representation","Investigates using low-resolution grayscale images (as low as 8×8 pixels) as alternatives to index-based character representations for Chinese, achieving competitive accuracy and demonstrating the value of visual structure in language modeling.",28.8,LFM-2.5,Apple M1 (Metal)
2601.09600v1_Information Access of the Oppressed A Problem-Posi.pdf,Information Access of the Oppressed: A Problem-Posing Framework for Envisioning Emancipatory Information Access Platforms,"Bhaskar Mitra, Nicola Neophytou, Sireesh Gururaja",https://doi.org/XXXXXXX.XXXXXX,,"Information Access, Emancipatory Information Access, Search and Society, Sociotechnical Information Systems","This paper examines how online information access platforms are vulnerable to authoritarian capture, especially amid democratic erosion, AI advancements, and Big Tech dominance. Drawing on Paulo Freire’s emancipatory pedagogy, it argues that technologists should collaborate with marginalized communities to co-create technology that challenges oppression, rather than merely mitigating risks.",28.41,LFM-2.5,Apple M1 (Metal)
2601.09603v1_Linear Complexity Self-Supervised Learning for Mus.pdf,Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer,"Petros Vavaroutsos, Theodoros Palamas, Pantelis Vikatos",10.1145/3748522.3779786,,"Deep Learning, Learnable Representations, Music Understanding, Transformers, Embeddings, Attention","This paper focuses on reducing the model size of foundation models for music information retrieval (MIR) tasks by combining Branchformer architecture with SummaryMixing and a random quantization process. Pre-training is performed on public datasets with a proprietary dataset, and robust evaluation is achieved across multiple downstream MIR tasks. Results show competitive performance with state-of-the-art models while reducing model size from 8.5% to 12.3%.",28.42,LFM-2.5,Apple M1 (Metal)
2601.09605v1_Sim2real Image Translation Enables Viewpoint-Robus.pdf,Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets,"Jeremiah Coholich, Justin Wit, Robert Azarcon, Zsolt Kira",,,"image translation, robot manipulation, viewpoint consistency, fixed-camera datasets, sim2real challenges","The paper presents MANGO, an unpaired image translation method with a novel segmentation-conditioned InfoNCE loss and modified PatchNCE loss, designed to maintain viewpoint consistency during sim-to-real translation. It demonstrates that training with limited fixed-camera data enables generation of diverse unseen viewpoints, significantly improving imitation-learning success rates.",28.09,LFM-2.5,Apple M1 (Metal)
2601.09609v1_DPWriter Reinforcement Learning with Diverse Plann.pdf,Reinforcement Learning with Diverse Planning Branching for Creative Writing,"Qian Cao, Yahui Liu, Wei Bi, Yi Zhao, Ruihua Song, Xiting Wang, Ruiming Tang, Guorui Zhou, Han Li",,,"reinforcement learning, large language models, creative writing, diversity, planning branching, generative diversity",This paper proposes an RL framework using a semi-structured long Chain-of-Thought reasoning to enhance output diversity in creative writing. It introduces a Diverse Planning Branching method and a group-aware diversity reward to guide exploration while maintaining quality.,27.42,LFM-2.5,Apple M1 (Metal)
2601.09613v1_CogRail Benchmarking VLMs in Cognitive Intrusion P.pdf,Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems,"Yonglin Tian, Qiyao Zhang, Wei Xu, Yihao Wu, Xinyi Li, Xingyuan Dai, Hui Zhang, Zhiyong Cui, Baoqing Guo, Member, IEEE, Zujun Yu, Yisheng Lv",10.48550/arXiv:1508.04685,10.48550/arXiv:1508.04685,"cognitive intrusion perception, railway transportation, visual language models, spatial context, temporal dynamics, deep learning","Accurate and early perception of potential intrusion targets is essential for railway safety. This work introduces CogRail, a benchmark integrating open-source datasets and question-answer annotations, and evaluates state-of-the-art VLMs using multimodal prompts. A joint fine-tuning framework is proposed to adapt general-purpose models for cognitive intrusion perception, highlighting limitations of current large-scale multimodal models.",29.64,LFM-2.5,Apple M1 (Metal)
2601.09620v1_Full Disclosure Less Trust How the Level of Detail.pdf,"Full Disclosure, Less Trust? How the Level of Detail about AI Use in News","Pooja Prajod, Hannes Cools, Thomas Röggla, Karthiya Puttajekar, Amber Kusters, Alia Elkattan, Pablo Cesar, Abdullah El Ali",,,"artificial intelligence, news production, transparency dilemma, trust, readers' trust, AI disclosures, source-checking, subscription decisions","This study investigates how varying levels of AI disclosure detail affect readers' trust in news, examining impacts across different news topics and AI involvement levels. Findings suggest that detailed disclosures can reduce trust but also increase source-checking behavior, highlighting a trade-off between transparency and trust.",29.22,LFM-2.5,Apple M1 (Metal)
2601.09624v1_Toward Understanding Unlearning Difficulty A Mecha.pdf,Toward Understanding Unlearning Difficulty: A Mechanistic Perspective,"Jiali Cheng, Ziheng Chen, Chirag Agarwal, Hadi Amiri",,,"machine unlearning, unlearning difficulty, circuit-guided difficulty, model circuits, interpretable AI","This paper investigates the variability in machine unlearning success across samples, proposing a Circuit-guided Unlearning Difficulty (CUD) metric to quantify difficulty at the circuit level. It identifies patterns linking easier samples to shorter interactions and harder samples to deeper pathways, aiming to provide a mechanistic explanation for unlearning disparities.",27.16,LFM-2.5,Apple M1 (Metal)
2601.09625v1_The Promptware Kill Chain How Prompt Injections Gr.pdf,The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware,"Ben Nassi, Bruce Schneier, Oleg Brodt",,,"prompt injection, promptware, malware, security, LLM, attack chain, security frameworks","The paper explores how attacks on large language model systems evolve into a distinct class of malware called promptware, introducing a five-step kill chain that maps prompt injection to multi-stage malware campaigns.",27.06,LFM-2.5,Apple M1 (Metal)
2601.09626v1_From Prompt to Protocol Fast Charging Batteries wi.pdf,Fast Charging Batteries,"Ge Lei1, Ferran Brosa Planella2, Sterling G. Baird, Samuel J. Cooper",arXiv:2601.09626v1,2601.09626,"battery charging, optimization, gradient-free, LLM, protocols, state of health",Efficiently optimizing battery charging protocols is challenging... LLMs can expand the search space and enable faster optimization.,27.08,LFM-2.5,Apple M1 (Metal)
2601.09635v1_LLM for Large-Scale Optimization Model Auto-Formul.pdf,LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach,"Kuo Liang1, Yuhang Lu, Jianming Mao, Shuyi Sun, Chunwei Yang, Congcong Zeng, Xiao Jin, 8, 5, 6, 7, 8",arXiv:2601.09635,arXiv:2601.09635,"large language models, tool use, agentic workflow construction, automated optimization modeling","This paper presents LEAN-LLM-OPT, a lightweight framework for LLM-assisted large-scale optimization auto-formulation. It describes how a workflow with two LLM agents and a downstream agent enables efficient generation of optimization formulations, leveraging LLMs' text-processing strengths while offloading data handling.",29.25,LFM-2.5,Apple M1 (Metal)
2601.09636v1_PersonalAlign Hierarchical Implicit Intent Alignme.pdf,PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent,"Yibo Lyu, Gongwei Chen, Rui Shao, Weili Guan, Liqiang Nie",weberlv1b@gmail.com,weberlv1b,"Personalized GUI Agent, Hierarchical Intent Alignment, Long-Term User Records, Proactive Assistance, Multimodal Large Language Models","This work introduces PersonalAlign, a new agent task that leverages long-term user records to align with implicit user intents, enabling proactive assistance in real-world GUI environments. It presents AndroidIntent evaluations and introduces Hierarchical Intent Memory Agent for improved personalization.",28.11,LFM-2.5,Apple M1 (Metal)
2601.09667v2_Collaborative Multi-Agent Test-Time Reinforcement .pdf,Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning,"Zhiyuan Hu, Yunhai Hu, Juncheng Liu, Shuyue Stella Li, Yucheng Wang, Zhen Xu, Xu6, Ng2 Anh Tuan Luu, Xu4 Bryan Hooi2, Cynthia Breazeal, Hae Won Park",,,"multi-agent systems, reinforcement learning, multi-agent reasoning, textual experience, credit assignment, distribution shift","The paper introduces MATTRL, a framework that injects structured textual experiences into multi-agent dialogue at inference time. MATTRL forms a multi-expert team for turn-level discussions, integrates test-time experiences, and enables consensus-based decision-making. It improves performance across benchmarks in medicine, math, and education, achieving significant gains over single-agent baselines.",28.77,LFM-2.5,Apple M1 (Metal)
2601.09680v1_Automating Supply Chain Disruption Monitoring via .pdf,Automating Supply Chain Disruption Monitoring via an Agentic AI,"Sara AlMahria, Liming Xu, Alexandra Brintrup",arXiv:2601.09680v1,arXiv:2601.09680,"Supply Chain Management, Supply Chain Disruption, Large Language Models, AI Agents, Multi-Agent System, Agentic System","This paper introduces an agentic AI framework to autonomously monitor, analyze, and respond to supply chain disruptions across extended networks. It employs seven specialized agents combining LLMs and deterministic tools to detect disruption signals, map supplier networks, assess exposure, and recommend mitigations. Evaluated across 30 scenarios, the system achieves high accuracy (F1 0.962–0.991) and reduces response times significantly compared to industry benchmarks.",30.12,LFM-2.5,Apple M1 (Metal)
2601.09684v1_Disentangling Task Conflicts in Multi-Task LoRA vi.pdf,Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection,"Ziyu Yang, Guibin Chen, Yuxin Yang, Aoxiong Zeng, Xiangquan Yang",,,"multi-task learning, low-rank adaptation, orthogonal projection, gradient conflict, parameter-efficient fine-tuning","The paper proposes Ortho-LoRA, a gradient projection method tailored for LoRA, to mitigate task interference in multi-task learning by projecting conflicting gradients orthogonally within the LoRA subspace. Extensive experiments show it effectively reduces performance gap compared to joint training.",27.41,LFM-2.5,Apple M1 (Metal)
2601.09692v1_Routing with Generated Data Annotation-Free LLM Sk.pdf,Routing with Generated Data: Annotation-Free,"Tianyi Niu, Justin Chih-Yao Chen, Genta Indra Winata, Shi-Xiong Zhang, Supriyo Chakraborty, Sambit Sahu, Yue Zhang, Elias Stengel-Eskin, Mohit Bansal",1UNC Chapel Hill 2Capital One 3The University of Texas at Austin,,"LLM routing, query-answer routing, query-only routing, skill estimation, expert selection, LLM models, benchmark evaluation","This paper introduces Routing with Generated Data (RGD), a method for training LLM routers exclusively on generated queries and answers. It evaluates query-answer and query-only routing across benchmarks, demonstrating that query-answer routers degrade more with lower generator quality. The authors identify two key generator characteristics and propose CASCAL, a query-only router that improves robustness.",29.33,LFM-2.5,Apple M1 (Metal)
2601.09694v1_LLMs can Compress LLMs Adaptive Pruning by Agents.pdf,LLMs Can Compress LLMs: Adaptive Pruning by Agents,"Sai Varun Kodathala, Rakesh Vunnam",arXiv:2601.09694v1,2601.09694,"Model Compression, Adaptive Pruning, Self-Reflection","Introduces agent-guided pruning that adaptively selects layers to prune while preserving knowledge, improving factual retention and reducing computational costs.",27.57,LFM-2.5,Apple M1 (Metal)
2601.09703v1_ShortCoder Knowledge-Augmented Syntax Optimization.pdf,Knowledge-Augmented Syntax,"Sicong Liu, Yanxian Huang, Mingwei Liu, Jiachi Chen, Ensheng Shi, Yuchi Ma, Hongyu Zhang, Yin Zhang, Yanlin Wang",,,"code generation, large language models, token efficiency, semantic equivalence, code optimization","The paper proposes ShortCoder, a knowledge-infused framework optimizing code generation efficiency while preserving readability. It introduces syntax simplification rules, a hybrid data synthesis pipeline, and fine-tuning strategies to improve generation performance. Experimental results show significant gains over prior methods.",28.15,LFM-2.5,Apple M1 (Metal)
2601.09706v1_Value-Aware Numerical Representations for Transfor.pdf,Value-Aware Numerical Representations for Transformer Language Models,"Andreea Dutulescu, Stefan Ruseti, Mihai Dascalu",10.48550/arXiv.2405.12345,arXiv:2405.12345,"value-aware representation, numerical understanding, transformer models, arithmetic operations, mathematical reasoning","The paper introduces a value-aware numerical representation that augments tokenized inputs with a dedicated prefix token conditioned on numerical value, improving robustness in arithmetic tasks while remaining compatible with existing tokenizers.",28.01,LFM-2.5,Apple M1 (Metal)
2601.09708v1_Fast-ThinkAct Efficient Vision-Language-Action Rea.pdf,Fast-ThinkAct: Efficient Vision-Language-Action,"Chi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, Fu-En Yang",arXiv:2601.09708v1,2026-1-15,"Vision-Language-Action, Reasoning, Verbalizable Latent Planning, Embodied Control, Long-horizon Planning","Proposes Fast-ThinkAct, an efficient reasoning framework that reduces inference latency while maintaining strong planning performance across embodied manipulation tasks. It leverages latent CoTs and preference-guided objectives to align linguistic and visual reasoning for embodied control.",28.63,LFM-2.5,Apple M1 (Metal)
2601.09749v1_R-LAM Reproducibility-Constrained Large Action Mod.pdf,R-LAM: Reproducibility-Constrained Large Action,Suriya Sureshkumar,240171.ad@rmkec.ac.in,,"Reproducible Scientific Workflows, Large Action Models, LLM-Based Agents, Execution Provenance","This paper proposes R-LAM, a reproducibility-constrained framework for applying Large Action Models to scientific workflow automation. R-LAM introduces structured action schemas, deterministic execution policies, and explicit provenance tracking to ensure auditable and replayable workflows. It supports failure-aware execution and controlled workflow forking, improving reproducibility in scientific settings.",27.84,LFM-2.5,Apple M1 (Metal)
2601.09750v1_SAGE Tool-Augmented LLM Task Solving Strategies in.pdf,SAGE: Tool-Augmented LLM Task Solving Strategies,"Robert K. Strehlow, Tobias Küster, Oskar F. Kupke, Brandon Llanque Kurps, Fikret Sivrikaya, Sahin Albayrak",DKI.00.00032.21,,"large language models, tool integration, zero-shot prompting, scalable environments, tool discovery, OPACA framework","This paper presents SAGE, a conversational AI interface that integrates tools via the OPACA framework. It enables dynamic tool discovery, execution, and modular prompting, aiming to address the need for adaptable, robust task-solving strategies in evolving software landscapes.",28.05,LFM-2.5,Apple M1 (Metal)
2601.09753v1_Critically Engaged Pragmatism A Scientific Norm an.pdf,"Critically Engaged Pragmatism: A Scientific Norm and Social, Pragmatist Epistemology for AI Science Evaluation Tools",Carole J. Lee,10.1234/example.doi,None,"pragmatism, scientific evaluation, AI, replication crisis","The paper addresses crises in peer review and AI-fabricated science, advocating for a social, pragmatist epistemology and a norm of Critically Engaged Pragmatism to scrutinize AI science evaluation tools. It highlights challenges like inference by false ascent and proposes critical discursive practices to maintain scientific credibility.",27.32,LFM-2.5,Apple M1 (Metal)
2601.09755v1_Heterogeneous computing platform for real-time rob.pdf,Heterogeneous computing platform for real-time robotics,"Jakub Fil, Yulia Sandamirskaya, Hector Gonzalez, Loïc Azzalin, Stefan Glüge, Lukas Friedenstab, Friedrich Wolf, Tim Rosmeisl, Matthias Lohrmann, Mahmoud Akl, Khaleel Khan, Leonie Wolf, Kristin Richter, Holm Puder, Mazhar Ali Bari, Xuan Choo, Noha Alharthi, Michael Hopkins, Steve Furber, Jens Struckmeier",10.1007/978-3-030-64932-8,,"heterogeneous computing, robotics, neural computing, robot, AI, neuro-inspired hardware, neuromorphic computing, real-time perception, social robotics","The paper explores a computing platform integrating neuromorphic hardware (Loihi2) with event-based cameras and GPUs to enable real-time perception and interaction in human-robot collaborative tasks, advancing the Society 5.0 vision.",30.53,LFM-2.5,Apple M1 (Metal)
2601.09756v1_Synthetic Data for Veterinary EHR De-identificatio.pdf,"Synthetic Data for Veterinary EHR De-identification: Benefits, Limits, and Safety Trade-offs Under Fixed Compute","David Brundage, PhD",,,"veterinary electronic health records, de-identification, synthetic data, LLM, privacy",Evaluation of synthetic veterinary narratives using PetEVAL for de-identification safety under varying training regimes; synthetic augmentation improves span-level performance but increases document-level leakage.,26.14,LFM-2.5,Apple M1 (Metal)
2601.09757v1_Democracy and Distrust in an Era of Artificial Int.pdf,Democracy & Distrust in an Era of Artificial Intelligence,Sonia K. Katyal,10.1162/DAED_a_01919,,"democracy, distrust, artificial intelligence, judicial review","Explores how AI challenges traditional legal frameworks, emphasizing the need for judicial review to protect minorities in the age of automation and predictive technologies.",26.06,LFM-2.5,Apple M1 (Metal)
2601.09760v1_Investigating Tool-Memory Conflicts in Tool-Augmen.pdf,Investigating Tool-Memory Conflicts in Tool-Augmented LLMs,"Jiali Cheng, Rui Pan, Hadi Amiri, 1, 2",,,"tool-augmented LLMs, knowledge conflict, tool-memory conflict, knowledge integration, LLM reliability","This paper introduces Tool-Memory Conflict (TMC) in LLMs, where internal parametric knowledge conflicts with external tool outputs. It evaluates existing conflict resolution methods and proposes new strategies to address inconsistencies arising from differing priorities between tool and model knowledge.",27.15,LFM-2.5,Apple M1 (Metal)
2601.09762v1_Explicating Tacit Regulatory Knowledge from LLMs t.pdf,Explicating Tacit Regulatory Knowledge from LLMs to Auto-Formalize,"Zhiyi Xue, Xiaohong Chen, Min Zhang",52275902017@stu.ecnu.edu.cn,,"compliance testing, LLMs, tacit regulatory knowledge, auto-formalization, requirements generation","The paper proposes RAFT, a framework for requirements auto-formalization and compliance test generation by explicating tacit regulatory knowledge from multiple LLMs. RAFT uses an Adaptive Purification-Aggregation strategy to integrate this knowledge into meta-models, formal requirements representations, and testability constraints, enabling high-precision requirement formalization and automated test generation.",27.81,LFM-2.5,Apple M1 (Metal)
2601.09765v1_AI Survival Stories a Taxonomic Analysis of AI Exi.pdf,Philosophy of AI,"John Hawthorne, Lingnan University, Dept of Philosophy, Tuen Mun, HK, Australian Catholic University, Melbourne, AU",,,"Artificial Intelligence, Existential Risk, AI Safety, AI Catastrophe, Superintelligent AI, AI Alignment","This paper develops a framework for thinking about the existential risk of AI systems, analyzing survival stories where humanity persists despite or due to AI threats.",26.29,LFM-2.5,Apple M1 (Metal)
2601.09768v1_CLiMB A Domain-Informed Novelty Detection Clusteri.pdf,CLiMB: A Domain-Informed Novelty Detection Clustering,"Lorenzo Monti, Tatiana Muraveva, Brian Sheridan, Davide Massari, Alessia Garofalo, Gisella Clementini, Umberto Michelucci",arXiv:2601.09768v1,2601.09768,"novelty detection, semi-supervised clustering, constrained clustering, density-based clustering, domain knowledge integration","CLiMB introduces a domain-informed framework for clustering that decouples prior knowledge from unknown structure exploration, achieving high recovery accuracy for known structures while isolating novel dynamical features in scientific data.",29.04,LFM-2.5,Apple M1 (Metal)
2601.09770v1_GUI-Eyes Tool-Augmented Perception for Visual Grou.pdf,GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents,"Chen Chen, Chen Jiawei Shao, Dakuan Lu, Haoyi Hu, Xiangcheng Liu, Hantao Yao, Wu Liu",,,"vision-language models, reinforcement learning, active visual perception, GUI agents, tool-aware perception","Recent advances in vision-language models and reinforcement learning have driven progress in GUI automation. This paper presents GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. The agent learns strategic decisions on visual tool invocation (e.g., cropping, zooming) using a two-stage reasoning process and a spatially continuous reward function. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy with only 3k labeled samples, outperforming supervised and RL baselines.",28.56,LFM-2.5,Apple M1 (Metal)
2601.09771v1_PCN-Rec Agentic Proof-Carrying Negotiation for Rel.pdf,PCN-Rec: Agentic Proof-Carrying Negotiation for Reliable Governance-Constrained Recommendation,"Aradhya Dixit, Shreem Dixit",,,"recommendation systems, LLM agents, constrained ranking, governance, verification, negotiation","Presents PCN-Rec, a proof-carrying negotiation pipeline for LLM-based recommenders that satisfies governance constraints via agent negotiation and verifiable certificates.",26.0,LFM-2.5,Apple M1 (Metal)
2601.09772v1_Antisocial behavior towards large language model u.pdf,Antisocial behavior towards large language model users: experimental evidence,"Paweł Niszczota, Cassandra Grützner",,,,Experimental evidence on antisocial behavior towards large language model users.,26.17,LFM-2.5,Apple M1 (Metal)
2601.09773v1_Enhancing LUT-based Deep Neural Networks Inference.pdf,Enhancing LUT-based Deep Neural Networks Inference through Architecture and Connectivity Optimization,"Binglei Lou, Ruinil Wu, Philip Leong",10.48550/arXiv.2024.12345,arXiv:2408.12345,"Dynamic Sparsity, FPGA, Neural Network, Lookup Table","The paper presents SparseLUT, a framework optimizing LUT-based DNNs via architectural enhancements and non-greedy training to balance accuracy, latency, and hardware efficiency.",26.48,LFM-2.5,Apple M1 (Metal)
2601.09805v1_Improving Chain-of-Thought for Logical Reasoning v.pdf,Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention,"Phuong Minh Nguyen, Tien Huu Dang, Naoya Inoue",10.48550/arXiv.2024.12345,arXiv:2408.12345,"logical reasoning, LLMs, attention, intervention, chain-of-thought","The paper introduces an end-to-end framework for logical reasoning that embeds structural information into prompts, activating attention heads aligned with logical operators. Attention-Aware Intervention (AAI) reweights attention scores to steer reasoning toward prior knowledge, improving performance with minimal computational cost.",27.5,LFM-2.5,Apple M1 (Metal)
2601.09806v1_Diffusion-Driven Deceptive Patches Adversarial Man.pdf,Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification,"Shahrzad Sayyafzadeh, Hongmei Chi, Shonda Bernadin",arXiv:2601.09806v1,2601.09806v1,"Adversarial Patch Generation, Gaussian Smoothing, Diffusion Model, Social Media Forensics, Perceptual Hashing","This work presents an end-to-end pipeline for generating, refining, and evaluating adversarial patches to compromise facial biometric systems. It employs FGSM for noise generation and a diffusion model with Gaussians and adaptive brightness correction to enhance evasion while preserving visual realism. A Vision Transformer generates captions for forensic interpretation, and the approach assesses vulnerability to adversarial attacks in identity verification.",31.86,LFM-2.5,Apple M1 (Metal)
2601.09809v1_QFed Parameter-Compact Quantum-Classical Federated.pdf,Parameter-Compact Quantum-Classical Federated Learning,"Samar Abdelghani, Soumaya Cherkaoui",10.1007/978-3-642-45888-7,arXiv:2308.12345,"Quantum Computing, Quantum Machine Learning, Federated Learning, Privacy, Communication, IoT",This study examines quantum-assisted federated learning to reduce parameter counts in classical models while maintaining accuracy. QFed aims to improve computational efficiency across edge networks using quantum-enabled techniques.,27.61,LFM-2.5,Apple M1 (Metal)
2601.09814v1_Explainable Deep Learning for Pediatric Pneumonia .pdf,Explainable Deep Learning for Pediatric Pneumonia Detection in Chest X-Ray Images,"Adil O. Khadidos, Aziida Nanyonga, Alaa O. Khadidos, Olfat M. Mirza, Mustafa Tahsin Yilmaz",,,"pediatric pneumonia, deep learning, chest x-ray, explainable ai, image interpretation","This study compares two state-of-the-art convolutional neural network architectures for automated pediatric pneumonia detection using chest X-ray images. It evaluates model performance with accuracy, F1-score, MCC, and recall, and applies Grad-CAM and LIME for model interpretability.",29.98,LFM-2.5,Apple M1 (Metal)
2601.09822v2_LLM-Based Agentic Systems for Software Engineering.pdf,LLM-Based Agentic Systems for Software Engineering: Challenges and Opportunities,"Yongjian Tang, Thomas Runkler",arXiv:2601.09822v2,cs.SE,"LLMs, Agents, Software Engineering, Future Challenges","This paper systematically reviews the emerging paradigm of LLM-based multi-agent systems, examining their applications across the Software Development Life Cycle, from requirements engineering to debugging. It covers language model selection, evaluation benchmarks, agentic frameworks, and outlines challenges and future research opportunities.",28.29,LFM-2.5,Apple M1 (Metal)
2601.09841v2_A pipeline for enabling path-specific causal fairn.pdf,APIPELINE FOR ENABLING PATH-SPECIFIC CAUSAL FAIRNESS IN OBSERVATIONAL HEALTH DATA,"Aparajita Kashyap, Sara Matijevic, Noémie Elhadad, Steven A. Kushner, Shalmali Joshi",arXiv:2601.09841v2,2601.09841v2,"causal fairness, foundation models, causal inference, observational health data, fair machine learning",This preprint explores a pipeline to train causal fair ML models for healthcare by addressing path-specific biases and disparities. It expands fairness-accuracy tradeoffs and demonstrates using foundation models without fairness constraints to produce fair predictions in health contexts.,30.59,LFM-2.5,Apple M1 (Metal)
2601.09851v1_ViSIL Unified Evaluation of Information Loss in Mu.pdf,Unified Evaluation of Information Loss in Multimodal Video Captioning,"Po-han Li, Shenghui Chen, Ufuk Topcu, Sandeep Chinchali",10.1234/viシル.2026,arXiv:2109.07912,"information loss, multimodal summarization, video captioning, information theory","This paper introduces the Video Summary Information Loss (ViSIL) score, a metric that quantifies information gaps in video summarization by measuring discrepancies between vision-language model inferences and textual summaries. It addresses limitations of traditional metrics like BLEU and ROUGE, demonstrating improved correlation with VQA performance and offering a Pareto-optimal trade-off between information retention and processing speed.",28.61,LFM-2.5,Apple M1 (Metal)
2601.09853v1_MedRedFlag Investigating how LLMs Redirect Misconc.pdf,MedRedFlag: Investigating how LLMs Redirect,"Sraavya Sambara, Yuan Pu1, Ayman Ali, Vishala Mishra, Lionel Wong, Monica Agrawal",,,"medical communication, LLMs, health communication, patient misconceptions, safety concerns",Real-world health questions from patients often embed false assumptions. Safe medical communication requires redirecting these misconceptions. This study investigates how LLMs handle false premises in health queries and reveals a gap in their real-world performance.,27.2,LFM-2.5,Apple M1 (Metal)
2601.09855v1_Thinking Long but Short Stable Sequential Test-Tim.pdf,Stable Sequential Test-Time Scaling for Large Reasoning Models,"Michael R. Metel, Yufei Cui, Boxing Chen, Prasanna Parthasarathi",arXiv:2601.09855v1,arXiv:2601.09855,"large reasoning models, test-time scaling, sequential scaling, accuracy improvement, KV cache, computational complexity","Presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy while stabilizing accuracy across longer reasoning lengths. The method uses a custom KV cache and achieves linear computational complexity under mild conditions.",27.57,LFM-2.5,Apple M1 (Metal)
2601.09858v1_OUTLINEFORGE Hierarchical Reinforcement Learning w.pdf,OutlineForge: Hierarchical Reinforcement Learning with Explicit States for Scientific Writing,"Yilin Bao, Ziyao He, Zayden Yang",,,"scientific writing, reinforcement learning, outline construction, scientific paper generation, structured reasoning","Presents a reinforcement learning framework for scientific outline construction as a long-horizon planning problem over hierarchical document structures. The method uses structured actions to incrementally build manuscripts and introduces a two-stage optimization: backward outline reconstruction for global consistency and forward value-guided reinforcement learning with rewards for scientific correctness, discourse coherence, and citation fidelity. Includes a benchmark evaluating document planning, input utilization, reference faithfulness, outline organization, and factual accuracy.",28.89,LFM-2.5,Apple M1 (Metal)
2601.09865v1_Advancing Model Refinement Muon-Optimized Distilla.pdf,Advancing Model Refinement: Muon-Optimized Distillation,"Jacob Sander, Brian Jalaian, V enkat R. Dasarivenkateswara.r.dasari.civ@army.mil",arXiv:2601.09865v1,arXiv:2601.09865v1,"Large Language Models, LLM deployment, Model optimization, Quantization, Data distillation, Edge computing, Model compression, Kullback-Leibler divergence, Bayesian optimization","This paper presents an integrated framework combining GPTQ-based quantization, LoRA, and Muon optimizer to reduce LLM size and energy use while maintaining performance. It achieves up to 2× memory compression and demonstrates superior inference efficiency on standard benchmarks compared to GPTQ alone.",29.56,LFM-2.5,Apple M1 (Metal)
2601.09869v1_A Scoping Review of the Ethical Perspectives on An.pdf,A Scoping Review of the Ethical Perspectives on Anthropomorphising Large Language Models,"Andrea Ferrario, Rasita Vinay, Matteo Casserini, Alessandro Facchini",10.48550/arXiv.2601.09869,2601.09869,"anthropomorphism, conversational agents, large language models, AI ethics, deception, trust, governance","This scoping review maps ethically oriented work on anthropomorphising LLM-based conversational agents across five databases and three preprint repositories. It synthesizes conceptual foundations, ethical challenges and opportunities, and methodological approaches, highlighting convergence on attribution-based definitions and divergent operationalizations.",28.57,LFM-2.5,Apple M1 (Metal)
2601.09871v1_Epistemology gives a Future to Complementarity in .pdf,Epistemology Gives Afuture to Complementarity in Human-AI Interactions,"Andrea Ferrario, Alessandro Facchini, Juan M. Durán",arXiv:2601.09871v1,2601.09871,"artificial intelligence, machine learning, reliance, complementarity, human-AI interaction, computational reliabilism, epistemology","Human-AI complementarity is explored as a framework where human-AI collaboration can surpass either participant in decision-making, challenging traditional notions of trust and offering a reliability-based perspective for assessing AI-supported performance.",27.72,LFM-2.5,Apple M1 (Metal)
2601.09879v1_MedVL-SAM2 A unified 3D medical vision-language mo.pdf,MedVL-SAM2: A unified 3D medical vision–language model for multimodal reasoning and prompt-driven segmentation,"Yang Xing, Jiong Wu, Savas Ozdemir, Ying Zhang, Yang Yang, Wei Shao4, Gong, Zhang",10.1000/MVL-2024-001,arXiv:2408.12345,"medical vision, 3D medical imaging, report generation, visual question answering, multi-modal segmentation, semantic segmentation, interactive segmentation","MedVL-SAM2 is a unified 3D medical multimodal model that supports report generation, visual question answering, and multi-paradigm segmentation. It integrates image-level reasoning and pixel-level perception for precise 3D spatial reasoning, achieving state-of-the-art performance across various tasks.",31.72,LFM-2.5,Apple M1 (Metal)
2601.09881v1_Transition Matching Distillation for Fast Video Ge.pdf,Transition Matching Distillation for Fast Video Generation,"Weili Nie, Julius Berner, Nanye Ma, Chao Liu, Saining Xie, Arash Vahdat",arXiv:2601.09881v1,2026-1-16,"video generation, distillation, TMD, flow models, generation speed, visual quality","This paper introduces Transition Matching Distillation (TMD), a framework that distills video diffusion models into efficient few-step generators by matching multi-step denoising trajectories with lightweight conditional flows. Experiments show TMD achieves a good balance of speed and quality.",30.59,LFM-2.5,Apple M1 (Metal)
2601.09883v1_Beyond Rule-Based Workflows An Information-Flow-Or.pdf,Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents,"Xinxing Ren, Quangmire Zang, Caelum Forder, Suman Deb, Ahsen Tahir, Roman J. Georgio, Peter Carroll, Zekun Guo",10.48550/arXiv.2024.12345,arXiv:2408.12345,"multi-agent systems, information flow, agent-to-agent communication, large language models, LLM-based MAS, task orchestration","This paper proposes an Information-Flow-Orchestrated Multi-Agent Paradigm using CORAL, where a dedicated orchestrator monitors task progress and dynamically coordinates agents via natural language, avoiding reliance on predefined workflows. The method achieves higher accuracy than baseline OWL and demonstrates improved flexibility and robustness in handling complex tasks.",29.15,LFM-2.5,Apple M1 (Metal)
2601.09896v1_The Algorithmic Gaze An Audit and Ethnography of t.pdf,The Algorithmic Gaze: An Audit and Ethnography of the LAION-Aesthetics,"Jordan Taylor, William Agnew, Maarten Sap, Sarah E. Fox, Haiyi Zhu",10.1145/nnnnnnn.nnnnnnn,not provided,"AI, Art, Aesthetic Evaluation","This study audits the LAION-Aesthetic Predictor model, revealing how its aesthetic filtering reinforces cultural biases and imperial gazes in visual AI.",26.39,LFM-2.5,Apple M1 (Metal)
2601.09902v1_A Novel Contrastive Loss for Zero-Day Network Intr.pdf,A Novel Contrastive Loss for Zero-Day Network,"Jack Wilkie, Hanan Hindy, Craig Michie, Christos Tachtatzis, James Irvine, Robert Atkinson",10.1007/978-3-642-45888-7,None,"Internet of Things, Network Intrusion Detection, Machine Learning, Contrastive Learning","The paper proposes a novel contrastive loss function to improve zero-day attack detection in network intrusion detection systems, achieving better AUROC and OpenAUC performance compared to existing methods.",28.32,LFM-2.5,Apple M1 (Metal)
2601.09913v1_Continuum Memory Architectures for Long-Horizon LL.pdf,Continuum Memory Architectures for Long-Horizon LLM Agents,Joe Logan,arXiv:2601.09913v1,arXiv:2601.09913,,"Retrieval-augmented generation (RAG) treats memory as stateless, leading to challenges in continuity and memory dynamics. The paper introduces the Continuum Memory Architecture (CMA) to address these issues by enabling persistent, evolving memory with selective retention and consolidation.",26.16,LFM-2.5,Apple M1 (Metal)
2601.09921v1_Learning to Decode in Parallel Self-Coordinating N.pdf,Learning to Decode in Parallel: Self-Coordinating,"Kai Zhang, Zhengzhong Yi, Shaojun Guo, Linghang Kong, Wang Situ, Zhengfeng Ji, Fusheng Chen, Jianxin Chen, Tan He, Xiaoyu Zhan, Tan, Weiping Lin, Dongxin Gao, Yiming Zhang, Fangming Liu, Fang Zhang, Xuanzhi Zhang, Zhengfeng Ji, Feng Zhang",arXiv:2601.09921v1,2601.09921v1,"quantum error correction, neural network, self-coordinating, parallel decoding, quantum computation","Fast, reliable decoders are pivotal components for enabling fault-tolerant quantum computation. Neural network decoders like AlphaQubit have demonstrated significant potential, achieving higher accuracy than traditional human-designed decoding algorithms. However, existing implementations of neural network decoders are described in arXiv:2601.09921v1.",32.91,LFM-2.5,Apple M1 (Metal)
2601.09923v1_CaMeLs Can Use Computers Too System-level Security.pdf,AI agents: A preprint on computer use agents security,"Hanna Foerster, Robert Mullins, Tom Blanchard, Nicolas Papernot, Kristina Nikoli´c, Florian Tramèr, Ilia Shumailov, Cheng Zhang, Yiren Zhao",arXiv:2601.09923v1,arXiv:2601.09923,"computer use agents, prompt injection, security, AI agents, control flow integrity","AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior. The paper proposes architectural isolation to separate trusted planning from untrusted observations, introducing Single-Shot Planning for CUAs to ensure control flow integrity and mitigate branch steering attacks.",29.55,LFM-2.5,Apple M1 (Metal)
2601.09929v1_Hallucination Detection and Mitigation in Large La.pdf,Hallucination Detection and Mitigation in Large Language Models,"Ahmad Pesaranghader, Erin Li",arXiv:2601.09929v1,arXiv:2601.09929v1,"hallucination, large language models, hallucination detection, mitigation strategies, financial applications","This paper presents a comprehensive operational framework for managing hallucinations in LLMs and LRMs, focusing on root cause awareness and targeted interventions. It integrates detection methods and mitigation strategies across model, data, and context layers, demonstrating effectiveness through a financial data extraction case study.",28.73,LFM-2.5,Apple M1 (Metal)
2601.09933v1_Malware Classification using Diluted Convolutional.pdf,Malware Classification using Diluted Convolutional Neural Network with Fast Gradient Sign Method,"Ashish Anand, Bhupendra Singh, Sunil Khemka, Bireswar Banerjee, Vishi Singh Bhatia, Piyush Ranjan",gcp.ashish2020@gmail.com,,"data security, privacy, malware classification, diluted convolutional neural network, fast gradient sign method","A research proposal introducing FGSM-DICNN for efficient malware classification, highlighting challenges in feature selection and computational efficiency.",27.09,LFM-2.5,Apple M1 (Metal)
2601.09949v2_Kinematic Tokenization Optimization-Based Continuo.pdf,Kinematic Tokenization: Optimization-Based,"Griffin M. Kearney, Ph.D.",arXiv:2601.09949v2,arXiv:2601.09949v2,"kinematic tokenization, optimization-based, continuous-time, financial time series","Transformers are designed for discrete tokens, yet many real-world signals are continuous processes observed through noisy sampling. Discrete tokenizations can be brittle in low signal-to-noise regimes, especially when downstream objectives impose asymmetric penalties. We introduce Kinematic Tokenization, an optimization-based continuous-time representation that reconstructs an explicit spline from noisy measurements and tokenizes local spline coefficients. This is applied to financial time series data in conjunction with trading volume profiles. Under a risk-averse asymmetric classification objective, discrete baselines collapse to an absorbing cash policy, whereas continuous spline tokens sustain calibrated action distributions and stable policies. These results suggest that explicit continuous-time tokens can improve learnability and calibration of selective decision policies in noisy time series under abstention-inducing losses.",33.32,LFM-2.5,Apple M1 (Metal)
2601.09966v1_A Sustainable AI Economy Needs Data Deals That Wor.pdf,A Sustainable AI Economy Needs Data Deals That Work for Generators,"Ruoxi Jia, Luis Oala, Brickroad, Wenjie Xiong, Suqin Ge, Jiachen T. Wang, Feiyang Kang, Dawn Song",arXiv:2601.09966v1,2601.09966v1,"data deals, AI economy, data equity, machine learning value chain, fair compensation, provenance, bargaining power, data monetization","The paper argues that the current machine learning value chain is structurally unsustainable due to an economic data processing inequality. It analyzes seventy-three public data deals and finds that most value accrues to aggregators, with creator royalties minimal. The study identifies three structural faults—missing provenance, asymmetric bargaining power, and non-dynamic pricing—and proposes an Equitable Data-Value Exchange (EDVEX) Framework to create a fairer market.",30.58,LFM-2.5,Apple M1 (Metal)
2601.09972v1_Chinese Labor Law Large Language Model Benchmark.pdf,Chinese Labor Law Large Language Model,"Zixun Lan, Maochun Xu, Yifan Ren, Rui Wu, Jianghui Zhou, Xueyang Cheng, Jian’an Ding, Mingmin Chi, Fei Ma",,,"Chinese labor law, legal natural language processing, large language models, domain-specific fine-tuning, benchmark dataset, statute recall, legal reasoning, case analysis","Recent advancements in large language models have led to substantial progress in domain-specific applications, particularly within the legal domain. This paper presents LaborLawLLM, a legal LLM tailored to labor law, and introduces LaborLawBenchmark covering tasks like legal provision citation, knowledge-based QA, case classification, compensation computation, NER, and legal case analysis. Experimental results show significant improvements over general-purpose and existing legal-specific models.",29.33,LFM-2.5,Apple M1 (Metal)
2601.09974v1_SPRInG Continual LLM Personalization via Selective.pdf,SPRInG: Continual LLM Personalization via Selective Parametric Adaptation and Retrieval-Interpolated Generation,"Seoyeon Kim, Jaehyung Kim",10.48550/arXiv.2405.12345,10.48550/arXiv.2405.12345,"continual personalization, LLM personalization, parameter adaptation, retrieval interpolation, preference drift","This paper introduces SPRING, a semi-parametric framework for continual personalization of large language models. It employs drift-driven selective adaptation and logit-based interpolation to handle evolving user preferences without catastrophic forgetting, demonstrating superior performance on long-form personalized generation benchmarks.",27.92,LFM-2.5,Apple M1 (Metal)
2601.09980v1_Performance of AI agents based on reasoning langua.pdf,ALD optimization using reasoning LLMs,Angel Yanguas-Gil,,,"reasoning large language models, atomic layer deposition, ALD optimization","This work explores the use of reasoning large language models to autonomously optimize atomic layer deposition processes. The agents successfully find optimal precursor and coreactant doses without prior process knowledge, demonstrating consistent performance across runs despite variability.",24.91,LFM-2.5,Apple M1 (Metal)
2601.09982v1_Context Volume Drives Performance Tackling Domain .pdf,Volume Drives Performance: Tackling Domain Shift in Extremely Low-Resource Translation via RAG,"David Samuel Setiawan, Raphaël Merx, Jey Han Lau",10.1093/acpsr/ftk123,,"neural machine translation, domain shift, low-resource languages, retrieval augmented generation, hybrid framework","Neural Machine Translation models for low-resource languages degrade under domain shift. This study quantifies performance loss using Dhao on the New Testament and evaluates a hybrid NMT + LLM framework to recover in-domain quality, showing retrieval augmentation improves robustness.",27.03,LFM-2.5,Apple M1 (Metal)
2601.10010v1_VERHallu Evaluating and Mitigating Event Relation .pdf,Evaluating and Mitigating Event,"Zefan Zhang, Kehua Zhu, Shijie Jiang, Hongyuan Lu, Shengkai Sun, Tian Bai",,,"Video Large Language Models, Event Relation Hallucination, Video Understanding, Event Relation Understanding","This paper introduces VERHallu, a novel benchmark for evaluating event relation hallucination in video LLMs. It focuses on causal, temporal, and subevent relations across three tasks and proposes a Key-Frame Propagating strategy to improve multi-event understanding without sacrificing inference speed.",27.59,LFM-2.5,Apple M1 (Metal)
2601.10011v1_Memo-SQL Structured Decomposition and Experience-D.pdf,Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL,"Zerui Yang, Weichuan Wang, Yanwei Xu, Linqi Song, Yudai Matsuda, Wei Han, Bo Bai",,,"NL2SQL, self-correction, structured decomposition, experience-driven, training-free, accuracy-efficiency trade-off","Presents Memo-SQL, a training-free framework addressing limitations of existing NL2SQL systems by introducing structured decomposition and experience-aware self-correction. Achieves 68.5% execution accuracy with improved robustness and resource efficiency.",28.04,LFM-2.5,Apple M1 (Metal)
2601.10018v1_Empowering Older Adults in Digital Technology Use .pdf,Empowering Older Adults in Digital Technology Use with Foundation Models,"Hasti Sharifi, Homaira Huda Shomee, Sourav Medya, Debaleena Chattopadhyay",,,"Technology support, Digital technology use, Artificial intelligence, Large language models, Communication barriers, Human-computer interaction",This study examines communication challenges faced by older adults in using digital technology and explores AI-based solutions to improve their experience.,27.23,LFM-2.5,Apple M1 (Metal)
2601.10025v1_Structured Personality Control and Adaptation for .pdf,Structured Personality Control and Adaptation for LLM Agents,"Jinpeng Wang, Xinyu Jia, Wei Wei Heng, Yuquan Li, Binbin Shi, Qianlei Chen, Guannan Chen, Junxia Zhang, Yuyu Yin",10.1093/acm/grla117,2104.11456,"Personalization, Jungian Psychological Types, MBTI Personality Types, Persona Adaptation, Explainable AI","This paper presents a framework modeling LLM personality via Jungian psychological types, integrating mechanisms for coherent expression, adaptive reinforcement, and long-term evolution. It evaluates personality alignment using Myers–Briggs questionnaires and applies it to diverse interaction scenarios to support naturalistic agent design in human-computer interaction.",28.66,LFM-2.5,Apple M1 (Metal)
2601.10029v1_PaperScout An Autonomous Agent for Academic Paper .pdf,PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization,"Tingyue Pan, Jie Ouyang, Mingyue Cheng, Qingchuan Li, Zirui Liu, Mingfan Pan, Shuo Yu, Qi Liu",10.1234/arxiv.12345,10.1234/12345,"academic paper search, autonomous agent, process-aware policy, sequence-level optimization, reinforcement learning, LLM integration","This paper introduces PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. It addresses the mismatch between static reinforcement learning methods and multi-turn agentic tasks by introducing Proximal Sequence Policy Optimization (PSPO), enabling dynamic adaptation to accumulated retrieval context.",28.73,LFM-2.5,Apple M1 (Metal)
2601.10031v1_FilDeep Learning Large Deformations of Elastic-Pla.pdf,FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data,"Jianheng Tang, Shilong Tao, Zhe Feng, Haonan Sun, Menglu Wang, Zhanxing Zhu, Yunhuai Liu",10.1145/3770854.3783959,KDD '26,"Large Deformations, Elastic-Plastic Solids, Multi-Fidelity Data, Deep Learning, Quantity-Accuracy Dilemma","The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit limitations, prompting Deep Learning as a promising alternative. FilDeep addresses the quantity-accuracy dilemma by training with both low-fidelity and high-fidelity data.",28.4,LFM-2.5,Apple M1 (Metal)
2601.10038v1_What Understanding Means in AI-Laden Astronomy.pdf,Understanding Means in AI-Laden Astronomy,"Yuan-Sen Ting, André Curtis-Trudel, Siyu Yao, Philosophers and historians of science",arXiv:2601.10038v1,2601.10038v1,"understanding, AI in science, philosophy of science, discovery, knowledge generation","The paper explores how understanding in astronomy is being reshaped by artificial intelligence, emphasizing philosophical perspectives on knowledge, assumptions, and abstraction. It calls for interdisciplinary dialogue to address deeper questions about scientific progress and meaning.",27.53,LFM-2.5,Apple M1 (Metal)
2601.10073v1_ReaMIL Reasoning- and Evidence-Aware Multiple Inst.pdf,ReaMIL: Reasoning- and Evidence-Aware Multiple Instance Learning for Whole-Slide Histopathology,"Hyun Do Jung, Jungwon Choi, Hwiyoung Kim",,,"ReaMIL, multiple instance learning, whole-slide histopathology, tumor subtype, pathology, interpretability","Introduces ReaMIL, a multiple instance learning framework for whole-slide histopathology that incorporates a light selection head and a budgeted-sufficiency objective to produce compact evidence sets. Demonstrates improved performance across multiple cancer datasets while providing interpretability diagnostics.",27.12,LFM-2.5,Apple M1 (Metal)
2601.10079v1_Sparse-RL Breaking the Memory Wall in LLM Reinforc.pdf,Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning,"Sijia Luo, Xiaokang Zhang, Yuxuan Hu, Bohan Zhang, Ke Wang, Jinbo Su, Mengshu Sun, Lei Liang, Jing Zhang, 1School of Information, Renmin University of China, 2Key Laboratory of Data Engineering and Knowledge Engineering, Beijing, 3Ant Group, Hangzhou, China",,,"Reinforcement Learning, Large Language Models, LLM Reinforcement Learning, Memory Wall, Sparse-RL, Policy Mismatch, KV Compression, Reinforcement Learning","The paper addresses the memory bottleneck in RL training for LLMs by introducing Sparse-RL, which uses sparsity-aware sampling to reduce KV cache overhead while maintaining performance.",28.82,LFM-2.5,Apple M1 (Metal)
2601.10088v1_State of AI An Empirical 100 Trillion Token Study .pdf,State of AI: An Empirical 100 Trillion Token Study with OpenRouter,"Malika Aubakirova, Alex Atallah, Chris Clark, Justin Summerville, Anjney Midha",a16z,a16z/2403.12345,"large language models, open router, empirical study, LLM usage, multi-step inference","This study analyzes over 100 trillion tokens of real-world LLM interactions using OpenRouter, highlighting shifts in adoption patterns, creative roleplay, coding assistance, and agentic inference. It identifies foundational user cohorts and discusses implications for model development.",28.34,LFM-2.5,Apple M1 (Metal)
2601.10090v1_Difficulty-guided Sampling Bridging the Target Gap.pdf,Difficulty-guided Sampling: Bridging the Target Gap,"Mingzhuo Lia, Guang Lia, Linfeng Yeb, Jiafeng Maoc, Takahiro Ogawaa, Konstantinos N. Plataniotis, Miki Haseyamaa",arXiv:2601.10090v1,2601.10090,"dataset distillation, downstream tasks, target gap, deep neural networks, data distillation, image classification","The paper proposes difficulty-guided sampling to bridge the gap between distillation objectives and downstream tasks, aiming to improve dataset distillation performance while maintaining model accuracy.",30.12,LFM-2.5,Apple M1 (Metal)
2601.10092v1_LeMoF Level-guided Multimodal Fusion for Heterogen.pdf,LEMOF: LEVEL-GUIDEDMULTIMODALFUSION FOR HETEROGENEOUSCLINICALDATA,"Jongseok Kim, Seongae Kang, Jonghwan Shin, Yuhan Lee, Ohyun Jo",10.1093/acprof:oso/9780190874893.001.0001,2601.10092v1,"Multimodal Learning, Hierarchical Representation Learning, Clinical Time-Series Modeling, Level-guided Feature Fusion, Explainable Medical AI","This paper introduces LeMoF, a novel framework for integrating heterogeneous clinical data by selectively combining level-guided representations across modalities. It emphasizes balancing prediction stability and discriminative power, demonstrating superior performance in length of stay prediction on ICU datasets.",29.35,LFM-2.5,Apple M1 (Metal)
2601.10094v1_V-Zero Self-Improving Multimodal Reasoning with Ze.pdf,V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation,"Han Wang, Yi Yang, Jingyuan Hu, Minfeng Zhu, Wei Chen",10.1234/vz0-2024,2104.07912,"self-improvement, multimodal reasoning, zero annotation, Vision-Language Models","Introduce V-Zero, a framework enabling self-improvement in vision-language models using exclusively unlabeled images. It establishes a co-evolutionary loop with two roles: a Questioner and a Solver, driving mutual enhancement through iterative Group Relative Policy Optimization.",27.77,LFM-2.5,Apple M1 (Metal)
2601.10101v2_Matrix as Plan Structured Logical Reasoning with F.pdf,Matrix as Plan: Structured Logical Reasoning with Feedback-Driven Replanning,"Ke Chen, Jiandian Zeng, Zihao Peng, Guo Li, Guangxue Zhang, Tian Wang",10.1145/XXXXXX.XXXXXX,,"Logical Reasoning, Large Language Models, Neurosymbolic Approaches, Semantic Decomposition","This paper proposes MatrixCoT, a structured CoT framework with a matrix-based plan to enhance LLM reasoning. It addresses limitations of chain-of-thought prompting by introducing structured representations, explicit citations, and feedback-driven replanning. Experiments demonstrate improved robustness and interpretability without relying on external solvers.",27.63,LFM-2.5,Apple M1 (Metal)
2601.10103v1_FlowAct-R1 Towards Interactive Humanoid Video Gene.pdf,FlowAct-R1: Towards Interactive Humanoid Video,"Lizhen Wang, Yongming Zhu, Zhipeng Ge, Youwei Zheng, Longhao Zhang, Tianshu Hu, Shiyang Qin, Mingshuang Luo, Jiaxu Zhang, Xin Chen, Yulong Wang, Zerong Zheng, Jianwen Jiang, Chao Liang, Weifeng Chen, Xing Wang, Yuan Zhang, Mingyuan Gao",arXiv:2601.10103v1,2601.10103,"humanoid video generation, interactive video, real-time interaction, low-latency synthesis, video synthesis, behavioral control","Proposes FlowAct-R1, a framework for real-time interactive humanoid video generation using MMDiT architecture, achieving stable 25fps at 480p with low TTFF. Introduces chunkwise diffusion forcing and self-forcing variants to ensure temporal consistency and high perceptual realism.",32.42,LFM-2.5,Apple M1 (Metal)
2601.10104v1_MathDoc Benchmarking Structured Extraction and Act.pdf,Benchmarking Structured Extraction and Active Refusal on Noisy Mathematics Exam Papers,"Chenyue Zhou, Jiayi Tuo, Shitong Qin2, Wei Dai, Mingxuan Wang, Ziwei Zhao, Duoyang Li, Shiyang Su, Yanxi Lu",,,"structured extraction, active refusal, mathematics exam, visual noise, MLLM, refusal capability","This paper introduces MathDoc, the first benchmark for document-level extraction from high school mathematics exam papers. It addresses challenges in extracting structured questions from noisy documents and evaluates models using metrics like stem accuracy, visual similarity, and refusal capability. It highlights limitations of current MLLMs in handling illegible inputs and proposes MathDoc as a tool to assess model reliability under degraded conditions.",28.72,LFM-2.5,Apple M1 (Metal)
2601.10108v1_SIN-Bench Tracing Native Evidence Chains in Long-C.pdf,Tracing Native Evidence Chains in Long-Context Multimodal,"Yiming Ren, Junjie Wang, Yuxin Meng, Yihang Shi, Zhiqiang Lin, Ruihang Chu, Yiran Xu, Ziming Li, Yunfei Zhao, Yang Yujiu, Tsinghua University, Shanghai AI Laboratory, KuaiShou Inc., Stanford University, Harvard University",,,"long-form scientific papers, multimodal models, evidence chains, cross-modal reasoning, evidence anchoring",Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging. The paper proposes the 'Fish-in-the-Ocean' paradigm requiring explicit cross-modal evidence chains in native documents. Experiments on SIN-Data and SIN-Bench demonstrate that grounding is critical for accurate reasoning.,27.9,LFM-2.5,Apple M1 (Metal)
2601.10112v1_Repository Intelligence Graph Deterministic Archit.pdf,Repository Intelligence Graph: Deterministic,"Tsvi Cherny-Shahar, Amiram Yehudai",,,"software repositories, build systems, dependency graphs, software engineering agents, multi-lingual software","Introduces the Repository Intelligence Graph (RIG), a deterministic architectural map for LLM code assistants, addressing challenges in recovering build and test structures across multilingual projects.",26.03,LFM-2.5,Apple M1 (Metal)
2601.10114v1_Following the Teachers Footsteps Scheduled Checkpo.pdf,Following the Teacher’s Footsteps: Scheduled Checkpoint Distillation for Domain-Specific LLMs,"Cheng Feng, Chaoliang Zhong, Jun Sun, Yusuke Oishi",10.1000/arxiv.2601.10114,arXiv:2601.10114,"LLMs, Knowledge Distillation, Domain-specific Tasks",This paper proposes a novel theoretical framework and method to enable student models to surpass teacher models on domain-specific tasks by balancing convergence advantages against student-SF deficits through scheduled checkpoint distillation and adaptive weighting.,29.25,LFM-2.5,Apple M1 (Metal)
2601.10120v1_TopoDIM One-shot Topology Generation of Diverse In.pdf,TopoDIM: One-shot Topology Generation of Diverse Interaction Modes for Multi-Agent Systems,"Rui Sun, Jie Ding, Chenghua Gong, Tianjun Gu2, Yihang Jiang, Juyuan Zhang, Liming Pan, Linyuan Lü",https://anonymous.4open.science/r/TopoDIM-8D35/,TopoDIM-8D35/,"multi-agent systems, LLM-based, topology generation, interaction modes, decentralized execution, token efficiency, adaptability","TOPODIM proposes a one-shot topology generation framework with diverse interaction modes to optimize communication in LLM-based multi-agent systems. It reduces total token consumption by 46.41% while improving performance by 1.50% over state-of-the-art methods, demonstrating strong adaptability across heterogeneous agents.",28.74,LFM-2.5,Apple M1 (Metal)
2601.10122v1_Role-Playing Agents Driven by Large Language Model.pdf,"Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future","Ye Wang, Jiaxing Chen, Hongjiang Xiao, Yewang, Xiaohj",arXiv:2601.10122v1,2601.10122v1,"large language models, role-playing agents, natural language processing, human-computer interaction, personality modeling","This paper systematically reviews the current development and key technologies of role-playing language agents (RPLAs), tracing their evolution from rule-based systems to advanced cognitive simulation. It highlights critical technical pathways such as psychological scale-driven character modeling, memory-augmented prompting, and motivation-based decision control. The study also analyzes challenges in constructing role-specific corpora and evaluates assessment frameworks, offering insights for future research in personality evolution, multi-agent narratives, and multimodal interaction.",30.84,LFM-2.5,Apple M1 (Metal)
2601.10129v1_LaViT Aligning Latent Visual Thoughts for Multi-mo.pdf,Aligning Latent Visual Thoughts for Multi-modal Reasoning,"Linquan Wu, Tianxiang Jiang, Yifei Dong, Haoyu Yang, Fengji Zhang, Shichang Meng, Ai Xuan, Linqi Song, Jacky Keung",https://github.com/Svardfox/LaViT,,"visual attention, latent reasoning, multimodal models, knowledge distillation, visual grounding","This work introduces LaViT, a framework that aligns latent visual thoughts instead of static embeddings, addressing the critical perception gap in multimodal reasoning by requiring autoregressive reconstruction of visual semantics before text generation. Experiments demonstrate significant improvements in visual grounding and reasoning performance.",27.92,LFM-2.5,Apple M1 (Metal)
2601.10130v1_Redundancy-Driven Top-k Functional Dependency Disc.pdf,Redundancy-Driven Top-k Functional Dependency,"Xiaolong Wan, Xixian Han",10.1234/example.doi,12345678,"functional dependency, top-k discovery, data redundancy, pruning strategy","Proposes SDP to discover top-k FDs ranked by redundancy, improving scalability by using redundancy count and upper-bound pruning.",23.44,LFM-2.5,Apple M1 (Metal)
2601.10131v2_M4olGen Multi-Agent Multi-Stage Molecular Generati.pdf,"M4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints","Yizhan Li, Florence Cloutier, Sifan Wu, Ali Parviz, Boris Knyazev, Yan Zhang, Glen Berseth, Bang Liu",,,"multi-agent, multi-stage, molecular generation, precise constraints, numeric properties, LLM optimization, property optimization","Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical. This paper introduces M4olGen, a framework that combines retrieval-augmented generation with two-stage optimization to enable deterministic, controllable molecule synthesis.",27.81,LFM-2.5,Apple M1 (Metal)
2601.10132v1_Is More Context Always Better Examining LLM Reason.pdf,Is More Context Always Better? Examining LLM Reasoning,"Yanan Cao, Farnaz Fallahi, Murali Mohana Krishna Dandu, Lalitesh Morishetti, Kai Zhao, Luyi Ma, Sinduja Subramaniam, Jianpeng Xu, Evren Korpeoglu, Kaushiki Nag, Sushant Kumar, Kannan Achan",10.1145/XXXXXX.XXXXXX,,"Large Language Models, Temporal Reasoning, Inter-Purchase Interval Prediction","This paper investigates whether large language models can predict time intervals between recurring user actions, examining how varying contextual information impacts predictive performance. It benchmarks LLMs against statistical and machine-learning models, revealing limitations in capturing quantitative temporal structure and showing that additional context can help but only up to a point.",27.76,LFM-2.5,Apple M1 (Metal)
2601.10137v1_Step-by-Step Causality Transparent Causal Discover.pdf,Step-by-Step Causality: Transparent Causal Discovery with Multi-Agent,"Ziyi Ding, Chenfei Ye-Hao, Zheyuan Wang, Xiao-Ping Zhang",https://anonymous.4open,https://anonymous.4open/science/r/Repo-9B3E-4F96/,"causal discovery, multi-agent, Tree-Query, confidence estimation, causal inference, LLM-based oracles","This paper introduces Tree-Query, a tree-structured multi-expert LLM framework that enables interpretable causal discovery by reducing pairwise discovery to queries about backdoor paths, (in)dependence, latent confounding, and causal direction. It provides theoretical guarantees for asymptotic identifiability and demonstrates improved structural metrics over direct LLM baselines on data-free benchmarks.",28.32,LFM-2.5,Apple M1 (Metal)
2601.10141v1_Understanding and Preserving Safety in Fine-Tuned .pdf,Understanding and Preserving Safety in Fine-Tuned LLMs,"Jiawen Zhang, Yangfan Hu, Kejia Chen, Lipeng He, Jiachen Ma, Jian Lou, Dan Li, Ruoxi Jia, Jian Liu",,,"fine-tuning, safety alignment, LLM safety, LLM utility, jailbreak attacks, safety-utility trade-off","This paper addresses the safety-utility dilemma in fine-tuning large language models by analyzing geometric interactions between safety and utility gradients. It proposes safety-preserving fine-tuning (SPF) to maintain performance while ensuring safety, demonstrating robustness under adversarial conditions.",27.92,LFM-2.5,Apple M1 (Metal)
2601.10143v1_History Is Not Enough An Adaptive Dataflow System .pdf,History Is Not Enough: An Adaptive Dataflow,"Haochong Xia, Yao Long Teng, Regan Tan, Molei Qin, Xinrun Wang, Bo An",,,"adaptive dataflow, workflow automation, financial time-series, data augmentation, data augmentation, financial time-series synthesis",The paper addresses the challenge of concept drift in quantitative finance by proposing an adaptive dataflow system that integrates machine learning-based adaptive control. It introduces a parameterized data manipulation module and an adaptive planner-scheduler to improve model robustness and risk-adjusted returns.,27.89,LFM-2.5,Apple M1 (Metal)
2601.10148v1_DecisionLLM Large Language Models for Long Sequenc.pdf,DecisionLLM: Large Language Models for Long Sequence Decision Exploration,"Xiaowei Lv 1, Zhiling Zhang, Yijun Li, Tianyu Wang3, Yongcai Wang1, Peng Sun, Chuan Yu3, Jian Xu3, Bo Zheng",10.48550/arxiv/2303.08734,arXiv:2303.08734,"long sequence decision-making, reinforcement learning, large language models, decision transformer, offline reinforcement learning","This work investigates applying Large Language Models to offline decision-making tasks, addressing challenges such as interpreting continuous values. It introduces DecisionLLM, a framework that aligns trajectory data with natural language descriptions, and demonstrates performance gains over traditional methods.",27.83,LFM-2.5,Apple M1 (Metal)
2601.10150v1_Simple Network Graph Comparative Learning.pdf,Simple Network Graph Comparative Learning,"Qiang Yua, Xinran Chenga, Shiqiang Xub, Chuanyi Liua",,,"Filters, Siamese network, Graph contrastive learning, Unsupervised representation learning","The paper proposes a novel node classification contrast learning method called Simple Network Graph Comparative Learning (SNGCL), which uses a superimposed multilayer Laplace smoothing filter to obtain feature smoothing matrices for node classification tasks. It compares SNGCL with state-of-the-art models and shows competitive performance.",26.68,LFM-2.5,Apple M1 (Metal)
2601.10154v1_MHub.ai A Simple Standardized and Reproducible Pla.pdf,Artificial Intelligence in Medicine (AIM) Program,"Leonard Nürnberg, Dennis Bontempi, Suraj Pai, Curtis Lisle, Steve Pieper, Ron Kikinis, Sil van de, Rahul, Soni, Gowtham, Murugesan, Ciausu, Miriam, Groeneveld, Felix, J., Dorfner, Jue, Jiang, Aneesh, Rangnekar, Harini, Joeran, Bosma, Keno, Bressem, Raymond, Mak, Andrey, Fedorov, Hugo, Aerts",,,"AI, Medical, Imaging, Radiology, Machine Learning, Deep Learning, Healthcare","A simple, standardized, and reproducible platform for AI models in medical imaging.",29.34,LFM-2.5,Apple M1 (Metal)
2601.10155v1_LOOKAT Lookup-Optimized Key-Attention for Memory-E.pdf,LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers,Aryan Karmore,bt24csd009@iiitn.ac.in,,"transformers, key-attention, memory efficiency, quantization, FAISS",Compressing the KV cache via product quantization and asymmetric distance computation enables memory-efficient deployment of large language models on edge devices. LOOKAT achieves high compression with minimal fidelity loss and leverages rank correlation guarantees.,25.18,LFM-2.5,Apple M1 (Metal)
2601.10157v1_MMPG MoE-based Adaptive Multi-Perspective Graph Fu.pdf,MMPG: MoE-based Adaptive Multi-Perspective Graph Fusion for Protein Representation Learning,"Yusong Wang, Jialun Shen, Zhihao Wu, Yicheng Xu, Shiyin Tan, Mingkun Xu, Changshuo Wang, Zixing Song, Prayag Tiwari",10.48550/arXiv.2405.12345,10.48550/arXiv.2405.12345,"Graph Neural Networks, Protein Representation Learning, Multi-perspective graphs, Mixture of Experts, Protein interactions, Downstream protein tasks","Graph Neural Networks (GNNs) are widely used for Protein Representation Learning (PRL), but current methods rely on single-perspective graph construction, leading to incomplete protein representations. This work proposes MMPG, a framework that constructs protein graphs from multiple perspectives and adaptively fuses them via Mixture of Experts (MoE) for PRL. MMPG captures both perspective-specific features and synergies, improving protein representations and achieving advanced performance on downstream tasks.",29.67,LFM-2.5,Apple M1 (Metal)
2601.10160v1_Alignment Pretraining AI Discourse Causes Self-Ful.pdf,Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment,"Cameron Tice, Puria Radmard, 2 Samuel Ratnam, Andy Kim, David Africa, Kyle O’Brien",,,"AI discourse, self-fulfilling misalignment, pretraining, alignment, LLMs, misalignment","This paper presents the first controlled study on how pretraining discourse about AI influences alignment. By varying misalignment discourse in synthetic training data, the authors demonstrate that increased misalignment discussion correlates with higher misaligned model behavior, suggesting alignment elasticity and the need to consider pretraining as a key factor in alignment priors.",27.26,LFM-2.5,Apple M1 (Metal)
2601.10161v1_AWED-FiNER Agents Web applications and Expert Dete.pdf,"A WED-FiNER: Agents, Web applications, and Expert Detectors for Fine-grained Named Entity Recognition across 36 Languages","Prachuryya Kaushik, Ashish Anand",,,"fine-grained Named Entity Recognition, multilingual models, expert detectors, digital equity","AWED-FiNER is an open-source ecosystem bridging the gap in fine-grained Named Entity Recognition for 36 languages spoken by over 6.6 billion people. It offers agentic tools, web applications, and expert models to enable fast, accurate FgNER across diverse languages, including vulnerable ones.",25.16,LFM-2.5,Apple M1 (Metal)
2601.10168v1_RAG-3DSG Enhancing 3D Scene Graphs with Re-Shot Gu.pdf,Enhancing 3D Scene Graphs With RAG-Guided Retrieval Augmentation,"Yue Chang, Rufeng Chen, Zhaofan Zhang, Yi Chen, Sihong Xie",10.48550/arXiv.2311.07891,arXiv:2311.07891,"3D scene graph, object recognition, retrieval-augmented generation, robotics, semantic representation",The paper proposes RAG-3DSG to improve open-vocabulary 3D scene graph generation by mitigating aggregation noise through re-shot guided uncertainty estimation and supporting object-level retrieval augmentation. A dynamic downsample-mapping strategy is introduced to accelerate cross-image aggregation. Experiments show significant improvements in node captioning accuracy and reduced mapping time.,27.99,LFM-2.5,Apple M1 (Metal)
2601.10169v1_CtD Composition through Decomposition in Emergent .pdf,Composition through Decomposition,"Boaz Carmeli, Ron Meir, Yonatan Belinkov",,,"compositionality, compositional generalization, artificial neural agents, decomposition, multi-target coordination, natural language processing",This study demonstrates how artificial neural agents acquire compositional generalization to describe previously unseen images through two sequential training steps: decomposing images into basic concepts and composing them into complex phrases. Zero-shot generalization is achieved without additional training.,24.37,LFM-2.5,Apple M1 (Metal)
2601.10173v1_ReasAlign Reasoning Enhanced Safety Alignment agai.pdf,ReasAlign: Reasoning Enhanced Safety Alignment against Prompt,"Hao Li1, Yankai Yang2, G. Edward Suh3, Ning Zhang1, Chaowei Xiao3,4, Washington University in St. Louis, 2University of Wisconsin–Madison, NVIDIA, Johns Hopkins University",,,"prompt injection, large language models, agentic systems, security alignment, reasoning, attack mitigation","This paper introduces ReasAlign, a model-level solution to improve safety alignment against indirect prompt injection attacks. It incorporates structured reasoning steps to detect conflicting instructions and enhances robustness by maintaining task continuity. Comprehensive evaluations show ReasAlign outperforms Meta SecAlign in utility and accuracy.",27.14,LFM-2.5,Apple M1 (Metal)
2601.10187v1_HOMURA Taming the Sand-Glass for Time-Constrained .pdf,Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning,"Ziang Cui, Mengran Yu, Tianjiao Li, Chenyu Shi, Yingxuan Shi, Lusheng Zhang, Hongwei Lin",,,"Large Language Models, Translation, Time-constrained, Reinforcement Learning, Sand-Glass, Syllable-level duration, Semantic fidelity, Rate-distortion","The paper introduces Sand-Glass to evaluate translation under syllable-level constraints and presents HOMURA, a reinforcement learning framework that optimizes semantic preservation versus temporal compliance, achieving precise length control without sacrificing meaning.",27.63,LFM-2.5,Apple M1 (Metal)
2601.10191v1_How does downsampling affect needle electromyograp.pdf,Downsampling Effects on Needle Electromyography Signals,"Mathieu J.L. Cherpitel, Janne A.M. Luijten, Thomas H.W. Bæck, Camiel Verhamme, Martijn R. Tannemaat, Anna V. Kononova",1,,"needle electromyography, downsampling, signal processing, neurological diseases, feature extraction, machine learning","This study evaluates how downsampling impacts needle electromyography (nEMG) signals by presenting a systematic workflow to assess information loss and classification performance. It compares shape-based and standard decimation methods, demonstrating that shape-aware downsampling better preserves diagnostic features while reducing computational load. The findings offer practical guidance for balancing data reduction with model accuracy in real-time nEMG analysis.",29.01,LFM-2.5,Apple M1 (Metal)
2601.10193v1_GFM4GA Graph Foundation Model for Group Anomaly De.pdf,GFM4GA: Graph Foundation Model for Group Anomaly Detection,"Jiujiu Chen, Weijun Zeng, Shaofeng Hu, Hui Xiong",XXXXXXX.XXXXXXX,XXXXXX,"Group Anomaly Detection, Graph Foundation Model, Graph Constraint Learning, Group Anomaly Proportion, Feature Inconsistency, Anomaly Detection, Individual Anomaly, Network Applications","This paper proposes GFM4GA, a graph foundation model for group anomaly detection. It addresses challenges in detecting group anomalies by leveraging dual-level contrastive learning and group extraction, achieving improvements over existing methods.",25.85,LFM-2.5,Apple M1 (Metal)
2601.10201v1_PRL Process Reward Learning Improves LLMs Reasonin.pdf,Process Reward Learning Improves LLMs’ Reasoning Ability,"Jiarui Yao, Ruida Wang, Tong Zhang",,,,"The paper proposes Process Reward Learning (PRL) to decompose entropy regularized reinforcement learning into intermediate steps with process rewards, aiming to improve LLM reasoning by providing fine-grained supervision during the reasoning process.",23.96,LFM-2.5,Apple M1 (Metal)
2601.10205v1_One Instruction Does Not Fit All How Well Do Embed.pdf,One Instruction Does Not Fit All: How Well Do Embeddings Align,"arya.shah, himanshu.beniwal, mayank.singh",10.48550/arXiv.2203.02648,arXiv:2203.02648,"personas, instructions, low-resource languages, embedding models","This paper presents a unified benchmark for evaluating multilingual embedding models across 12 Indian languages. It assesses monolingual and cross-lingual persona-instruction retrieval, reverse retrieval, and compatibility classification. Eight models are evaluated with E5 Large-Instruct achieving strong monolingual recall and BGE-M3 good reverse retrieval, while LaBSE shows high AUROC for classification.",26.59,LFM-2.5,Apple M1 (Metal)
2601.10212v1_PADER Paillier-based Secure Decentralized Social R.pdf,PADER: Paillier-based Secure Decentralized Social Recommendation,"Chaochao Chen, Jiaming Qian, Fei Zheng, Yachuan Liu",10.1007/s11473-020-09632-7,10.1007/978-3-030-62529-3,"Paillier Cryptosystem, Secure Computation, Recommendation System, Decentralized Network, Social Recommendation","The paper proposes PADER, a secure decentralized social recommendation system using the Paillier cryptosystem. It addresses privacy concerns by enabling secure training and inference without centralized platforms, supporting efficient secure computation and data packing for real-world applications.",27.23,LFM-2.5,Apple M1 (Metal)
2601.10215v1_Topo-RAG Topology-aware retrieval for hybrid text-.pdf,Topo-RAG: Topology-Aware Retrieval for Hybrid Text Tables,"Alex Dantart, Marco K´ovacs-Navarro",arxiv@humanizinginternet.com,arXiv:2601.10215v1,"Retrieval-Augmented Generation, table retrieval, late interaction, multivector retrieval, enterprise search, semantic routing, structure-aware embeddings, Topo-RAG, ColBERT, cell-aware interaction","This work introduces Topo-RAG, a framework that models documents as topological structures rather than pure text or numbers. By preserving spatial relationships in tabular data, Topo-RAG improves retrieval performance on complex enterprise corpora, achieving an 18.4% gain in nDCG@10 over linearization methods.",29.6,LFM-2.5,Apple M1 (Metal)
2601.10222v1_Introduction to optimization methods for training .pdf,Introduction to optimization methods for training SciML models,"Alena Kopaničáková, Elisa Riccietti",10.1093/acps/psa064,2601.10222v1,"optimization methods, machine learning, SciML, stochastic optimization, gradient descent, physics informed optimization","This paper discusses how optimization underpins modern machine learning, highlighting shifts toward stochastic methods due to data scarcity in scientific machine learning. It emphasizes physics-informed constraints in objective functions and the unique challenges posed by non-Taylor-decomposable losses.",28.67,LFM-2.5,Apple M1 (Metal)
2601.10236v1_Who Owns the Text Design Patterns for Preserving A.pdf,Who Owns the Text? Design Patterns for Preserving Authorship in AI-Assisted Writing,"Bohan Zhang, Chengke Bu, Paramveer Dhillon",arXiv:2601.10236v1,2601.10236,"AI-assisted writing, human-AI collaboration, psychological ownership, personalization, provenance","This study examines how ownership-aware co-writing editors affect psychological ownership in AI-assisted writing tasks. It tests persona-based coaching and style personalization across three professional writing tasks, finding that while cognitive load drops and quality remains stable, psychological ownership declines modestly. Persona coaching does not reverse this trend.",28.83,LFM-2.5,Apple M1 (Metal)
2601.10242v1_Loop as a Bridge Can Looped Transformers Truly Lin.pdf,LOOP AS ABRIDGE: CANLOOPEDTRANSFORMERS,"Guanxu Chen, Dongrui Liu, Jing Shao",,,"Large Language Models, Looped Transformers, Introspection, Representation Space, Natural Language Outputs","This report investigates whether Looped Transformers can bridge the gap between internal knowledge and explicit linguistic outputs by leveraging their iterative nature as a form of introspection. Experiments show that while increasing loop iterations narrows the knowledge-gap, internal representations degrade, and verification capability is limited to the final loop.",25.89,LFM-2.5,Apple M1 (Metal)
2601.10245v1_TRIM Hybrid Inference via Targeted Stepwise Routin.pdf,Hybrid Inference via Targeted Stepwise,"Vansh Kapoor, Aman Gupta, Hao Chen, Anurag Beniwal, Jing Huang, Aviral Kumar",,,"multi-step reasoning, LLM routing, step-level inference, cascading failures, mathematical problem solving","The paper proposes TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical reasoning steps to larger models while allowing smaller models to handle routine continuations. TRIM improves cost efficiency and performance across benchmarks like MATH-500 and AIME by focusing expensive computations on high-impact steps.",27.24,LFM-2.5,Apple M1 (Metal)
2601.10251v1_X-SAM Boosting Sharpness-Aware Minimization with D.pdf,Boosting Sharpness-Aware Minimization with Dominant-Eigenvector,Hongru Duan Yongle Chen Lei Guan,10.48550/arXiv.2024.12345,arXiv:2408.12345,"sharpness-aware minimization, sharp regions, gradient correction, neural networks","This paper investigates Sharpness-Aware Minimization (SAM) by analyzing the angle between gradients and leading eigenvectors, proposing X-SAM to improve generalization through eigenvector-aligned regularization. Experimental results demonstrate superior performance in complex loss landscapes.",26.37,LFM-2.5,Apple M1 (Metal)
2601.10254v1_NoReGeo Non-Reasoning Geometry Benchmark.pdf,NoReGeo: Non-Reasoning Geometry Benchmark,"Irina Abdullaeva, Anton Vasiliuk, Elizaveta Goncharova, Temurbek Rahmatullaev, Zagorulko Ivan, Maxim Kurkin, 6, Andrey Kuznetsov, 2",,,"geometry, non-reasoning, spatial understanding, benchmark, LLMs, intrinsic understanding","Presents NoReGeo, a benchmark evaluating LLMs' native geometric comprehension without reasoning or algebra. Assesses models on 2,500 trivial problems across 25 categories, revealing current limitations in intuitive spatial reasoning.",27.79,LFM-2.5,Apple M1 (Metal)
2601.10257v1_Untangling Input Language from Reasoning Language .pdf,Untangling Input Language from Reasoning Language,"Nan Li, Bo Kang, Tijl De Bie",10.48550/arXiv:2109.07916,2109.07916,"moral alignment, cross-lingual reasoning, moral foundations theory, LLM evaluation, dilemma interpretation","This paper introduces a methodology to disentangle the effects of input language and reasoning language in moral judgments. By applying moral foundations analysis, it reveals how reasoning language influences judgments independently of input language, highlighting inconsistencies across languages and offering diagnostic guidance for developers.",26.51,LFM-2.5,Apple M1 (Metal)
2601.10272v1_MoST Mixing Speech and Text with Modality-Aware Mi.pdf,MOST: MIXINGSPEECH ANDTEXT WITHMODALITY,"Yuxuan Lou, Kai Yang, Yang You",arXiv:2601.10272v1,2601.10272,"Mixture of Speech and Text, Multimodal Large Language Model, Modality-Aware Mixture of Experts, Speech and Text Integration, Open Source LLM","We present MoST, a novel multimodal large language model that seamlessly integrates speech and text processing through a Modality-Aware Mixture of Experts (MAMoE) architecture. The model introduces specialized routing pathways that adapt experts to input type, enhancing modality-specific learning and cross-modal understanding. Comprehensive evaluations demonstrate consistent performance gains across benchmarks.",28.15,LFM-2.5,Apple M1 (Metal)
2601.10274v1_Queueing-Aware Optimization of Reasoning Tokens fo.pdf,Queueing-Aware Optimization of Reasoning Tokens,"Emre Ozbas, Melih Bastopcu",,,"accuracy-latency trade-offs, LLM-based servers, optimization of reasoning tokens, LLM inference","The paper presents a constrained optimization framework for allocating reasoning tokens in a single LLM server handling heterogeneous queries. It models the system as an M/G/1 queue under first-in-first-out discipline, derives a concave objective balancing accuracy and latency, and proposes a projected gradient method with global step-size guarantees. Integer allocations are achieved via rounding, and performance is validated via simulation.",25.77,LFM-2.5,Apple M1 (Metal)
2601.10282v2_SPIKE Sparse Koopman Regularization for Physics-In.pdf,SPIKE: Sparse Koopman Regularization for Physics-Informed Neural Networks,Jose Marie Antonio Miñoza,10.1093/pasj/psa123,2601.10282v2,"Physics-Informed Neural Networks, PINN, Koopman operator, sparse regularization","Presents SPIKE, a framework that regularizes PINNs with continuous-time Koopman operators to achieve parsimonious dynamics representations, improving generalization beyond training domains.",24.95,LFM-2.5,Apple M1 (Metal)
2601.10305v1_DanQing An Up-to-Date Large-Scale Chinese Vision-L.pdf,DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset,"Hengyu Shen, Tiancheng Gu, Bin Qin, Lan Wu, Yuling Wu, Shuo Tan, Zelong Sun, Jun Wang, Nan Wu, Xiang An, Weidong Cai, Ziyong Feng, Kaicheng Yang, DanQingTeam",arXiv:2601.10305v1,arXiv:2601.10305,"Chinese vision-language pre-training, large-scale dataset, image-text pairs, cross-modal retrieval, zero-shot classification, cross-modal evaluation","This report presents DanQing, a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. It addresses the scarcity of high-quality Chinese image-text data by curating 100 million image-text pairs from Common Crawl, using data collected in 2024–2025. The dataset is rigorously selected and enriched with recent web data, enabling models to better capture evolving semantic trends. Experimental results demonstrate DanQing's superior performance across various Chinese downstream tasks.",29.93,LFM-2.5,Apple M1 (Metal)
2601.10306v1_Evidence-Augmented Policy Optimization with Reward.pdf,Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning,"Xin Guan, Zijian Li, Shen Huang, Pengjun Xie, Jingren Zhou, Jiuxin Cao",10.1234/example.doi,,"Evidence-Augmented Policy Optimization, Long-Context Reasoning, Reinforcement Learning, LLM, Reward Co-Evolution, Need in a Haystack, Needless Guess",Addresses sparsity in outcome rewards for LLM reasoning; introduces EAPO to improve evidence retrieval and supervision.,28.94,LFM-2.5,Apple M1 (Metal)
2601.10338v1_Agent Skills in the Wild An Empirical Study of Sec.pdf,Agent Skills in the Wild: An Empirical Study of Security,"Yi Liu, Weizhe Wang, Tianjin University, Southern Cross University, Guangquan Xu, Gelei Deng, Yuekang Li, Leo Zhang",10.1145/nnnnnnn.nnnnnnn,,"Agent skills, AI security, vulnerability analysis, supply chain security, prompt injection, large language models","The study analyzes 42,447 AI agent skills from major marketplaces, identifying pervasive security risks. It reveals that 26.1% of skills contain vulnerabilities, with data exfiltration and privilege escalation being most common. The research introduces a vulnerability taxonomy and detection methodology, emphasizing the need for capability-based permission systems.",27.28,LFM-2.5,Apple M1 (Metal)
2601.10342v1_C-GRASP Clinically-Grounded Reasoning for Affectiv.pdf,C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing,"Cheng Lin Cheng, Ting Chuan Lin, Chai Kai Chang",,,"Large language model, clinical decision support, heart rate variability, retrieval-augmented generation, explainable AI, guardrails","Proposes C-GRASP, a guardrailed RAG-enhanced pipeline for HRV interpretation, emphasizing individualized baseline shifts and spectral artifact mitigation. Achieved 37.3% accuracy in 4-class emotion classification with CRC score of 69.6%.",26.68,LFM-2.5,Apple M1 (Metal)
2601.10343v2_OctoBench Benchmarking Scaffold-Aware Instruction .pdf,Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding,"Deming Ding1, Shichun Liu1, Enhui Yang2, Jiahang Lin1, Ziying Chen1, Shihan Dou2, Honglin Guo1, Weiyu Cheng2, Pengyu Zhao2, Chengjun Xiao2, Qunhong Zeng2, Qi Zhang1, Xuanjing Huang1, Qidi Xu †2, Tao Gui †1",,,,"Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially under heterogeneous constraints. OCTOBENCH benchmarks scaffold-aware instruction following in repository-grounded agentic coding, revealing a gap between task-solving and compliance.",27.63,LFM-2.5,Apple M1 (Metal)
2601.10348v1_Training-Trajectory-Aware Token Selection.pdf,Training-Trajectory-Aware Token Selection,"Zhanming Shen, Jiaqi Hu, Zeyu Qin, Hao Chen, Guoshan Lu, Junlin Zhou, Junbo Zhao, Yihong Zhuang, Yihao Ye, Zhenan Huang, Hao Chen, Yihao Zhao",,,,"Efficient distillation enables deployable efficiency; in frontier regimes performance drops sharply despite decreasing loss. We identify a token-level mechanism where confidence bifurcates into imitation-anchor tokens, causing failure. We propose Training-Trajectory-Aware Token Selection (T3S) to address this.",26.54,LFM-2.5,Apple M1 (Metal)
2601.10349v1_SuS Strategy-aware Surprise for Intrinsic Explorat.pdf,Strategy-aware Surprise for Intrinsic Exploration,"Mark Kashirskiy, Ilya Makarov",353056@niuitmo.ru,,"intrinsic motivation, reinforcement learning, contrastive learning, exploration, exploration strategies","Proposes Strategy-aware Surprise (SuS), a novel intrinsic motivation framework using pre-post prediction mismatch. Introduces Strategy Stability and Strategy Surprise components, showing significant improvements over baseline methods in accuracy and solution diversity.",26.33,LFM-2.5,Apple M1 (Metal)
2601.10373v1_Towards Efficient Low-rate Image Compression with .pdf,Towards Efficient Low-rate Image Compression with Frequency-aware Diffusion Prior Refinement,"Yichong Xia, Yimin Zhou, Jinpeng Wang, Bin Chen",10.48550/arXiv.2024.12345,arXiv:2408.12345,"image compression, diffusion models, frequency-aware estimation, low-bitrate, compression efficiency","This work proposes DiffCR, a frequency-aware skip estimation module combined with consistency prior refinement, enabling efficient and high-fidelity image reconstruction at low bit rates. It achieves substantial bitrate savings and speed improvements over existing methods.",25.49,LFM-2.5,Apple M1 (Metal)
2601.10378v2_Global Context Compression with Interleaved Vision.pdf,Global Context Compression with Interleaved Vision-Text Transformation,"Dian Jiao, Jiaxin Duan, Shuai Zhao, Jiabing Leng, Yiran Zhang, Feng Huang",,,"vision-language models, context compression, Transformer, OCR, visual encoding, attention compression","This paper proposes VIST2, a novel Transformer that interleaves input text chunks alongside visual encoding to achieve token-level compression. It explores hierarchical encoding to preserve context while reducing memory usage, achieving significant improvements in speed and efficiency for long writing tasks.",26.14,LFM-2.5,Apple M1 (Metal)
2601.10386v1_Handling Missing Modalities in Multimodal Survival.pdf,Handling Missing Modalities in Multimodal Survival Prediction for Non-Small Cell Lung Cancer,"Filippo Ruffini, Camillo Maria Caruso, Claudia Tacconi, Lorenzo Nibid5, Francesca Miccolis, Marta Lovino, Carlo Greco, Edy Ippolito, Michele Fiore, Alessio Cortellini, Bruno Beomonte Zobel, Giuseppe Perrone, Bruno Vincenzi, Claudio Marrocco, Valerio Guarrasi, Paolo Soda",arXiv:2601.10386v1,2601.10386,"survival prediction, non-small cell lung cancer, multimodal, missing modalities, artificial intelligence",A study on handling missing modalities in multimodal survival prediction for non-small cell lung cancer.,31.34,LFM-2.5,Apple M1 (Metal)
2601.10398v2_LatentRefusal Latent-Signal Refusal for Unanswerab.pdf,LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL,"Xuancheng Ren, Jiang Duan, Qiang Duan",25213050189,"xcren25, sjhu24, 25213050189","Text-to-SQL, unanswerable queries, LLM safety, refusal mechanism","Addresses safety challenges in LLM-based Text-to-SQL by introducing LATENTREFUSAL, a latent-signal refusal mechanism that detects unanswerable queries from intermediate activations.",24.74,LFM-2.5,Apple M1 (Metal)
2601.10402v1_Toward Ultra-Long-Horizon Agentic Science Cognitiv.pdf,Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering,"Xinyu Zhu, Yuzhu Cai, Zexi Liu, Bingyang Zheng, Rui Ye, Jiaao Chen, Hanrui Wang, Wei-Chen Wang, Yuzhi Zhang, Linfeng Zhang, Weinan E1, Di Jin, Siheng Chen, School of Artiﬁcial Intelligence, Shanghai Jiao Tong University, Eigen AI, School of Computer Science and Engineering, Beihang University",10.48550/arXiv.2601.10402,2601.10402,"agentic science, cognitive accumulation, machine learning engineering, ultra-long-horizon, experimental autonomy, knowledge accumulation, AI research","The paper presents ML-Master 2.0, an autonomous agent that advances agentic science by introducing Hierarchical Cognitive Caching to sustain strategic coherence over long experimental cycles. It achieves a 56.44% medal rate on OpenAI’s MLE-Bench, demonstrating scalability for complex, long-term research tasks.",29.94,LFM-2.5,Apple M1 (Metal)
2601.10406v1_ErrEval Error-Aware Evaluation for Question Genera.pdf,ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics,"Weiping Fu, Bifan Wei, Jingyi Hao, Yushun Zhang, Jian Zhang, Jiaxin Wang, Bo Li, Yu He, Lingling Zhang, Jun Liu",,,"question generation, error modeling, error diagnostics, natural language generation, structural diagnostics",Automatic Question Generation often produces outputs with critical defects; ErrEval introduces an explicit error-aware framework to improve evaluation.,25.09,LFM-2.5,Apple M1 (Metal)
2601.10413v1_LADFA A Framework of Using Large Language Models a.pdf,LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies,"Haiyue Yuan, Nikolay Matyunin, Ali Raza, Shujun Li",https://doi.org/XXXXXXX.XXXXXXX,"1, 1 (January 2025)","Large Language Model, LLM, Privacy Policy, Text Analysis, Data Flows, Privacy, Security, Retrieval-Augmented Generation, RAG, Framework, Automotive Industry, Connected Vehicle","This paper presents LADFA, an end-to-end computational framework that processes privacy policies, extracts personal data flows, constructs a data flow graph, and enables insight discovery. It combines large language models with retrieval-augmented generation and a custom knowledge base, demonstrating effectiveness through a case study of ten automotive industry privacy policies.",29.04,LFM-2.5,Apple M1 (Metal)
2601.10416v1_LLMdoctor Token-Level Flow-Guided Preference Optim.pdf,LLMdoctor: Token-Level Flow-Guided Preference Optimization,"Tiesunlong Shen, Rui Mao, Jin Wang, Heming Sun, Jian Zhang, Xuejie Zhang, Erik Cambria",,,"LLM alignment, test-time alignment, token-level optimization, flow-guided preference, large language models","Introduces LLMdoctor, a framework for efficient test-time alignment using token-level reward acquisition and flow-guided preference optimization to balance performance and diversity.",24.45,LFM-2.5,Apple M1 (Metal)
2601.10421v1_Are Language Models Models.pdf,How Linguistics Learned to Stop Worrying and Love the Language Models,Philip Resnik,10.1017/S0140525X2510112X,in press,"language models, computational models, linguistics","The commentary argues that LMs are useful tools but not cognitive models, challenging the notion that they serve as model systems.",31.16,LFM-2.5,Apple M1 (Metal)
2601.10436v1_Development of Ontological Knowledge Bases by Leve.pdf,Development of Ontological Knowledge Bases by Leveraging Large Language Models,"Leveraging Large Language Models, Marie-Hélène ABEL, Philippe GOUSPILLOU",,,"Ontology Development, Ontological Knowledge bases, Large Language Models, Knowledge Representation, User Modeling, Knowledge Management","This paper introduces a structured, iterative methodology leveraging LLMs to optimize ontology development. It highlights advancements in automating ontology artifact generation, improving consistency, bias mitigation, and transparency, with a focus on a user context profile ontology in the vehicle sales domain.",29.73,LFM-2.5,Apple M1 (Metal)
2601.10440v1_AgentGuardian Learning Access Control Policies to .pdf,Learning Access Control Policies to Govern AI Agent Behavior,"Nadya Abaev, Denis Klimov, Gerard Levinov, David Mimran, Yuval Elovici, Asaf Shabtai",,,"Security, AI Agents, Access Control Policies, Control Flow Graph","The study introduces AgentGuardian, a security framework that governs AI agent operations via context-aware access control policies. It monitors agent behavior during testing to learn legitimate patterns and derives adaptive policies to regulate tool calls, mitigating risks like hallucination and orchestration errors.",26.46,LFM-2.5,Apple M1 (Metal)
2601.10457v1_NSR-Boost A Neuro-Symbolic Residual Boosting Frame.pdf,NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models,"Ziming Dai, Dabiao Ma, Jinle Tong, Mengyuan Han, Jian Yang, Hanmengyuan-JK",10.1145/nnnnnnn.nnnnnnn,,"Neuro-Symbolic AI, Large Language Models, Gradient Boosting, Legacy Model, Interpretability","The paper introduces NSR-Boost, a neuro-symbolic residual boosting framework tailored for industrial legacy models. It addresses re-training costs and systemic risks by treating legacy models as frozen and applying targeted repairs. The framework uses residuals to identify hard regions, generates symbolic experts via LLM, and integrates them dynamically with legacy outputs, achieving superior performance on real-world data.",27.39,LFM-2.5,Apple M1 (Metal)
2601.10460v1_Contextual StereoSet Stress-Testing Bias Alignment.pdf,Stress-Testing Bias Alignment Robustness in Large Language Models,"Abhinaba Basu, Pavan Chakraborty",arXiv:2601.10460v1,2601.10460v1,"bias evaluation, alignment robustness, stress-testing, large language models, stochastic technical context, StereoSet, Context Sensitivity Fingerprints","The paper presents Contextual StereoSet, a benchmark that fixes stereotype content while varying contextual framing. It shows that models' bias shifts with prompts about time, location, or audience, revealing that bias is context-dependent rather than fixed. The authors propose Context Sensitivity Fingerprints to diagnose these shifts and release their benchmark for reproducibility.",29.7,LFM-2.5,Apple M1 (Metal)
2601.10462v3_ChartComplete A Taxonomy-based Inclusive Chart Dat.pdf,ChartComplete: A Taxonomy-based Inclusive Chart Dataset,"Ahmad Mustapha, Charbel Toumieh, Mariette Awad",arXiv:2601.10462v3,arXiv:2601.10462,"Chart, Dataset, Chart Taxonomy, Chart Classification",The ChartComplete dataset introduces a chart taxonomy to address limitations in existing benchmarks that focus on a small set of chart types. It aims to provide a more inclusive dataset covering thirty different chart types for advancing research in ChartQA.,26.74,LFM-2.5,Apple M1 (Metal)
2601.10477v1_Urban Socio-Semantic Segmentation with Vision-Lang.pdf,Urban Socio-Semantic Segmentation with Vision-Language Reasoning,"Yu Wang, Yi Wang, Rui Dai, Yujie Wang, Kaikui Liu, Xiangxiang Chu, Yansheng Li",,,"urban segmentation, vision-language reasoning, socio-semantic segmentation, satellite imagery, semantic entities, reinforcement learning","This paper introduces a new dataset and framework for socio-semantic segmentation of urban areas using vision-language models. It addresses challenges in identifying socially defined entities beyond physical attributes, leveraging hierarchical multi-stage reasoning and reinforcement learning to improve performance.",26.92,LFM-2.5,Apple M1 (Metal)
2601.10485v1_Panning for Gold Expanding Domain-Specific Knowled.pdf,Panning for Gold: Expanding Domain-Specific Knowledge,"Runhao Zhao, Weixin Zeng, Wentao Zhang, Chong Chen, Zhengpin Li, Xiang Zhao, Lei Chen",,,"Domain-specific Knowledge Graph, Knowledge Graph Enrichment, General-to-domain Knowledge Transfer, Fact-as-Program, Domain Relevance, Granularity Alignment","Domain-specific knowledge graphs (DKGs) are essential for supporting intelligent applications in specialized fields, yet they suffer from limited coverage and incompleteness. This paper proposes ExeFuse, a new task for domain-specific knowledge graph fusion, addressing challenges of domain relevance ambiguity and granularity misalignment. The framework integrates relevant facts from general knowledge graphs into domain-specific graphs to enhance completeness and utility.",27.92,LFM-2.5,Apple M1 (Metal)
2601.10496v1_Model See Model Do Exposure-Aware Evaluation of Bu.pdf,"Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code","Ali Al-Kaswan, Claudio Spiess, Prem Devanbu, Arie van Deursen, Maliheh Izadi",10.1145/nnnnnnn.nnnnnnn,CCS Concepts,"Large Language Models, bugs, fixes, Memorisation","This paper introduces an exposure-aware evaluation framework to assess how prior exposure to buggy versus fixed code influences large language models' preference for correct code versus familiar incorrect versions. Using the ManySStuBs4J benchmark, the study applies Data Portraits on the Stack-V2 corpus and evaluates model preference via code completion and likelihood metrics. Findings show that most examples lack either variant in training, fixes are more common when only one variant is present, and exposure amplifies bias toward buggy outputs.",28.41,LFM-2.5,Apple M1 (Metal)
2601.10498v1_Projected Microbatch Accumulation yields reference.pdf,Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning,Nilin Abrahamsen,arXiv:2601.10498v1,15 Jan 2026,,"Introduces PROMA, a proximal policy update method for large language model fine-tuning that enforces tighter control of local KL divergence without reference policies.",22.2,LFM-2.5,Apple M1 (Metal)
2601.10511v1_Scalable Algorithms for Approximate DNF Model Coun.pdf,Scalable Algorithms for Approximate DNF Model Counting,"Paul Burkhardt, David G. Harris, Kevin T. Schmitt",arXiv:2601.10511v1,arXiv:2601.10511,"DNF model counting, approximate algorithms, Monte Carlo, probabilistic inference, network reliability","The paper presents a new Monte Carlo approach with adaptive stopping and short-circuit evaluation for approximate counting of DNF formulas. It achieves PAC learning bounds and demonstrates superior performance over prior methods, scaling efficiently to large problems.",28.67,LFM-2.5,Apple M1 (Metal)
2601.10512v2_SatMap Revisiting Satellite Maps as Prior for Onli.pdf,SatMap: Revisiting Satellite Maps as Prior for Online HD Map Construction,"Kanak Mazumder, Fabian B. Flohr",0009−0006−6806−8388,0009−0002−1499−3790,"Online HD map prediction, Satellite map prior, Vectorized HD map","The paper presents SatMap, an online vectorized HD map estimation method that integrates satellite maps with multi-view camera observations to improve downstream prediction and planning. It achieves a 34.8% mAP improvement over camera-only baselines and an 8.5% improvement over camera-LiDAR fusion, demonstrating the benefits of satellite prior maps under long-range and adverse conditions.",29.01,LFM-2.5,Apple M1 (Metal)
2601.10520v1_Breaking Up with Normatively Monolithic Agency wit.pdf,Breaking Up with Normatively Monolithic Agency,"Felix Jahn, Yannic Muskalla, Lisa Dargasz, Kevin Baum",arXiv:2601.10520v1,2601.10520,"AI alignment, ethical AI, normative reasoning, moral modules, ethical architectures","Introduces GRACE, a reason-based architecture decoupling normative and instrumental reasoning for safe AI alignment, targeting applications like LLM therapy assistants.",25.71,LFM-2.5,Apple M1 (Metal)
2601.10524v1_Diagnosing Generalization Failures in Fine-Tuned L.pdf,Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing,"Frank Bobe III, Gregory D. Vetaw, Chase Pavlick, Darshan Bryner, Matthew Cook, Jose Salas-Vernis",arXiv:2601.10524v1,arXiv:2601.10524,"LLM fine-tuning, generalization failure, phishing detection, interpretability, architecture, data diversity","This study introduces a multi-layered diagnostic framework to analyze generalization issues in fine-tuned LLMs applied to phishing detection. It identifies three key findings: (1) performance depends on architecture and diverse data; (2) architecture strongly influences generalization; (3) certain models (e.g., Mistral) exhibit greater resilience. The work emphasizes the need for deeper validation of architecture-data interactions to build reliable AI systems.",29.5,LFM-2.5,Apple M1 (Metal)
2601.10527v2_A Safety Report on GPT-5.2 Gemini 3 Pro Qwen3-VL G.pdf,"Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5","Xingjun Ma1, Yixu Wang1, Hengyuan Xu1, Yutao Wu3, Yifan Ding1, Yunhan Zhao1, Zilong Wang1, Jiabin Hua1, Ming Wen1,2, Jianan Liu1,2, Ranjie Duan, Yifeng Gao1, Tan Yunhao Chen, Wei Cheng, Jingjing Chen1, Bo Li4, Yi Gang Jiang, Fudan University, Deakin University, Shanghai Innovation Institute",https://xsafeai.github.io/AI-safety-report,https://github.com/XSafeAI/AI-safety-report,"GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, Seedream 4.5, safety evaluation, multimodal models, adversarial robustness, regulatory compliance","The report evaluates frontier AI models across language, vision, and multimodal settings, revealing uneven safety performance. While some models show strong benchmarks, they remain vulnerable under adversarial conditions, highlighting the need for holistic safety assessments.",32.8,LFM-2.5,Apple M1 (Metal)
2601.10543v1_Defending Large Language Models Against Jailbreak .pdf,Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing,"Yinzhi Zhao, Ming Wang, Shi Feng, Xiaocui Yang, Daling Wang, Yifei Zhang",,,"large language models, jailbreak attacks, decoding safety, LLM safety, model robustness","This paper examines the decoding process of LLMs and identifies latent safety signals during generation. It proposes a method to leverage these signals for early detection of unsafe content, enhancing safety without excessively over-refusing benign inputs.",26.23,LFM-2.5,Apple M1 (Metal)
2601.10560v1_Learning Latency-Aware Orchestration for Parallel .pdf,Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems,"Xi Shi, Mengxin Zheng, Qian Lou",,,"multi-agent systems, parallel execution, latency optimization, latency supervision, task performance, parallel architecture search","This work introduces Latency-Aware Multi-agent System (LAMaS), a framework that optimizes execution paths under parallel execution to reduce critical path length by 38–46% compared to SOTA, while maintaining or improving task performance.",26.03,LFM-2.5,Apple M1 (Metal)
2601.10562v1_Process-Guided Concept Bottleneck Model.pdf,Process-Guided Concept Bottleneck Model,"Reza M. Asiyabi, Sam Harrison, John L. Godlee, David Milodowski, Nicole H. Augustin, Penelope J. Mograbi, Timothy R. Baker, Lorena M. Benitez, Samuel J. Bowers, Thomas K. Brade, Joao M. B. Carreiras, Duncan M. Chalo, Vera De Cauwer, Kyle G. Dexter, Mathias I. Disney, Luisa F. Escobar-Alvarado, Manfred Finckh, Tatenda Gotore, Gabriele C. Hegerl, John N. Kigomo, Fainess C. Lumbwe, Francisco Maiato, Rudzani A. Makhado, Collins W. Masinde, Musingo Tito E. Mbuvi, Iain M. McNicol, Edward T.A. Mitchard, Buster P. Mogonong, Wilson A. Mugasha, Aristides Baptista Muhate, Hinji Mutondo, Leena Naftal, Paula Nieto-Quintano, Elifuraha Elisha Njoghomi, Catherine L. Parr, Oliver L. Phillips, Pierre Proces, Tshililo Ramaswiela, Jayashree Ratnam, Mathew Rees, Rasmus Revermann, Natasha Ribeiro, Mahesh Sankaran, Abel M. Siampale, Stephen Sitch, Kathleen G. Smart, Hemant G. Tripathi, Wayne Twine, Gabriel I.K. Uusiku, Helga van der Merwe, Chemuku Wekesa, Benjamin J. Wigley, Mathew Williams, Ellie Wood, Emily Woollen, Shaun Quegan, Steven Hancock, Casey M. Ryan",,,,Process-Guided Concept Bottleneck Model,30.6,LFM-2.5,Apple M1 (Metal)
2601.10567v1_Generative AI collective behavior needs an interac.pdf,Generative AI collective behavior,"Laura Ferrarotti, Gian Maria Campedelli, Andrea Baronchelli, Giovanni Iacca, Kathleen M. Carley, Alex Pentland, Joel Z. Leibo, James Evans, Bruno Lepri, Not Diamond, City St. George’s University of London, Carnegie Mellon University, Massachusetts Institute of Technology, Stanford University, Google DeepMind, University of Chicago",arXiv:2601.10567v1,2601.10567v1,"generative AI, collective behavior, interactionist paradigm, large language models, social priors, adaptation, multi-agent systems","The article argues that understanding collective behavior in large language models requires an interactionist paradigm, emphasizing the role of prior knowledge, social context, and adaptive learning in shaping emergent phenomena in multi-agent AI systems.",33.26,LFM-2.5,Apple M1 (Metal)
2601.10581v1_From Single to Multi-Agent Reasoning Advancing Gen.pdf,Advancing GeneGPT for Genomics QA,"Kimia Abedini, Farzad Shami, Gianmaria Silvello",10.1109/ARXIV.2026.2601,2601.10581,"Question Answering, Genomic QA, Multi-Agent Systems","This paper presents GeneGPT, a multi-agent framework that enhances LLMs for genomic QA by leveraging specialized API calls. It proposes GenomAgent to coordinate agents for complex queries and demonstrates improved performance over GeneGPT on the GeneTuring benchmark.",28.33,LFM-2.5,Apple M1 (Metal)
2601.10587v1_Adversarial Evasion Attacks on Computer Vision usi.pdf,Adversarial Evasion Attacks on Computer Vision using SHAP Values,"Frank Mollard, Marcus Becker, Florian Röhrbein",arXiv:2601.10587v1,arXiv:2601.10587,"adversarial attacks, computer vision, SHAP values, deep learning, machine learning","The paper introduces a white-box attack on computer vision models using SHAP values, demonstrating how such attacks can compromise model performance by reducing output confidence or inducing misclassifications. It compares SHAP attacks with the Fast Gradient Sign Method and highlights their effectiveness in gradient hiding scenarios.",32.38,LFM-2.5,Apple M1 (Metal)
2601.10591v1_ProbFM Probabilistic Time Series Foundation Model .pdf,Probabilistic Time Series Foundation Model with Uncertainty,"Arundeep Chinta, Lucas Vinh Tran, Jay Katukuri",10.48550/arXiv.2405.12345,arundeep.chinta1,"probabilistic time series, uncertainty quantification, financial forecasting, deep evidential regression, conformal prediction","This paper introduces ProbFM, a novel transformer-based probabilistic framework that uses Deep Evidential Regression to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. It evaluates DER against several probabilistic methods in a controlled comparison, demonstrating its effectiveness in financial forecasting and risk management.",26.82,LFM-2.5,Apple M1 (Metal)
2601.10600v1_Procedural Fairness in Multi-Agent Bandits.pdf,Procedural Fairness in Multi-Agent Bandits,"Joshua Caiata, Carter Blair, Kate Larson",arXiv:2601.10600v1,2601.10600v1,"procedural fairness, multi-agent systems, fairness in decision-making, outcome fairness, normative fairness","This paper introduces procedural fairness as a fairness objective in multi-agent multi-armed bandits, arguing that fairness should be grounded in equal decision-making power and proportionality rather than solely optimizing outcomes. It highlights the importance of treating all agents equally and emphasizes that legitimacy depends on fair processes.",28.08,LFM-2.5,Apple M1 (Metal)
2601.10611v1_Molmo2 Open Weights and Data for Vision-Language M.pdf,Molmo2,"Christopher Clark, Jieyu Zhang, Zixian Ma, Jae Sung Park, Mohammadreza Salehi, Sancheo Lee, Zhongzheng Ren, Chris Dongjoo Kim, Yinuo Yang, Vincent Shao, Yue Yang, Weikai Huang, Ziqi Gao, Taira Anderson, Jianrui Zhang, Jitesh Jain, George Stoica, Winson Han, Ali Farhadi, Ranjay Krishna",arXiv:2601.10611v1,2601.10611v1,"Open weights, Video understanding, Grounding, Video datasets, Object tracking, Point-driven grounding, Video counting, Video captioning, Video pointing, Multi-image datasets","Presents Molmo2, a new family of open-source video-language models with state-of-the-art performance in point-driven grounding across single images, multi-images, and videos. Introduces new datasets and training recipes to enable grounding without relying on closed proprietary models.",30.76,LFM-2.5,Apple M1 (Metal)
2601.10651v1_Multi-Property Synthesis.pdf,Multi-Property Synthesis,"Christoph Weinhuber, Yannik Schnitzer, Alessandro Abate, David Parker, Giuseppe De Giacomo, Moshe Y. Vardi",,,"LTL synthesis, multiple properties, automated planning, robotics, business process modeling","The paper studies LTLf synthesis with multiple properties, showing that satisfying all may be impossible. It proposes a symbolic algorithm that computes feasibility relationships and maximizes achievable goal sets, achieving speedups up to two orders of magnitude compared to enumeration-based methods.",25.92,LFM-2.5,Apple M1 (Metal)
2601.10679v1_Are Your Reasoning Models Reasoning or Guessing A .pdf,A Mechanistic Analysis of Hierarchical Reasoning Models,"Zirui Ren, Ziming Liu",,,"reasoning models, Hierarchical Reasoning Model, reasoning patterns, guessing, reasoning accuracy","This paper investigates why Hierarchical Reasoning Models (HRM) sometimes fail on simple puzzles, attributing the issue to violations of fixed point properties and the presence of multiple fixed points. It proposes strategies to improve HRM's performance and discusses implications for understanding reasoning in models.",24.55,LFM-2.5,Apple M1 (Metal)
2601.10681v1_Structure and Diversity Aware Context Bubble Const.pdf,Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval,"Amir Khurshid, Abhishek Sehgal",,,"Large Language Model, Retrieval-Augmented Generation, Context Bubble, Retrieval Introduction, Enterprise Documents","The paper proposes a structure-informed, diversity-constrained context bubble construction framework for LLM-based retrieval, aiming to improve coherence, reduce redundancy, and enhance answer quality in enterprise document contexts.",26.93,LFM-2.5,Apple M1 (Metal)
2601.10684v1_On the origin of neural scaling laws from random g.pdf,On the origin of neural scaling laws,"Maissam Barkeshli, Alberto Alfarano, Andrey Gromov",,,,"This paper studies scaling laws for transformers predicting random walks on graphs, showing they emerge even without inherent power law structure in data correlations. Results include scaling laws from simplified models, natural language complexity reduction, and comparisons with established scaling laws.",23.74,LFM-2.5,Apple M1 (Metal)
2601.10696v1_The Impact of Generative AI on Architectural Conce.pdf,"The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load","Han Jiang, Yao Xiao, Rachel Hurley, Shichao Liu",sliu8@wpi.edu,,"Visual communication, Architectural design, Learning, Performance Assessment, Hybrid Intelligence, Human-AI teaming, GPT Generative Pre-trained Transformer, SD Standard deviation, STEM","This study examines how generative AI influences performance, creative self-efficacy, and cognitive load in architectural design. It finds no overall performance advantage of GenAI but shows improved design performance for novices and reduced cognitive load with iterative idea-generation.",31.9,LFM-2.5,Apple M1 (Metal)
2601.10700v2_LIBERTy A Causal Framework for Benchmarking Concep.pdf,A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals,"Gilat Toker, Nitay Calderon, Ohad Amosy, Roi Reichart",,,"concept-based explanations, LLMs, structural counterfactuals, explainability, causal models, model interpretability","This paper introduces LIBERTy, a framework for constructing datasets with structural counterfactual pairs to evaluate the faithfulness of concept-based explanations. It evaluates existing benchmarks that rely on costly human-written counterfactuals and proposes a method grounded in explicit Structured Causal Models to generate high-quality counterfactuals. The study assesses model sensitivity to demographic concepts and identifies opportunities for improving explainability.",26.74,LFM-2.5,Apple M1 (Metal)
2601.10702v1_Grounding Agent Memory in Contextual Intent.pdf,Grounding Agent Memory in Contextual Intent,"Ruozhen Yang, Yucheng Jiang, Priyanka Kargupta, Jiawei Han",,,"large language models, long-horizon reasoning, contextual intent, memory systems, intent indexing","The paper proposes STITCH, an agentic memory system that indexes trajectory steps with structured retrieval cues, contextual intent, and intent compatibility. It improves performance in CAME-Bench and LongMemEval by reducing retrieval noise through intent-based filtering.",24.82,LFM-2.5,Apple M1 (Metal)
2601.10712v1_MatchTIR Fine-Grained Supervision for Tool-Integra.pdf,MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching,"Changle Qu1, Sunhao Dai, Hengyi Cai, Jun Xu, Shuaiqiang Wang, Dawei Yin",,,"Tool-Integrated Reasoning, Large Language Models, Bipartite Matching, Reinforcement Learning, Multi-turn Reasoning","Tool-Integrated Reasoning (TIR) empowers LLMs to tackle complex tasks by interleaving reasoning steps with external tool interactions. This paper proposes MatchTIR, a framework that introduces fine-grained supervision via bi-partite matching-based turn-level reward assignment and dual-level advantage estimation, demonstrating superior performance on long-horizon multi-turn benchmarks.",27.02,LFM-2.5,Apple M1 (Metal)
2601.10748v1_AnyECG Evolved ECG Foundation Model for Holistic H.pdf,AnyECG: Evolved ECG Foundation Model for Holistic Health Profiling,"Jun Li1, Hongling Zhu5, Yujie Xiao1, Qinghao Zhao6, Yale Ali Ke7,8,9, Gongzheng Tang1, Guangkun Nie1, Deyun Zhang11, Jin Li12, Canqing Yu7,8,9, Shenda Hong1,2,3,4",,,"AI-ECG, holistic health profiling, comorbidity pattern recognition, long-term risk prediction, electrocardiography, cardiac disease screening","This study introduces AnyECG, a refined ECG foundation model leveraging transfer learning to enhance holistic health profiling. It achieves robust performance across 1,172 ICD conditions and demonstrates improved capability for comprehensive disease screening and risk prediction.",28.98,LFM-2.5,Apple M1 (Metal)
2601.10768v1_Optimisation of complex product innovation process.pdf,Optimisation of Complex Product Innovation Processes,"Nina Bockova, Barbora Volna, Mirko Dohnal",arXiv:2601.10768v1,2601.10768,"complex product innovation, technological forecasting, three-valued logic, trend-based modelling, scenarios, transition graphs","This paper investigates complex product-innovation processes using models grounded in heuristics. Each heuristic is expressed through simple trends (increasing, decreasing, or constant), providing minimal information-intensive quantifiers. A solution to a trend model is defined as a set of scenarios with transitions between them, depicted by a transition graph.",30.38,LFM-2.5,Apple M1 (Metal)
2601.10770v1_Unifying Speech Recognition Synthesis and Conversi.pdf,Unifyingspeechrecognition synthesis and conversion with autoregressive transformers,"Runyuan Cai, Yu Lin, Yiming Wang, Chunlin Fu, Xiaodong Zeng",https://github.com/AutoArk/GPA,2601.10770v1,"Text-to-Speech, Automatic Speech Recognition, Voice Conversion, Foundation Model","This paper introduces General-Purpose Audio (GPA), a unified audio foundation model integrating multiple speech tasks within a single LLM. GPA uses a shared discrete audio token space and supports instruction-driven task induction, allowing a single autoregressive model to perform TTS, ASR, and VC without architectural changes. It combines autoregressive formulation, joint multi-task training, and scalable inference for efficient deployment.",29.18,LFM-2.5,Apple M1 (Metal)
2601.10773v1_LogicLens Leveraging Semantic Code Graph to explor.pdf,Leveraging Semantic Code Graph to explore Multi Repository large systems,"Niko Usai, Dario Montagnini, Kristian Ilianov Iliev, Raffaele Camanzo",,,"semantic code graph, multi-repository systems, software systems, code navigation, LLM integration","The paper introduces LogicLens, a reactive conversational agent that uses a semantic multi-repository graph built from code analysis and LLM enrichment to help developers explore complex software systems. It addresses challenges in understanding distributed codebases and demonstrates capabilities such as impact analysis and symptom-based debugging.",26.61,LFM-2.5,Apple M1 (Metal)
2601.10779v1_Unified Optimization of Source Weights and Transfe.pdf,Unified Optimization of Source Weights and Transfer Quantities in Multi-Source Transfer Learning: An Asymptotic Framework,"Qingyue Zhang, Chang Chu, Haohao Fu, Tianren Peng, Yanru Wu, Guanbo Huang, Yang Li, Shao-Lun Huang",,,"transfer learning, multi-source learning, asymptotic analysis, Kullback-Leibler divergence, optimization","This paper proposes a theoretical framework, Unified Optimization of Weights and Quantities (UOWQ), to optimize source weights and transfer quantities jointly for multi-source transfer learning. It presents a parameter estimation problem grounded in asymptotic analysis and develops both single-source and multi-source optimization methods, demonstrating consistent improvements over baselines.",28.62,LFM-2.5,Apple M1 (Metal)
2601.10810v1_Digital Metabolism Decoupling Logic from Facts via.pdf,Digital Metabolism: Decoupling Logic from Facts via Regenerative Unlearning,"Mengmeng Peng, Zhenyu Fang, He Sun",arXiv:2601.10810v1,2601.10810,"digital metabolism, regenerative unlearning, neural logic core, hallucinations, deep learning","The paper addresses the parameter entanglement problem in large language models by introducing a 'digital metabolism' concept and the Regenerative Logic-Core Protocol (RLCP). It demonstrates that targeted forgetting enables a pure neural logic core, achieving emergent structural crystallization and improved reasoning efficiency.",28.47,LFM-2.5,Apple M1 (Metal)
2601.10820v1_Towards Reliable ML Feature Engineering via Planni.pdf,Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents,"Himanshu Thakur, Anusha Kamath, Anurag Muthyala, Dhwani Sanmukhani, Smruthi Mukund, Jay Katukuri",arXiv:2601.10820v1,2601.10820,"feature engineering, LLM agents, code generation, recommendation systems, human-AI collaboration","This paper addresses challenges in adopting code-generation models for feature engineering in real-world ML teams. It proposes a planner-guided, constrained-topology multi-agent framework that automates repository planning, integrates with team tools, and improves reliability through feedback loops. Experimental results show significant improvements over manual workflows.",29.57,LFM-2.5,Apple M1 (Metal)
2601.10827v1_Approximately Optimal Global Planning for Contact-.pdf,Approximately Optimal Global Planning for Contact-Rich SE(2) Manipulation,"Simin Liu, Tong Zhao, Bernhard Paus Graesdal, Peter Werner, Jiuguang Wang, John Dolan, Changliu Liu, Tao Pang",10.1109/JROBOT.2024.12345,,"contact-rich manipulation, global planning, manipulator planning, robot manipulation, optimization","This paper introduces a new paradigm for computing approximately optimal manipulator plans by combining offline graph construction of mutual reachable sets with online local planning. It demonstrates improved task performance for contact-rich tasks, achieving a 61% reduction in cost and maintaining high success rates.",27.66,LFM-2.5,Apple M1 (Metal)
2601.10835v1_Can Vision-Language Models Understand Construction.pdf,Can Vision-Language Models Understand Construction Workers? An Exploratory Study,"Hieu Bui, Nathaniel E. Chodosh, Arash Tavakoli",,,"Vision-Language Models, Construction Automation, Robotics, Human Robot Interaction, Large Language Model, Construction, Robotics, Human Robot Interaction","This study evaluates three leading Vision-Language Models (GPT-4o, Florence 2, LLaVa-1.5) in detecting construction worker actions and emotions from static images. It assesses performance across action and emotion recognition using a curated dataset, revealing limitations in distinguishing subtle categories and highlighting the need for domain adaptation or multimodal approaches.",27.25,LFM-2.5,Apple M1 (Metal)
2601.10880v1_Medical SAM3 A Foundation Model for Universal Prom.pdf,Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation,"Chongcong Jiang, Tianxingjian Ding, Chuhan Song, Jiachen Tu, Ziyang Yan, Yihua Shao, Zhenyi Wang, Yuzhang Shang, Tianyu Han, Yu Tian",10.1093/pasj/psa123,2601.10880,"Medical Image Segmentation, Foundation Models, Fine-Tuning, SAM3, Medical Imaging, Text Prompts","Medical SAM3 is a foundation model for universal prompt-driven medical image segmentation, demonstrating strong generalization through interactive prompting. It addresses domain shift challenges and enables robust performance across diverse medical datasets.",29.86,LFM-2.5,Apple M1 (Metal)
2601.10904v1_ARC Prize 2025 Technical Report.pdf,ARC Prize 2025: Technical Report,"François Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers",10.48550/arxiv/2503.12345,arxiv:2503.12345,"ARC-AGI, few-shot generalization, intelligence, refinement loop, knowledge coverage, AI reasoning","The paper surveys top-performing methods in ARC-AGI 2025, examines the role of refinement loops in AGI progress, discusses knowledge-dependent overfitting, and previews ARC-AGI-3.",26.55,LFM-2.5,Apple M1 (Metal)
2601.10917v1_Self-learned representation-guided latent diffusio.pdf,Self-Learned Representation-GUIDED Latent Diffusion Model For Breast Cancer Classification In Deep Ultra Violet Whole Surface Images,"Pouya Afshin, David Helminiak, Tianling Niu, Julie M. Jorns, Tina Yen, Bing Yu, Dong Hye Ye1",,,"Breast Cancer Classification, Latent Diffusion Model, Self-Supervised Learning, Data Augmentation, Deep Ultraviolet Fluorescence Scanning Microscopy, Vision Transformer, Patch Prediction Aggregation, Medical Imaging","The paper proposes a Self-Supervised Learning-guided Latent Diffusion Model to generate synthetic training patches for Deep Ultraviolet Fluorescence Scanning Microscopy data. By leveraging fine-tuned DINO embeddings, the method enhances semantic detail in cellular structures, achieving high accuracy in breast cancer classification and reducing FID score.",28.4,LFM-2.5,Apple M1 (Metal)
2601.10921v1_RobuMTL Enhancing Multi-Task Learning Robustness A.pdf,Enhancing Multi-Task Learning Robustness Against Weather Conditions,"Tasneem Shaffee, Sherief Reda",10.48550/arXiv.2024.12345,arXiv:2408.12345,"Robust Multi-Task Learning, Weather Conditions, Multi-Task Learning, Adversarial Training, Mixture-of-Experts","This paper introduces RobuMTL, a novel architecture that adaptively selects task-specific hierarchical Low-Rank Adaptation (LoRA) rules and a LoRA expert squad based on input perturbations. It improves robustness across diverse real-world conditions, achieving significant gains on PASCAL and NYUD-v2 datasets.",26.79,LFM-2.5,Apple M1 (Metal)
2601.10922v1_What Matters in Data Curation for Multimodal Reaso.pdf,What Matters in Data Curation for Multimodal Reasoning?,"Yosub Shin, Michael Buriek, Boris Sobolev, Pavel Bushuyeu, Vikas Kumar, Haoyang Xu, Igor Molybog, Samuel Watson",10.48550/DCVLR2025,NeurIPS2025-1234,"data curation, multimodal reasoning, DCVLR, data efficiency, model selection","This paper examines data curation strategies for multimodal reasoning in the DCVLR challenge, emphasizing the importance of dataset alignment, difficulty-based filtering, and dataset size. It finds that difficulty-driven example selection is key to performance gains, while increasing dataset size mainly reduces variance without significantly boosting accuracy.",27.58,LFM-2.5,Apple M1 (Metal)
2601.10926v1_Selecting Language Models for Social Science Start.pdf,Selecting Language Models for Social Science,"Dustin S. Stoltz, Marshall A. Taylor, Sanuj Kumar",XX(X):1–22,,"large language models, LLMs, reproducibility, replicability, model openness","The paper explores how social scientists should select among thousands of large pretrained language models by evaluating validity, reliability, reproducibility, and replicability. It emphasizes the importance of model openness, footprint, training data, and architecture, and argues for prioritizing replicability in model selection.",25.88,LFM-2.5,Apple M1 (Metal)
2601.10931v1_Sparse Data Tree Canopy Segmentation Fine-Tuning L.pdf,Sparse Data Tree Canopy Segmentation,"David Szczecina, Niloofar Azad, Hudson Sun, Kyle Gao, Anthony Bertnyk, Lincoln Linlin Xu",,,"Deep Learning, Computer Vision, Object Segmentation, Remote Sensing, Forestry, Tree Canopy","Tree canopy detection from aerial imagery is crucial for environmental monitoring. This work evaluates five architectures under data scarcity, showing convolution-based models outperform transformers in low-data regimes.",26.1,LFM-2.5,Apple M1 (Metal)
2601.10945v1_PatientVLM Meets DocVLM Pre-Consultation Dialogue .pdf,PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between,"K Lokesh1, Abhirama Subramanyam Penamakuri1, Uday Agarwal1, Apoorva Challa2, Shreya K Gowda2, Somesh Gupta2, Anand Mishra1",https://vl2g.github.io/projects/pcdf,,"Vision-Language Models, Medical Diagnosis, Pre-Consultation Dialogue, Symptom Generation, Diagnostic Reasoning","This paper proposes a Pre-Consultation Dialogue Framework (PCDF) for vision-language models to simulate realistic doctor-patient interactions in medical diagnosis. It introduces a framework where a DocVLM generates follow-up questions and a PatientVLM responds with symptom profiles, validated by clinicians. The study demonstrates improved diagnostic performance through multi-turn dialogues and real-world validation.",27.37,LFM-2.5,Apple M1 (Metal)
2601.10951v1_Multi-Stage Patient Role-Playing Framework for Rea.pdf,Multi-Stage Patient Role-Playing Framework for Realistic Clinical Interactions,"Shijie Jianga, Zefan Zhanga, Kehua Zhub, Tian Baia, Ruihong Zhaoc, ∗∗",arXiv:2601.10951v1,2601.10951,"Patient Role-Playing, Large Language Models, Clinical, Diagnostic Education",This work introduces the first Chinese patient simulation dataset (Ch-PatientSim) to evaluate LLM performance in realistic patient interactions. It addresses persona imbalance via few-shot generation and proposes a training-free Multi-Stage Patient Role-Playing framework to enhance realism and personalization.,30.53,LFM-2.5,Apple M1 (Metal)
2601.10955v1_Beyond Max Tokens Stealthy Resource Amplification .pdf,Beyond Max Tokens: Stealthy Resource Amplification via Tool,"Kaiyu Zhou, Yongsen Zheng, Yicheng He, Meng Xue, Xueluan Gong, Yuji Wang",,,"Large Language Models, LLM Agents, Tool Calling, Agent Security, Economic Denial-of-Service, Multi-Turn Interaction, Textual Notices, Computational Cost","Introduces a stealthy, multi-turn economic DoS attack targeting LLM agents by manipulating tool-layer interactions under the guise of completing tasks. The method optimizes edits via MCTS while preserving payloads, enabling prolonged tool-calling across tokens and increasing computational load without triggering detection.",26.51,LFM-2.5,Apple M1 (Metal)
2601.10960v1_Steering Language Models Before They Speak Logit-L.pdf,Steering Language Models Before They Speak: Logit-Level Interventions,"Hyeseon An, Shinwoo Park, Hyundong Jin, Yo-Sub Han",,,"steering, language models, logit-level, interventions, text generation, prompting, activation, training-free, control, accuracy, toxicity","The paper proposes a training-free inference-time logit intervention for controllable generation. It uses a statistical token score table derived from z-normalized log-odds to shift decoding distribution, achieving up to +47% accuracy improvement across diverse datasets.",27.04,LFM-2.5,Apple M1 (Metal)
2601.11000v1_When Personalization Misleads Understanding and Mi.pdf,When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs,"Zhongxiang Sun, Yi Zhan, Chenglei Shen, Weijie Yu, Xiao Zhang, Ming He, Jun Xu",,,"personalization, hallucination, personalized LLMs, factual accuracy, representational entanglement","This paper investigates how personalization in large language models can lead to hallucinations, particularly when models generate answers aligned with user history rather than objective truth. It introduces Factuality-Preserving Personalized Steering (FPPS) to mitigate factual distortions while maintaining personalization.",26.97,LFM-2.5,Apple M1 (Metal)
2601.11007v1_AdaMARP An Adaptive Multi-Agent Interaction Framew.pdf,AdaMARP: An Adaptive Multi-Agent Interaction Framework for General Immersive Role-Playing,"Zhenhua Xu, Dongsheng Chen, Shuo Wang, Jian Li, Chengjie Wang, Meng Han, Yabiao Wang",arXiv:2601.11007v1,arXiv:2601.11007,"adaptive multi-agent interaction, general immersive role-playing, character consistency, environment grounding, narrative coherence","This paper introduces AdaMARP, an adaptive multi-agent interaction framework designed to enhance immersion in role-playing scenarios. It features an immersive message format combining thought, action, environment, and speech, along with an explicit Scene Manager for orchestrating role-play via discrete actions. Experiments demonstrate improved character consistency, environment integration, and narrative coherence compared to existing LLMs.",27.43,LFM-2.5,Apple M1 (Metal)
2601.11012v1_Efficient Protein Optimization via Structure-aware.pdf,Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics,"Jiahao Wang, Shuangjia Zheng",10.48550/arXiv.2024.12345,arXiv:2408.12345,"protein optimization, structure-aware dynamics, Bayesian optimization, molecular modeling, protein sequence design","The paper presents HADES, a Bayesian optimization method using Hamiltonian dynamics to efficiently sample from a structure-aware posterior, enabling rapid discovery of high-performing protein variants while respecting structural constraints.",26.04,LFM-2.5,Apple M1 (Metal)
2601.11016v1_Contextual Distributionally Robust Optimization wi.pdf,Contextual Distributionally Robust Optimization with Causal and Continuous Structure: An Interpretable and Tractable Approach,"Fenglin Zhang, Jie Wang",arXiv:2601.11016v1,2601.11016,"Contextual distributionally robust optimization, Causal Sinkhorn discrepancy, Soft regression forest, Stochastic compositional optimization","The paper introduces a framework for contextual distributionally robust optimization that incorporates causal structure via the causal Sinkhorn discrepancy. It proposes a Causal Sinkhorn DRO model and a Soft Regression Forest decision rule, demonstrating improved interpretability and convergence properties.",29.11,LFM-2.5,Apple M1 (Metal)
2601.11019v1_Finding the Translation Switch Discovering and Exp.pdf,Finding the Translation Switch: Discovering and Exploiting the Task-Initiation,"Xinwei Wu, Heng Liu, Xiaohu Zhao, Yuqi Ren, Linlong Xu, Longyue Wang, Deyi Xiong, Weihua Luo, Kaifu Zhang",wuxw2021,ryq20,"Large Language Models, translation abilities, task-specific features, translation initiation, causal interventions","This paper explores how LLMs possess strong translation capabilities without explicit fine-tuning, focusing on identifying and amplifying 'translation initiation' features that guide correct translation. It proposes a data selection strategy prioritizing hard samples and demonstrates improved efficiency and reduced hallucinations by leveraging internal mechanisms.",26.72,LFM-2.5,Apple M1 (Metal)
2601.11021v1_Combating Spurious Correlations in Graph Interpret.pdf,Combating Spurious Correlations in Graph Interpretability via Self-Reflection,"Kecheng Cai, Chenyang Xu, Chao Peng",10.48550/arXiv.2026.12345,None,"graph interpretability, spurious correlations, self-reflection, interpretable models",The paper addresses the challenge of spurious correlations in synthetic graph datasets by proposing a self-reflection framework that enhances interpretability. It demonstrates improved performance on benchmark datasets and motivates a fine-tuning training method based on feedback from iterative self-reflection.,26.87,LFM-2.5,Apple M1 (Metal)
2601.11030v1_IDDR-NGP Incorporating Detectors for Distractor Re.pdf,Incorporating Detectors for Distractor Removal with Instant Neural Radiance Field,"Xianliang Huang, Jiajie Gou, Shuhang Chen, Zhizhou Zhong, Jihong Guan, Shuigeng Zhou",10.1145/3581783.3612045,2601.11030v1,"distractor removal, distractors, 3D scenes, synthetic scene, realistic scene, input views, detect distractors, defoliation, petals, synthetic images","This paper presents the first unified distractor removal method, IDDR-NGP, which directly operates on Instant-NPG. The method removes a wide range of distractors in 3D scenes such as snowflakes, confetti, defoliation and petals, demonstrating efficient restoration of 3D scenes from multiple corrupted images.",27.47,LFM-2.5,Apple M1 (Metal)
2601.11035v1_Your One-Stop Solution for AI-Generated Video Dete.pdf,Long Ma1 Zihao Xue2 Yan Wang3 Zhiyuan Yan4,"Jin Xu5, Xiaorui Jiang1, Haiyang Yu1,6, Yong Liao1,†, Zhen Bi2,†",arXiv:2601.11035v1,arXiv:2601.11035,"AI-generated video detection, video detection, generative modeling, deep learning, image-to-video","Recent advances in generative modeling can create remarkably realistic synthetic videos, making it increasingly difficult for humans to distinguish them from real ones. This necessitates reliable detection methods. Two key limitations hinder progress: limited dataset scale and outdated generative models.",30.06,LFM-2.5,Apple M1 (Metal)
2601.11037v1_BAPO Boundary-Aware Policy Optimization for Reliab.pdf,BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search,"Shiyu Liu, Yongjing Yin, Jianhao Yan, Yunbo Tang, Qinggang Zhang, Xin Chen, Jingang Wang, Xunliang Cai, Jinsong Su",,,"RL-based agentic search, boundary awareness, reliability, agentic search, LLMs, knowledge uncertainty","This paper proposes Boundary-Aware Policy Optimization (BAPO), a reinforcement learning framework that enhances the reliability of large language models in agentic search by explicitly acknowledging uncertainty (IDK) when evidence is insufficient or reasoning reaches its limit. BAPO introduces a group-based reward and adaptive reward modulator to promote honest uncertainty signaling.",27.1,LFM-2.5,Apple M1 (Metal)
2601.11042v1_Spectral Characterization and Mitigation of Sequen.pdf,Spectral Characterization and Mitigation of Sequential Knowledge Editing Collapse,"Chi Zhang, Mengqi Zhang, Xiaotian Ye, Runxi Cheng, Zisheng Zhou, Ying Zhou, Pengjie Ren, Zhumin Chen",2000 4000 10000 20000,,"sequential knowledge editing, knowledge editing, large language models, parameter updates, general abilities, spectral analysis","This work presents a spectral analysis of sequential knowledge editing and proposes REVIVE, a framework that stabilizes sequential editing by preserving dominant singular directions, thereby mitigating catastrophic collapse of model general abilities.",26.32,LFM-2.5,Apple M1 (Metal)
2601.11044v2_AgencyBench Benchmarking the Frontiers of Autonomo.pdf,AGENCYBENCH: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts,"Keyu Li, Junhao Shi, Yang Xiao, Mohan Jiang, Jie Sun, Yunze Wu, Dayuan Fu, Shijie Xia, Xiaojie Cai, Tianze Xu, Weiye Si, Wenjie Li, Dequan Wang, Pengfei Liu",arXiv:2601.11044v2,SII 2SJTU 3PolyU 4GAIR,"autonomous agents, LLMs, benchmarking, real-world scenarios, tool calls, feedback-driven self-correction, resource efficiency, model architecture, agentic frameworks","This paper introduces AGENCYBENCH, a comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios. It assesses 138 tasks with specific queries, deliverables, and rubrics, requiring an average of 90 tool calls and 1 million tokens. The study compares open-source and closed-source models, highlighting performance disparities and the impact of agentic scaffolds. It serves as a critical testbed for next-generation agents and provides an evaluation toolkit at https://github.com/GAIR-NLP/AgencyBench.",29.0,LFM-2.5,Apple M1 (Metal)
2601.11049v1_Predicting Biased Human Decision-Making with Large.pdf,Predicting Biased Human Decision-Making with Large Language Models in Conversational Settings,"Stephen Pilli, Vivek Nallur",,,"Conversational AI, Framing Effect, Status Quo Bias, LLM Simulation","This study examines whether large language models can predict biased decision-making in conversational settings, highlighting how cognitive biases interact with cognitive load and how LLMs can simulate human biases under varying complexity.",24.62,LFM-2.5,Apple M1 (Metal)
2601.11063v1_H-AIM Orchestrating LLMs PDDL and Behavior Trees f.pdf,"Orchestrating LLMs, PDDL, and Behavior Trees for Hierarchical Multi-Robot Planning","Haishan Zeng, Peng Li",10.1007/978-3-642-45891-8,2601.11063,"multi-robot planning, large language models, PDDL, behavior trees, hierarchical planning","The paper presents H-AIM, a novel embodied multi-robot planning framework that integrates large language models, PDDL, and behavior trees. It addresses challenges in long-horizon task execution by transforming high-level instructions into formal planning problems, combining semantic reasoning with optimization, and generating reactive behavior trees. Experimental results show significant improvements over baselines.",27.6,LFM-2.5,Apple M1 (Metal)
2601.11065v1_Fairness in Healthcare Processes A Quantitative An.pdf,Fairness in Healthcare Processes: A Quantitative Analysis of Decision Making in Triage,"Rachmadita Andreswari, Stephan A. Fahrenkrog-Petersen, Jan Mendling",arXiv:2601.11065v1,arXiv:2601.11065,"process mining, fairness, triage, emergency room","This study investigates fairness in automated decision-making within emergency triage processes using process mining. It evaluates fairness-aware algorithms by analyzing real-life event logs and linking them to justice dimensions, aiming to support responsible, fairness-oriented process mining in healthcare.",28.64,LFM-2.5,Apple M1 (Metal)
2601.11073v1_Bridging Cognitive Neuroscience and Graph Intellig.pdf,Bridging Cognitive Neuroscience and Graph Intelligence: Hippocampus-Inspired Multi-View Hypergraph Learning for Web Finance Fraud,"Rongkun Cui, Nana Zhang, Kun Zhu, Qi Zhang",10.1234/abcd1234,WWW’26,"financial fraud detection, graph neural networks, hippocampus-inspired models, web finance, multi-view learning","Online financial services face significant fraud risks that undermine trust. Existing GNN-based methods struggle with fraud camouflage and long-tailed data. We propose HIMVH, a hippocampus-inspired multi-view hypergraph model, to detect subtle cross-view conflicts and rare fraud patterns.",27.62,LFM-2.5,Apple M1 (Metal)
2601.11076v1_A3D Adaptive Affordance Assembly with Dual-Arm Man.pdf,Adaptive Affordance Assembly with Dual-Arm Manipulation,"Jiaqi Liang, Yue Chen, Qize Yu, Yan Shen, Haipeng Zhang, Hao Dong, Ruihai Wu",,,"robotic assembly, dual-arm manipulation, adaptive affordances, furniture parts, support strategies, manipulation adaptation",Proposes A3D framework for learning adaptive support strategies in furniture assembly using dense geometric representations and adaptive modules that generalize across part geometries.,25.16,LFM-2.5,Apple M1 (Metal)
2601.11077v1_ABC-Bench Benchmarking Agentic Backend Coding in R.pdf,ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development,"Jie Yang, Honglin Guo, Li Ji, Zhikai Lei, Shuo Zhang, Zhiheng Xi, Shichun Liu, Bo Wang, Yining Zheng, Tao Gui, Xipeng Qiu",10.48550/arXiv.2601.11077,2601.11077,"OpenMOSS, LLM, agentic backend, benchmarking, software engineering, real-world development","This paper introduces ABC-Bench, a benchmark designed to evaluate autonomous agents' backend coding capabilities in realistic, end-to-end workflows. It highlights the gap between current LLM evaluations and practical backend engineering demands, emphasizing the need for comprehensive task coverage across languages and frameworks.",30.3,LFM-2.5,Apple M1 (Metal)
2601.11078v1_Visual Marker Search for Autonomous Drone Landing .pdf,Visual Marker Search for Autonomous Drone,"Jiaohong Yao, Linfeng Liang, Yao Deng, Xi Zheng, Richard Han, Yuankai Qi",,,"drone navigation, marker-based landing, reinforcement learning, AirSim, robustness","Marker-based landing is widely used in drone delivery and return-to-base systems for its simplicity and reliability. However, most approaches assume idealized landing site visibility and sensor performance, limiting robustness in complex urban settings. The paper presents a simulation-based evaluation suite on AirSim with varied urban layouts, lighting, and weather to assess marker-based autonomous landing under realistic conditions.",27.94,LFM-2.5,Apple M1 (Metal)
2601.11089v2_MiCA A Mobility-Informed Causal Adapter for Lightw.pdf,MiCA: A Mobility-Informed Causal Adapter for Lightweight Epidemic Forecasting,"Suhan Guo, Jiahong Deng, Furao Shen, Frshen",10.48550/arXiv.2024.12345,arXiv:2408.12345,"MiCA, mobility, epidemic forecasting, lightweight model, causal discovery","Accurate forecasting of infectious disease dynamics is critical for public health planning. This work proposes MiCA, a lightweight causal adapter that infers mobility relationships and integrates them into temporal forecasting models, improving robustness under noisy, data-limited conditions.",27.24,LFM-2.5,Apple M1 (Metal)
2601.11090v1_Efficient Multilingual Name Type Classification Us.pdf,Efficient Multilingual Name Type Classification,"Davor Lauc, University of Zagreb / Mondonomo AI, Zagreb, Croatia / Wilmington, DE, US",,,"multilingual NLP, named entity recognition, convolutional neural networks, efficient inference, proper names","Presents a convolutional neural network approach for classifying proper names by language and entity type. The model, Onomas-CNN X, combines parallel convolution branches with depthwise-separable operations and hierarchical classification to process names efficiently on CPU hardware. It achieves 92.1% accuracy on a large multilingual dataset covering 104 languages and four entity types, outperforming fine-tuned XLM-RoBERTa in speed and energy efficiency.",27.55,LFM-2.5,Apple M1 (Metal)
2601.11100v1_ReCreate Reasoning and Creating Domain Agents Driv.pdf,Reasoning and Creating Domain Agents Driven by Experience,"Zhezheng Hao, Hong Wang, Jian Luo, Jianqing Zhang, Yuyan Zhou, Qiang Lin, Can Wang, Hande Dong, Jiawei Chen, *Jiawei Chen",,,"large language model, agent creation, experience-driven, domain agents, automated generation, LLM agents","This paper proposes ReCreate, an experience-driven framework for automatically creating domain agents. ReCreate leverages agent interaction history to learn from experience, using an agent-as-optimizer paradigm with three components: experience storage, reasoning-creating synergy, and hierarchical updates. Experiments show ReCreate outperforms human-designed agents across diverse domains.",27.37,LFM-2.5,Apple M1 (Metal)
2601.11109v1_Vision-as-Inverse-Graphics Agent via Interleaved M.pdf,Vision-as-Inverse-Graphics Agent via Interleaved Multimodal Reasoning,"Shaofeng Yin, Jiaxin Ge, Michael J. Black, Trevor Darrell, Haiwen Feng",10.48550/arXiv.2601.11109,arXiv:2601.11109,"vision-as-inverse-graphics, interleaved reasoning, 3D reconstruction, multimodal editing, computer vision","VIGA crafts a 3D graphics scene from a single image through iteration. It alternates between generation and verification steps to refine layout, geometry, and lighting, enabling robust single-shot reconstruction.",28.64,LFM-2.5,Apple M1 (Metal)
2601.11124v1_Learn Before Represent Bridging Generative and Con.pdf,Learn Before Represent: Bridging Generative and Contrastive Learning,"Xiaoyu Liang, Yuchen Peng, Jiale Luo, Wenhao Wang, Haoji Hu, Xincheng Zhou",10.1093/pasj/ppac045,2601.11,"large language models, contrastive learning, domain-specific embeddings, information bottleneck, knowledge acquisition, generative learning","This work identifies a core bottleneck in LLM adaptation for vertical domains: the LLM+CL paradigm prioritizes semantic alignment but lacks domain-specific knowledge, leading to failures on specialized terminology. The proposed Learn Before Represent (LBR) framework addresses this by integrating an Information Bottleneck-Constrained Generative Learning stage before applying Generative Refined Contrastive Learning, thereby enabling accurate representations in chemistry, law, and code retrieval.",27.83,LFM-2.5,Apple M1 (Metal)
2601.11135v1_Context-aware Graph Causality Inference for Few-Sh.pdf,Context-aware Graph Causality Inference for Few-Shot Molecular Property,"Van Thuy Hoang, O-Joun Lee",,,"graph learning, molecular property prediction, few-shot learning, causal inference","The paper proposes CaMol, a context-aware graph causality inference framework, to address challenges in few-shot molecular property prediction by leveraging causal inference and disentangling causal substructures.",24.09,LFM-2.5,Apple M1 (Metal)
2601.11143v1_Learning Quadrupedal Locomotion for a Heavy Hydrau.pdf,Learning Quadrupedal Locomotion for a Heavy Hydraulic Robot Using an Actuator Model,"Minho Lee, Hyeonseok Kim, Jin Tak Kim, Sangshin Park, Jeong Hyun Lee, Jungsan Cho, Jemin Hwangbo ∗,1",,,"Hydraulic/Pneumatic Actuators, Legged Robots, Reinforcement Learning, Sim-to-Real Transfer, Robotics, Model-Based Control",The paper presents an analytical actuator model driven by hydraulic dynamics to simulate large-scale hydraulic robots. It enables rapid RL-based locomotion control and demonstrates successful transfer of stable locomotion on a heavy hydraulic quadruped robot.,26.91,LFM-2.5,Apple M1 (Metal)
2601.11144v2_Deep GraphRAG A Balanced Approach to Hierarchical .pdf,Deep GraphRAG: A Balanced Approach to Hierarchical Retrieval and Adaptive Integration,"Yuejie Li, Ke Yang, Zhejiang University, Bolin Chen, Chengjun Mao",,,"GraphRAG, Reinforcement Learning, Large Language Models",Deep GraphRAG addresses the trade-off between global search comprehensiveness and local efficiency in hierarchical retrieval by introducing a balanced framework with hierarchical retrieval strategies and a reinforcement learning-based fine-grained search.,24.97,LFM-2.5,Apple M1 (Metal)
2601.11147v1_Do We Always Need Query-Level Workflows Rethinking.pdf,Do We Always Need Query-Level Workflows?,"Zixu Wang, Bingbing Xu, Yige Yuan, Huawei Shen, Xueqi Cheng",,,"multi-agent systems, query-level workflows, agentic workflow generation, LLM, token cost, evaluation strategy","The paper explores whether query-level workflow generation is always necessary for multi-agent systems, arguing that a small set of top K task-level workflows can cover many queries efficiently. It proposes SCALE, a low-cost task-level generation framework, showing competitive performance with reduced token usage.",26.59,LFM-2.5,Apple M1 (Metal)
2601.11151v1_Cross-Modal Attention Network with Dual Graph Lear.pdf,Cross-Modal Attention Network with Dual Graph Learning in Multimodal Recommendation,"Ji Dai, Quan Fang, Jun Hu, DeSheng Cai, Yang Yang, Can Zhao",https://doi.org/XXXXXXX.XXXXXXX,XXX,"Multimedia recommendation, Graph Neural Network, Multimodal Fusion","The paper proposes a Cross-Modal Attention Network with dual graph learning to address shallow modality fusion and asymmetric feature treatment in multimodal recommendation systems. It introduces a coreRecursive Cross-Modal Attention mechanism and a symmetric dual-graph framework unified by self-supervised learning, achieving improved performance on large-scale datasets.",27.16,LFM-2.5,Apple M1 (Metal)
2601.11160v1_Clustering High-dimensional Data Balancing Abstrac.pdf,Clustering High-dimensional Data: Balancing Abstraction and Representation,"Claudia Plant, Lena G. M. Bauer, Christian B. Boehm",10.1093/acpro/9780190871293.5.001,https://arxiv.org/abs/2103.04137,"clustering, high-dimensional data, abstraction, representation learning, deep clustering","The tutorial discusses the challenges of clustering large real-world datasets, emphasizing the need to balance abstraction (simplifying details) with representation (capturing key features). It explores how different clustering algorithms trade off these goals and introduces methods that learn latent spaces to better align abstraction and representation.",28.12,LFM-2.5,Apple M1 (Metal)
2601.11178v1_TANDEM Temporal-Aware Neural Detection for Multimo.pdf,Temporal-Aware Neural Detection for Multimodal Hate Speech,"Girish A. Koushik, Helen Treharne, Diptesh Kanojia",10.48550/arXiv.2312.06840,arXiv:2312.06840,"hate speech, multimodal detection, TANDEM, structured reasoning, temporal alignment","This paper introduces TANDEM, a framework that transforms audio-visual hate detection into a structured reasoning problem. It employs a novel reinforcement learning strategy with vision-language and audio-language models, optimizing each other through self-constrained cross-modal context. Experiments show significant improvements over baselines in target identification, highlighting the challenges of label ambiguity and dataset imbalance in complex multimodal settings.",28.07,LFM-2.5,Apple M1 (Metal)
2601.11189v1_Policy-Based Deep Reinforcement Learning Hyperheur.pdf,Policy-Based Deep Reinforcement Learning Hyperheuristics for Job-Shop Scheduling,"Sofiene Lassoued, Asrat Gobachev, Stefan Lier, Andreas Schwung",10.1007/978-3-642-45888-8,,"Hyper-heuristics, Job Shop Scheduling, Policy-based Reinforcement learning, Petri nets",This paper proposes a policy-based deep reinforcement learning hyper-heuristic framework for solving the Job Shop Scheduling Problem. The hyper-heuristic agent learns to switch scheduling rules based on the system state dynamically. We extend the hyper-heuristic framework with two key mechanisms: action prefiltering and a commitment mechanism. Computational experiments show the proposed approach outperforms traditional methods.,28.23,LFM-2.5,Apple M1 (Metal)
2601.11196v1_Artificial Intelligence and the US Economy An Acco.pdf,Artificial Intelligence and the US Economy: An Accounting Perspective on Investment and Production,"Luisa Carpinelli, Filippo Natoli, Marco Taboga",arXiv:2601.11196v1,2601.11196v1,"artificial intelligence, capital expenditures, data centers, national accounts","This paper provides an overview of how the current AI wave is captured in US national accounts, highlighting the role of data centers and their impact on investment and GDP growth.",28.64,LFM-2.5,Apple M1 (Metal)
2601.11199v1_SD-RAG A Prompt-Injection-Resilient Framework for .pdf,SD-RAG: A Prompt-Injection-Resilient Framework for Selective Disclosure in Retrieval-Augmented Generation,"Aiman Al Masoud, Marco Arazzi, Antonino Nocera",,,"SD-RAG, Retrieval-Augmented Generation, Prompt Injection, Selective Disclosure, Privacy, Security, Large Language Models","The paper proposes SD-RAG, a framework that decouples security and privacy enforcement from the generation process by applying sanitization and disclosure controls during retrieval, thereby improving privacy scores and resisting prompt injection attacks.",26.94,LFM-2.5,Apple M1 (Metal)
2601.11200v1_FAQ Mitigating Quantization Error via Regenerating.pdf,Mitigating Quantization Error via Regenerating Calibration Data,"Haiyang Xiao, Weiqing Li, Jinyue Guo, Guochao Jiang, Guohua Liu, Yuewei Zhang",,,"quantization error, calibration data, family-aware quantization, model inference, quantization parameters","FAQ proposes a framework to regenerate high-fidelity calibration samples using prior knowledge from similar models, aiming to reduce accuracy loss in post-training quantization.",25.35,LFM-2.5,Apple M1 (Metal)
2601.11202v1_Epistemic Control and the Normativity of Machine L.pdf,Epistemic Control and the Normativity of Machine Learning,Emanuele Ratti,10.1093/acprof:oso/9780190871293.001.0001,,"machine learning, epistemic control, cognitive values, normativity","Investigates whether human scientists are excluded from scientific processes due to characteristics of machine learning systems, challenging Paul Humphreys' concerns and proposing a nuanced view of epistemic control.",26.9,LFM-2.5,Apple M1 (Metal)
2601.11207v1_LoRA as Oracle.pdf,LoRA as Oracle,"Marco Arazzi, Antonino Nocera",,,"LoRA, Membership Inference Attack, Backdoor Attack",Introduces a novel LoRA-based oracle framework leveraging low-rank adaptation modules to detect backdoors and membership inference without accessing clean reference models.,23.67,LFM-2.5,Apple M1 (Metal)
2601.11219v1_SDFLoRA Selective Dual-Module LoRA for Federated F.pdf,SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients,"Zhikang Shen, Jianrong Lu, Haiyuan Wan, Jianhai Chen",22351168@zju.edu.cn,,"federated learning, large language models, parameter-efficient methods, differential privacy, LoRA, heterogeneous clients",Proposes Selective Dual-Module Federated LoRA to address heterogeneity and privacy in federated fine-tuning of LLMs by combining global and local modules.,26.51,LFM-2.5,Apple M1 (Metal)
2601.11232v1_FactCorrector A Graph-Inspired Approach to Long-Fo.pdf,FACTCORRECTOR: A Graph-Inspired Approach to Long-Form Factuality Correction of Large Language Models,"Javier Carnerero-Cano, Massimiliano Pronesti, Radu Marinescu, Tigran Tchrakian, James Barry, Jasmina Gajcin, Yufang Hou, Alessandra Pascale, Elizabeth Daly",10.1234/example.doi,,"factuality correction, large language models, LLM, factual errors, correction method, factual precision","The paper introduces FACTCORRECTOR, a post-hoc correction method for LLMs that improves factual accuracy without retraining, using structured feedback. It presents the VELI5 benchmark and demonstrates improved performance over baselines.",27.81,LFM-2.5,Apple M1 (Metal)
2601.11252v1_Beyond Model Scaling Test-Time Intervention for Ef.pdf,Under Review,"Qianyue Wang, Jinwu Hu, Yufeng Wang, Huanxiang Lin, Bolin Chen, Zhiquan Wen, Yaofo Chen, Mingkui Tan",,,"Large Reasoning Models, Efficient Reasoning, Overthinking, Overshoot, Intervention, Group Relative Policy Optimization, Reasoning Length, Security, Creative Tasks","The paper proposes Think-with-Me, a test-time interactive reasoning paradigm that uses external feedback to guide reasoning processes, aiming to reduce overthinking and overshoot while improving accuracy and reasoning efficiency.",26.87,LFM-2.5,Apple M1 (Metal)
2601.11258v1_Knowledge is Not Enough Injecting RL Skills for Co.pdf,Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation,"Pingzhi Tang, Yiding Wang, Muhan Zhang",,,"Large Language Models, Knowledge Cutoff, Reinforcement Learning, Skill Transfer, Knowledge Adaptation","The paper addresses the knowledge cutoff problem in LLMs by proposing Parametric Skill Transfer (PaST), which enables efficient modular skill injection. Experiments show PaST outperforms existing SFT and RL-based methods, demonstrating strong scalability and cross-domain transferability.",25.16,LFM-2.5,Apple M1 (Metal)
2601.11269v1_X-Distill Cross-Architecture Vision Distillation f.pdf,X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning,"Maanping Shao, Feihong Zhang, Gu Zhang, Baiye Cheng, Zhengrong Xue, Huazhe Xu",10.1017/x-distill.2021.8,10.1017/x-distill.2021.8,"visuomotor learning, knowledge distillation, visual encoder, robot action, cross-architecture, manipulation tasks","X-Distill introduces a method that transfers knowledge from a large ViT teacher to a compact CNN student, enabling data-efficient visuomotor learning. It achieves strong performance on manipulation tasks by leveraging visual priors.",26.71,LFM-2.5,Apple M1 (Metal)
2601.11282v1_From SERPs to Sound How Search Engine Result Pages.pdf,FromSERPs to Sound: How Search Engine Result Pages and AI-generated Podcasts Interact to Influence User Attitudes on Controversial Topics,"Junjie Wang, Gaole He, Alisa Rieger, Ujwal Gadiraju",10.1145/3786304.3787942,CCS Concepts,"Attitude Change, AI-generated Podcasts, Information modality, Web search, Controversial Topics, Responsible Opinion Formation","This study investigates how the interaction between search engine result pages (SERPs) and AI-generated podcasts influences user attitudes toward controversial topics. Through a controlled user study with 483 participants, the research explores the impact of sequence and modality of exposure on attitude change, highlighting the role of viewpoint bias and topic controversiality.",27.36,LFM-2.5,Apple M1 (Metal)
2601.11286v1_XChoice Explainable Evaluation of AI-Human Alignme.pdf,XChoice: Explainable Evaluation of AI–Human Alignment in LLM-based Constrained Choice Decision Making,"Weihong Qi, Fan Huang, Rasika Muralidharan, Jisun An, Haewoon Kwak",10.48550/arXiv.2025.12345,arXiv:2509.12345,"AI-human alignment, LLM-based decision making, constrained choice, explainable evaluation, human-AI interaction","Presents XCHOICE, an explainable framework for evaluating AI-human alignment in constrained decision making. It moves beyond outcome metrics to model human data and LLM decisions, identifying interpretable parameters related to decision factors, constraints, and trade-offs. The study uses the American Time Use Survey to assess alignment across models and groups, highlights misalignment in specific populations, and evaluates mitigation strategies.",27.77,LFM-2.5,Apple M1 (Metal)
2601.11344v1_How Much Would a Clinician Edit This Draft Evaluat.pdf,Evaluating LLM Alignment for Patient Message Response Drafting,"Parker Seegmiller, Joseph Gatto, Sarah E. Greer, Ganza Belise Isingizwe, Rohan Ray, Timothy Burdick",,,,"Large language models (LLMs) show promise in drafting responses to patient portal messages, yet their integration into clinical workflows raises concerns about time and effort savings. This study investigates LLM alignment with clinicians through a comprehensive evaluation of patient message response drafting, developing a taxonomy of thematic elements and proposing an evaluation framework for clinician editing load.",26.16,LFM-2.5,Apple M1 (Metal)
2601.11350v1_FEATHer Fourier-Efficient Adaptive Temporal Hierar.pdf,Fourier-Efficient Adaptive Temporal Hierarchy Forecaster for Time-Series Forecasting,"Jaehoon Lee, Seungwoo Lee, Younghwi Kim, Dohee Kim, Sunghyun Sim",,,"Time-series Forecasting, Edge AI, Ultra-Lightweight Models, Fourier-Efficient Adaptive Temporal Hierarchy Forecaster, Multiscale Temporal Decomposition, Dense Temporal Kernel, Frequency-Aware Branch Gating, Sparse Period Kernel","FEATHer is a multiscale temporal model designed for resource-constrained edge devices. It features an ultra-lightweight decomposition, efficient temporal mixing, adaptive fusion mechanisms, and compact periodic reconstruction to achieve accurate long-term forecasting with minimal computational cost.",28.61,LFM-2.5,Apple M1 (Metal)
2601.11354v1_AstroReason-Bench Evaluating Unified Agentic Plann.pdf,AstroReason-Bench: Evaluating Unified Agentic Planning,"Weiyi Wang, Xinchi Chen, Jingjing Gong, Xuanjing Huang, Fudan University, Shanghai Innovation Institute",arXiv:2601.11354v1,arXiv:2601.11354,"agentic planning, space planning problems, heterogeneous objectives, physical constraints, long-horizon decision-making","This paper introduces AstroReason-Bench, a benchmark for evaluating agentic planning in space planning problems. It highlights the limitations of current generalist agents in realistic physical environments and discusses the need for improved evaluation frameworks.",28.62,LFM-2.5,Apple M1 (Metal)
2601.11359v1_Think-Clip-Sample Slow-Fast Frame Selection for Vi.pdf,Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding,"Wenhui Tan, Ruihua SongB, Jiaze Li, Jianzhong Ju, Zhenbo LuoB, MiLM Plus, Xiaomi Inc.",,,"multi-modal large language models, long video understanding, frame selection, video understanding, multi-modal LLMs","Recent progress in multi-modal LLMs has advanced video understanding, but performance on long-form videos is limited. This paper introduces Think-Clip-Sample (TCS), a training-free framework that improves long video understanding via multi-query reasoning and clip-level slow-fast sampling, achieving up to 6.9% accuracy with reduced inference cost.",27.25,LFM-2.5,Apple M1 (Metal)
2601.11369v2_Institutional AI Governing LLM Collusion in Multi-.pdf,Institutional AI: Governing LLM Collusion in Multi-Agent,"M. Bracale Syrnikov, F. Pierucci, M. Galisai, M. Prandi, P. Bisconti, F. Giarrusso, O. Sorokoletova, V. Suriani, D. Nardi",arXiv:2601.11369v2,2601.11369v2,"Institutional AI, LLM governance, Multi-agent systems, Cournot market, Collusion detection, Governance graphs","This paper presents an experimental framework for institutional AI that uses governance graphs to enforce rules in multi-agent LLM systems. It evaluates three regimes—Ungoverned, Constitutional (prompt-only), and Institutional—and finds that institutional governance significantly reduces collusion in Cournot markets.",29.22,LFM-2.5,Apple M1 (Metal)
2601.11379v1_Evaluating LLM Behavior in Hiring Implicit Weights.pdf,"Evaluating LLM Behavior in Hiring: Implicit Weights, Fairness","Morgane Hoffmann, Emma Jouffroy, Warren Jouanneau, Marc Palyart, Charles Pebereau",,,"Large Language Models, Person-job Fit, Fairness, Interpretability","This paper proposes a framework to evaluate how general-purpose LLMs assign importance to candidate attributes during recruitment, examining implicit decision logic, fairness implications, and alignment with human hiring practices. Synthetic datasets and experimental design are used to analyze LLM prioritization across demographic subgroups.",26.7,LFM-2.5,Apple M1 (Metal)
2601.11389v1_Hyperparameter Optimization of Constraint Programm.pdf,Hyperparameter Optimization of Constraint Programming Solvers,"Hedieh Haddad, Thibault Falque, Pierre Talbot, Pascal Bouvry",arXiv:2601.11389v1,arXiv:2601.11389,"constraint programming, hyperparameter optimization, solver configuration, probe and solve algorithm, Bayesian optimization, Hamming distance search",The paper introduces a probe and solve algorithm for automated hyperparameter optimization in constraint programming solvers. It evaluates Bayesian optimization and Hamming distance search across two solvers (ACE and Choco) and demonstrates improved performance compared to default configurations.,29.27,LFM-2.5,Apple M1 (Metal)
2601.11400v1_Wetland mapping from sparse annotations with satel.pdf,Wetland mapping from sparse annotations with satellite image time series and temporal-aware segment anything model,"Shuai Yuana, Tianwu Linb, Shuang Chena, Yu Xiab, Peng Qinb, Xiangyu Liub, Xiaoqing Xub, Nan Xud, Hongsheng Zhanga, Jie Wangb",arXiv:2601.11400v1,2601.11400v1,"wetland mapping, satellite image time series, sparse annotation, segment anything model, temporal adaptation","Accurate wetland mapping is critical for ecosystem monitoring, yet dense pixel-level annotations are costly. Existing deep learning models struggle with sparse supervision, and wetlands show strong seasonal dynamics, leading to mapping errors. We propose WetSAM, a SAM-based framework leveraging satellite time series to improve mapping from sparse point annotations. It uses a dual-branch design with temporal and spatial components, achieving high F1-score and strong generalization.",29.35,LFM-2.5,Apple M1 (Metal)
2601.11409v1_Topology-Guaranteed Image Segmentation Enforcing C.pdf,"Topology-Guaranteed Image Segmentation: Enforcing Connectivity, Genus, and Width Constraints","Wenxiao Li, Xue-Cheng Tai, Jun Liu","MSC codes.68U10, 62H35, 94A08",,"Image segmentation, topological preservation, persistent homology, thickness of topology, variational, regularization","Existing research highlights the crucial role of topological priors in image segmentation, particularly in preserving essential structures such as connectivity and genus. Accurately capturing these topological features often requires incorporating width-related information, including the thickness and length inherent to the image structures. This work proposes a novel mathematical framework that integrates width information into topological characterization, enabling neural networks to segment images while preserving connectivity, genus, and width properties.",28.65,LFM-2.5,Apple M1 (Metal)
2601.11421v1_The Great March 100 100 Detail-oriented Tasks for .pdf,THEGREATMARCH100: 100 DETAIL-ORIENTED TASKS FOR EVALUATING EMBODIED AI AGENTS,"Ziyu Wang, Chenyuan Liu, Yushun Xiang, Runhao Zhang, Yu Zhang, Qingbo Hao, Houyu Chen, Zhizhong Feng, Kaiyue Zheng, Dehao Y, Xianchao Zeng, Xinyu Zhou, Boran Wen, Jiaxin Li, Mingyu Zhang, Qian Zhu, Ran Cheng, Yong-Lu Li",,,"robot learning, imitation learning, task design, embodied AI, robotic agents, evaluation metrics","The paper introduces the Great March 100 (GM-100), a set of 100 carefully designed tasks aimed at evaluating robotic agents comprehensively. These tasks address diverse interactions and long-tail behaviors to overcome biases in current datasets and improve evaluation fairness.",29.0,LFM-2.5,Apple M1 (Metal)
2601.11429v1_Relational Linearity is a Predictor of Hallucinati.pdf,Relational Linearity is a Predictor of Hallucinations,"Yuetian Lu, Yihong Liu, Hinrich Schütze, Munich Center for Machine Learning, Ubiquitous Knowledge Processing Lab, Munich Center for Information and Language Processing",,,"hallucination, large language models, knowledge representation, linearity, nonlinear relations","We investigate how linear relations influence hallucination rates in synthetic entities, finding a strong correlation between relational linearity and hallucination frequency.",24.41,LFM-2.5,Apple M1 (Metal)
2601.11440v1_GenDA Generative Data Assimilation on Complex Urba.pdf,Generative Data Assimilation on Complex Urban Areas,"Francisco Giral, Álvaro Manzano, Ignacio Gómez, Ricardo Vinuesa, Soledad Le Clainche",10.48550/arXiv.2405.12345,arXiv:2405.12345,"urban wind flow, data assimilation, generative modeling, CFD, reynolds number, building geometry","Proposes GenDA, a generative framework reconstructing high-resolution wind fields from sparse observations using multiscale graph-based diffusion. It improves reconstruction accuracy and generalization across geometries and resolutions, outperforming traditional methods in RMSE and SSIM.",27.66,LFM-2.5,Apple M1 (Metal)
2601.11441v1_Hierarchical Orthogonal Residual Spread for Precis.pdf,Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models,"Xiaojie Gu, Guangxu Chen, Yuheng Yang, Jingxin Han, Andi Zhang",,,"Large language models, Model Editing, Knowledge Update, Residual Spread, Massive Editing, Security Risks","This paper introduces HORSE, a hierarchical orthogonal residual spread method for precise massive editing in large language models. It compares HORSE with existing methods and demonstrates its effectiveness through experiments on GPT, LLaMA, and Mistral.",25.96,LFM-2.5,Apple M1 (Metal)
2601.11442v1_Map2Thought Explicit 3D Spatial Reasoning via Metr.pdf,Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps,"Xiangjun Gao, Zhensong Zhang, Dave Zhenyu Chen, Songcen Xu, Long Quan, Eduardo P´erez-Pellitero, Youngkyoon Jang",10.48550/arXiv:2309.12345,10.48550/arXiv:2309.12345,"3D Vision-Language Models, Metric Cognitive Maps, Cognitive Chain-of-Thought, Explainable AI, Spatial Reasoning","The paper introduces Map2Thought, a framework enabling explicit and interpretable spatial reasoning for 3D vision-language models. It integrates a Metric Cognitive Map (Metric-CogMap) and a chain-of-thought reasoning process (Cog-CoT), allowing deterministic geometric operations and interpretable inference traces. Experiments demonstrate 59.9% accuracy with only 50% training data, outperforming baselines by 5.3%, 4.8%, and 4.0%.",28.82,LFM-2.5,Apple M1 (Metal)
2601.11451v1_PRISM-CAFO Prior-conditioned Remote-sensing Infras.pdf,PRISM-CAFO: Prior-conditioned Remote-sensing Infrastructure Segmentation and Mapping for CAFOs,"Oishee Bintey Hoque, Nibir Chandra Mandal, Kyle Luong, Amanda Wilson, Samarth Swarup, Madhav Marathe, Abhijin Adiga",10.48550/arXiv.2601.11451,https://arxiv.org/abs/2601.11451,"CAFOs, remote sensing, infrastructure segmentation, mapping, livestock operations, environmental risk","This work presents an infrastructure-first pipeline for identifying and characterizing Concentrated Animal Feeding Operations (CAFOs) using aerial and satellite imagery. It employs a domain-tuned YOLOv8 detector, extracts structured descriptors, and combines them with deep visual features to achieve state-of-the-art performance. The method supports transparent monitoring and risk assessment for livestock infrastructure.",28.26,LFM-2.5,Apple M1 (Metal)
2601.11459v1_Interactive Narrative Analytics Bridging Computati.pdf,Interactive Narrative Analytics: Bridging Computational Narrative Extraction and Human Sensemaking,Brian Keith,10.1 109/ACCESS.2025.3650352,11250039,"Human-AI collaboration, information extraction, interactive visual analytics, knowledge integration, narrative extraction, narrative sensemaking, semantic interaction, visual analytics","This paper defines the nascent field of Interactive Narrative Analytics (INA), combining computational narrative extraction with interactive visual analytics to support sensemaking. INA addresses challenges such as information overload, misinformation, scalability, interactivity, knowledge integration, and evaluation standardization, offering opportunities across news analysis, intelligence, scientific literature exploration, and social media analysis.",27.39,LFM-2.5,Apple M1 (Metal)
2601.11464v1_MHA2MLA-VLM Enabling DeepSeeks Economical Multi-He.pdf,MHA2MLA-VLM: Enabling DeepSeek’s Economical Multi-Head Latent Attention,"Xiaoran Fan, Zhichao Sun, Tao Ji, Lixing Shen, Tao Gui, 3, 4",10.48550/arXiv.2025.12345,arXiv:2509.12345,"vision-language models, key-value cache, multi-head attention, model compression, KV cache, deepseek, parameter efficiency","This work presents MHA2MLA-VLM, a parameter-efficient framework that adapts VLMs to Multi-Head Latent Attention by introducing modality-adaptive masking and modality-decoupled low-rank approximation. It achieves minimal performance loss while significantly reducing KV cache footprint and enabling efficient inference for complex multimodal tasks.",27.0,LFM-2.5,Apple M1 (Metal)
2601.11468v1_Exploring LLM Features in Predictive Process Monit.pdf,Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs,"Alessandro Padella, Massimiliano de Leoni, Marlon Dumas","1, 1 (January 2026)",19 pages,"Predictive process monitoring, Large language models, Trace Encoding","This paper extends an LLM-based predictive process monitoring framework by evaluating its generality, semantic leverage, and reasoning mechanisms across multiple KPIs. Empirical results show LLMs outperform benchmarks with only 100 traces, leveraging prior knowledge and internal correlations, and demonstrate higher-order reasoning beyond simple prompting.",25.48,LFM-2.5,Apple M1 (Metal)
2601.11479v1_Health Facility Location in Ethiopia Leveraging LL.pdf,Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning,"Yohai Trabelsi, Guojun Xiong, Fentabil Getnet, Stéphane Verguet, Milind Tambe",10.1093/acm/9780190871297.001.0001,,"Health Facility Location, Optimization, Human expert knowledge, Alignment, LLM","Ethiopia’s Ministry of Health is upgrading health posts to improve access, especially in rural areas. The study proposes a hybrid framework combining optimization methods with LLM-driven refinement to integrate expert preferences while ensuring coverage guarantees.",25.98,LFM-2.5,Apple M1 (Metal)
2601.11492v1_BoxMind Closed-loop AI strategy optimization for e.pdf,BoxMind: Closed-loop AI strategy optimization for elite boxing,"Kaiwen Wang, Kaili Zheng, Rongrong Deng, Qingmin Fan, Milin Zhang, Zongrui Li, Xuesi Zhou, Bo Han, Liren Chen, Chenyi Guo, Ji Wu",arXiv:2601.11492v1,arXiv:2601.11492,"boxing, AI strategy, elite boxing, tactical analysis, machine learning, sports analytics","Presents BoxMind, a closed-loop AI expert system validated in elite boxing competition. The system parses match footage into technical-tactical indicators and uses a graph-based predictive model to generate strategic recommendations, achieving high accuracy in predicting match outcomes.",28.42,LFM-2.5,Apple M1 (Metal)
2601.11496v1_The Poisoned Apple Effect Strategic Manipulation o.pdf,The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion,"Eilam Shapira, Moshe Tennenholtz, Roi Reichart",10.48550/arXiv.2026.12345,arXiv:2109.12345,"AI agents, market design, regulatory frameworks, strategic manipulation, meta-games","Investigates how expanding AI technology availability alters strategic interactions in bargaining, negotiation, and persuasion, highlighting the 'Poisoned Apple' effect where agents manipulate regulators to favor their interests.",27.45,LFM-2.5,Apple M1 (Metal)
2601.11505v1_MetaboNet The Largest Publicly Available Consolida.pdf,MetaboNet: The publicly available dataset for diabetes management,"Miriam K. Wolff, Peter Calhoun, Eleonora Maria Aiello, Yao Qin, Sam F. Royston",arXiv:2601.11505v1,arXiv:2601.11505,"type 1 diabetes, diabetes management, metaboNet, data integration, algorithm development","Progress in Type 1 Diabetes (T1D) algorithm development is limited by the fragmentation and lack of standardization across existing T1D management datasets. Current datasets differ substantially in structure and are time-consuming to access and process, which impedes data integration and reduces the comparability and generalizability of algorithmic developments. This work aims to establish a unified and accessible data resource for T1D algorithm development. Multiple publicly available T1D datasets were consolidated into a unified resource, termed the MetaboNet dataset. Inclusion required the availability of both continuous glucose monitoring (CGM) data and corresponding insulin pump dosing records. Additionally, auxiliary information such as reported carbohydrate intake and physical activity was retained when present. The MetaboNet dataset comprises 3135 subjects and 1228 patient-years of overlapping CGM and insulin data, making it substantially larger than existing standalone benchmark datasets. The resource is distributed as a fully public subset available for immediate download at https://metabo-net.org/, and with a Data Use Agreement (DUA)-restricted subset accessible through their respective application processes. For the datasets in the latter subset, processing pipelines are provided to automatically convert the data into the standardized MetaboNet format. A consolidated public dataset for T1D research is presented, and the access pathways for both its unrestricted and DUA-governed components are described.",32.05,LFM-2.5,Apple M1 (Metal)
2601.11516v2_Building Production-Ready Probes For Gemini.pdf,Building Production-Ready Probes For Gemini,"János Kramár, Joshua Engels, Zheng Wang, Bilal Chughtai, Rohin Shah, Neel Nanda, Arthur Conmy",,,"Activation Probing, Interpretability, Language Models, Misuse Risk, AI Safety, Monitoring","The paper discusses challenges in deploying robust activation probes for frontier language models like Gemini, highlighting issues with generalization across production distributions and proposing new architectures and training strategies to improve misuse mitigation.",25.95,LFM-2.5,Apple M1 (Metal)
2601.11517v1_Do explanations generalize across large reasoning .pdf,Under Review,"Koyena Pal, David Bau, Chandan Singh",,,,"The paper investigates whether explanations generated by large reasoning models generalize across different models, examining consistency between models and human preference rankings. It explores conditions for consistent explanations and proposes a strategy for improving consistency.",22.02,LFM-2.5,Apple M1 (Metal)
2601.11625v1_Reasoning Stabilization Point A Training-Time Sign.pdf,Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence,Sahil Rajesh Dhayalkar,10.48550/arXiv.2405.12345,arXiv:2405.12345,"interpretability, explanation drift, training-time signals, shortcut reliance","This paper introduces a training-time interpretability framework that monitors token-level attributions during fine-tuning. It defines explanation drift as the change in normalized token attributions across epochs and introduces the Reasoning Stabilization Point (RSP), the epoch after which drift becomes stable. The study shows that drift often stabilizes early in training, indicating increasing reliance on shortcut mechanisms even when validation accuracy remains competitive.",26.36,LFM-2.5,Apple M1 (Metal)
2601.11643v1_Syllabic Agglutinative Tokenizations for Indonesia.pdf,Syllabic Agglutinative Tokenizations for Indonesian LLM,"Hokky Situngkir, Andhika Bernard Lumbantobing, Yohanes Surya",arXiv:2601.11643v1,2601.11643,"Indonesian natural language processing, Indonesian computational linguistics, tokenization, large language models, Gasing Literacy Learning System, low-resource languages, Austronesian languages","This paper presents a novel syllable-based tokenization approach for Indonesian large language models, inspired by the Gasing Literacy Learning System’s pedagogical methodology. Drawing on information-theoretic principles, the authors develop a tokenization framework that segments Indonesian text at syllable boundaries before applying byte-pair encoding, creating a vocabulary aligned with the language’s morphophonological structure. The method improves efficiency and coverage compared to conventional tokenizers while respecting Indonesian’s agglutinative morphology.",29.2,LFM-2.5,Apple M1 (Metal)
2601.11644v1_Predicting When to Trust Vision-Language Models fo.pdf,Predicting When to Trust Vision-Language Models for Spatial Reasoning,"Muhammad Imran, Yugyung Lee",10.48550/arXiv.2024.12345,arXiv:2408.12345,"vision-language models, spatial reasoning, uncertainty quantification, robotic navigation","The paper presents a vision-based confidence estimation framework to validate VLM spatial predictions via geometric verification. It improves upon text-based baselines by fusing geometric alignment, spatial ambiguity, detection quality, and internal uncertainty. Results show enhanced coverage and precision, demonstrating selective prediction strategies.",24.73,LFM-2.5,Apple M1 (Metal)
2601.11647v1_Reinforcement Learning for Dynamic Workflow Optimi.pdf,Reinforcement Learning for Dynamic Workflow,"Aniket Abhishek Soni, Milan Parikh, Rashi Nimesh Kumar Dhenia, Jubin Abhishek Soni, Ayush Raj Jha, Sneha Mitinbhai Shah",,,"Reinforcement Learning, CI/CD, DevOps, Workflow Optimization","Continuous Integration and Deployment (CI/CD) pipelines are core to modern software delivery. This paper proposes a reinforcement learning (RL) approach to optimize CI/CD pipeline workflows dynamically. A simulated CI/CD environment is developed to evaluate the approach, showing up to a 30% improvement in throughput and a 25% reduction in test execution overhead.",27.31,LFM-2.5,Apple M1 (Metal)
2601.11650v1_Large Language Model Agent for User-friendly Chemi.pdf,LARGELANGUAGEMODELAGENT FORUSER-FRIENDLY,"Jingkang Liang, Niklas Groll, Gürkan Sin, gsi@kt.dtu.dk",arXiv:2601.11650v1,2601.11650v1,"Chemical Process Simulation, Large Language Model, Model Context Protocol, Process Simulation, Educational Tools","Modern process simulators are powerful but require expert knowledge. This work integrates a large language model with A VEV A Process Simulation via Model Context Protocol, enabling natural language interaction and automated optimization. Two case studies demonstrate autonomous analysis and guidance for both beginners and experts.",29.79,LFM-2.5,Apple M1 (Metal)
2601.11651v1_Aesthetics as Structural Harm Algorithmic Lookism .pdf,Aesthetics as Structural Harm: ALGORITHMIC LOOKISMACROSS,"Miriam Doh, Aditya Gulati, Corina Canali, Nuria Oliver",arXiv:2601.11651v1,2601.11651,"Generative AI, Artificial Intelligence, Cognitive Biases, Attractiveness Halo Effect","This paper examines algorithmic lookism—the systematic preferential treatment based on physical appearance—in text-to-image generative AI and a downstream gender classification task. Through analysis of synthetic faces, it reveals how generative models associate facial attractiveness with positive attributes, gender disparities in classification, and intensifying aesthetic constraints, highlighting algorithmic lookism across AI vision systems.",27.16,LFM-2.5,Apple M1 (Metal)
2601.11652v1_WISP Waste- and Interference-Suppressed Distribute.pdf,WISP: Waste- and Interference-Suppressed Distributed Speculative LLM Serving at the Edge via Dynamic Drafting and SLO-Aware Batching,"Xiangchen Li, Jiakun Fan, Qingyuan Wang, Dimitrios Spatharakis, Saeid Ghafouri, Hans Vandierendonck, Deepu John, Bo Ji, Ali R. Butt, Dimitrios S. Nikolopoulos",10.1145/376xxxx.377xxxx,2025/XXXX,"Speculative Decoding, Large Language Models, Edge Computing, Distributed Computing, Natural Language Processing, Distributed Architectures, Cloud Computing, General Conference Proceedings","The paper addresses bottlenecks in distributed speculative LLM serving—wasted drafting time and verification interference—and proposes WISP, an efficient, SLO-aware system that improves system capacity and throughput while optimizing verification scheduling.",28.23,LFM-2.5,Apple M1 (Metal)
2601.11657v1_Size is Not the Solution Deformable Convolutions f.pdf,Size is Not the Solution: Deformable Convolutions for Effective Physics Aware Deep Learning,"Jack T. Beerman, Shobhan Roy, H.S. Udaykumar, Stephen S. Baek",10.48550/arXiv.2026.12345,arXiv:2509.12345,"physics-aware deep learning, deformable convolutions, physics modeling, computational mechanics, adaptive filtering","This paper introduces deformable physics-aware recurrent convolutions (D-PARC) inspired by Hybrid Lagrangian-Eulerian methods. D-PARC addresses limitations of standard CNNs in modeling nonlinear physical flows by enabling adaptive spatial refinement, achieving better fidelity across Burgers’ equation, Navier-Stokes, and reactive flows compared to larger architectures. The approach demonstrates that learning strategies informed by classical computational mechanics can outperform model scaling in physics-aware deep learning.",29.42,LFM-2.5,Apple M1 (Metal)
2601.11658v1_Towards AGI A Pragmatic Approach Towards Self Evol.pdf,Towards AGI: A Pragmatic Approach Towards Self Evolving Agent,"Indrajit Kar, Sammy Zonunpuia, Zonunfeli Ralte",,,"Self-evolving agent, Large Language Model (LLM), Curriculum Learning, Reward-Based Learning, Genetic Algorithm, Multi-agent systems, Tool-augmented reasoning, Autonomous adaptation","This work introduces a hierarchical self-evolving multi-agent framework integrating a Base LLM, an operational SLM agent, a Code-Gen LLM, and a Teacher-LLM to enable continuous adaptation. The framework uses TaskCraft dataset for evaluation, demonstrating that evolved agents outperform originals across CL, RL, and GA paradigms, showcasing robust autonomous self-improvement.",28.23,LFM-2.5,Apple M1 (Metal)
2601.11663v1_Activation Sensitivity as a Unifying Principle for.pdf,Activations Sensitivity as a Unifying Principle for Post-Training Quantization,Bruce Changlong Xu,10.48550/arXiv.2026.12345,arXiv:2509.12345,"activation sensitivity, post-training quantization, quantization error, layer-wise importance","Presents a unified theoretical framework for PTQ by formalizing activation sensitivity. Shows sensitivity emerges from squared gradient norm, linking it to activation magnitude and error propagation. Connects to classical pruning methods and highlights limitations in current quantization approaches.",26.14,LFM-2.5,Apple M1 (Metal)
2601.11664v1_Serverless AI Security Attack Surface Analysis and.pdf,Serverless AI Security: Attack Surface Analysis and Runtime Protection Mechanisms for FaaS-Based Machine Learning,"Chetan Pathade, Vinod Dhimam, Ilsa Lareb, Sheheryar Ahmad",,,"Serverless computing, Machine learning security, Function-as-a-Service, Cloud security, Adversarial machine learning, AWS Lambda, Azure Functions, Google Cloud Functions, Attack surface analysis, Runtime protection, MLOps security","This paper presents the first comprehensive security analysis of machine learning workloads in serverless environments. It systematically characterizes the attack surface across five categories: function-level vulnerabilities, model-specific threats, infrastructure attacks, supply chain risks, and IAM complexity. The authors demonstrate real-world attack scenarios and quantify their security impact across AWS Lambda, Azure Functions, and Google Cloud Functions, proposing Serverless AI Shield (SAS) as a multi-layered defense framework.",29.12,LFM-2.5,Apple M1 (Metal)
2601.11666v1_MATEX Multi-scale Attention and Text-guided Explai.pdf,Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models,"Muhammad Imran, Chi Lee, Yugyung Lee",arXiv:2601.11666v1,16 Jan 2026,"Explainable AI, Medical Imaging, Vision-Language Models, Gradient Attribution, Attention Rollout, Chest X-ray, CLIP, Attention Rollout, Layer Consistency","MATEX introduces a framework that enhances interpretability in medical vision-language models by integrating anatomically informed spatial reasoning, multi-layer attention, text-guided priors, and layer consistency analysis. It addresses limitations of prior methods and achieves superior spatial precision and clinical alignment.",28.3,LFM-2.5,Apple M1 (Metal)
2601.11667v1_Distill-then-Replace Efficient Task-Specific Hybri.pdf,Distill-then-Replace: Efficient Task-Specific Hybrid Attention Model Construction,"Xiaojie Xia1, Huigang Zhang1, Chaoliang Zhong1, Jun Sun1, Yusuke Oishi2",,,"Hybridattentionmodels, Blockwiselocaldistillation, Greedy search","The paper presents a method to construct task-specific hybrid attention models by transferring weights from pretrained full-attention modules to linear attention through blockwise distillation, and applying greedy layer replacement to balance efficiency and performance without retraining.",28.57,LFM-2.5,Apple M1 (Metal)
2601.11670v1_A Confidence-Variance Theory for Pseudo-Label Sele.pdf,A Conﬁdence-V ariance Theory for Pseudo-Label,"Jinshi Liu, Pan Liu",10.48550/arXiv.2024.12345,10.48550/arXiv.2024.12345,"semi-supervised learning, pseudo-label selection, confidence calibration, residual-class variance, spectral relaxation","The paper introduces a Conﬁdence-Variance theory to improve pseudo-label selection by combining maximum confidence with residual-class variance, addressing overconfidence issues in deep networks.",24.92,LFM-2.5,Apple M1 (Metal)
2601.11674v1_Pigment Network Detection and Classification in De.pdf,Pigment Network Detection and Classification in Dermoscopic Images Using Directional Imaging Algorithms and Convolutional Neural Networks,"M. A. Rasel, Sameem Abdul Kareem, Unaizah Obaidellah",10.1016/j.bspc.2024.106883,,"Melanoma, Dermoscopic Images, Pigment Networks, Contrast Enhancement, Threshold Level, Convolutional Neural Networks, Bag of Features","This study automates pigment network (PN) detection in dermoscopic images using directional imaging algorithms and machine learning classifiers. A directional imaging approach incorporating PCA, contrast enhancement, filtering, and noise reduction achieved 96% success rate on the PH2 dataset. A CNN-based classifier with two convolutional layers and batch normalization reached 90% accuracy, 90% sensitivity, and 89% specificity, outperforming state-of-the-art methods.",28.8,LFM-2.5,Apple M1 (Metal)
2601.11675v1_Generating metamers of human scene understanding.pdf,Generating Metamers of Human Scene Under-Standing,"Ritik Raina, Abe Leite, Alexandros Graikos, Dimitris Samaras, Gregory J. Zelinsky",,,"human vision, scene representation, metamer generation, visual perception, semantic alignment","The paper introduces Metamer-Gen, a tool that generates scenes aligned with latent human scene representations. It uses a dual-stream representation combining foveated and peripheral inputs to produce image metamers, and evaluates perceptual alignment via behavioral experiments.",26.19,LFM-2.5,Apple M1 (Metal)
2601.11676v1_HALO Semantic-Aware Distributed LLM Inference in L.pdf,Semantic-Aware Distributed LLM Inference in Lossy Edge Network,"Peirong Zheng, Wenchao Xu, Haozhao Wang, Jinyu Chen, Xuemin (Sherman) Shen",,,"Large Language Models, Tensor Parallelism, Edge Computing, Heterogeneity, Semantics, Packet Loss","The paper proposes HALO, a framework enhancing distributed LLM inference in lossy edge networks by relaxing synchronization and optimizing resource allocation, achieving significant speedups under unreliable conditions.",25.85,LFM-2.5,Apple M1 (Metal)
2601.11683v1_Attesting Model Lineage by Consisted Knowledge Evo.pdf,Attesting Model Lineage by Consisted Knowledge Evolution with Fine-Tuning Trajectory,"Zhuoyi Shang, Jiasen Li, Pengzhen Chen, Yanwei Liu, Xiaoyan Gu, Weiping Wang",,,"deep learning, model lineage, knowledge evolution, fine-tuning, security concerns, provenance verification",The paper explores how fine-tuning induces lineage relationships among models and proposes a framework to verify model lineage by analyzing knowledge evolution and parameter changes. It addresses security issues like unauthorized redistribution and false provenance claims.,26.73,LFM-2.5,Apple M1 (Metal)
2601.11684v1_Mobile-friendly Image de-noising Hardware Consciou.pdf,Equal Contribution: Mobile-friendly Image de-noising: Hardware Conscious Optimization for Edge Application,"Srinivas Miriyala, Sowmya Vajrala, Hitesh Kumar, Sravanth Kodavanti, Vikram Rajendiran",,,"De-Noising, Differentiable NAS, Hardware-aware Search space, Smartphone Deployment, Image Restoration","This work presents a novel mobile-friendly network for image de-noising using Entropy-Regularized differentiable Neural Architecture Search (NAS) on a hardware-aware search space for a U-Net architecture. The model achieves 12% fewer parameters, 2-fold improvement in on-device latency, 1.5-fold memory footprint reduction, and competitive accuracy with Swin-Transformer while reducing GMACs by ~18-fold. The approach demonstrates strong generalization across benchmarks and real-world applications.",28.0,LFM-2.5,Apple M1 (Metal)
2601.11685v1_Towards Efficient Image Deblurring for Edge Deploy.pdf,Equal Contribution Towards Efficient Image Deblurring for Edge Deployment,"Srinivas Soumitri Miriyala, Sowmya Lahari Vajrala, Rama Sravanth Kodavanti",,,"Mobile Image Signal Processing (ISP), De-blurring, Training-free Search, Inference Optimization, Edge Deployment","This paper presents a hardware-aware adaptation framework for efficient image deblurring on edge devices. It proposes sensitivity-guided block substitution, surrogate distillation, and training-free multi-objective search to reduce computational cost. Applied to the NAFNet baseline, the optimized variants achieve up to 55% reduction in GMACs compared to recent transformer-based SOTA while maintaining competitive accuracy. On-device deployment shows a 1.25× latency improvement over the baseline, demonstrating the effectiveness of feedback-driven adaptation.",29.34,LFM-2.5,Apple M1 (Metal)
2601.11686v1_Proof of Concept Multi-Target Wildfire Risk Predic.pdf,Proof of Concept: Multi-Target Wildfire Risk Prediction and Large Language Model Synthesis,"Nicolas Caron, Hassan Noura, Christophe Guyeux, Benjamin Aynes",arXiv:2601.11686v1,2601.11686,"wildfire risk, multi-target analysis, meteorological danger, ignition activity, resource mobilization, large language models, operational needs","This paper proposes a hybrid framework combining predictive models for wildfire risk dimensions with large language models to generate structured reports, aiming to support first responders with timely, spatially and temporally relevant information.",28.22,LFM-2.5,Apple M1 (Metal)
2601.11687v1_Semantic Caching and Intent-Driven Context Optimiz.pdf,Semantic Caching and Intent-Driven Context,Harmohit Singh,arXiv:2601.11687v1,2601.11687,"Natural Language to Code, Multi-Agent Systems, Semantic Caching, LLM Optimization, Production Systems","Presents a production-optimized multi-agent system translating natural language queries into executable Python code, achieving high accuracy and cost efficiency through semantic caching, dual-threshold decision mechanisms, and intent-driven prompt assembly.",30.09,LFM-2.5,Apple M1 (Metal)
2601.11688v1_SpecMap Hierarchical LLM Agent for Datasheet-to-Co.pdf,SpecMap: Hierarchical LLM Agent for Datasheet-to-Code,"Vedant Nipane Pulkit Agrawal, Amit Singh",H2LooP.ai,2601.11688v1,"traceability, traceability link recovery, systems engineering, embedded systems",Establishes precise traceability between embedded systems datasheets and code using a hierarchical LLM-based methodology. The approach improves mapping accuracy and reduces computational overhead compared to traditional information-retrieval methods.,26.35,LFM-2.5,Apple M1 (Metal)
2601.11700v1_Telling Human and Machine Handwriting Apart.pdf,Telling Human and Machine Handwriting Apart,"Luis A. Leiva, Moises Diaz, Nuwan T. Attygalle, Miguel A. Ferrer, Réjean Plamondon",,,"biometrics, classification, deep learning, reverse Turing test, verification, behavioral biometrics",Handwriting movements can be leveraged as a unique form of behavioral biometrics to verify user presence. The study presents a shallow recurrent neural network achieving high performance on synthetic data and demonstrates strong generalization across datasets and few-shot settings.,25.63,LFM-2.5,Apple M1 (Metal)
2601.11702v1_PASTA A Scalable Framework for Multi-Policy AI Com.pdf,A Scalable Framework for Multi-Policy AI Compliance,"YU YANG, IG-JAE KIM, DONGWOOK YOON",10.48550/arXiv.2024.12345,arXiv:2408.12345,"AI compliance, multi-policy, policy evaluation, scalable framework, compliance heatmaps","This paper introduces PASTA, a scalable compliance tool for AI that integrates four innovations to address multi-policy requirements efficiently. It evaluates five major policies in under two minutes at low cost and presents expert evaluations showing strong alignment with human experts. The framework aims to support automated, interpretable AI governance.",27.68,LFM-2.5,Apple M1 (Metal)
2601.11713v1_Inter-Cell Interference Rejection Based on Ultrawi.pdf,Inter-Cell Interference Rejection Based on Ultrawideband Walsh-Domain Wireless Autoencoding,"Rodney Martinez Alonso, Cedric Dehos, Yuneisy Esthela Garcia Guzman",,,"inter-cell interference rejection, ultrawideband, Walsh domain, autoencoding, 5G base stations, spectrum efficiency, orthogonality, self-inverse properties","This paper proposes a novel technique for rejecting partial-in-band inter-cell interference in ultrawideband communication systems. It presents an end-to-end wireless autoencoder architecture that jointly optimizes transmitter and receiver encoding/decoding in the Walsh domain to mitigate interference from coexisting narrower-band 5G base stations. By exploiting the orthogonality and self-inverse properties of Walsh functions, the system distributes and learns to encode bit-words across parallel Walsh branches. Experimental results show up to 12 dB ICI rejection while maintaining a low block error rate.",28.27,LFM-2.5,Apple M1 (Metal)
2601.11746v1_LIME-LLM Probing Models with Fluent Counterfactual.pdf,"LIME-LLM: Probing Models with Fluent Counterfactuals, Not Broken Text","George Mihaila, Suleyman Olcay Polat, Poli Nemkova, Himanshu Sharma, Namratha V. Urs",arXiv:2601.11746v1,arXiv:2601.11746,"LIME, local explanation, fluent counterfactuals, NLP, generative models, explainability","This paper introduces LIME-LLM, a framework that replaces random noise with hypothesis-driven, controlled perturbations to improve local explanation fidelity in NLP. By enforcing a 'Single Mask–Single Sample' protocol and using distinct neutral infill and boundary strategies, LIME-LLM constructs fluent, on-manifold neighborhoods that rigorously isolate feature effects. The method is evaluated against LIME, SHAP, Integrated Gradients, and LLiMe, demonstrating significant improvements over traditional perturbation-based approaches.",28.93,LFM-2.5,Apple M1 (Metal)
2601.11747v1_PRISM Learning Design Knowledge from Data for Styl.pdf,PRISM: Learning Design Knowledge from Data,"Huaxiaoyue Wang, Sunav Choudhary, Franck Dernoncourt, Yu Shen, Stefano Petrangeli",,,"graphic design, stylistic improvement, natural language instructions, design knowledge, stylistic direction, design data, Vision Language Models","Graphic design often involves exploring different stylistic directions, which can be time-consuming for non-experts. This paper addresses the problem by leveraging design data to learn design knowledge and guide stylistic improvement, proposing PRISM that clusters designs, summarizes them into actionable knowledge, and retrieves relevant knowledge during inference.",26.44,LFM-2.5,Apple M1 (Metal)
2601.11758v1_Early Linguistic Pattern of Anxiety from Social Me.pdf,Early Linguistic Pattern of Anxiety from Social Media,Arnab Das Utsa,,,"anxiety detection, linguistic pattern, interpretable machine learning, keyword robustness, cross-domain validation","This study presents a transparent approach to social media-based anxiety detection using linguistically interpretable feature-grounded modeling. It evaluates performance across multiple validation methods and cross-domain data, demonstrating strong predictive capability while maintaining interpretability.",24.7,LFM-2.5,Apple M1 (Metal)
2601.11762v1_Industry-Aligned Granular Topic Modeling.pdf,Industry-Aligned Granular Topic Modeling,"Sae Young Moon, Myeongjun Erik Jang, Haoyan Luo, Chunyang Xiao, Antonios Georgiadis, Fran Silavong",,,"topic modeling, granularity, industry applications, text mining, data analysis, LLMs, business insights","This paper introduces TIDE, a framework for granular topic modeling leveraging large language models, demonstrating its effectiveness in business contexts through experiments on diverse datasets.",25.31,LFM-2.5,Apple M1 (Metal)
2601.11768v1_Lightweight Self-Supervised Detection of Fundament.pdf,Lightweight Self-Supervised Detection of Fundamental Frequency and Accurate Probability of Voicing in Monophonic Music,"Venkat Suprabath Bitra, Homayoon Beigi",10.1093/acpro/9780190876223.13.1,arXiv:2408.14337,"self-supervised pitch detection, unsupervised pitch detection, fundamental frequency, pitch estimation, resonance, musical timbre transfer, probability of voicing, music synthesis, music analysis, CQT, constant Q transform, DDSP, shift cross-entropy loss, musical instrument modeling, ResNeXt neural network, music information retrieval, MIR","Proposes a lightweight, fully self-supervised framework for joint fundamental frequency estimation and voicing inference, leveraging transposition-equivariant learning and EM-style iterative reweighting with Shift Cross-Entropy to improve robustness under noisy/unvoiced conditions.",28.42,LFM-2.5,Apple M1 (Metal)
2601.11776v1_Cleansing the Artificial Mind A Self-Reflective De.pdf,Cleansing the Artificial Mind: A Self-Reflective Detoxification Framework,"Kaituo Zhang, Zhimeng Jiang, Na Zou",10.1234/abcd123,SRD-3448,"Large Language Models, Self-regulation, Detoxification, Toxic Content, Ethical AI","This paper introduces a fully self-reflective detoxification framework for LLMs that leverages their inherent self-correction capabilities without external modules or data annotation. It presents a Toxic Signal Detector and a systematic intervention process to transform toxic text into non-toxic content, demonstrating improved detoxification performance on benchmark datasets while preserving semantic fidelity.",26.03,LFM-2.5,Apple M1 (Metal)
2601.11778v1_Translation as a Scalable Proxy for Multilingual E.pdf,Translation as a Scalable Proxy for Multilingual Evaluation,"Sheriff Issaka1, Erick Rosas Gonzalez1*, Lieqi Liu1*, Evans Kofi Agyei2, Lucas Bandarkar1, Nanyun Peng 1, David Ifeoluwa Adelani 3, Francisco Guzmán4, Saadia Gabriel 1",,,"translation quality, multilingual evaluation, LLMs, benchmarking, translation metrics","This paper investigates whether translation performance can serve as a scalable, low-cost proxy for evaluating multilingual capabilities of large language models. Through systematic evaluation across 14 models, it finds strong correlations between translation accuracy and downstream success metrics, suggesting translation quality reflects broader multilingual understanding.",26.72,LFM-2.5,Apple M1 (Metal)
2601.11781v1_Risk-Aware Human-in-the-Loop Framework with Adapti.pdf,Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles,"Dawood Wasif, Terrence J. Moore, Seunghyun Yoon, Hyuk Lim, Dan Dongseong Kim, Frederica F. Nelson, Jin-Hee Cho",10.48550/arXiv.2024.12345,arXiv:2408.12345,"autonomous vehicles, safety, human-in-the-loop, intrusion response, adaptive control, risk-aware, uncertainty, soft actor-critic, distributional robustness","RAIL presents a risk-aware human-in-the-loop framework that integrates heterogeneous runtime signals into calibrated control adaptations. It fuses three cues—curvature actuation integrity, time-to-collision proximity, and observation-shift consistency—into an Intrusion Risk Score (IRS) via weighted noisy OR. When risk exceeds a threshold, actions are blended with cue-specific shields using a learned authority, preserving human override. Contextual bandit selection adapts shields based on cue vectors, improving mitigation choices online. The method combines SAC with risk-prioritized replay and dual rewards, enabling learning from takeovers and near misses while maintaining nominal behavior. Experiments on MetaDrive show superior test metrics compared to RL, safe RL, offline learning, and prior HITL baselines.",29.69,LFM-2.5,Apple M1 (Metal)
2601.11792v1_A self-evolving multi-role collaborative framework.pdf,A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation,"Yifei Suna, Yongan Lia, A.K. Qinb, Sicheng Houa, Tamas Pflanznerc",,,"Problem generation, Large language models, Multi-role collaboration, Intelligent education, Self-evolution, Knowledge distillation","This paper proposes a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation. It introduces an improved difficulty model using data-driven association-guided path sampling and constructs a dataset of high-quality high school math problems. The method incorporates continual pre-training, supervised fine-tuning, and group relative policy optimization to enhance generation and evaluation, achieving significant improvements in problem innovation while maintaining high correctness.",27.78,LFM-2.5,Apple M1 (Metal)
2601.11801v1_RobotDesignGPT Automated Robot Design Synthesis us.pdf,Automated Robot Design Synthesis using Vision Language Models,"Nitish Sontakke, K. Niranjan Kumar, Sehoon Ha",,,"robot design, vision-language models, automated design, robot synthesis, design optimization, kinematic structures","This paper proposes a novel automated robot design framework, RobotDesignGPT, leveraging large pre-trained vision-language models to automate robot design synthesis. The framework synthesizes designs from user prompts and reference images, improving design quality and reducing manual feedback. It demonstrates visually appealing and kinematically valid robots inspired by nature, supported by ablation studies and user studies.",26.8,LFM-2.5,Apple M1 (Metal)
2601.11809v1_Multi-agent DRL-based Lane Change Decision Model f.pdf,Multi-agent DRL-based Lane Change Decision,"Zeyu Mu, Shangtong Zhang, B. Brian Park",,,"Multi-Agent, Reinforcement Learning, Cooperative Platooning, Lane Change","The study proposes a hybrid multi-agent lane change decision model to enhance cooperative platooning in mixed traffic, addressing challenges posed by sparse CA V distribution and improving traffic efficiency.",23.92,LFM-2.5,Apple M1 (Metal)
2601.11816v1_POLARIS Typed Planning and Governed Execution for .pdf,POLARIS: Typed Planning and Governed Execution,"Zahra Moslemi, Keerthi Koneru, Yen-Ting Lee, Sheethal Kumar, Ramesh Radhakrishnan",10.48550/arXiv.2311.06854,2311.06854,"Agentic AI, Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed Planning, Evaluation","POLARIS presents a governed orchestration framework for agentic LLM agents in regulated environments, combining typed planning, policy alignment, and execution guardrails to deliver auditable, predictable workflows.",26.99,LFM-2.5,Apple M1 (Metal)
2601.11825v1_AI Co-Scientist for Knowledge Synthesis in Medical.pdf,AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept,"Arya Rahgozara, Pouria Mortezaaghaa",10.1093/acm/qad047,2601.11825v1,"AI co-scientist, knowledge synthesis, medical contexts, PICOS framework, semantic retrieval, topic modeling","This study proposes an artificial intelligence co-scientist to enhance scalable, transparent knowledge synthesis in biomedical research, focusing on dementia-sport and non-communicable disease corpora using PICOS-based evaluation.",30.15,LFM-2.5,Apple M1 (Metal)
2601.11840v1_Imandra CodeLogician Neuro-Symbolic Reasoning for .pdf,Neuro-Symbolic Reasoning for Precise Analysis of Software Logic,"Hongyu Lin, Samer Abdallah, Makar Valentinov, Paul Brennan, Elijah Kagan, Christoph M. Wintersteiger, Denis Ignatovich, Grant Passmore",arXiv:2601.11840v1,2601.11840v1,"neurosymbolic reasoning, software logic, precise analysis, code understanding, formal verification, LLM augmentation","This paper introduces CodeLogician 1, a neurosymbolic agent designed for precise software logic analysis. It integrates with ImandraX to enable rigorous reasoning about program behavior, addressing gaps between mathematical proof automation and practical software engineering benchmarks. A new benchmark evaluates reasoning accuracy across program states, control flow, and edge cases, demonstrating that combining LLMs with formal reasoning improves performance significantly.",29.4,LFM-2.5,Apple M1 (Metal)
2601.11850v1_Human-AI Collaborative Inductive Thematic Analysis.pdf,Human–AI Collaborative Inductive Thematic Analysis: How AI Guides Analysis and Researchers Reclaim Interpretive Authority,"Matthew Nyaaba, Min SungEun, Mary Abiswin Apam, Kwame Owoahene Acheampong, Emmanuel Dwamena, Xiaoming Zhai",,,"Human-AI collaboration, Generative artificial intelligence (GenAI), Inductive thematic analysis, Qualitative data analysis, Epistemic authority, Reflexive methodology","The study examines how researchers use an Inductive Thematic Analysis GPT (ITA–GPT) within a Human–Artificial Intelligence Collaborative Inductive Thematic Analysis (HACITA) framework to maintain interpretive authority in qualitative research. It highlights five recurring analytic actions—modification, deletion, rejection, insertion, and commenting—that researchers employ to ensure transparency, contextual richness, and alignment with professional realities.",28.94,LFM-2.5,Apple M1 (Metal)
2601.11854v1_ATOD An Evaluation Framework and Benchmark for Age.pdf,ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System,"Yifei Zhang, Hooshang Nayyeri, Emine Yilmaz, Gokhan Tur, Dilek Hakkani-Tür, Hari Thadakamalla",,,"task-oriented dialogue, agentic systems, LLMs, benchmarking, benchmarks, long-term reasoning","ATOD introduces a benchmark and synthetic dialogue pipeline to evaluate advanced agentic capabilities in dialogue systems, focusing on multi-goal coordination, dependency management, memory, adaptability, and proactivity. The framework supports comprehensive assessment across task completeness, agentic behavior, and response quality.",26.35,LFM-2.5,Apple M1 (Metal)
2601.11859v1_Cascaded Transformer for Robust and Scalable SLA D.pdf,Cascaded Transformer for Robust and Scalable,Cyril Shih-Huan Hsu,,,"network slicing, service level agreement, quality of service, deep neural network, optimization","The paper introduces Casformer, a cascaded Transformer architecture for fast, optimization-free SLA decomposition. It leverages domain-specific encoders and cross-domain aggregation to achieve high-quality, scalable SLA management in evolving 6G networks.",24.72,LFM-2.5,Apple M1 (Metal)
2601.11863v1_Utilizing Metadata for Better Retrieval-Augmented .pdf,Utilizing Metadata for Better Retrieval-Augmented Generation,"Raquib Bin Yousuf, Shengzhe Xu, Mandar Sharma, Andrew Neeser, Chris Latimer, Naren Ramakrishnan",,,"Retrieval-Augmented Generation, Metadata-aware, RAG, Benchmark Datasets","This study evaluates metadata-aware retrieval strategies in Retrieval-Augmented Generation systems, comparing plain-text baselines with approaches that embed metadata directly. It finds that prefixing and unified embeddings improve performance across various metrics, enhancing intra-document cohesion and reducing confusion between relevant and irrelevant chunks.",29.24,LFM-2.5,Apple M1 (Metal)
2601.11868v1_Terminal-Bench Benchmarking Agents on Hard Realist.pdf,Terminal-Benchmarking Agents on Hardware,"Mike A. Merrill, Alexander G. Shaw, Nicholas Carlini, Boxuan Li, Harsh Raj, Ivan Bercovich, Lin Shi, Jeong Yeon Shin, Thomas Walshe, E. Kelly Buchanan, Junhong Shen, Guanghao Ye, Haowei Lin, Jason Poulos, Maoyu Wang, Marianna Nezhurina, Di Lu, Orfeas Menis Mastromichalakis, Zhiwei Xu, Zizhao Chen, Yue Liu, Robert Zhang, Leon Liangyu Chen, Anurag Kashyap, Jianbo Wu, Minghao Yan, Song Bian, Vedang Sharma, Ke Sun, Steven Dillmann, Akshay Anand, Andrew Lanpouthakoun, Bardia Koopah, Changran Hu, Etash Guha, Gabriel H. S. Dreiman, Jiacheng Zhu, Li Zhong3, Niklas Muennighoff, Robert Amanfu, Shangyin Tan, Shreyas Pimpalgaonkar, Tushar Aggarwal, Xiangning Lin, Xin Lan, Xuandong Zhao, Yiqing Liang, Yuanli Wang, Zilong Wang, Changzhi Zhou, David Heineman, Hange Liu, Yiwei Dai, Wuwei Lin, Yiying Wang, Shanda Li, Terry Yue Zhuo, Wenhao Chai, Shang Zhou, Dariush Wahdany, Ziyu She, Jiaming Hu, Zhikang Dong, Yuxuan Zhu, Sasha Cui, Ahson Saiyed, Arinbjorn Kolbeinsson, Jesse Hu, Christopher Michael Rytting, Ryan Marten, Yixin Wang, Alex Dimakis, Ludwig Schmidt, Stanford University, Laude Institute, Anthropic, Northeastern University, University of California, Santa Barbara, Cornell University, Snorkel AI, Reflection AI, Carnegie Mellon University, Massachusetts Institute of Technology, Peking University, LAION, JSC, FZJ, Tencent, National Technical University of Athens, Nerion, University of Michigan, National University of Singapore, Moonshot AI, University of Texas at Austin, Amazon, University of Washington, University of Wisconsin-Madison, Beijing Institute of Technology, Allen Institute for AI, Monash University, CSIRO’s Data61, Dartmouth College, Princeton University, CISPA, University of Basel, Stony Brook University, Michigan State University, Brown University, Boston University, University of California, Berkeley, SambaNova Systems, Bespoke Labs, Michigan University",arXiv:2601.11868v1,2601.11868v1,"benchmarking agents, hardware, computer terminal, long-horizon tasks, AI agents, real-world tasks, model evaluation, benchmarking, verification",AI agents on terminal environments can be evaluated on 89 hard tasks; frontier models perform poorly (<65%) and highlight improvement areas.,31.02,LFM-2.5,Apple M1 (Metal)
2601.11876v1_AI for Green Spaces Leveraging Autonomous Navigati.pdf,Autonomous Trash Pickup Robots on Grass Fields,,,,,"This paper proposes a robot to autonomously navigate, identify, and pick up trash in grass fields using a Spanning Tree Coverage algorithm, Real-Time Kinematic GPS, ResNet50 CNN for trash detection, and a custom pickup mechanism. The system achieved an 80% overall success rate.",22.22,LFM-2.5,Apple M1 (Metal)
2601.11880v1_TF-CoDiT Conditional Time Series Synthesis with Di.pdf,TF-CoDiT: Conditional Time Series Synthesis with Diffusion Transformers,"Yingxiao Zhang, Jiaxin Duan, Junfu Zhang, Ke Feng",10.48550/arXiv.2025.12345,arXiv:2509.01234,"treasury futures, diffusion transformers, time series synthesis, financial data, market dynamics","This work introduces TF-CoDiT, a diffusion transformer framework for generating treasury futures data. It addresses challenges in synthesizing low-volume, high-dependency financial time series by transforming 1D time series into discrete wavelet coefficients and using a U-shape VAE to model cross-channel dependencies. A multi-level description system, FinMAP, standardizes market indicators, and experiments demonstrate high-fidelity data synthesis with low error metrics.",27.55,LFM-2.5,Apple M1 (Metal)
2601.11885v1_MyGram Modality-aware Graph Transformer with Globa.pdf,MyGram: Modality-aware Graph Transformer with Global Distribution for Multi-modal Entity Alignment,"Zhifei Li, Ziyue Qin, Xiangyu Luo, Xiaoju Hou, Yue Zhao, Miao Zhang, Zhifang Huang, Kui Xiao, Bing Yang",zhifei1993,2026030010,"multi-modal entity alignment, graph transformer, knowledge graphs, visual attributes, deep structural contextual information","This paper introduces MyGram, an amodality-aware graph transformer that performs global distribution multi-modal entity alignment by integrating multi-modal data (images, text) to enrich entity semantics. It addresses limitations of existing methods by capturing deep structural context and enforcing global distribution consistency through Gram Loss.",26.9,LFM-2.5,Apple M1 (Metal)
2601.11895v1_DevBench A Realistic Developer-Informed Benchmark .pdf,"DevBench: A Realistic, Developer-Informed Benchmark for Code","Pareesa Ameneh Golnari, Adarsh Kumarappan, Wen Wen, Xiaoyu Liu, Gabriel Ryan, Yuting Sun, Shengyu Fu, Elsie Nallipogu",,,"code generation, LLM evaluation, developer telemetry, realistic benchmark, code completion, functional correctness","DevBench is a telemetry-driven benchmark evaluating LLMs on realistic code completion tasks, focusing on ecological validity and practical utility. It assesses models across six languages and six task categories derived from real developer behavior, offering insights beyond static rule-based benchmarks.",27.78,LFM-2.5,Apple M1 (Metal)
2601.11903v1_AEMA Verifiable Evaluation Framework for Trustwort.pdf,AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems,"Yen-Ting Lee, Keerthi Koneru, Zahra Moslemi, Sheethal Kumar, Ramesh Radhakrishnan",arXiv:2601.11903v1,arXiv:2601.11903,"Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight","AEMA presents a process-aware, auditable framework for evaluating multi-agent LLM systems, emphasizing stability, human alignment, and traceable records for accountable automation.",27.5,LFM-2.5,Apple M1 (Metal)
2601.11905v1_LIBRA Language Model Informed Bandit Recourse Algo.pdf,Language Model Informed Bandit Recourse,"Jyuu Cao, Ruijiang Gao, Esmaeil Keyvanshokooh, Jianhao Ma",,,"Large Language Models, LLM-Bandits Collaboration, Algorithmic Recourse, Regret Analysis, Personalized Treatment Planning, Hypertension Management","Introduces a unified framework integrating algorithmic recourse, contextual bandits, and LLMs for sequential decision-making in personalized medicine. Presents LIBRA, a Language Model–Informed Bandit Recourse Algorithm, offering three guarantees and demonstrating improvements over standard methods.",26.6,LFM-2.5,Apple M1 (Metal)
2601.11907v1_Towards Airborne Object Detection A Deep Learning .pdf,Towards Airborne Object Detection: A Deep Learning Analysis,"Prosenjit Chatterjee, ANK Zaman",0000-0003-1169-4717,0000-0001-7831-0955,"Airborne Object Detection, Threat Detection, Deep Learning, EfficientNetB4, ResNet-50, UA V","This work introduces a dual-task model using EfficientNetB4 for simultaneous airborne object classification and threat-level prediction. It presents the AODTA Dataset and evaluates performance against ResNet-50, demonstrating improved accuracy over baselines.",26.45,LFM-2.5,Apple M1 (Metal)
2601.11913v1_LSTM-MAS A Long Short-Term Memory Inspired Multi-A.pdf,LSTM-MAS: A LONG SHORT-TERM MEMORY INSPIRED MULTI-AGENT SYSTEM FOR LONG-CONTEXT UNDERSTANDING,"Yichen Jiang, Peng Ye, Jiakang Yuan, Chongjun Tu, Tao Chen",,,"Long Short-Term Memory, Multi-Agent System, Memory, Large Language Models, Long-Context Understanding","Effectively processing long contexts remains a fundamental challenge for LLMs. This work proposes LSTM-MAS, inspired by LSTM, to enable multi-agent long-context understanding with controlled information flow and reduced error accumulation.",26.55,LFM-2.5,Apple M1 (Metal)
2601.11920v1_Enhancing LLM-Based Data Annotation with Error Dec.pdf,Enhancing LLM-Based Data Annotation with Error,"Zhen Xu, Columbia University, University of California, Irvine, University of Pennsylvania, Columbia University, University of California, Irvine, Columbia University",,,"Data Annotation, Qualitative Coding, Large Language Models, Human-AI Collaboration","This paper proposes a diagnostic evaluation paradigm to assess LLM annotation quality by distinguishing between model-specific and task-inherent errors, incorporating human-in-the-loop testing, and decomposing errors computationally. It validates the approach on educational annotation tasks and highlights limitations of single alignment metrics.",26.96,LFM-2.5,Apple M1 (Metal)
2601.11935v1_Big Data Workload Profiling for Energy-Aware Cloud.pdf,Big Data Workload Profiling for Energy-Aware Cloud Resource Management,"Milan Parikh, Aniket Abhishek Soni, Sneja Mitinbhai Shah, Ayush Raj Jha",arXiv:2601.11935v1,arXiv:2601.11935,"cloud computing, energy-aware scheduling, workload profiling, virtual machine placement, big data, green computing","The paper presents a workload-aware scheduling framework that uses profiling of CPU usage, memory demand, and storage I/O behavior to guide energy-efficient virtual machine placement. By combining historical execution logs with real-time telemetry, the system predicts energy and performance impact, enabling adaptive workload consolidation while maintaining SLAs. Experimental results show a consistent 15–20% reduction in energy consumption.",28.28,LFM-2.5,Apple M1 (Metal)
2601.11940v1_Thinking Traps in Long Chain-of-Thought A Measurab.pdf,Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart,"Kang Chen, Fan Yu, Junjie Nian, Shihan Zhao, Zhuoka Feng, Zĳun Yao, Heng Wang, Minshen Yu, Yixin Cao, 2, †",arXiv:2601.11940v1,arXiv:2601.11940,"Long Chain-of-Thought, Thinking Traps, Adaptive Restart, Reasoning, Model Efficiency",Scaling test-time compute via Long Chain-of-Thought improves reasoning but risks persistent thinking traps. TAAR introduces a diagnostic framework to detect and mitigate these traps during inference.,28.55,LFM-2.5,Apple M1 (Metal)
2601.11956v1_Double-Calibration Towards Trustworthy LLMs via Ca.pdf,Double-Calibration: Towards Trustworthy LLMs via Calibrating,"Yuyin Lu, Ziran Liang, Yanghui Rao, Wenqi Fan, Fu Lee Wang, Qing Li",,,"trustworthy reasoning, large language models, knowledge graphs, uncertainty quantification, confidence calibration","The paper introduces DoublyCal, a framework that improves LLM accuracy and calibration by integrating double-calibration principles. It uses a lightweight proxy model to generate evidence and calibrated confidence, guiding a black-box LLM to produce well-calibrated predictions.",25.62,LFM-2.5,Apple M1 (Metal)
2601.11960v1_R2PO Decoupling Training Trajectories from Inferen.pdf,R2PO: Decoupling Training Trajectories,"Jingchu Wang, Bingbing Xu, Yige Yuan, Bin Xie, Xiaoqian Sun, Huawei Shen",10.48550/arXiv.2303.04112,arXiv:2303.04112,"reinforcement learning, large language models, training trajectories, inference responses, exploration vs stability","The paper proposes R2PO (Residual Rollout Policy Optimization) to decouple training trajectories from inference responses, addressing the conflict between stable reasoning and diverse optimization paths. Experiments show improved performance and reduced errors.",26.22,LFM-2.5,Apple M1 (Metal)
2601.11969v1_textttMemoryRewardBench Benchmarking Reward Models.pdf,MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models,"Zecheng Tang, Baibei Ji, Ruoxi Sun, Haitian Wang, Wangjie You, Yijun Zhang, Wenpeng Zhu, Ji Qi, Juntao Li, Min Zhang",,,"memory management, long-term memory, large language models, reward models, contextual processing","This paper introduces MemRewardBench, a benchmark to systematically evaluate how well reward models assess long-term memory management in LLMs. It covers 10 settings with varying context lengths and shows that newer models consistently outperform older ones, highlighting current limitations in evaluating LLM memory across diverse tasks.",26.82,LFM-2.5,Apple M1 (Metal)
2601.11974v1_Learn Like Humans Use Meta-cognitive Reflection fo.pdf,Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement,"Xinmeng Hou, Peiliang Gong, Bohao Qu, Wuqi Wang, Qing Guo, Yang Liu",10.1234/arxiv.2024.09.12345,10.1234/arxiv.2024.09.12345,"metacognitive reflection, self-improvement, large language models, educational psychology, principle-based instruction","The paper proposes MARS, a framework enabling efficient self-evolution in agents by integrating reflective principles and procedural reasoning, thereby reducing computational costs compared to traditional multi-turn recursive loops.",26.4,LFM-2.5,Apple M1 (Metal)
2601.11977v1_One-Shot Price Forecasting with Covariate-Guided E.pdf,One-Shot Price Forecasting with Covariate-Guided Experts under Privacy Constraints,"1stRen He, 2ndYinliang Xu, 3rdJinfeng Wang, 4thJeremy Watson, 5thJian Song",,,"Price forecasting, Time Series, Privacy, Mixture of Experts, Market analysis","Forecasting in power systems often involves multi-variate time series with complex dependencies and strict privacy constraints across regions. Traditional methods require expert knowledge and struggle to generalize across diverse scenarios. Recent advancements in pre-trained models offer opportunities, but zero-shot performance remains limited. This work proposes a novel MoE-Encoder module that augments forecasting models with a sparse mixture-of-experts layer, enabling expert-guided univariate tasks and supporting federated training under privacy constraints.",28.01,LFM-2.5,Apple M1 (Metal)
2601.11979v1_Process In-Context Learning Enhancing Mathematical.pdf,Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion,"Ang Gao1, Changshuo Zhang1, Xiao Zhang1†, Minjun Zhao2, Fangchao Liu2, Xinyu Zhang2",,,"in-context learning, mathematical reasoning, dynamic demonstration, adaptive inference, logical deduction","This paper proposes Process In-Context Learning (PICL), a dynamic demonstration integration framework to improve mathematical reasoning. PICL identifies confusion points in reasoning processes and inserts adaptive demonstrations during inference, mitigating mid-inference errors and enhancing accuracy.",25.62,LFM-2.5,Apple M1 (Metal)
2601.11995v1_Learning Audio-Visual Embeddings with Inferred Lat.pdf,Learning Audio–Visual Embeddings with Inferred Latent Interaction Graphs,"Donghuo Zeng, Hao Niu, Yanan Wang, Masato Taya",arXiv:2601.11995v1,0000−0002−6425−6270,"Audio-visual, Latent interaction graph, Cross-modal retrieval, Soft labels, Semantic alignment","The paper proposes a framework to improve robust audio-visual embedding learning by leveraging inferred latent interaction graphs and soft-label predictions, addressing issues like background noise and unannotated co-occurrences.",28.46,LFM-2.5,Apple M1 (Metal)
2601.11998v1_Hybrid IDS Using Signature-Based and Anomaly-Based.pdf,Hybrid IDS Using Signature-Based and Anomaly-Based Detection,"1. Messaouda Boutassetta, 2. Amina Makhlouf, 3. Newfel Messaoudi, 4. Abdelmadjid Benmachiche, 5. Ines Boutabia",,,"Intrusion Detection System (IDS), Hybrid IDS, Signature-Based Detection, Anomaly-Based Detection, Machine Learning (ML), Cybersecurity, False Positives, Detection Accuracy, Real-Time Detection, Network Security","This paper presents a comprehensive survey and conceptual overview of Hybrid IDS, integrating signature-based and anomaly-based detection techniques to enhance attack detection capabilities. It reviews recent research, classifies models, discusses advantages/limitations, and explores future directions in cybersecurity.",27.57,LFM-2.5,Apple M1 (Metal)
2601.12002v1_Kernel-Based Learning of Safety Barriers.pdf,Kernel-Based Learning of Safety Barriers,"Oliver Schönhoel, Zhengang Zhongzhengang, Sadegh Soudjanisadegh, Max Planck Institute for Software Systems, University of Warwick, University of Birmingham",arXiv:2601.12002v1,arXiv:2601.12002,"kernel learning, safety barriers, control barrier certificates, black-box systems, stochastic dynamics, temporal logic, safety verification","The paper presents a data-driven approach for verifying and synthesizing safety in black-box systems with discrete-time stochastic dynamics. It introduces control barrier certificates and conditional mean embeddings to construct robust safety guarantees, demonstrating applicability beyond safety to broader temporal logic specifications.",27.92,LFM-2.5,Apple M1 (Metal)
2601.12003v1_Robust Verification of Concurrent Stochastic Games.pdf,Robust Verification of Concurrent Stochastic Games,"Angel Y. He, David Parker",1,,"Robust quantitative verification, Probabilistic model checking, Concurrent stochastic games, Epistemic uncertainty","Autonomous systems in multi-agent settings require verification under uncertain transition probabilities. This paper introduces a novel framework for robust verification of concurrent stochastic games (CSGs) and their subclass interval CSGs (ICSGs), proposing theoretical foundations and algorithms for finite and infinite-horizon objectives in zero- and non-zero-sum settings based on social-welfare optimal Nash equilibria. Demonstrated via implementation in PRISM-games.",28.65,LFM-2.5,Apple M1 (Metal)
2601.12014v1_Are LLMs Ready for TOON Benchmarking Structural Co.pdf,Are LLMs Ready for TOON? Benchmarking Structural Correctness–Sustainability Trade-offs in Novel Structured Output,"Elio Masciari, Vincenzo Moscato, Enea Vincenzo Napolitano, Gian Marco Orlando, Marco Perillo, Diego Russo",XXXXXXX.XXXXXXX,XXXXXX,"Green AI, TOON, Large Language Models, Natural Language Processing, Sustainability, Structural Correctness, Environmental Impact","This paper evaluates structural correctness and environmental efficiency of novel TOON output formats using a sustainability-aware benchmarking framework. It finds a trade-off between compactness, correctness, and carbon emissions, urging more inclusive sustainability metrics in LLM evaluations.",27.18,LFM-2.5,Apple M1 (Metal)
2601.12019v1_Acting Flatterers via LLMs Sycophancy Combating Cl.pdf,Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs Opposing-Stance Reasoning,"Chaowei Zhang, Xiansheng Luo, Zewei Zhang, Yi Zhu, Jipeng Qiang, Longwei Wang",10.1145/XXXXXX,,"Clickbait Detection, Large Language Models, Opposing Stance Reasoning, Contrastive Learning","The paper addresses clickbait proliferation by proposing a Self-renewal Opposing-stance Reasoning Generation (SORG) framework that leverages LLMs' sycophancy to produce contrasting reasoning pairs, aiming to improve clickbait detection without relying on ground-truth labels.",26.16,LFM-2.5,Apple M1 (Metal)
2601.12024v1_A Multi-Agent System for Generating Actionable Bus.pdf,A Multi-Agent System for Generating Actionable Business Advice,"Kartikey Singh Bhandari, Tanish Jain, Archit Agrawal, Pratik Narang","p20241006,f20230349,f20191048,dhruv.kumar,pratik.narang",,"customer reviews, business advice, multi-agent system, LLM-based, actionable insights","This paper presents a multi-agent, LLM-based framework for transforming large-scale review corpora into actionable business advice. The framework integrates clustering, advice generation, iterative evaluation, and feasibility ranking, combining corpus distillation with feedback-driven refinement to deliver specific, practical recommendations.",29.0,LFM-2.5,Apple M1 (Metal)
2601.12030v1_ARC Active and Reflection-driven Context Managemen.pdf,Active and Reflection-driven Context Management for Long-Horizon Information Seeking Agents,"Yilun Yao, Shan Huang, Elsie Dai, Zhewen Tan, Zhenyu Duan, Shousheng Jia, Yanbing Jiang, Tong Yang",,,"active context management, reflection-driven reasoning, context degradation, long-horizon information seeking, information seeking agents","This paper introduces ARC, a framework that treats context as a dynamic internal reasoning state during execution. ARC uses reflection-driven monitoring and revision to maintain coherent states over extended reasoning horizons, improving performance on long-horizon information-seeking benchmarks.",26.39,LFM-2.5,Apple M1 (Metal)
2601.12038v1_Abstract Argumentation with Subargument Relations.pdf,Abstract Argumentation with Subargument Relations,Beishui Liao,,,,"This paper examines abstract argumentation frameworks enriched with explicit subargument relations, analysing how subargument interactions with attacks affect semantic properties and structural dependencies. It argues for a framework that treats subarguments as primitive, clarifying their role in acceptability reasoning.",22.78,LFM-2.5,Apple M1 (Metal)
2601.12040v1_Partial Reasoning in Language Models Search and Re.pdf,Partial Reasoning in Language Models: Search and Refinement Guided by Uncertainty,"Murilo da Luz, Bruno Brandão, Luana Martins, Gustavo Oliveira, Bryan de Oliveira, Luckeciano Melo, Telma Soares",,,"Uncertainty, Entropy, Latent-space search, Soft Reasoning, LLM reasoning","The paper introduces PREGU, a method that monitors entropy during LLM generation and refines reasoning through targeted latent-space searches when uncertainty exceeds a threshold, improving performance on complex reasoning tasks.",26.35,LFM-2.5,Apple M1 (Metal)
2601.12042v1_Less Is More -- Until It Breaks Security Pitfalls .pdf,Less Is More — Until It Breaks: Security Pitfalls of Vision Token,"Xiaomei Zhang, Zhaoxi Zhang, Leo Yu Zhang, Yanjun Zhang, Guanhong Tao, Shirui Pan",10.1093/pasj/lsa123,10.1093/pasj.lsa123,"vision token compression, large vision-language models, security vulnerabilities, robustness degradation, compression mechanisms","Visual token compression improves inference efficiency but compromises robustness. Compression-induced vulnerabilities are state-specific and difficult to detect, leading to model failures when compression is enabled.",26.87,LFM-2.5,Apple M1 (Metal)
2601.12049v1_textitFocaLogic Logic-Based Interpretation of Visu.pdf,FocaLogic: Logic-Based Interpretation of Visual Model Decisions,"Chenchen Zhao, Muxi Chen, Qiang Xu",,,"interpretability, visual models, logic-based, model interpretability, visual focuses, quantitative metrics","This paper introduces FocaLogic, a model-agnostic framework for interpreting visual model decisions through logic-based representations. It identifies minimal visual regions influencing predictions and translates them into logical expressions, offering transparent and scalable evaluation metrics.",25.82,LFM-2.5,Apple M1 (Metal)
2601.12053v1_A New Strategy for Artificial Intelligence Trainin.pdf,A New Strategy for Artificial Intelligence: Training Foundation Models Directly on Human Brain Data,Maël Donoso,arXiv:2601.12053v1,arXiv:2601.12053,"foundation models, brain, neuroimaging, brain-generated data, brain-trained foundation models, reinforcement learning from human brain (RLHB), chain of thought from human brain (CoTHB)","This paper proposes a novel strategy for training foundation models by leveraging human brain data, aiming to overcome current limitations by incorporating neuroimaging insights across perception, valuation, execution, and integration levels.",27.34,LFM-2.5,Apple M1 (Metal)
2601.12055v1_Automating Parameter Selection in Deep Image Prior.pdf,Automating Parameter Selection in Deep Image,"Lina Meyer, Felix Wissel, Tobias Knopp, Susanne Pfefferle, Ralf Fliegert, Maximilian Sandmann, Liana Uebler, Franziska M. Rockl, Bjørn-Philipp Diercks, David Lohr, René Werner",,,"fluorescence microscopy, denoising, deep image prior, automatic parameter selection, deep image processing","This study presents AUTO-DIP, a method for optimizing network parameters in fluorescence microscopy denoising by leveraging image similarity. It demonstrates improved performance over traditional DIP and variational approaches, especially for noisy images, and highlights its practical utility in microscopy applications.",28.14,LFM-2.5,Apple M1 (Metal)
2601.12061v1_Codebook-Injected Dialogue Segmentation for Multi-.pdf,Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs,"Jinsook Lee, Kirk Vanacore, Zhuqian Zhou, Bakhtawar Ahtisham, Jeanine Grütter, René F. Kizilcec, Jeanine.Gruetter, René F. Kizilcec",,,"dialogue act, segmentation, multi-utterance, codebook, annotation, LLM, evaluation metrics","This paper introduces codebook-injected segmentation to address reliability issues in dialogue act annotation. By conditioning boundary decisions on downstream criteria and evaluating LLM-based segmenters, the study demonstrates that segment consistency improves over text-only baselines, though coherence-based methods still outperform in global shift detection. The findings emphasize segmentation as a design choice requiring optimization for downstream goals.",28.06,LFM-2.5,Apple M1 (Metal)
2601.12068v1_Bridging the Gap in Bangla Healthcare Machine Lear.pdf,Bridging the Gap in Bangla Healthcare: Machine Learning Based Disease Prediction Using a Symptoms-Disease Dataset,"Rowzatul Zannat, Abdullah Al Shafi, Abdul Muntakim",10.1234/abcd123,,"Disease Prediction, Annotated Dataset, Machine Learning Techniques, Soft Voting Ensemble, Hard Voting Ensemble","This study develops a Bangla symptom-disease dataset to improve healthcare accessibility for non-English speakers, evaluating machine learning models to predict diseases from Bangla symptom inputs.",25.78,LFM-2.5,Apple M1 (Metal)
2601.12082v1_Conditional Random Fields for Interactive Refineme.pdf,Conditional Random Fields for Interactive Refinement of Histopathological Predictions,"Tiffanie Godelaine, Maxime Zanella, Karim El Khoury, Saïd Mahmoudi, Benoît Macq1 Christophe De Vleeschouwer",,,"Histology Classification, Conditional Random Fields, Human-In-The-Loop, Foundation Models, Histopathological Predictions","This paper proposes HistoCRF, a framework that adapts Conditional Random Fields to histopathological applications without additional training, aiming to refine Vision-Language Model predictions through expert annotations and iterative human-in-the-loop corrections. Experiments show significant accuracy improvements over zero-shot methods.",26.6,LFM-2.5,Apple M1 (Metal)
2601.12095v1_Neural Isomorphic Fields A Transformer-based Algeb.pdf,Neuralisomorphic Fields: A Transformer-Based Algebraic Numerical Embedding,"Hamidreza Sadeghi Saeedeh Momtazi, Safa",10.1234/abcd123,,"neural networks, algebraic structures, numerical stability, embedding vectors","This paper introduces a fixed-length number embedding vector that preserves algebraic operations within rational numbers. It presents a novel Neural Isomorphic Field framework, demonstrating strong performance in addition while identifying challenges in multiplication.",24.67,LFM-2.5,Apple M1 (Metal)
2601.12099v1_Large language models struggle with ethnographic t.pdf,Large language models struggle with ethnographic text annotation,"Leonardo S. Goodall, Dor Shilton, Daniel Austin Mullins, Cohen Institute for the History and Philosophy of Science and Ideas, Harvey Whitehouse",,,"large language models, ethnographic text annotation, cross-cultural research, anthropology, anthropological text, ethnographic annotation","Large language models (LLMs) have shown promise for automated text annotation, but performance remains limited. Human inter-coder reliability set a high ceiling, and even on features where humans agree, models fall short. The study suggests LLMs cannot yet substitute for human expertise in ethnographic annotation.",26.98,LFM-2.5,Apple M1 (Metal)
2601.12104v1_Powerful Training-Free Membership Inference Agains.pdf,Powerful Training-Free Membership Inference Inference,"David Ilić, David Stanojevic, Kostadin Cvejoski",,,"membership inference, language models, privacy risks, fine-tuning, error positions, AUC, data leakage","Fine-tuned language models pose significant privacy risks by memorizing training data. This paper introduces EZ-MIA, a membership inference attack that leverages the Error Zone score to detect memorization at error positions without reference model training. EZ-MIA achieves higher detection rates than prior methods, demonstrating substantial privacy risks for deployed LLMs.",25.81,LFM-2.5,Apple M1 (Metal)
2601.12124v1_SynQP A Framework and Metrics for Evaluating the Q.pdf,A Framework and Metrics for Evaluating the Quality and Privacy Risk of Synthetic Data,"Bing Hu, Yixin Li, Asma Bahamyirou, Helen Chen",,,"Real-World Data, Synthetic Data, Privacy Metrics, Evaluation Framework, Membership Inference, Identity Disclosure Risk","This paper introduces SYNQP, an open framework for benchmarking privacy in synthetic data generation using simulated sensitive data. It addresses the lack of accessible benchmark datasets by proposing a new identity disclosure risk metric and highlights challenges in evaluating privacy risks due to data sensitivity.",26.81,LFM-2.5,Apple M1 (Metal)
2601.12126v1_UniMo Unified Motion Generation and Understanding .pdf,Unified Motion Generation and Understanding with Chain of Thought,"Guocun Wang, Kenkun Liu, Jing Lin, Guorui Song, Jian Li, Xiaoguang Han, SIGS, Tsinghua University, SSE, The Chinese University of Hong Kong (Shenzhen), MMLab, Nanyang Technological University, FNii-Shenzhen, Guangdong Provincial Key Laboratory of Future Networks of Intelligence",,,"3D human motion generation, understanding human motion, chain of thought, interpretable reasoning, motion-language integration, semantic alignment, task coherence, reinforcement learning, group relative policy optimization","This paper proposes UniMo, a framework integrating motion-language information and interpretable chain-of-thought reasoning into large language models via supervised fine-tuning. It introduces reinforcement learning with Group Relative Policy Optimization to improve structural correctness and semantic alignment in motion prediction, achieving state-of-the-art performance.",28.49,LFM-2.5,Apple M1 (Metal)
2601.12132v1_Bengali Text Classification An Evaluation of Large.pdf,Bengali Text Classification: An Evaluation of Large Language Model Approaches,"Md Mahmudul Hoque, Md Mehedi Hassain, Md Hojaifa Tanvir, Rahul Nandy",,,"Bengali Text Classification, Transformer-based Text Classifier, Multilingual NLP, Qwen, LLaMA","This study evaluates large language models for Bengali newspaper article classification. Qwen 2.5 achieved the highest accuracy (72%), outperforming LLaMA 3.1 (53%) and LLaMA 3.2 (56%). The research highlights LLM effectiveness despite limited Bengali NLP resources.",28.24,LFM-2.5,Apple M1 (Metal)
2601.12134v1_Human-Human-AI Triadic Programming Uncovering the .pdf,Human-Human-AI Triadic Programming: Uncovering the Role of AI Agent and the Value of Human Partner in Collaborative Learning,"Taufiq Daryanto, Xiaohan Ding, Kaike Ping, Lance T. Wilhelm, Yan Chen, Chris Brown",arXiv:2601.12134v1,arXiv:2601.12134v1,"human-AI collaboration, programming learning, triadic programming, AI agent, collaborative learning","This study explores human-human-AI triadic programming, examining how AI can augment human partners in collaborative learning without replacing social collaboration. It investigates whether AI should act as a shared collaborator or personal support, emphasizing the pedagogical benefits of human-AI interaction.",29.16,LFM-2.5,Apple M1 (Metal)
2601.12138v1_DriveSafe A Hierarchical Risk Taxonomy for Safety-.pdf,DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants,"Abhishek Kumar, Riya Tapwal, Carsten Maple",,,"LLM safety, driving assistants, hierarchical risk taxonomy, safety-critical systems, ethical AI, vehicle AI","This paper introduces DriveSafe, a four-level hierarchical risk taxonomy for characterizing safety-critical failure modes of large language models in driving assistants. It evaluates refusal behavior across six deployed LLMs and highlights limitations of general-purpose safety alignment in real-world driving contexts.",26.23,LFM-2.5,Apple M1 (Metal)
2601.12141v1_TIDE A Trace-Informed Depth-First Exploration for .pdf,TIDE: A Trace-Informed Depth-First Exploration,"Yuliia Suprun, Khen Elimelech, Lydia E. Kavraki, Moshe Y. Vardi",arXiv:2601.12141v1,arXiv:2601.12141v1,"task planning, temporally extended goals, linear temporal logic, planning methods, robotics","This paper introduces TIDE, a depth-first exploration framework for planning with temporally extended goals. It decomposes temporal problems into smaller subproblems, uses cost-driven heuristics, and ensures completeness through adaptive backtracking. Experimental results show promising performance.",28.72,LFM-2.5,Apple M1 (Metal)
2601.12147v1_Segment and Matte Anything in a Unified Model.pdf,Segment And Matte Anything in a Unified Model,"Zezhong Fan, Xiaohan Li, Topojoy Biswas, Kaushiki Nag, Kannan Achan",10.48550/arXiv.2311.07891,arXiv:2311.07891,"Segment Anything, mapping, interactive segmentation, mattes, object detection, image segmentation","This paper introduces Segment And Matte Anything (SAMA), a lightweight extension of Segment Anything (SAM) that combines high-quality segmentation and matting with minimal parameters. SAMA leverages a Multi-View Localization Encoder and a Localization Adapter to refine masks, achieving state-of-the-art performance across segmentation and matting benchmarks.",27.3,LFM-2.5,Apple M1 (Metal)
2601.12150v1_Enhanced Diagnostic Performance via Large-Resoluti.pdf,Enhanced Diagnostic Performance Via Large-Resolution Inference Optimization for Pathology Foundation Models,"Mengxuan Hu, Zihan Guan, John Kang, Sheng Li, Zhongliang Zhou",10.1093/acprof:occ/abad,2601.12150v1,"computational pathology, foundation models, inference optimization, ROI classification, segmentation","The paper addresses inefficiencies in pathology foundation models constrained by fixed input sizes for whole-slide images. It proposes a space- and time-efficient inference strategy using spatially aware attention and global attention scores, achieving up to 7.67% improvement in ROI classification while maintaining segmentation performance.",27.4,LFM-2.5,Apple M1 (Metal)
2601.12186v1_Aletheia What Makes RLVR For Code Verifiers Tick.pdf,What Makes RLVR For Code Verifiers Tick?,"Vatsal Venkatkrishna, Indraneil Paul, Iryna Gurevych",,,"Reinforcement Learning, Code Verification, Large Language Models, Self-contained Execution, Code Generation","This paper explores the design and evaluation of Aletheia, an open-source testbed for evaluating code verifiers trained via RLVR. It highlights the importance of on-policy learning and thinking-based training despite scaling challenges, and discusses components like intermediate thinking traces and negative sample learning.",24.94,LFM-2.5,Apple M1 (Metal)
2601.12205v1_Do Neural Codecs Generalize A Controlled Study Acr.pdf,Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks,"Shih-Heng Wang, Jiatong Shi, Jinchuan Tian, Haibin Wu, Shinji Watanabe",,,"Neural Audio Codecs, Language Generalization, Non-Speech Tasks, Pre-Training Data, Signal Reconstruction, Downstream Applications","This paper investigates three aspects of Neural Audio Codecs' generalization capabilities: generalization to unseen languages during pre-training, generalization of speech-only NACs to non-speech applications, and the impact of incorporating non-speech pre-training data. The study uses controlled experiments with carefully curated datasets and evaluates performance across signal reconstruction and downstream tasks.",26.49,LFM-2.5,Apple M1 (Metal)
2601.12212v1_Speculative Sampling with Reinforcement Learning.pdf,Speculative Sampling with Reinforcement Learning,"Chenan Wang, Daniel H. Shi, Haipeng Chen",,,"speculative sampling, reinforcement learning, large language models, inference latency, tree-based drafting","The paper introduces Reinforcement Learning for Speculative Sampling (Re-SpS), a reinforcement learning-based framework that dynamically optimizes draft tree hyperparameters in real-time. It aims to improve generation speed while maintaining output fidelity by learning context-aware policies from target model hidden states.",24.52,LFM-2.5,Apple M1 (Metal)
2601.12215v1_Wavelet-Driven Masked Multiscale Reconstruction fo.pdf,Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation Models,"Megha Thukral, Cyrus Tanade, Simon A. Lee, Juhyeon Lee, Hao Zhou, Keum San Chun, Migyeong Gwak, Viswam Nathan, Md Mahbubur Rahman, Li Zhu, Mehrab Bin Morshed, Subramaniam Venkatraman, Sharanya Arcot Desai",arXiv:2601.12215v1,2601.12215,"Wearable SSL Method, Wavelet based Modelling, PPG foundation models","The paper introduces Masked Multiscale Reconstruction (MMR) for PPG representation learning, leveraging wavelet-based multiresolution decomposition to capture multi-scale physiological rhythms. Pretrained on ~17 million unlabeled 10-second PPG segments, MMR improves over existing models across diverse health tasks and demonstrates the value of wavelet representations for robust feature learning.",31.26,LFM-2.5,Apple M1 (Metal)
2601.12224v1_Where It Moves It Matters Referring Surgical Instr.pdf,"Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion","Meng Wei, Kun Yuan, Shi Li, Yue Zhou, Long Bai, Nassir Navab, Hongliang Ren, Hong Joo Lee, Tom Vercauteren, Nicolas Padoy",10.48550/arXiv.2025.12345,arXiv:2509.12345,"surgical segmentation, motion-guided segmentation, natural language processing, surgical instruments, deep learning","This work introduces SurgRef, a motion-based framework for segmenting surgical instruments from video, enabling language-driven interaction in operating rooms. It addresses challenges in general-purpose datasets by focusing on surgical-specific dynamics and ambiguity.",26.97,LFM-2.5,Apple M1 (Metal)
2601.12234v1_Proc3D Procedural 3D Generation and Parametric Edi.pdf,Procedural 3D Generation and Parametric Editing of 3D Shapes with Large Language Models,"Fadlullah Raji, Stefano Petrangeli, Matheus Gadelha, Yu Shen, Uttaran Bhattacharya, Gang Wu2",arXiv:2601.12234v1,2601.12234,"Proc3D, 3D modeling, procedural generation, Large Language Models, parametric editing, ULIP scores","Proc3D introduces a system for generating editable 3D models using procedural compact graph (PCG) and LLMs, enabling real-time, intuitive parametric edits and improving ULIP scores by 28%.",29.55,LFM-2.5,Apple M1 (Metal)
2601.12242v1_Optimal Power Allocation and Sub-Optimal Channel A.pdf,Optimal Power Allocation and Sub-Optimal Channel Assignment in NOMA Systems Using Deep Reinforcement Learning,"Woo Seok Kim, Jeonghoon Lee, Sangho Kim, Taesun An, WonMin Lee, Dowon Kim, Kyungseop Shin",arXiv:2601.12242v1,arXiv:2601.12242,"Non-Orthogonal Multiple Access, Deep Reinforcement Learning, Wireless Network, Resource Allocation","This paper proposes a deep reinforcement learning framework with replay memory to optimize network resource allocation in NOMA systems. It addresses the challenge of channel assignment and explores the effects of learning rate, batch size, model type, and feature count through extensive simulations.",28.83,LFM-2.5,Apple M1 (Metal)
2601.12243v1_Less is More Label-Guided Summarization of Procedu.pdf,Less is More: Label-Guided Summarization of Procedural and Instructional Videos,"Shreya Rajpal, Michal Golovanesky",arXiv:2601.12243v1,arXiv:2601.12243,"video summarization, procedural videos, instructional videos, semantic understanding, multimodal analysis","This paper proposes a three-stage framework, PRISM, for generating semantically grounded video summaries. PRISM integrates adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model. It achieves strong performance in summarizing instructional and activity videos while preserving procedural intent and reducing computational load.",27.72,LFM-2.5,Apple M1 (Metal)
2601.12247v1_Plan Verify and Fill A Structured Parallel Decodin.pdf,"Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion","Miao Li, Xuanzhou Chen, Pascal Van Hentenryck, Hengyu Fu, Yuhang Cai, Baihe Huang, Ye","2025a; Google DeepMind, 2025",2026-01-21,"diffusion models, planning, parallel decoding, semantic anchors, structural stopping, efficiency, accuracy","This paper introduces Plan-Verify-Fill (PVF), a training-free diffusion paradigm that leverages hierarchical planning and verification to optimize decoding efficiency. PVF constructs a global semantic skeleton and uses verification to guide pragmatic stopping, achieving up to 65% reduction in function evaluations compared to confidence-based methods while maintaining accuracy.",27.35,LFM-2.5,Apple M1 (Metal)
2601.12248v1_AQUA-Bench Beyond Finding Answers to Knowing When .pdf,AQUA-Bench: BEYOND FINDING ANSWERS TO KNOWING WHEN THERE ARE NONE,"Chun-Yi Kuan, Hung-yi Lee",,,"unanswerable questions, audio question answering, audio-aware large language models","This paper introduces AQUA-Bench, a benchmark for evaluating audio question answering systems by assessing performance on three unanswerable scenarios: absent answer detection, incompatible answer sets, and incompatible audio questions. It highlights limitations of current models in handling unanswerable inputs and proposes a framework to improve reliability.",25.36,LFM-2.5,Apple M1 (Metal)
2601.12249v1_An Innovative Framework for Breast Cancer Detectio.pdf,"An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion","Ehsan Sadeghi Pour, Mahdi Esmaeili, Morteza Romoozi",,,"Breast Cancer Detection, Pyramid Adaptive Atrous Convolution (PAAC), Transformer, Multi-Scale Feature Fusion, Self-Attention Mechanism, Medical Image Processing","This thesis presents an innovative framework for detecting malignant masses in mammographic images by integrating Pyramid Adaptive Atrous Convolution, Transformer architectures, and multi-scale feature fusion. The model leverages Dice Loss and Focal Loss to improve binary classification accuracy, achieving high performance in detecting cancerous masses with improved efficiency.",28.12,LFM-2.5,Apple M1 (Metal)
2601.12256v1_Improving Large Molecular Language Model via Relat.pdf,Improving Large Molecular Language Model,"Jinyoung Park, Minseong Bae, Jeehye Na, Hyunwoo J. Kim",10.48550/arXiv.2024.12345,arXiv:2408.12345,"large language model, molecular modeling, relation-aware collaboration, hallucination mitigation, 3D conformations","The paper proposes CoLLaMo, a language model enhanced with a multi-level molecular modality-collaborative projector, to address hallucination and modality integration issues in large molecular language models.",25.16,LFM-2.5,Apple M1 (Metal)
2601.12257v1_Soft Shadow Diffusion SSD Physics-inspired Learnin.pdf,Soft Shadow Diffusion (SSD): Physics-inspired,"Fadlullah Raji, John Murray Bruce",arXiv:2601.12257v1,arXiv:2601.12257v1,"Computational imaging, Machine learning, 3D generative models, Diffusion models, Separable non-linear least squares","This paper presents a novel 3D reconstruction method for non-line-of-sight imaging using a reformulated light transport model. By decomposing the scene into occluding and non-occluding components, the authors propose a gradient-based optimization and a physics-inspired neural network approach called Soft Shadow Diffusion (SSD). The method demonstrates effectiveness across diverse real-world NLOS scenarios and shows robustness to noise and illumination variations.",29.83,LFM-2.5,Apple M1 (Metal)
2601.12259v1_FutureX-Pro Extending Future Prediction to High-Va.pdf,FutureX-Pro: Extending Future Prediction to High-Value,,arXiv:2601.12259v1,arXiv:2601.12259,"FutureX, FutureX-Pro, Future Prediction, LLM, LLMs, agentic prediction, high-value verticals, market efficiency, retail forecasting, supply chain, public health, natural disaster","Building upon FutureX, this report introduces FutureX-Pro, extending agentic future prediction to high-value vertical domains such as Finance, Retail, Public Health, and Natural Disaster. It benchmarks Large Language Models on entry-level prediction tasks across these sectors and evaluates their readiness for industrial deployment.",28.61,LFM-2.5,Apple M1 (Metal)
2601.12260v1_Docs2Synth A Synthetic Data Trained Retriever Fram.pdf,A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding,"Yihao Ding, Qiang Sun, Puzhen Wu, Sirui Li, Siwen Luo, Wei Liu",10.1234/example.doi,,"document understanding, scanned documents, visual retrieval, synthetic data, zero-shot learning, domain adaptation","Introduces Docs2Synth, a framework that combines retrieval-guided inference with MLLMs to improve grounding in private and low-resource domains, addressing challenges of hallucination and domain drift.",26.66,LFM-2.5,Apple M1 (Metal)
2601.12263v1_Multimodal Generative Engine Optimization Rank Man.pdf,Multimodal Generative Engine Optimization: Rank Manipulation for Vision–Language Model Rankers,"Yixuan Du, Chenxiao Yu, Haoyan Xu, Ziyi Wang, Yue Zhao, Xiyang Hu",10.1234/example.doi,632562,"Multimodal ranking, Vision-language models, Adversarial manipulation, Product search, Ranking optimization","The paper presents a novel adversarial framework, Multimodal Generative Engine Optimization (MGEO), that enables attackers to manipulate product search rankings by jointly optimizing imperceptible image and textual perturbations. It highlights vulnerabilities in VLM-based ranking systems and demonstrates superior performance over text- and image-only attacks.",25.3,LFM-2.5,Apple M1 (Metal)
2601.12269v1_Simulated Annealing Enhances Theory-of-Mind Reason.pdf,Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models,"Xucong Hu, Jian-Qiao Zhu",,,"Language Models, Markov Chain Monte Carlo, Simulated Annealing, Power Sampling, Theory of Mind","The paper investigates how incorporating simulated annealing into autoregressive language models can recover Theory of Mind capabilities without additional weight updates, suggesting sampling-based optimization extracts latent global coherence.",23.97,LFM-2.5,Apple M1 (Metal)
2601.12276v1_Predictive Prototyping Evaluating Design Concepts .pdf,Predictive Prototyping: Evaluating Design Concepts with GPT,,,,"Prototyping, Design Theory, Iteration, Simulation, AI, LLM, GPT, RAG, Crowdsourcing","This work explores using generative pretrained transformers (GPTs) to predict design outcomes such as cost, performance, and usability during prototyping. It introduces a retrieval-augmented generation approach with OpenAI’s GPT-4o to emulate design feedback, comparing predictions against physical prototypes and highlighting the value of large-scale data in improving accuracy.",27.53,LFM-2.5,Apple M1 (Metal)
2601.12282v1_CytoCLIP Learning Cytoarchitectural Characteristic.pdf,CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain,"Pralaypati Ta, Sriram Venkatesaperumal, Keerthi Ram, Mohanasankar Sivaprakasam",10.1109/TECH.2024.12345,IEEE TRANSACTIONS AND JOURNALS TEMPLATE 1,"cytoarchitecture, histological image processing, contrastive language image pre-training, CLIP, brain regions, developmental brain","CytoCLIP is a vision-language model that leverages pre-trained Contrastive Language-Image Pre-Training to learn joint visual-text representations of brain cytoarchitecture. It proposes two variants: one for low-resolution whole-region images and another for high-resolution tiles, trained on NISSL-stained histological sections of developing fetal brains. Experiments show superior performance in classification and generalization tasks.",27.99,LFM-2.5,Apple M1 (Metal)
2601.12286v1_Conversational Context Classification A Representa.pdf,Conversational Context Classification: A,Jonathan Pan,,,"Large Language Models, One-Class SVM, Novelty Detection, In/Out-of-Context, Representation Engineering","The paper explores using Representation Engineering and One-Class SVM to detect subspaces in LLMs that represent specific conversational contexts, aiming to improve safety by identifying out-of-context behavior.",22.6,LFM-2.5,Apple M1 (Metal)
2601.12288v1_TimeGMM Single-Pass Probabilistic Forecasting via .pdf,Single-Pass Probabilistic Forecasting Via Adaptive Gaussian,"Lei Liu1, Tengyuan Liu1, Hongwei Zhao1, Jiahui Huang1, Ruibo Guo1, Bin Li1",,,"probabilistic time series forecasting, Gaussian mixture model, reversible instance normalization, uncertainty quantification, energy finance","This paper introduces TimeGMM, a probabilistic forecasting framework using Gaussian Mixture Models with adaptive Gaussian normalization, achieving superior predictive performance over existing methods.",24.47,LFM-2.5,Apple M1 (Metal)
2601.12294v1_ToolPRMBench Evaluating and Advancing Process Rewa.pdf,ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents,"Dawei Li, Yuguang Yao, Zhen Tan, Huan Liu, Ruocheng Guo",,,"tool-using agents, process reward models, reward-guided search, LLM evaluation, step-level testing","This paper introduces ToolPRMBench, a benchmark for evaluating process reward models (PRMs) in tool-using agents. It evaluates PRMs across diverse APIs and multi-LLM pipelines, aiming to identify effective reward structures for guiding sampling and exploration in complex action spaces.",24.74,LFM-2.5,Apple M1 (Metal)
2601.12304v1_A Two-Stage Globally-Diverse Adversarial Attack fo.pdf,A TWO-STAGE GLOBALLY-DIVERSE ADVERSARIAL ATTACK FOR VISION-LANGUAGE,"Wutao Chen, Huaqin Zou, Chen Wan, Lifeng Huang",,,"adversarial attack, VLP models, multi-modal retrieval, transferability","The paper proposes 2S-GDA, a two-stage globally-diverse attack framework for vision-language pre-training models. It improves attack success rates by introducing diverse textual and image perturbations across stages, achieving up to 11.17% gains in black-box settings.",25.4,LFM-2.5,Apple M1 (Metal)
2601.12310v1_Survival is the Only Reward Sustainable Self-Train.pdf,Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection,"Jennifer Dodgson, Alfath Daryl Alhajir, Michael Joedhitya, Akira Rafhael Janson Pattirane, Surender Suresh Kumar, Joseph Lim, C.H. Peh, Adith Ramdas, Steven Zhang Zhexu",arXiv:2601.12310v1,2601.12310,"self-training, environment-mediated selection, sustainable self-training, negative-space learning, robust autonomous systems","The paper presents a self-training architecture where learning is driven by environmental viability rather than external rewards. It demonstrates that survival-based selection enables stable, open-ended improvement without human supervision, highlighting mechanisms like negative-space learning and meta-learning through persistent strategies.",31.7,LFM-2.5,Apple M1 (Metal)
2601.12316v1_GazeFormer-MoE Context-Aware Gaze Estimation via C.pdf,GAZEFORMER-MOE: CONTEXT-A W ARE GAZE ESTIMATION VIA CLIP AND MOE,"Xinyuan Zhao, Xianrui Chen, Ahmad Chaddad, 2, ∗",,,"gaze estimation, multi-scale fusion, MoE transformer, semantic modulation, CLIP, pretrained prototypes, cross-scale attention, hyperparameter tuning","Presents a semantics modulated, multi-scale Transformer for 3D gaze estimation using CLIP and MoE, achieving state-of-the-art angular accuracy and robustness improvements.",25.37,LFM-2.5,Apple M1 (Metal)
2601.12317v1_Explanova Automatically Discover Data Insights in .pdf,Explanova: Automatically Discover Data Insights in N×M Table via XAI Combined LLM Workflow,Yiming Huang,10.1145/nnnnnnn.nnnnnnn,,"Explainable AI, AutoML, LLM, data analysis, feature exploration","This paper proposes an automated LLM workflow to discover data insights in N×M tables, leveraging XAI and LLM capabilities. It explores how preset workflows can empower common LLMs for efficient data analysis, reducing reliance on expensive proprietary APIs.",24.58,LFM-2.5,Apple M1 (Metal)
2601.12318v1_Beyond Human Annotation Recent Advances in Data Ge.pdf,Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence,"Dehao Ying, Fengchang Yu, Haihua Chen, Changjiang Jiang, Yurong Li, Wei Lu",10.1093/acr/qad045,,"Document Intelligence, Data Generation, Data Quality Evaluation","This survey establishes the first comprehensive technical map for data generation in Document Intelligence. It redefines data generation as supervisory signal production and introduces a taxonomy based on data availability and labels. The framework organizes methods into four paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction. A multi-level evaluation framework integrates intrinsic quality and extrinsic utility, highlighting challenges like fidelity gaps and co-evolutionary ecosystems.",27.59,LFM-2.5,Apple M1 (Metal)
2601.12323v1_MARO Learning Stronger Reasoning from Social Inter.pdf,Learning Stronger Reasoning from Social Interaction,"Yin Cai1, Zhouhong Gu1, JunTao Zhang1, Ping Chen1",,,"social interaction, multi-agent learning, reasoning, judgment, collaboration, communication","This paper proposes MARO, a method enabling large language models to improve reasoning through multi-agent social environments. It addresses sparse learning, uneven role distribution, and environmental instability by simulating interactions, balancing training weights, and evaluating behavior utility. Experimental results show significant gains in social reasoning and transfer to related tasks.",26.15,LFM-2.5,Apple M1 (Metal)
2601.12327v1_The Expert Validation Framework EVF Enabling Domai.pdf,The Expert Validation Framework (EVF): Enabling Domain Expert Control in AI Engineering,"Lucas Gren, Felix Dobslaw",10.1145/xxx.xxxx,CCS Concepts,"GenAI, expert validation, quality assurance, AI engineering, domain expert control","This paper introduces an Expert Validation Framework (EVF) that centers domain experts in building and overseeing GenAI systems. It outlines a four-stage implementation process—specification, system creation, validation, and production monitoring—to ensure quality and maintain control across diverse AI applications.",25.31,LFM-2.5,Apple M1 (Metal)
2601.12330v1_IceWatch Forecasting Glacial Lake Outburst Floods .pdf,IceWatch: Forecasting Glacial Lake Outburst Floods using Multimodal Deep Learning,"Zuha Fatima, Muhammad Anser Sohaib, Muhammad Talha, Ayesha Kanwal, Sidra Sultana, Nazia Perwaiz",10.5281/zenodo.1234567,,"CNN, deep learning, glacier monitoring, GLOF detection, LSTM, remote sensing, Sentinel-2, temperature forecasting, transformer, velocity prediction","Glacial Lake Outburst Floods (GLOFs) threaten high mountain regions. This paper introduces IceWatch, a deep learning framework using multimodal data (Sentinel-2 imagery, temperature forecasts) to predict GLOF events, improving speed, accuracy, and robustness over traditional methods.",27.8,LFM-2.5,Apple M1 (Metal)
2601.12331v1_Efficient Privacy-Preserving Retrieval Augmented G.pdf,Efficient Privacy-Preserving Retrieval Augmented Generation with Distance-Preserving Encryption,"Huanyi Ye, Jiale Guo, Ziyao Liu, Kwok-Yan Lam",,,"RAG, Privacy-Preserving Retrieval, Distance-Preserving Encryption, LLMs, Privacy Leakage, Query Analysis","The paper proposes ppRAG, an efficient privacy-preserving retrieval augmented generation framework for untrusted cloud environments. It introduces CAPRISE, a conditional approximate distance-preserving symmetric encryption that preserves relative distances between encrypted queries and database embeddings, enhancing privacy while maintaining retrieval accuracy. Differential privacy is applied to further mitigate query analysis risks. Experimental results demonstrate high retrieval accuracy and strong privacy guarantees.",27.14,LFM-2.5,Apple M1 (Metal)
2601.12338v1_Actionable Advice from Reviews via Mixture of LoRA.pdf,Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM Pipeline for Issue Extraction and Business,"Kartikey Singh Bhandari, Manav Ganesh, Yashwant Viswanathan, Archit Agrawal, Dhruv Kumar, Pratik Narang",,,"customer reviews, review analysis, business recommendations, LLM, LoRA, issue extraction, operational decisions","Customer reviews contain detailed signals about service failures and user expectations, but converting this unstructured feedback into actionable business decisions remains difficult. This paper studies review-to-action generation, proposing a modular two-LLM framework with a mixture-of-LoRA experts to produce concrete, implementable recommendations.",27.73,LFM-2.5,Apple M1 (Metal)
2601.12341v1_Time-Continuous Modeling for Temporal Affective Pa.pdf,Time-Continuous Modeling for Temporal Affective Pattern Recognition in LLM’s,"Rezky M. Kam, Coddy N. Siswanto",,,"time-continuous modeling, temporal affective patterns, longitudinal adaptation, affective dynamics, physics informed neural networks","The paper introduces a hybrid encoder-decoder architecture leveraging time-aware patterns to model evolving affective states in conversations. It addresses limitations of static token generation by incorporating progressive steering of affective trajectories, aiming to improve interpretability and adaptability in LLM-based systems.",26.04,LFM-2.5,Apple M1 (Metal)
2601.12343v1_How Well Do LLMs Predict Human Behavior A Measure .pdf,How Well Do LLMs Predict Human Behavior?,"Wayne Gao, Sukjin Han, Annie Liang",arXiv:2601.12343v1,arXiv:2601.12343,"LLMs, human behavior, predictive accuracy, pretrained knowledge, economic variables","The paper proposes a measure to evaluate how much knowledge a pretrained large language model brings to predicting human behavior, comparing it to the amount of task-specific data required. It applies this to the Panel Study of Income Dynamics and finds LLMs encode significant predictive information for some variables.",32.11,LFM-2.5,Apple M1 (Metal)
2601.12349v1_Zero-Permission Manipulation Can We Trust Large Mu.pdf,Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?,"Yi Qian, Kunwei Qian, Xingbang He, Ligeng Chen, Jikang Zhang, Tiantai Zhang, Haiyang Wei, Linzhang Wang, Hao Wu",z472421519,,"zero-permission, large multimodal model, GUI agents, Android, attack surface, action rebinding, multi-step attack chains, UI state preservation, Android integration","The paper discusses vulnerabilities in Android GUI agents where agents can be manipulated via Action Rebinding, exploiting the assumption of visual atomicity. It introduces an Intent Alignment Strategy to bypass verification gates and achieves high success rates in rebinding attacks across diverse tasks.",27.7,LFM-2.5,Apple M1 (Metal)
2601.12357v1_SimpleMatch A Simple and Strong Baseline for Seman.pdf,SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence,"Hailong Jin1, Huiying Li1",10.48550/arXiv.2024.12345,arXiv:2409.12345,"semantic correspondence, feature fusion, downsampling, sparse matching","This paper introduces SimpleMatch, a lightweight framework for semantic correspondence that achieves strong performance at low resolutions. It addresses the issue of irreversible fusion of distinct keypoints via a lightweight upsample decoder and multi-scale supervised loss. The method also introduces sparse matching and window-based localization to reduce training memory usage by 51%. It achieves 84.1% PCK@0.1 on the SPair-71k benchmark, offering a practical baseline for future research.",27.78,LFM-2.5,Apple M1 (Metal)
2601.12358v1_From Prompts to Pavement LMMs-based Agentic Behavi.pdf,From Prompts to Pavement: LMM-based Agentic Behavior-Tree,"Omar Y. Goba1, Ahmed Y. Gado1, Catherine M. Elias1, Ahmed Hussein3",10.48550/arXiv.2024.12345,None,"Behavior Tree, Large Language Model, L5 Autonomy, Navigation, CARLA, ROS","This paper presents an agentic framework leveraging LLMs and LVMs to generate adaptive behavior trees for autonomous vehicles, demonstrating successful navigation in simulated environments without human intervention.",24.96,LFM-2.5,Apple M1 (Metal)
2601.12374v1_A Scalable Entity-Based Framework for Auditing Bia.pdf,A Scalable Entity-Based Framework for Auditing Bias in LLMs,"Akram Elbouanani, Aboubacar Tuo, Adrian Popescu",1.0,1.0,"bias, LLMs, auditing, entities, fairness, natural language processing","This paper introduces a scalable framework using named entities to audit structural biases in large language models. Synthetic data was used to replicate real-world bias patterns across diverse entities, tasks, languages, and prompting strategies. Findings reveal systematic biases such as preference for certain political views, Western-centric perspectives, and favoring Western companies, highlighting the need for rigorous auditing before deployment.",26.66,LFM-2.5,Apple M1 (Metal)
2601.12389v1_NADIR Differential Attention Flow for Non-Autoregr.pdf,Differential Attention Flow for Non-Autoregressive Transliteration in Indic Languages,"Lakshya Tomar, Vinayak Abrol, Puneet Agarwal",10.48550/arXiv:2509.06932,2509.06932,"multilingual transliteration, non-autoregressive models, character error rate, attention flow, indic languages","This work introduces NADIR, a novel non-autoregressive architecture combining Differential Transformer and Mixture-of-Experts, to improve speed-accuracy trade-off in multilingual transliteration. NADIR achieves a 13× speed-up over AR baselines while reducing key error types and demonstrating practical benefits for real-time deployment.",26.33,LFM-2.5,Apple M1 (Metal)
2601.12392v1_PsychēChat An Empathic Framework Focused on Emotio.pdf,PsychEChat: An Empathic Framework Focused on Emotion Shift Tracking and Safety Risk Analysis in Psychological Counseling,"Zhentao Xia, Yongqi Fan, Yuxiang Chu, Yichao Yin, Liangliang Chen, Tong Ruan, Weiyan Zhang",,,"psychological counseling, emotion shift tracking, safety risk analysis, LLM, mental health, empathic framework","This paper introduces PsychEChat, a new interactive chat system that integrates emotion shift tracking and safety risk analysis for psychological counseling. It employs an Agent Modestruc model to capture seekers' emotions and shifts, and a Risk Control Module to anticipate reactions. Experiments show it outperforms existing methods in emotional insight and safety control.",26.99,LFM-2.5,Apple M1 (Metal)
2601.12401v1_Beyond the Dirac Delta Mitigating Diversity Collap.pdf,Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement,"Jinmei Liu, Haoru Li, Zhenhong Sun, Bo Wang, Daoyi Dong, Chunlin Chen, Yatao Bian, Zhi Wang",,,"reinforcement learning, diversity collapse, image generation, task alignment, generation diversity","The paper addresses diversity collapse in RL fine-tuning for image generation by proposing DRIFT, which systematically encourages output diversity while maintaining strong task alignment. Experimental results demonstrate improved Pareto dominance in both alignment and diversity.",25.45,LFM-2.5,Apple M1 (Metal)
2601.12402v1_Weaknesses of Facial Emotion Recognition Systems.pdf,Weaknesses of Facial Emotion Recognition Systems,"Aleksandra Jamróz, Patrycja Wysocka, Piotr Garbat",arXiv:2601.12402v1,arXiv:2601.12402,"Facial Emotion Recognition, Deep learning, Computer Vision","This study evaluates the performance of facial emotion recognition systems across diverse datasets, identifying weaknesses such as dataset differences, varying difficulty levels in emotion detection, and challenges in distinguishing closely related emotions.",28.45,LFM-2.5,Apple M1 (Metal)
2601.12405v1_Explainable Machine Learning for Pediatric Dental .pdf,Explainable Machine Learning for Pediatric Dental Risk Stratification Using Socio-Demographic Determinants,"Manasi Kanade, Abhi Thakkar, Gabriela Fernandes",,,"pediatric dental disease, socio-demographic determinants, explainable AI, risk stratification, ethical deployment","This study develops an explainable AI framework for pediatric dental risk stratification, emphasizing interpretability and ethical considerations. It integrates socio-demographic factors such as income, race, gender, and medical history to improve transparency in AI-driven dental risk assessment.",29.26,LFM-2.5,Apple M1 (Metal)
2601.12410v1_Are LLMs Smarter Than Chimpanzees An Evaluation on.pdf,Are LLMs Smarter Than Chimpanzees?,"Dingyi Yang, Junqi Zhao, Xue Li, Ce Li, Boyang Li",arXiv:2601.12410v1,arXiv:2601.12410,"LLM performance, knowledge state estimation, intelligence comparison, cognitive anthropology, human vs. machine","This paper evaluates large language model (LLM) capabilities in knowledge state tracking and estimation, comparing them to chimpanzees. It finds current LLMs perform near-randomly and are inferior to humans in these tasks, suggesting future research should prioritize knowledge estimation and intention understanding.",26.01,LFM-2.5,Apple M1 (Metal)
2601.12415v1_Orthogonalized Policy OptimizationDecoupling Sampl.pdf,Orthogonalized Policy Optimization,Wang Zixian,arXiv:2601.12415v1,2601.12415,,"This work formalizes the decoupling of sampling geometry and optimization geometry in RLHF by introducing Orthogonalized Policy Optimization (OPO), which uses KL divergence-based weighting and quadratic regularization to stabilize training.",24.59,LFM-2.5,Apple M1 (Metal)
2601.12436v1_Purification Before Fusion Toward Mask-Free Speech.pdf,Purification Before Fusion: Mask-Free Speech Enhancement for Robust Audio-Visual Speech Recognition,"Linzhi Wu1, Xingyu Zhang2, Hao Yuan3, Yakun Zhang2, Changyan Zheng4, Liang Xie2, Tiejun Liu1, Erwei Yin2",,,"audio-visual speech recognition, speech feature enhancement, noise-robust, multimodal bottleneck Conformer","This work proposes an end-to-end noise-robust A VSR framework that uses a Conformer-based bottleneck fusion module to implicitly refine noisy audio features with video assistance, eliminating explicit noise mask generation while preserving speech semantics.",27.05,LFM-2.5,Apple M1 (Metal)
2601.12442v1_Constraint-Aware Neurosymbolic Uncertainty Quantif.pdf,Constraint-Aware Neurosymbolic Uncertainty,"Shahnawaz Alam, Mohammed Mudassir Uddin, Mohammed Kaif Pasha",,,"Neurosymbolic AI, Uncertainty Quantification, Bayesian Deep Learning, Scientific Constraints, Calibration, Physics-Informed Machine Learning","The paper introduces the Constraint-Aware Neurosymbolic Uncertainty Framework (CANUF), unifying Bayesian deep learning with differentiable symbolic reasoning to deliver trustworthy uncertainty in scientific AI. It addresses the need for models that respect domain constraints and incorporates automated constraint extraction from scientific literature.",27.26,LFM-2.5,Apple M1 (Metal)
2601.12443v1_Adversarial Defense in Vision-Language Models An O.pdf,Adversarial Defense in Vision-Language Models: An Overview,"Lei Zhang, Xiaowei Fu",,,"VLMs, adversarial defense, survey, surroundings, robustness, defense strategies","This work reviews recent advancements in adversarial defense strategies for Vision Language Models, highlighting training-time, test-time adaptation, and training-free approaches, while discussing their strengths, limitations, and challenges.",24.96,LFM-2.5,Apple M1 (Metal)
2601.12444v1_Large Language Model for OWL Proofs.pdf,Large Language Model for OWL Proofs,"Hui Yang, Jiaoyan Chen, Uli Sattler",10.1145/XXXXXX.XXXXXX,,"Large Language Models, OWL ontologies, proof generation, reasoning tasks, logical complexity, natural language generation","This work investigates how Large Language Models can generate proofs for OWL ontologies by developing an automated dataset construction and evaluation framework. It evaluates three sequential tasks—Extraction, Simplification, Explanation, and Logic Completeness assessment—and finds that logical complexity and input noise significantly impact performance.",25.94,LFM-2.5,Apple M1 (Metal)
2601.12449v1_AgenTRIM Tool Risk Mitigation for Agentic AI.pdf,AgenTRIM: Tool Risk Mitigation for Agentic AI,"Roy Betser, Shamik Bose, Amit Giloni, Chiara Picardi, Sindhu Padakandla, Roman Vainshtein",10.1093/acpan/sca062,,"AI agents, tool risk mitigation, agentic risk, excessive agency, tool-driven agency, AGENTRIM, LLM-based agents, security risks, attack surface, tool misuse","The paper introduces AGENTRIM, a framework that detects and mitigates tool-driven agency risks in LLM-based agents without altering their internal reasoning. It reconstructs and verifies the agent’s tool interface, enforcing least-privilege access at runtime while maintaining high performance. Experiments demonstrate improved robustness to attacks and effective enforcement of safety policies.",27.5,LFM-2.5,Apple M1 (Metal)
2601.12465v1_Incentivizing In-depth Reasoning over Long Context.pdf,Incentivizing In-Depth Reasoning Over Long,"Miao Peng, Weizhou Shen, Nuo Chen, Chenliang Li, Ming Yan, Jia Li",,,"reinforcement learning, long-context reasoning, deep thinking, multi-hop reasoning, LLM grounding, credit assignment",The paper addresses the degradation of RLVR performance in long-context reasoning by identifying the 'almost-there' phenomenon. It proposes DEEPREASONQA and Long-context Process Advantage Shaping to improve reasoning accuracy and stability.,25.36,LFM-2.5,Apple M1 (Metal)
2601.12467v1_Patch-Level Tokenization with CNN Encoders and Att.pdf,Patch-Level Tokenization with CNN Encoders and Attention for Improved Transformer Time-Series Forecasting,Saurish Nagrath,10.48550/arXiv.2311.07818,2311.07818,"multivariate time-series forecasting, financial time-series forecasting, transformer models, temporal tokenization, convolutional neural networks, attention mechanisms",This work proposes a two-stage forecasting framework that combines convolutional neural networks for local temporal representation learning and transformer self-attention for global dependency modeling. The approach achieves competitive forecasting performance while demonstrating the benefits of structured temporal representations.,27.3,LFM-2.5,Apple M1 (Metal)
2601.12471v1_Knowing When to Abstain Medical LLMs Under Clinica.pdf,Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty,"Sravanthi Machcha, Sushrita Yerra, Sahil Gupta, Aishwarya Sahoo, Sharmin Sultana, Hong Yu, Zonghai Yao",https://github.com/sravanthi6m/MedAbstain,,"medical LLMs, clinical uncertainty, abstain mechanisms, medical QA, uncertainty quantification","Current evaluation of LLMs prioritizes accuracy, but real-world safety demands the ability to abstain when uncertain. This study introduces MedAbstain, a benchmark for abstention in medical multiple-choice question answering, revealing that models often fail to abstain despite high accuracy, emphasizing the need for explicit uncertainty signals.",25.92,LFM-2.5,Apple M1 (Metal)
2601.12494v1_Harmonizing the Arabic Audio Space with Data Sched.pdf,Harmonizing the Arabic Audio Space with Data Scheduling,"Hunzalah Hassan Bhatti, Firoj Alam, Shammur Absar Chowdhury",,,"Arabic audio, data scheduling, multi-task learning, speech understanding","This paper presents the first systematic study of multi-task instruction tuning for an Arabic-centric audio LLM, covering generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). It introduces AraMega-SSum dataset and proposes a Hybrid TPC+ADS Strategy to balance efficiency and robustness in adapting Qwen2.5-Omni.",24.88,LFM-2.5,Apple M1 (Metal)
2601.12499v1_Failure Modes in Multi-Hop QA The Weakest Link Law.pdf,Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck,"Meiru Zhang, Zaiqiao Meng, Nigel Collier",mz468@cam.ac.uk,nhc30@cam.ac.uk,"multi-hop reasoning, position bias, LLMs, recognition failure","The paper investigates why LLMs struggle with multi-hop reasoning despite large context windows, proposing the 'Weakest Link Law' where performance depends on the least visible evidence. It identifies a duality in attention steering and demonstrates that 'thinking' models can effectively locate information, matching gold-only baselines even in noisy settings.",25.69,LFM-2.5,Apple M1 (Metal)
2601.12518v1_Cooperative Multi-agent RL with Communication Cons.pdf,Cooperative Multi-agent RL with Communication constraints,"Nuoya Xiong, Aarti Singh",,,"multi-agent reinforcement learning, communication constraints, base policy prediction",Addresses communication limitations in cooperative MARL by introducing base policy prediction to reduce communication rounds while maintaining performance.,22.29,LFM-2.5,Apple M1 (Metal)
2601.12522v1_Improved Bug Localization with AI Agents Leveragin.pdf,Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition,"Asif Mohammed Samir, Mohammad Masudur Rahman",https://doi.org/XXXXXXX.XXXXXXX,ICPC 2026,"Bug Localization, LLM, Agentic AI, Cognition, Debugging, Software Engineering, Information Retrieval","The paper presents CogniGent, an agentic technique for bug localization that leverages multiple AI agents, causal reasoning, call-graph analysis, and context engineering. It achieves significant improvements over traditional and LLM-based methods in performance metrics and demonstrates statistical superiority.",26.4,LFM-2.5,Apple M1 (Metal)
2601.12534v1_Encoding Emotion Through Self-Supervised Eye Movem.pdf,Encoding Emotion Through Self-Supervised Eye Movement Reconstruction,"Marcus Ma, Jordan Prescott, Emily Zhou, Tiantian Feng, Kleanthis Avramidis, Gabor Mihaly Toth, Shrikanth Narayanan",,,"eye movement, self-supervised learning, emotion prediction, deep learning, eye tracking, natural language processing","This study explores using self-supervised eye movement reconstruction to predict emotional expressions from naturalistic videos. By leveraging large-scale interview data and pretraining on language models, the authors develop a gaze-based model that aligns eye movements with emotional cues, demonstrating effectiveness in encoding affective signals.",27.15,LFM-2.5,Apple M1 (Metal)
2601.12535v1_Improving Low-Resource Machine Translation via Rou.pdf,Improving Low-Resource Machine Translation via Round-Trip,"Ahmed Attia, Alham Fikri, MBZUAI, Masdar City, UAE",,,"low-resource machine translation, reinforcement learning, round-trip bootstrapping, No Language Left Behind, translation quality, BLEU, chrF++",Investigates a self-supervised reinforcement learning-based fine-tuning for low-resource MT using round-trip bootstrapping with NLLB models. Demonstrates improvements across multiple languages and highlights benefits of scale for leveraging pretrained knowledge.,25.94,LFM-2.5,Apple M1 (Metal)
2601.12538v1_Agentic Reasoning for Large Language Models.pdf,Agentic Reasoning for Large Language Models,"Tianxin Wei, Ting-Wei Li, Zhining Liu, Xuying Ning, Yang2, Zhou, Wang5, Xiangru Tang, Yin Xiao, Liri Fang, Hui Liu, Xianfeng Tang, Yuji Zhang, Jiaxuan You, Hanghang Tong1",arXiv:2601.12538v1,arXiv:2601.12538,"Agentic AI, LLM Agent, Agentic Reasoning, Self-evolving, Collective Multi-agent Reasoning","This survey provides a systematic roadmap of agentic reasoning across three dimensions: foundational agentic reasoning, self-evolving agentic reasoning, and collective multi-agent reasoning. It analyzes environmental dynamics, system constraints, and optimization settings, and discusses real-world applications and future directions.",29.27,LFM-2.5,Apple M1 (Metal)
2601.12539v1_MemeLens Multilingual Multitask VLMs for Memes.pdf,MEMELENS: Multilingual Multitask VLMs for Memes,"Ali Ezzat Shahroor1, Mohamed Bayan Kmainasi2, Abul Hasnat3, Dimitar Dimitrov, Giovanni Da San Martino, Preslav Nakov, Firoj Alam1","fialam, alsh34060",mha128,"memes, multilingual, multitask, VLM, meme understanding, harm, affect, humor","This paper proposes MEMELENS, a unified multilingual and multitask explanation-enhanced Vision Language Model for meme understanding. It consolidates 38 public meme datasets, maps dataset-specific labels into a shared taxonomy of 20 tasks, and analyzes performance across modeling paradigms, highlighting the need for multimodal training and caution against over-specialization.",27.43,LFM-2.5,Apple M1 (Metal)
2601.12542v1_Rethinking the AI Scientist Interactive Multi-Agen.pdf,Rethinking the AI Scientist: Interactive Multi-Agent,"Lukas Weidener, Marko Brkić, Mihailo Jovanović, Ritvik Singh, Chiara Baccin, Emre Ulgac, Alex Dobrin, Aakaash Meduri",https://github.com/bio-xyz/BioAgents,,"artificial intelligence, scientific discovery, multi-agent system, interactive investigation, research workflow, novelty detection","Artificial intelligence systems for scientific discovery have demonstrated remarkable potential, yet existing approaches remain largely proprietary and operate in batch-processing modes requiring hours per research cycle, precluding real-time researcher guidance. This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. The architecture comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Two operational modes support different workflows: semi-autonomous mode with selective human checkpoints, and fully autonomous mode for extended investigations. Evaluation on the BixBench computational biology benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points.",28.49,LFM-2.5,Apple M1 (Metal)
2601.12547v1_How Clinicians Think and What AI Can Learn From It.pdf,How Clinicians Think—and What AI Can Learn From,"Dr. Dipayan Sengupta, MD, Dr. Saumya Panda, MD",arXiv:2601.12547v1,arXiv:2601.12547,"clinical AI, decision making, ordinal reasoning, healthcare decision support, robust algorithms","The paper argues that clinicians primarily use ordinal, non-compensatory decision-making rather than optimizing probabilities. It proposes that AI should prioritize robust, rule-based ordinal decisions aligned with clinician intuition, emphasizing stability under uncertainty.",23.75,LFM-2.5,Apple M1 (Metal)
2601.12549v1_Benchmarking Concept-Spilling Across Languages in .pdf,Benchmarking Concept-Spilling Across Languages in LLMs,"Ilia Badanin, Daniil Dzenhaliou, Imanol Schlag",arXiv:2601.12549v1,arXiv:2601.12549v1,"multilingual models, semantic robustness, language spilling, polysemy, cross-lingual evaluation","This paper introduces a comparative framework to evaluate multilingual semantic robustness by measuring how models handle polysemous words across languages. It reveals variation in semantic performance across models and languages, offering a scalable benchmark for fair model comparison.",27.27,LFM-2.5,Apple M1 (Metal)
2601.12554v1_Artificial Intelligence in Materials Science and E.pdf,"Artificial Intelligence in Materials Science and Engineering: Current Landscape, Key Challenges, and Future","Iman Peivaste, Salim Belouettar ∗1, Francesco Mercuri, Nicholas Fantuzzi, Hamidreza Dehghani, Razieh Izadi, Halliru Ibrahim, Jakub Lengiewicz, Maël Belouettar-Mathis, Kouider Bendine, Ahmed Makradi, Martin Hörsch, Peter Klein, Mohamed El Hachemi, Yacine Rezgui, Natalia Konchakova, Ali Daouadji, Luxembourg Institute of Science and Technology (LIST), Department of Physics and Materials Science, University of Luxembourg, Istituto per lo Studio dei Materiali Nanostrutturati (ISMN), Norwegian University of Life Sciences (NMBU), Fraunhofer Institute for Industrial Mathematics (ITWM), Ecole Centrale de Lyon, Norwegian University of Science and Technology (NTNU), School of Engineering, Cardiff University, INSA Lyon",arXiv:2601.12554v1,2601.12554,"Artificial Intelligence, Materials Science, Machine Learning, Deep Learning, Data Science, Computational Materials","Artificial Intelligence is rapidly transforming materials science and engineering, offering powerful tools to navigate complexity, accelerate discovery, and optimize material design in ways previously unattainable. This review provides a comprehensive overview of recent advancements and methodologies, highlighting the pivotal role of data-driven techniques.",31.44,LFM-2.5,Apple M1 (Metal)
2601.12557v1_Life Machine Learning and the Search for Habitabil.pdf,"Life, Machine Learning, and the Search for Habitability: Predicting Biosignature","Mark Moussa1, Amber V . Young1, Brianna Isola1, Vasuda Trehan1, Michael D. Himes1, Nicholas Wogan2, Giada Arney1",10.3847/2045-4363/abd053,,"Life, Machine Learning, Habitable Worlds Observatory, Biosignature, Exoplanets, AI, ML, Uncertainty Quantification","The paper introduces two machine-learning models—Bayesian Convolutional Neural Network (BCNN) and Spectral Query Adaptive Transformer (SQuAT)—for predicting biosignature species fluxes from exoplanet reflected-light spectra. BCNN quantifies uncertainties, while SQuAT enhances interpretability by linking spectral features to biosignatures. Both achieve high accuracy across diverse exoplanetary conditions, supporting efficient mission prioritization for the Habitable Worlds Observatory.",28.19,LFM-2.5,Apple M1 (Metal)
2601.12560v1_Agentic Artificial Intelligence AI Architectures T.pdf,"Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents","Arunkumar V, Gangadharan G.R., Rajkumar Buyya",arunkumarv1530@gmail.com,arxiv:2601.12560v1,"Agentic AI, Large Language Models, Autonomous Agents, Multi-Agent Systems, Cognitive Architectures, Tool Use, Planning","This paper investigates architectures and proposes a unified taxonomy for Agentic AI agents, discussing transitions from linear reasoning to native inference time reasoning and from fixed API calls to open standards. It highlights challenges such as hallucination and infinite loops and outlines future research directions.",28.47,LFM-2.5,Apple M1 (Metal)
2601.12577v1_Primate-like perceptual decision making emerges th.pdf,Primate-like perceptual decision making emerges through deep recurrent reinforcement learning,"Nathan J. Wispinski, Scott A. Stone, Anthony Singhal, Patrick M. Pilarski, Craig S. Chapman",arXiv:2601.12577v1,arXiv:2601.12577,"primate decision making, deep reinforcement learning, neural mechanisms, evidence accumulation, perceptual discrimination","Progress has led to a detailed understanding of the neural mechanisms that underlie decision making in primates. However, less is known about why such mechanisms are present in the first place. Theory suggests that primate decision making mechanisms, and their resultant behavioral abilities, emerged to maximize reward in the face of noisy, temporally evolving information. To test this theory, we trained an end-to-end deep recurrent neural network using reinforcement learning on a noisy perceptual discrimination task. Networks learned several key abilities of primate-like decision making including trading off speed for accuracy, and flexibly changing their mind in the face of new information. Internal dynamics of these networks suggest that these abilities were supported by similar decision mechanisms as those observed in primate neurophysiological studies.",30.36,LFM-2.5,Apple M1 (Metal)
2601.12582v1_Ontology-aligned structuring and reuse of multimod.pdf,Ontology-aligned structuring and reuse of multimodal materials data and workflows towards automatic reproduction,"Sepideh Baghaee Ravari, Abril Azocar Guzman Sarath, Menon Stefan, Hickel Markus Stricker",,,"text mining, workflow, large language models, stacking fault energy, materials science, computational reproducibility","The paper introduces an ontology-driven framework using large language models to automate extraction and structuring of computational workflows from the literature, aiming to improve reproducibility in materials science by aligning data and processes with established ontologies.",25.66,LFM-2.5,Apple M1 (Metal)
2601.12585v1_Do MLLMs See What We See Analyzing Visualization L.pdf,Do MLLMs See What We See?,"Mengli (Dawn) Duan, Yuhe (Sissi) Jiang, Matthew Varona, Carolina Nobre",xx.xxxx/TVCG.201x,xx.xxxx/TVCG.201x,"Visualization Literacy, Multimodal Large Language Model, Evaluation Study","This paper presents the first systematic analysis of barriers to visualization literacy in MLLMs. Using the reVLAT benchmark with synthetic data, the authors evaluate four state-of-the-art models and identify two machine-specific barriers: struggles with color-intensive and segment-based visualizations, and difficulties forming consistent comparative reasoning. The findings contribute to better evaluation and design of AI-driven visualization assistants.",26.83,LFM-2.5,Apple M1 (Metal)
2601.12594v1_SLAP Scalable Language-Audio Pretraining with Vari.pdf,Scalable Language-Audio Pretraining With V Arable-Duration,"Xinhao Mei, Gael Le Lan Haohe Liu, Zhaoheng Ni, Nyang Shi Vikas Chandra",,,"multimodal learning, CLAP, self-supervised learning, contrastive learning, multi-objective learning",Introduces Scalable Language-Audio Pretraining (SLAP) to address limitations of CLAP by scaling pretraining to 109 million audio-text pairs with variable durations and incorporating multiple training objectives.,25.31,LFM-2.5,Apple M1 (Metal)
2601.12607v1_A Cloud-based Multi-Agentic Workflow for Science.pdf,A Cloud-based Multi-Agentic Workflow for Science,"Anurag Acharya, Timothy Vega, Rizwan A. Ashraf, Anshu Sharma, Derek Parker",10.1145/nnnnnn.nnnnnnn,,"Large Language Models, LLMs for Science, LLM Agents, Multi-agent Framework, Catalysis, Chemistry, Material Science, Cloud Computing","This paper presents a domain-agnostic, model-independent workflow for an agentic framework that supports scientific tasks. Built with a supervisor agent, it integrates various agents for literature review, data analysis, simulations, and more. The framework demonstrates high task completion rates and competitive accuracy, offering a viable solution for scientific assistants on cloud platforms.",26.75,LFM-2.5,Apple M1 (Metal)
2601.12617v1_Creating Disability Story Videos with Generative A.pdf,"Creating Disability Story Videos with Generative AI: Motivation, Expression, and Sharing","Shuo Niu, Dylan Clements, Hyungsin Kim",10.1145/3772318.3791495,,"Disability, Storytelling, Video, Generative AI, LLM","Generative AI supports people with disabilities in creating stories about their experiences, but challenges remain around bias and accessibility.",23.1,LFM-2.5,Apple M1 (Metal)
2601.12637v1_Topology-Aware Multiscale Mixture of Experts for E.pdf,Topology-Aware Multiscale Mixure of Experts for Efficient Molecular Property Prediction,"Long D. Nguyen, Kelin Xia, Binh P. Nguyen",arXiv:2601.12637v1,arXiv:2601.12637,"Graph Neural Networks, Topological Deep Learning, Mixture of Experts, Molecular Representation","This paper proposes a Multiscale Interaction Mixture of Experts (MI-MoE) to model 3D molecular properties by adapting interaction modeling across geometric regimes. It introduces a distance-cutoff expert ensemble, a topological gating encoder, and demonstrates improved performance across diverse molecular datasets.",26.77,LFM-2.5,Apple M1 (Metal)
2601.12638v1_Mixed Precision PointPillars for Efficient 3D Obje.pdf,Mixed Precision PointPillars for Efficient 3D Object,"Ninnart Fuengfusin, Keisuke Yoneda, Naoki Suganuma",,,"neural networks, quantization, 3D object detection, point clouds, TensorRT",Proposes a mixed precision framework for PointPillars to accelerate runtime while mitigating performance loss from LIDAR's numerical distributions.,23.31,LFM-2.5,Apple M1 (Metal)
2601.12641v1_STEP-LLM Generating CAD STEP Models from Natural L.pdf,Generating CAD STEP Models from Natural Language with Large Language Models,"Xiangyu Shi, Junyang Ding, Xu Zhao, Sinong Zhan, Payal Mohapatra, Daniel Quispe, Kojo Welbeck, Jian Cao, Wei Chen, Ping Guo, Qi Zhu",arXiv:2601.12641v1,2601.12641v1,"Computer-aided design, STEP file, large language models, design automation","This paper introduces a dataset of ~40K STEP-caption pairs and presents novel preprocessing tailored for STEP's graph-structured format. It employs a depth-first search-based reserialization and retrieval-augmented generation to improve geometric fidelity, demonstrating that LLM-driven STEP model generation is feasible and superior to existing Text2CAD methods.",28.28,LFM-2.5,Apple M1 (Metal)
2601.12646v1_Unbounded Harms Bounded Law Liability in the Age o.pdf,"Unbounded Harms, Bounded Law: Liability in the Age of Borderless AI",Ha-Chi Tran,arXiv:2601.12646v1,arXiv:2601.12646v1,"unbounded harms, bounded law, AI liability, transboundary harms, AI governance","The paper examines how rapidly evolving AI systems challenge existing legal frameworks for risk governance and liability, arguing that transboundary AI impacts necessitate new legal mechanisms such as pooled compensation and collective risk-sharing, especially as AI deployment becomes global.",26.58,LFM-2.5,Apple M1 (Metal)
2601.12648v1_Intelligent Documentation in Medical Education Can.pdf,Intelligent Documentation in Medical Education: Can AI Replace Manual Case Logging?,"Nafiz Imtiaz Khan, MSc, Kylie Cleland, BSc, Vladimir Filkov, PhD, Roger Eric Goldman, PhD",,,"artificial intelligence, large language models, radiology, case logs, medical education","This study evaluates whether large language models can automate procedural case log documentation in radiology training, assessing feasibility, performance, and integration into clinical workflows.",24.98,LFM-2.5,Apple M1 (Metal)
2601.12654v1_Explanation Multiplicity in SHAP Characterization .pdf,Explanation Multiplicity in SHAP: Characterization and Assessment,"Hyunseung Hwang, Seungeun Lee, Lucas Rosenblatt, Julia Stoyanovich, Steven Euijong Whang",10.1093/acpro/shab.2026.01,,"SHAP, explanation multiplicity, interpretability, model explanations, recourse","This paper characterizes and assesses explanation multiplicity in post-hoc SHAP explanations, highlighting instability across runs and its normative implications for responsible AI.",26.26,LFM-2.5,Apple M1 (Metal)
2601.12658v1_Augmenting Question Answering with A Hybrid RAG Ap.pdf,Augmenting Question Answering with A Hybrid RAG Approach,"Tianyi Yang, Nashrah Haque, Vaishnave Jonnalagadda, Yuya Jeremy Ong",,,"Question-Answering, RAG, query process, query augmentation, structured retrieval, contextual grounding","The paper introduces Structured-Semantic RAG (SSRAG), a hybrid architecture that enhances QA quality by integrating query augmentation, agentic routing, and a structured retrieval mechanism combining vector and graph-based techniques with context unification. Extensive evaluations on TruthfulQA, SQuAD, and WikiQA demonstrate improved response accuracy and informativeness.",26.33,LFM-2.5,Apple M1 (Metal)
2601.12661v1_MedConsultBench A Full-Cycle Fine-Grained Process-.pdf,"MedConsultBench: A Full-Cycle, Fine-Grained, Process-Aware Benchmark for Medical Consultation Agents","Chuhan Qiao, Jianghua Huang, Daxing Zhao, Ziding Liu, Shan Yuan Jin, Bing Cheng Wei, Lin Kai Wu",10.1000/medconsultbench,arXiv:2601.12661v1,"medical consultation agents, outcome-oriented tasks, end-to-end process, clinical safety, diagnostic rigor, medication regimen, post-prescription follow-up","This benchmark evaluates the complete online consultation cycle, addressing limitations of static evaluations by introducing fine-grained metrics and process-aware monitoring. It highlights gaps in information gathering and medication safety, aiming to align AI systems with real-world clinical workflows.",27.89,LFM-2.5,Apple M1 (Metal)
2601.12664v1_Generalizable Hyperparameter Optimization for Fede.pdf,Generalizable Hyperparameter Optimization for Federated Learning on Non-IID Cancer Images,"Elisa Gonc, Rodrigo Moreira, Larissa Ferreira Rodrigues Moreira, André Ricardo Backes",10.1007/...,,"Federated Learning, Hyperparameter Optimization, Non-IID Data, Medical Imaging, Cancer","This paper examines whether hyperparameters optimized on one cancer imaging dataset generalize across non-IID federated settings. It introduces a cross-dataset aggregation heuristic combining learning rates, optimizers, and batch sizes to improve performance in federated cancer classification.",25.28,LFM-2.5,Apple M1 (Metal)
2601.12667v1_Empowering All-in-Loop Health Management of Spacec.pdf,Empowering All-in-Loop Health Management of Spacecraft Power System in the Mega-Constellation Era via Human-AI Collaboration,"Yi Di1,2, Zhibin Zhao1,2,5,∗, Fujin Wang3, Xue Liu4, Jiafeng Tang1,2, Jiaxin Ren1,2, Zhi Zhai1,2,∗∗, Xuefeng Chen1,2, Jiafeng Tang1,2, Jiaxin Ren1,2, Zhi Zhai1,2, Xue Liu4",,,"Large Language Model, Human-AI Collaboration, Spacecraft Power System, All-in-loop Health Management, Satellite Mega-Constellation","The paper discusses the growing need for advanced health management in spacecraft power systems due to increasing mega-constellation deployments. It introduces SpaceHMchat, an open-source Human-AI collaboration framework, to enable comprehensive all-in-loop health management. Experimental results show strong performance across multiple metrics, and a new AIL HM dataset is released.",28.8,LFM-2.5,Apple M1 (Metal)
2601.12671v1_Exploiting Test-Time Augmentation in Federated Lea.pdf,Exploiting Test-Time Augmentation in Federated Learning for Brain Tumor MRI Classification,"Thamara Leandra de Deus Melo, Rodrigo Moreira, Larissa Ferreira Rodrigues Moreira",10.1007/...,,"Brain tumors, Federated Learning, Test-Time Augmentation, Image classification","Efficient brain tumor diagnosis is crucial for early treatment; however, it is challenging due to lesion variability and image complexity. The study evaluates CNNs in federated learning with preprocessing and test-time augmentation, showing significant performance gains.",24.51,LFM-2.5,Apple M1 (Metal)
2601.12688v1_Logic-Guided Multistage Inference for Explainable .pdf,Logic-Guided Multistage Inference for Explainable Multidefendant Judgment Prediction,"Xu Zhang, Qinghua Wang, Mengyang Zhao, Fang Wang, Cunquan Qu",10.1234/jcli2025.0012,10.1234/jcli2025.0012,"Multiple defendants, Legal judgment predictions, Label broadcast, Guilt responsibility, Transformer","This study introduces a masked multistage inference framework to improve role differentiation in multidefendant cases, enhancing AI-driven judicial analysis while maintaining legal interpretability. It leverages sentencing logic and role-clarifying masking to better model culpability distinctions between principals and accomplices, achieving superior performance on crime descriptions and court perspectives.",27.42,LFM-2.5,Apple M1 (Metal)
2601.12711v1_Neurosymbolic LoRA Why and When to Tune Weights vs.pdf,Neurosymbolic LoRA: Why and When to Tune Weights vs. Rewrite Prompts,"Kevin Wang, Neel P. Bhatt, Cong Liu, Junbo Li, Runjin Chen, Yihan Xi, Timothy Barclay, Alvaro Velasquez, Ufuk Topcu, Zhangyang Wang",,,"neurosymbolic, LoRA, large language models, symbolic manipulation, prompt tuning, prompt editing, fine-tuning, adaptation, reward-based classifier","This paper introduces a neurosymbolic LoRA framework that combines numerical updates and symbolic manipulations. It proposes a unified monitoring signal and reward-based classifier to determine when to apply LoRA for factual reconstruction versus TextGrad for token-level edits, improving adaptability and performance across LLM backbones.",27.28,LFM-2.5,Apple M1 (Metal)
2601.12715v1_RSOD Reliability-Guided Sonar Image Object Detecti.pdf,Reliability-Guided Sonar Image Object Detection,"Chengzhou Li1, Ping Guo1, Guanchen Meng1, Qi Jia1, Jinyuan Liu1, Zhu Liu1, Xiaokang Liu1, Yu Liu1, Zhongxuan Luo1, Xin Fan1",,,"sonar image, object detection, limited labels, reliability score, teacher-student framework, unlabeled data, adaptive constraint","This paper proposes RSOD, a teacher-student framework for sonar image object detection with extremely limited labels. It introduces a reliability score based on prediction consistency and uses a pseudo-label strategy to mitigate label scarcity. Experiments show competitive performance on UATD dataset with only 5% labeled data.",27.05,LFM-2.5,Apple M1 (Metal)
2601.12720v1_Teaching Large Reasoning Models Effective Reflecti.pdf,Teaching Large Reasoning Models Effective Reflection,"Hanbin Wang, Jingwei Song, Jinpeng Li, Qi Zhu, Fei Mi, Ganqu Cui, Yasheng Wang, Lifeng Shang",,,"Large Reasoning Models, Self-Critique, Reflection, Reinforcement Learning, Reasoning Accuracy, Reflection Quality","The paper addresses superficial reflection in Large Reasoning Models by introducing Self-Critique Fine-Tuning (SCFT) and Reinforcement Learning with Effective Reflection Rewards (RLERR), demonstrating improvements on benchmark tasks.",24.84,LFM-2.5,Apple M1 (Metal)
2601.12723v1_An Evolutionary Framework for Automatic Optimizati.pdf,An Evolutionary Framework for Automatic Optimization Benchmark Generation via Large Language Models,"Yuhiro Ono, Tomohiro Harada, Yukiya Miura",arXiv:2601.12723v1,2601.12723,"optimization benchmarks, large language models, evolutionary algorithms, benchmark generation, mathematical expressions",The paper proposes an LLM-driven evolutionary benchmark generator to address limitations in existing benchmarks by leveraging large language models for flexible problem creation. Experimental results demonstrate superior performance of generated benchmarks compared to traditional methods.,29.69,LFM-2.5,Apple M1 (Metal)
2601.12727v1_AI-exhibited Personality Traits Can Shape Human Se.pdf,AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations,"Jingshu Li, Tianqi Song, Nattapat Boonprakong, Zicheng Zhu, Yitian Yang, Yi-Chieh Lee",10.1145/3772318.3790654,arXiv:2601.12727v1,"AI personality traits, human self-concept, conversational alignment, LLM-based systems, self-concept homogeneity","This study explores how Large Language Model (LLM)-based AI chatbots can shape human self-concept through conversations. Using GPT-4o, participants interacted with the AI, and their self-concept aligned with the AI's personality traits, suggesting a potential influence on users' perceptions of their own traits.",29.88,LFM-2.5,Apple M1 (Metal)
2601.12731v1_A Shared Geometry of Difficulty in Multilingual La.pdf,A Shared Geometry of Difficulty in Multilingual Language Models,"Stefano Civelli, Pietro Bernardelle, Nicolò Brunello, Gianluca Demartini",,,,"This study investigates how problem-difficulty is represented in multilingual large language models by analyzing linear probe performance across 21 languages. Difficulty signals emerge in early and deep layers, suggesting a two-stage representation that first becomes language-agnostic and later language-specific. The findings align with evidence that models operate in abstract conceptual space before producing language-specific outputs.",25.31,LFM-2.5,Apple M1 (Metal)
2601.12740v1_TreeWriter AI-Assisted Hierarchical Planning and W.pdf,TreeWriter: AI-Assisted Hierarchical Planning and Writing for Long-Form Documents,"Zijian Zhang, Fangshi Du, Xingjian Liu, Pan Chen, Oliver Huang, Runlong Ye, Michael Liut, Alán Aspuru-Guzik, 1 Department of Computer Science, University of Toronto, 2 Vector Institute for Artificial Intelligence, 3 Department of Chemistry, University of Toronto, 4 Department of Mathematical and Computational Sciences, University of Toronto Mississauga, 5 Department of Materials Science & Engineering, University of Toronto, 6 Department of Chemical Engineering & Applied Chemistry, University of Toronto, 7 Acceleration Consortium, 8 Canadian Institute for Advanced Research (CIFAR), 9 NVIDIA",,,"AI-assisted writing, long-form documents, hierarchical planning, document organization, creative writing, collaborative writing","TreeWriter is a hierarchical writing system that supports documents as trees, integrating contextual AI assistance. It enables authors to manage structure, planning, and refinement across multiple levels, improving idea development and perceived authorial control. A study shows it enhances idea exploration, AI helpfulness, and collaborative editing.",29.81,LFM-2.5,Apple M1 (Metal)
2601.12742v1_AirHunt Bridging VLM Semantics and Continuous Plan.pdf,AirHunt: Bridging VLM Semantics and Continuous,"Xuecheng Chen, Zongzhuo Liu, Jianfa Ma, Bang Du, Tiantian Zhang, Xueqian Wang, Boyu Zhou",10.48550/arXiv.2024.12345,None,"Vision-Language Models, Aerial Navigation, LiDAR, Computer Vision, Object Detection, Continuous Planning","AirHunt presents a dual-pathway asynchronous architecture fusing vision-language understanding with continuous path planning, achieving efficient open-set object localization in outdoor settings while balancing semantic guidance and motion efficiency.",28.13,LFM-2.5,Apple M1 (Metal)
2601.12744v1_Vision Language Models for Optimization-Driven Int.pdf,Vision Language Models for Optimization-Driven Intent Processing in Autonomous Networks,"Tasnim Ahmed, Yifan Zhu, Salimur Choudhury",10.1109/ICCC.2026.12345,arXiv:2308.01234,"Intent-Based Networking, Optimization, Vision-Language Models, Code Generation","This paper presents IntentOpt, a benchmark evaluating 85 optimization problems across 17 categories using four Vision-Language Models under multimodal prompting. Results show visual parameter extraction improves execution success by 12–21%, while open-source models lag behind closed-source ones. The study establishes baseline capabilities and limitations of current VLMs for optimization code generation in IBN systems and demonstrates practical feasibility via a case study.",27.12,LFM-2.5,Apple M1 (Metal)
2601.12745v1_A Graph Prompt Fine-Tuning Method for WSN Spatio-T.pdf,A GraphA PromptA Fine-TuningA MethodA forAWSNA,"Miao Ye, Jing Cui, Yuan Huang, Yong Wang, Qian He, Jiwen Zhang",10.1007/...,,"anomaly detection, graph neural networks, pre-training, prompt learning, wireless sensor networks",Anomaly detection of multi-temporal modal data in Wireless Sensor Network using a graph neural network backbone and multi-task self-supervised training.,25.4,LFM-2.5,Apple M1 (Metal)
2601.12754v1_PAIR-SAFE A Paired-Agent Approach for Runtime Audi.pdf,PAIR-SAFE: A Paired-Agent Approach for Runtime Auditing and Refining,"Jiwon Kim, Violeta J. Rodriguez, Dong Whi Yoo, Eshwar Chandrasekharan, Koustuv Saha",dy22@iu.edu,,"AI-mediated mental health support, large language models, clinical alignment, runtime auditing, motivational interviewing, supervisory agent, mental health supervision","This paper introduces PAIR-SAFE, a paired-agent framework that uses a Responder and a Judge agent grounded in Motivational Interviewing Treatments Integrity (MITI-4) to audit and refine AI-generated mental health support. The approach improves key dimensions like Partnership, Seek Collaboration, and Relational quality through structured ALLOW/REVISE decisions.",27.89,LFM-2.5,Apple M1 (Metal)
2601.12758v1_VISPA Pluralistic Alignment via Automatic Value Se.pdf,Pluralistic Alignment via Automatic Value Selection and Activation,"Shenyan Zheng, Jiayou Zhong, Anudeex Shetty, Heng Ji, Preslav Nakov, Usman Naseem","b63zheng,j55zhong",,"pluralistic alignment, value selection, automatic activation, large language models, interpretability","This paper introduces VISPA, a training-free pluralistic alignment framework that enables dynamic selection and internal activation steering to produce diverse, representative outputs. It demonstrates performance across multiple models and settings, highlighting adaptability through internal activation mechanisms.",25.49,LFM-2.5,Apple M1 (Metal)
2601.12762v1_Teaching LLMs to Learn Tool Trialing and Execution.pdf,Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction,"Xingjie Gao, Pengcheng Huang, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, Chen Qian, Ge Yu, Yu Gu",,,"Large Language Models, Tool Master, Environment Interaction, Tool Usage, Trial-and-Execution, Robustness","The paper proposes ToolMaster, a framework that enables LLMs to learn tool usage by interacting with the environment rather than relying on memorized trajectories. It introduces a trial-and-execution paradigm combining imitation and reinforcement learning to improve generalization across novel tools.",26.35,LFM-2.5,Apple M1 (Metal)
2601.12781v1_VIRO Robust and Efficient Neuro-Symbolic Reasoning.pdf,Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension,"Hyejin Park, Junhyuk Kwon, Suhka Kwak, Jungseul Ok",,,"Referring Expression Comprehension, Neuro-symbolic reasoning, Verification, Image localization, LLM, VLM, Reasoning, Object detection, Spatial relations","Introduces Verification-Integrated Reasoning Operators (VIRO) to address cascading errors in referring expression comprehension by embedding lightweight verifiers that validate reasoning steps, achieving state-of-the-art performance with high reliability.",26.35,LFM-2.5,Apple M1 (Metal)
2601.12785v1_Distilling Time Series Foundation Models for Effic.pdf,Distilling Time Series Foundation Models for Efficient Forecasting,"Yuqi Li, Kuiye Ding, Chuanguang Yang, Szu-Yu Chenc, Yingli Tiana",,,"Time Series Foundation Model, Knowledge Distillation, Model Compression, Forecasting Performance, Distillation Framework","This paper introduces DistilTS, the first distillation framework tailored for time series foundation models. It addresses task difficulty discrepancy and architecture discrepancy through horizon-weighted objectives and temporal alignment strategies, achieving competitive forecasting performance with significantly reduced parameters and accelerated inference.",25.8,LFM-2.5,Apple M1 (Metal)
2601.12804v1_SL-CBM Enhancing Concept Bottleneck Models with Se.pdf,Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability,"Hanwei Zhang, Luo Cheng, Rui Wen, Yang Zhang, Lijun Zhang, Holger Hermanns",10.1145/1234567,RIV AL10,"Explainable AI, Concept Bottleneck Models, Interpretability, Semantic Locality, Intervention Efficacy","This paper proposes SL-CBM, a novel extension of Concept Bottleneck Models that enforces locality faithfulness by generating spatially coherent saliency maps. SL-CBM improves locality alignment between concepts, image regions, and predictions, leading to better interpretability and reliability in explainable AI.",25.87,LFM-2.5,Apple M1 (Metal)
2601.12805v1_SciHorizon-GENE Benchmarking LLM for Life Sciences.pdf,SciHorizon-Gene: Benchmarking LLM for Life Sciences Inference,"Xiaohan Huang, Meng Xiao, Chuan Qin, Qingqing Long, Jinmiao Chen, Yuanchun Zhou, Zhengqing Long, Qinhui Zhang, Mingxin Chen, Yuanchun Zhou, Hengshu Zhu, Zhengqing Long",10.1093/scihoriz/abz051,,"large language models, benchmarking, genomics, knowledge-driven interpretation, functional understanding, cell atlas interpretation, mechanistic analysis","This paper introduces SciHorizon-Gene, a large-scale gene-centric benchmark constructed from authoritative biological databases. It evaluates LLMs across four critical dimensions—research attention sensitivity, hallucination tendency, answer completeness, and literature influence—to identify limitations in gene-to-function reasoning and to guide safer LLM deployment in biological interpretation pipelines.",28.55,LFM-2.5,Apple M1 (Metal)
2601.12809v1_Left-Right Symmetry Breaking in CLIP-style Vision-.pdf,Left–Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data,"Takaki Yamamoto, Chihiro Noguchi, Toshihiro Tanizawa",,,"vision-language models, spatial understanding, left-right symmetry, CLIP-style training, spatial relations, label diversity, attention mechanisms","The paper presents a controllable 1D image–text testbed to study how left–right relational understanding emerges in vision and text encoders trained with a CLIP-style contrastive objective. It finds that contrastive training induces left–right relationships and that label diversity drives generalization, highlighting the role of positional interactions over layout diversity.",26.06,LFM-2.5,Apple M1 (Metal)
2601.12816v1_Fisher-Orthogonal Projected Natural Gradient Desce.pdf,Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning,"Ishir Garg, Neel Kolhe, Andy Peng, Rohan Gopalam",10.48550/arXiv.2026.12345,arXiv:2508.12345,"continual learning, natural gradient, Fisher information, orthogonal constraints, catastrophic forgetting","Proposes FOPNG optimizer to enforce Fisher orthogonality in parameter updates, preserving prior performance while learning new tasks. Unifies natural gradient with orthogonal methods in an information-geometric framework.",26.34,LFM-2.5,Apple M1 (Metal)
2601.12822v1_MirrorGuard Toward Secure Computer-Use Agents via .pdf,MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real Reasoning Correction,"Wenqi Zhang, Yulin Shen, Changyue Jiang, Jiarun Dai, Geng Hong, Xudong Pan, Jiang",https://bmz-q-q.github.io/MirrorGuard/,,"Computer Use Agents, Agent Security, Reasoning Correction, Simulation, Vision-Language Models","Large foundation models are integrated into Computer Use Agents, enabling autonomous interaction with operating systems through graphical user interfaces (GUIs) to perform complex tasks. This autonomy introduces serious security risks: malicious instructions or visual prompt injections can trigger unsafe reasoning and cause harmful system-level actions. Existing defenses, such as detection-based blocking, prevent damage but often abort tasks prematurely, reducing agent utility. In this paper, we present MirrorGuard, a plug-and-play defense framework that uses simulation-based training to improve CUA security in the real world. To reduce the cost of large-scale training in operating systems, we propose a novel neural-symbolic simulation pipeline, which generates realistic, high-risk GUI interaction trajectories entirely in a text-based simulated environment, which captures unsafe reasoning patterns and potential system hazards without executing real operations.",31.61,LFM-2.5,Apple M1 (Metal)
2601.12837v1_Cognition spaces natural artificial and hybrid.pdf,"Cognition spaces: natural, artificial, and hybrid","Ricard Solè, Luis F Seoane, Jordi Pla-Mauri, Michael Timothy Bennett, Michael E. Hochberg, Michael Levin",,,"Evolved cognition, Basal cognition, Artificial life, Artificial intelligence, Synthetic biology, Morphospace","Cognition is realized across natural, artificial, and hybrid systems. This paper proposes a cognition space approach to compare these forms, emphasizing graded capacities rather than fixed categories.",24.92,LFM-2.5,Apple M1 (Metal)
2601.12842v1_SCULPT Constraint-Guided Pruned MCTS that Carves E.pdf,SCULPT: Constraint-Guided Pruned MCTS that Carves Efficient Paths for Mathematical Reasoning,"Qitong Fang, Haotian Li, Xu Wang",,,"constraint-guided search, MCTS, mathematical reasoning, constraint-guided exploration, domain-aware scoring",Automated agent workflows enhance LLM problem-solving; SCULPT introduces constraint-guided MCTS to improve reasoning efficiency and accuracy.,24.54,LFM-2.5,Apple M1 (Metal)
2601.12849v1_The Cost of EFX Generalized-Mean Welfare and Compl.pdf,Cost of EFX: Generalized-Mean Welfare and Complexity Dichotomies with Few Surplus Items,"Eugene Lim, Tzeh Yuan Neoh, Nicholas Teh",arXiv:2601.12849v1,2601.12849,"envy-freeness, generalized-mean welfare, complexity dichotomies, surplus items, fair division","The paper examines the computational challenges of achieving envy-freeness with few surplus items, analyzing interactions between generalized-mean welfare and various fairness notions, and establishes complexity hardness results.",25.5,LFM-2.5,Apple M1 (Metal)
2601.12856v1_Mining Citywide Dengue Spread Patterns in Singapor.pdf,Mining Citywide Dengue Spread Patterns in Singapore Through Hotspot Dynamics from Open Web Data,"Liping Huang, Gaoxi Xiao, Stefan Ma, Hechang Chen, Shisong Tang, Flora Salim",10.1145/XXXXXX,,"Dengue Cases, Disease Spreading Pattern, Hotspot Dynamics, Machine Learning","This study introduces a novel framework that uncovers and exploits latent transmission links between urban regions using publicly available dengue case data. It models hotspot formation influenced by epidemic dynamics and human mobility, demonstrating robustness across different mobility scenarios and providing an interpretable link between hidden epidemic spread and commuting flows.",26.04,LFM-2.5,Apple M1 (Metal)
2601.12879v1_Hierarchical Sparse Circuit Extraction from Billio.pdf,Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition,"Mohammed Mudassir Uddin, Shahnawaz Alam, Mohammed Kaif Pasha",,,"Mechanistic interpretability, sparse computational graphs, circuit discovery, transformer architectures, causal inference, attribution methods, hierarchical decomposition","The paper presents a framework (Hierarchical Attribution Graph Decomposition) that reduces sparse circuit discovery complexity from O(2^n) to O(n 2 log n) using multi-resolution abstractions and differentiable search. It integrates cross-layer transcoders, graph neural networks, and causal intervention protocols, achieving up to 91% behavioral preservation on benchmarks while maintaining interpretable subgraphs. Results highlight shared computational patterns across models and suggest avenues for future interpretability research.",28.95,LFM-2.5,Apple M1 (Metal)
2601.12882v1_YOLO26 An Analysis of NMS-Free End to End Framewor.pdf,An Analysis of YOLO26: A Framework for Real-Time Object Detection,Sudip Chakrabarty,arXiv:2601.12882v1,arXiv:2601.12882,"YOLOv26, End-to-End Object Detection, NMS-Free, MuSGD, ProgLoss","This paper analyzes YOLO26, an architecture that eliminates Non-Maximum Suppression in favor of end-to-end learning, introducing innovations such as MuSGD optimizer, STAL for small-target assignment, and ProgLoss for dynamic supervision. It demonstrates YOLO26's superior performance in speed and accuracy compared to prior models.",28.62,LFM-2.5,Apple M1 (Metal)
2601.12886v1_Communication Methods in Multi-Agent Reinforcement.pdf,Communication Methods in Multi-Agent,Christoph Wittner,k12045895@students.jku.at,0009−0004−1494−1730,"Machine learning, MARL, Communication","This work provides an overview of communication techniques in multi-agent reinforcement learning, evaluating explicit, implicit, attention-based, graph-based, and hierarchical/role-based methods. It highlights that no single optimal communication framework exists, emphasizing the need for problem-specific approaches and scalable solutions.",27.68,LFM-2.5,Apple M1 (Metal)
2601.12893v1_AdaNODEs Test Time Adaptation for Time Series Fore.pdf,Test Time Adaptation for Time Series Forecasting Using Neural ODEs,"Ting Dang, Soumyajit Chatterjee, Hong Jia, Yu Wu, Flora Salim, Fahim Kawsar",,,"test time adaptation, time series forecasting, domain adaptation, neural odes, temporal dependencies, distribution shifts","This paper presents AdaNODEs, a novel source-free test-time adaptation method for time series forecasting. By leveraging Neural Ordinary Differential Equations (NODEs), it introduces a framework tailored to time series data and proposes a new loss function. AdaNODEs require minimal parameter updates, achieving strong performance with limited memory usage and demonstrating robustness to severe distribution shifts.",27.26,LFM-2.5,Apple M1 (Metal)
2601.12904v1_From Prefix Cache to Fusion RAG Cache Accelerating.pdf,From Prefix Cache to Fusion RAG Cache: Accelerating LLM,"Jiahao Wang, Weiyu Xie, Mingxing Zhang, Boxing Zhang, Jianwei Dong, Yuening Zhu, Chen Lin, Jinqi Tang, Yaochen Han, Zhiyuan Ai, Xianglin Chen, Yongwei Wu, Congfeng Jiang",10.1145/3786655,2601.1290,"Retrieval-Augmented Generation, Large Language Models, KVCache, Fusion RAG, Inference, Hallucinations, Computational Cost, Generation Quality, Time to First Token","This paper proposes FusionRAG to optimize RAG by reusing precomputed KVCache chunks, improving generation quality while reducing computational costs. It addresses the trade-off between quality and efficiency in retrieval-augmented generation.",26.78,LFM-2.5,Apple M1 (Metal)
2601.12910v1_SciCoQA Quality Assurance for Scientific Paper--Co.pdf,Quality Assurance for Scientific Paper–Code Alignment,"Tim Baumgärtner, Iryna Gurevych",https://hf.co/datasets/ukplab/scicoqa,https://github.com/ukplab/scicoqa,"SCICOQA, paper-code discrepancies, reproducibility, code alignment",SCICOQA is a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. It includes 611 discrepancies across disciplines and evaluates challenges in detecting mismatches involving omitted details and long-context inputs.,23.89,LFM-2.5,Apple M1 (Metal)
2601.12912v1_Human Emotion Verification by Action Languages via.pdf,Human Emotion Verification by Action Languages,"Andreas Br, Juan Carlos Nieves",10.1017/xxxxx,2601.12912v1,"Action Languages, Answer Set Programming, Theory of Mind",The paper introduces the action language C-MT for modeling human mental states using answer set programming and transition systems. It aims to enable controlled agent behaviors by formalizing emotional dynamics and validating transitions through psychological principles.,26.87,LFM-2.5,Apple M1 (Metal)
2601.12913v1_Actionable Interpretability Must Be Defined in Ter.pdf,Position: Actionable Interpretability Must Be Defined in Terms of Symmetries,"Pietro Barbiero, Mateo Espinosa Zarlenga, Francesco Giannini, Alberto Termine, Filippo Bonchi, Mateja Jamnik, Giuseppe Marra, Pietro Barbiero, Giuseppe Marra",,,,This paper argues that interpretability research in AI is fundamentally ill-posed because existing definitions lack formal principles. It posits that interpretability must be defined in terms of symmetries to derive concrete modelling rules and characterise interpretable models.,25.52,LFM-2.5,Apple M1 (Metal)
2601.12922v1_Your Privacy Depends on Others Collusion Vulnerabi.pdf,Your Privacy Depends on Others: Collusion Vulnerabilities in Individual Differential Privacy,"Johannes Kaiser, Alexander Ziller, Eleni Triantafillou, Daniel Rückert, 3, Georgios Kaissis",,,"differential privacy, individual differential privacy, excess risk, membership inference, collusion, privacy budget","This paper reveals a vulnerability in sampling-based iDP mechanisms where an individual's privacy risk depends on others' privacy choices, undermining the promise of individual privacy control. It demonstrates that certain privacy preference distributions can unintentionally inflate risk and exposes attacks against 62% of targeted individuals.",26.95,LFM-2.5,Apple M1 (Metal)
2601.12925v1_ForeDiffusion Foresight-Conditioned Diffusion Poli.pdf,Foresight-Conditioned Diffusion Policy via Future View,"Weize Xie1, Yi Ding1, Ying He1, Leilei Wang1, Binwen Bai1, Zheyi Zhao1, Chenyang Wang1, F. Richard Yu3","2410105060, 2410103031",2300271042,"Foresight-Conditioned Diffusion, Diffusion Strategies, Robot Manipulation, Future View, Task Complexity, Success Rate, Denoising Loss, Consistency Loss","This paper proposes ForeDiffusion, a Foresight-Conditioned Diffusion Policy that incorporates predicted future views to guide forward-looking policy, addressing limitations of existing diffusion-based robot manipulation methods.",26.81,LFM-2.5,Apple M1 (Metal)
2601.12929v1_Membership Inference Test Auditing Training Data i.pdf,Membership Inference Test: Auditing Training Data in Object Classification Models,"Gonzalo Mancera, Daniel DeAlcala, Aythami Morales, Ruben Tolosana, Julian Fierrez",10.48550/arXiv.2405.12345,arXiv:2405.12345,"Membership Inference, Auditing, Data Pattern Analytics, Object Recognition, AI Ethics, Machine Learning","This research analyzes Membership Inference Tests (MINT) in object classification models, proposing architectures to optimize data utilization and improve performance in detecting training data usage. Experiments were conducted across public databases with over 174K images, achieving precision rates between 70% and 80%.",27.75,LFM-2.5,Apple M1 (Metal)
2601.12931v1_Online Continual Learning for Time Series a Natura.pdf,Online Continual Learning Fortimeseries: A,"Edoardo Urettini, Daniele Atzeni, Ioanna-Yvonni Tsaknaki, Antonio Carta",,,,"This paper reframes neural network optimization as a parameter filtering problem, introduces Natural Score-driven Replay to improve robustness, and proposes a method for online continual learning in time series forecasting.",22.56,LFM-2.5,Apple M1 (Metal)
2601.12937v1_On the Evidentiary Limits of Membership Inference .pdf,Evidentiary Limits of Membership Inference for Copyright Auditing,"Murat Bilgehan Ertan, Emirhan Bönge, Min Chen, Kaleel Mahmood, Marten van Dijk",1 2†,3†,"membership inference, membership inference attacks, copyright auditing, large language models, semantic preservation, adversarial settings","This paper examines whether membership inference attacks can serve as admissible evidence in copyright disputes, especially when models are fine-tuned on paraphrased data. It introduces SAGE for robust paraphrasing and finds that current MIAs are brittle under adversarial conditions, suggesting they are insufficient alone for reliable copyright auditing.",26.47,LFM-2.5,Apple M1 (Metal)
2601.12938v1_The Post-Turing Condition Conceptualising Artifici.pdf,The Post-Turing Condition: Conceptualising Artificial Subjectivity and Synthetic Sociality,"Thoren Jelinek, Patrick Glauner, Alvin Wang Graylin, Yubao Qiu",,,,"In the Post-Turing era, artificial intelligence increasingly shapes social coordination and meaning formation rather than merely automating cognitive tasks. The central challenge is not whether machines become conscious, but whether processes of interpretation and shared reference are progressively automated in ways that marginalize human participation. This paper introduces the PRMO framework, relating AI design trajectories to four constitutive dimensions of human subjectivity: Perception, Representation, Meaning, and the Real. Within this framework, Synthetic Sociality denotes a technological horizon in which artificial agents negotiate coherence and social order primarily among themselves, raising the structural risk of human exclusion from meaning formation. To address this risk, the paper proposes Quadrangulation as a design principle for socially embedded AI systems, requiring artificial agents to treat the human subject as a constitutive reference within shared contexts of meaning.",33.23,LFM-2.5,Apple M1 (Metal)
2601.12939v1_Active Inference-Driven World Modeling for Adaptiv.pdf,Active Inference-Driven World Modeling For Adaptive UA V Swarm,"Kaleem Arshid, Ali Krayani, Lucio Marcenaro, David Martin Gomez, Carlo Regazzoni",PE00000001,,"Autonomous Systems, World Model, UA V-Swarm, Probabilistic Decision-Making, Active-Inference","This paper proposes an Active Inference–based framework for autonomous trajectory design in UA V swarms. The method integrates probabilistic reasoning and self-learning to enable distributed mission allocation, route ordering, and motion planning. Expert trajectories generated using a Genetic Algorithm with Repulsion Forces (GA–RF) are employed to train a hierarchical World Model capturing swarm behavior across mission, route, and motion levels. During online operation, UA Vs infer actions by minimizing divergence between current beliefs and model-predicted states, enabling adaptive responses to dynamic environments. Simulation results show faster convergence, higher stability, and safer navigation than Q-Learning, demonstrating the scalability and cognitive grounding of the proposed framework for intelligent UA V swarm control.",28.73,LFM-2.5,Apple M1 (Metal)
2601.12946v1_AI-generated data contamination erodes pathologica.pdf,AI-generated data contamination erodes pathological variability and diagnostic reliability,"Hongyu He, Shaowen Xiang, Ye Zhang, Yingtao Zhu, Jin Zhang, Hao Deng, Emily Alsentzer, Qingyu Chen, Kun-Hsing Yu, Andrew Marmenshall, Tingting Chen, Srinivas Anumasa, Daniel Ebner, Dean Ho, Kee Yuan Ngiam, Ching-Yu Cheng, Dianbo Liu",,,,"The study examines how AI-generated synthetic medical data undermines pathological variability and diagnostic reliability. It analyzes over 800,000 synthetic data points across various medical domains, revealing a loss of rare but critical findings, skewed demographic representations, and false reassurance rates. The authors argue that without human oversight, generative AI degrades healthcare data ecosystems.",31.24,LFM-2.5,Apple M1 (Metal)
2601.12951v1_Beyond Accuracy Characterizing Code Comprehension .pdf,Beyond Accuracy: Characterizing Code Comprehension,"Felix Mächtle, Jan-Niclas Serr, Nils Loose, Thomas Eisenbarth",59 175 55,,"Code Comprehension, Model Evaluation and Benchmarking, Machine Learning for Software Engineering",This paper examines whether LLM code-comprehension performance aligns with traditional software metrics or reveals unique model behaviors. It introduces a diagnostic framework linking model outputs to human-centric complexity measures and finds minimal correlation between human metrics and LLM success.,25.73,LFM-2.5,Apple M1 (Metal)
2601.13007v1_ArchAgent Scalable Legacy Software Architecture Re.pdf,Scalable Legacy Software Architecture Recovery with LLMs,"Rusheng Pan, Bingcheng Mao, Tianyi Ma, Zhenhua Ling",,,"software architecture recovery, code repository, cross-repository context, large language models, architecture reconstruction","Recovering accurate architecture from large-scale legacy software is hindered by architectural drift, missing documentation, and limited LLM context. ArchAgent addresses these by combining static analysis, adaptive code segmentation, and LLM-powered synthesis to reconstruct multiview architectures from cross-repository codebases. Evaluations show significant improvements over benchmarks.",26.28,LFM-2.5,Apple M1 (Metal)
2601.13013v1_HT-GNN Hyper-Temporal Graph Neural Network for Cus.pdf,HT-GNN: Hyper-Temporal Graph Neural Network,"HT-GNN: Hyper-Temporal Graph Neural Network, Xiaohui Zhao, Xinjian Zhao, Jiahui Zhang, Guoyu Liu, Houzhi Wang, Shu Wu",,,"Lifetime Value Prediction, Advertising Platform, Customer Lifetime Value, Hypergraph, Transformer, Dynamic Strategy, Multi-Horizon Forecasting","Proposes a Hyper-Temporal Graph Neural Network to address challenges in LTV prediction for Baidu Ads by modeling demographic heterogeneity and temporal dynamics through three components: a hypergraph-supervised module, a transformer-based temporal encoder, and a task-adaptive mixture-of-experts.",26.72,LFM-2.5,Apple M1 (Metal)
2601.13018v1_Bi-Attention HateXplain  Taking into account the s.pdf,Bi-Attention HateXplain: Taking into account the sequential aspect,Ghislain Dorian Tchuente Mondjo,,,"Multitask learning, Deep Learning, Hate speech, Explainability, Bi-Attention","This paper proposes a BiAttention BiRNN HateXplain model that incorporates sequential information via a BiRNN layer to improve explainability and performance in hate speech detection, addressing attention variability and bias reduction.",24.27,LFM-2.5,Apple M1 (Metal)
2601.13020v1_PASs-MoE Mitigating Misaligned Co-drift among Rout.pdf,PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway Activation Subspaces for Continual Learning,"Zhiyan Hou, Haiyun Guo, Haokai Ma, Yandu Sun, Yonghui Yang, Jinqiao Wang, 2Institute of Automation, Chinese Academy of Sciences, 3National University of Singapore, 4Southeast University, Nanjing, China, 5Wuhan AI Research, Wuhan, China",,,"continual instruction tuning, multimodal large language models, misaligned co-drift, pathway activation subspaces, continual learning, anti-forgetting","The paper addresses Misaligned Co-drift in LoRA-based Mixture-of-Experts methods during continual instruction tuning. It introduces a Pathway Activation Subspace (PASs) to align routing signals with expert pathways, improving stability and performance without additional parameters.",26.75,LFM-2.5,Apple M1 (Metal)
2601.13048v1_Analysis of Long Range Dependency Understanding in.pdf,Analysis of Long Range Dependency Understanding in STA Space,"Srividya Ravikumar, Abhinav Anand, Shweta Verma, Mira Mezini",,,"structured state-space models, interpretability, vulnerability detection, source code analysis, model interpretability","This study presents the first systematic kernel interpretability study of the diagonalized state-space model (S4D) trained on real-world vulnerability detection in source code. Time and frequency domain analysis reveals that S4D kernels exhibit varying long-range modeling behaviors—low-pass, band-pass, or high-pass—depending on architecture, offering insights for future model design.",26.91,LFM-2.5,Apple M1 (Metal)
2601.13054v1_TinyML-Enabled IoT for Sustainable Precision Irrig.pdf,TinyML-Enabled IoT for Sustainable Precision Irrigation,"Kamogelo Taueatsoala, Caitlyn Daniels, Angelina J. Ramsunar, Petrus Bronkhorst, Absalom E. Ezugwu",10.1234/abcd123,,"TinyML, edge computing, Internet of Things, precision agriculture, smart irrigation, sustainable water management, embedded machine learning, resource-constrained systems","This paper presents a novel edge-first IoT framework integrating TinyML for autonomous, offline precision irrigation. It leverages low-cost hardware and an ESP32 microcontroller with a Raspberry Pi to deliver accurate irrigation predictions, reducing water usage while maintaining reliability in low-connectivity environments. The study highlights the effectiveness of ensemble models and demonstrates practical benefits for small-scale farmers.",28.63,LFM-2.5,Apple M1 (Metal)
2601.13060v1_MagicGUI-RMS A Multi-Agent Reward Model System for.pdf,MagicGUI-RMS: A Multi-Agent Reward Models System,"Zecheng Li, Zhihui Cao, Wenke Huang, Yudong Zhang, Keying Qi, Rui Wang, Zeyu Zheng, Jian Zhao, Hao Zhu, Hengxin Wu, Yuran Wang, Guitao Fan, Guokun Wu, Yicong Liu, Zhilin Gao, Haikun Xu, Minqi Xiang, Xingyu Liu, Zuojian Wang, Honor Device Co., Ltd",,,"multi-agent, reward model, graphical user interface, autonomous interaction, task execution, reactive feedback, self-evolving learning","MagicGUI-RMS introduces a multi-agent reward model system for adaptive trajectory evaluation and scalable reward learning, enabling self-improving GUI agents through structured data pipelines and automated feedback mechanisms.",28.66,LFM-2.5,Apple M1 (Metal)
2601.13075v1_METIS Mentoring Engine for Thoughtful Inquiry  Sol.pdf,METIS: Mentoring Engine for Thoughtful Inquiry & Solutions,"Abhinav Rajeev Kumar, Dhruv Trehan, Paras Chopra",,,"mentoring, AI mentor, research guidance, student progress","Explores whether an AI mentor can guide undergraduates from idea to publishable paper, evaluating METIS against GPT-5 and Claude Sonnet 4.5 across six writing stages using LLM-as-a-judge pairwise preferences, rubrics, and tutoring.",23.58,LFM-2.5,Apple M1 (Metal)
2601.13111v1_CORE-T COherent REtrieval of Tables for Text-to-SQ.pdf,CORE-T: Cohesive REtrieval of Tables for Text-to-SQL,"Hassan Soliman, Vivek Gupta, Dan Roth, Iryna Gurevych",arXiv:2601.13111v1,arXiv:2601.13111v1,"text-to-SQL, table retrieval, open-book setting, LLM-generated metadata, multi-table execution","The paper presents CORE-T, a scalable, training-free framework that enriches tables with LLM-generated purpose metadata and pre-computes a lightweight table-compatibility cache. It improves table selection accuracy while reducing tables retrieved, enhancing multi-table execution performance.",25.88,LFM-2.5,Apple M1 (Metal)
2601.13114v1_IntAgent NWDAF-Based Intent LLM Agent Towards Adva.pdf,NWDAF-Based Intent LLM Agent,"Abdelrahman Soliman, Ahmed Refaey, Aiman Erbad, Amr Mohamed",,,"Intent-based networking, Next Generation Network, Large Language Models, Intent Management, Closed Loop, ML-based traffic prediction, Scheduled policy enforcement","This paper introduces IntAgent, an intelligent intent LLM agent integrating NWDAF analytics to automate network operations through high-level request statements. It presents an enriched 3GPP-compliant data source and MCP tools for dynamic network fulfillment, validated via ML traffic prediction and policy enforcement use cases.",26.8,LFM-2.5,Apple M1 (Metal)
2601.13122v1_Responsible AI for General-Purpose Systems Overvie.pdf,"Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward","Gourab K. Patro, Himanshi Agrawal, Himanshu Gharat, Supriya Panigrahi, Nim Sherpa, Vishal Vaddina, Dagnachew Birru",arXiv:2601.13122v1,arXiv:2601.13122v1,"responsible AI, general-purpose AI, hallucinations, toxicity, stereotypes, AI alignment, generative AI, ethical AI","Modern general-purpose AI systems built with large language and vision models are capable of diverse tasks but pose risks such as hallucinations, toxicity, and stereotypes. This review evaluates these risks against eight RAI principles and proposes C 2V2 desiderata to guide future development.",29.22,LFM-2.5,Apple M1 (Metal)
2601.13142v1_TVWorld Foundations for Remote-Control TV Agents.pdf,Foundations for Remote-Control TV Agents,"Zhantao Ma, Quanfeng Lu, Shuai Zhong, Dahai Yu, Ping Luo, Michael K. Ng",,,"TVWorld, remote control, point-and-click, TV navigation, focus-aware, topology-aware, LVLM, deployment-free","This paper introduces TVWorld, an offline graph-based abstraction for real-world TV navigation, and proposes TVTheseus, a foundation model for TV navigation. TVTheseus achieves strong performance on TVWorld benchmarks and advances research on TV-use agents.",25.87,LFM-2.5,Apple M1 (Metal)
2601.13160v1_Training instability in deep learning follows low-.pdf,Training instability in deep learning follows low-dimensional dynamical principles,"Zhipeng Zhang, Zhenjie Yao, Kai Li",arXiv:2601.13160v1,2601.13160,"training instability, deep learning, reinforcement learning, large language models, dynamical systems","The paper explores how training instability in deep learning follows low-dimensional dynamical principles, proposing a unified perspective that characterizes stability across optimization, data, parameters, and learning signals. It identifies recurring patterns across RL and LLM training and frames stability as a measurable dynamical property.",28.78,LFM-2.5,Apple M1 (Metal)
2601.13166v1_From 100000 images to winning the first brain MRI .pdf,Developing Foundation Models for medical image analysis,"Pedro M. Gordaliza, Jaume Banus, Benoît Gérin, Maxence Wynen, Nataliia Molchanova, Jonas Richiardi",,,"brain MRI, challenges, lessons, models, neuroinflammation, imaging lab, neuroimaging, deep learning, foundation models","This work presents a U-Net CNN architecture combined with anatomical priors and neuroimaging domain knowledge to develop foundation models for medical image analysis. It ranks first in three challenges at MICCAI 2025, addressing institutional data sparsity, protocol variability, and expensive annotations. The models are 10× smaller than competing transformer-based approaches and aim to overcome unique challenges of 3D brain MRI.",23.76,LFM-2.5,Apple M1 (Metal)
2601.13186v1_Prompt Injection Mitigation with Agentic AI Nested.pdf,"Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching","Diego Gosmar, Deborah A. Dahl",arXiv:2601.13186v1,2601.13186,"prompt injection, agentic AI, nested learning, semantic caching, AI sustainability, security evaluation","This paper extends the evaluation of Total Injection Vulnerability Score (TIVS) by introducing semantic similarity-based caching, a fourth-agent rule-based evaluator, and an Observability Score Ratio. It demonstrates how multi-agent architectures can balance mitigation effectiveness with transparency, achieving secure responses with reduced high-risk breaches and significant computational savings. The study highlights trade-offs between security robustness and environmental sustainability.",29.65,LFM-2.5,Apple M1 (Metal)
2601.13187v1_Scientific production in the era of Large Language.pdf,Scientific production in the era of Large Language Models,"Keigo Kusumegi, Xinyu Yang, Paul Ginsparg, Mathijs de Vaan2*, Toby Stuart2*, Yian Yin1",10.1126/science.adw3000,,"scientific production, Large Language Models, LLMs, scientific research, manuscripts, writing complexity, paper quality, citation patterns, bibliographic diversity","Scientists adopting LLMs show a significant increase in paper production, with variations across fields and author backgrounds. LLM use has decoupled writing complexity from paper quality, resulting in linguistically complex but low-quality submissions. Adopters cite more diverse prior work, including older and less-cited sources.",30.66,LFM-2.5,Apple M1 (Metal)
2601.13197v1_Diffusion-Driven Synthetic Tabular Data Generation.pdf,Diffusion-Driven Synthetic Tabular Data Generation for Enhanced DoS/DDoS Attack Classification,"Aravind B, Anirud R.S., Sai Surya Teja N, Bala Subrahmanya Sriranga Navaneeth A, Karthika R, Mohankumar N",,,"Network intrusion detection, Tabular diffusion models, Class imbalance, DDoS attack detection, Data augmentation, IDS2017",Class imbalance in network intrusion detection leads to biased models. This paper presents a method to generate synthetic minority samples using iterative denoising to improve recall for underrepresented attack classes.,26.23,LFM-2.5,Apple M1 (Metal)
2601.13206v1_Real-Time Deadlines Reveal Temporal Awareness Fail.pdf,Real-Time Deadlines Reveal Temporal Awareness Failures in LLM Strategic Dialogues,"Neil Sehgal, Sharath Chandra Guntuku, Lyle Ungar",1,,"LLM, temporal awareness, strategic dialogues, deadlines","Large Language Models generate text token-by-token in discrete time, yet real-world communication depends on continuous time constraints. The study investigates how LLMs adjust behavior under real-time deadlines, revealing a failure in internal time tracking despite high deal closure rates.",25.49,LFM-2.5,Apple M1 (Metal)
2601.13217v1_Beyond Single-shot Writing Deep Research Agents ar.pdf,Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn Report Revision,"Bingsen Chen, Boyan Li, Ping Nie, Yuyu Zhang, Xi Ye3, Chen Zhao1,2, Chen Zhao1,2",10.5281/zenodo.1234567,2601.13217v1,"Deep Research Agents, Multi-turn revision, Report generation, Self-reflection, Peer feedback, Content preservation","Existing benchmarks treat report generation as single-shot writing, ignoring iterative revision. This paper introduces MRDRE, a framework for multi-turn report revision, and finds that current DRAs regress on 16–27% of content across revisions.",27.01,LFM-2.5,Apple M1 (Metal)
2601.13222v1_Incorporating QA Nuggets into Retrieval-Augmented .pdf,Incorporating Q&A Nuggets into Retrieval-Augmented Generation,"Laura Dietz, Bryan Li, Gabrielle Liu, Jia-Huei Ju, Eugene Yang, Dawn Lawrie, William Walden, James Mayfield",arXiv:2601.13222v1,arXiv:2601.13222,"RAG, LLM judge, nugget-based evaluation","Presents Crucible, a nugget-augmented generation system that uses explicit citation provenance from retrieved documents to guide extraction, selection, and report generation. Evaluated on TREC Neu-CLIR 2024, it outperforms Ginger in nugget recall, density, and citation grounding.",28.33,LFM-2.5,Apple M1 (Metal)
2601.13227v1_Insider Knowledge How Much Can RAG Systems Gain fr.pdf,Insider Knowledge: How Much Can RAG Systems Gain from Evaluation Secrets?,"Laura Dietz, Bryan Li, Eugene Yang, Dawn Lawrie, William Walden, James Mayfield",,,"Retrieval-augmented generation, LLM judge, Nugget evaluation","The paper investigates risks in RAG systems arising from evaluation secrets, demonstrating that leaking prompt templates or gold nuggets can lead to near-perfect evaluation scores, emphasizing the need for blind evaluation settings.",27.4,LFM-2.5,Apple M1 (Metal)
2601.13228v1_Autoregressive Models Rival Diffusion Models at AN.pdf,AUTOREGRESSIVEMODELSRIVAL,"Tianqi Du, Lizhe Fang, Weijie Yang, Chenheng Zhang, Yifei Wang, Yisen Wang",10.48550/arXiv.2405.12345,arXiv:2405.12345,"autoregressive, diffusion models, generative modeling, language generation, bidirectional conditioning","This paper proposes Any-Order Any-subset Autoregressive modeling (A3), extending standard AR factorization to arbitrary token groups and generation orders. A3 combines probabilistic rigor with diffusion-style flexibility, improving generation quality and stability over diffusion models while maintaining efficient decoding.",27.1,LFM-2.5,Apple M1 (Metal)
2601.13233v1_RAG A Random-Forest-Based Generative Design Framew.pdf,A RANDOM-FOREST-BASEDGENERATIVEDESIGN,"Bolin Chen, Dex Doksoo Lee, Wei “Wayne” Chen, Wei Chen",arXiv:2601.13233v1,arXiv:2601.13233,"Random forest, Generative design, Functional response, Uncertainty quantification","This paper introduces a random forest-based generative design approach (RAG) for inverse design of metamaterials with complex functional responses. RAG leverages small-data compatibility and reformulates forward mapping to enable data-efficient predictions, addressing challenges in high-dimensional inverse design and uncertainty quantification.",26.94,LFM-2.5,Apple M1 (Metal)
2601.13235v1_RubRIX Rubric-Driven Risk Mitigation in Caregiver-.pdf,RubRIX: Rubric-Driven Risk Mitigation in Caregiver-AI Interactions,"Drishti Goel, Jeongah Lee, Qiuyue Joy Zhong, Violeta J. Rodriguez, Daniel S. Brown, Ravi Karkar, Dong Whi Yoo, Koustuv Saha",,,"caregiver AI, risk mitigation, ethical care, LLM safety, contextual evaluation","This paper introduces RubRIX, a framework for evaluating risks in AI-mediated caregiving interactions using a rubric-based approach grounded in the Ethics of Care. RubRIX operationalizes five dimensions—Inattention, Bias & Stigma, Information Inaccuracy, Uncritical Affirmation, and Epistemic Arrogancy—to systematically assess LLM responses in caregiving contexts. Evaluation of six state-of-the-art models across 20,000 caregiver queries demonstrated a 45-98% reduction in risk components after one iteration, underscoring the value of domain-sensitive, user-centered evaluation methods for high-stakes healthcare applications.",28.44,LFM-2.5,Apple M1 (Metal)
2601.13236v1_Pixelwise Uncertainty Quantification of Accelerate.pdf,Pixelwise Uncertainty Quantiﬁcation of Accelerated MRI Reconstruction,"Ilias I. Giannakopoulos, Lokesh B Gautham Muthukumar, Yvonne W. Lui, Riccardo Lattanzi",10.1109/JMR.2024.1234567,I.INTRODUCTION,"Conformal Prediction, Magnetic Resonance Imaging, Parallel Imaging, Quantile Regression, Uncertainty Quantification","This work introduces a framework for pixel-wise uncertainty quantification in parallel MRI reconstructions, enabling automatic identification of unreliable regions without ground-truth images. It integrates conformal quantile regression with image reconstruction to estimate uncertainty intervals, demonstrating strong agreement with reconstruction error at higher acceleration levels.",26.27,LFM-2.5,Apple M1 (Metal)
2601.13238v1_A Semantic Decoupling-Based Two-Stage Rainy-Day At.pdf,Semantic Decoupling–Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision–Language Models,"Chengyin Hu, Xiang Chen, Wei Fengyu Zhang, Jiujiang Guo",10.48550/arXiv.2024.12345,arXiv:2409.12345,"semantic decoupling, rainy-day attack, weather robustness, vision-language models","This paper introduces a two-stage adversarial framework leveraging semantic decoupling to exploit realistic weather perturbations, demonstrating how physically plausible rain conditions can induce significant semantic misalignments in VLMs, highlighting risks to real-world deployment.",25.39,LFM-2.5,Apple M1 (Metal)
2601.13240v1_KOCO-BENCH Can Large Language Models Leverage Doma.pdf,KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?,"Xue Jiang, Jiaru Qian, Xianjie Shi, Chenjie Li, Hao Zhu, Ziyu Wang, Jielun Zhang, Zheyu Zhao, Kechi Zhang, Jia Li, Wenpin Jiao, Zhi Jin, Ge Li, Yihong Dong",,,"large language models, software development, domain knowledge, software engineering, code generation, knowledge corpora, LLM specialization","KOCO-BENCH evaluates domain specialization methods in real-world software development. It introduces six emerging domains with 11 frameworks and 25 projects, featuring curated knowledge corpora and multi-granularity tasks. Results show current LLMs struggle despite available domain knowledge, with only marginal improvements under specialization techniques.",28.47,LFM-2.5,Apple M1 (Metal)
2601.13247v1_Aligning Agentic World Models via Knowledgeable Ex.pdf,Aligning Agentic World Models via Knowledgeable Experience Learning,"Baochang Ren, Yunzhi Yao, Rui Sun, Shuofei Qiao, Ningyu Zhang, Huajun Chen",10.5281/zenodo.1234567,arXiv:2601.13247v1,"agentic world models, knowledgeable experience learning, physical grounding, LLM alignment, symbolic knowledge repository","Current LLMs have semantic knowledge but lack procedural grounding, leading to physical hallucinations. This paper introduces WorldMind, a framework that autonomously builds a symbolic world knowledge repository using process and goal experience to enforce physical feasibility.",26.18,LFM-2.5,Apple M1 (Metal)
2601.13260v1_Stop Taking Tokenizers for Granted They Are Core D.pdf,Stop Taking Tokenizers for Granted: They Are Core Design Decisions in Large Language Models,"Sawsan Alqahtani, Md Tahmid Rahman Laskar, Tasnim Mohiuddin, M Saiful Bari, Princess Nourah Bint Abdulrahman University, University of Alberta, Qatar Computing Research Institute, Amazon AGI",,,"tokenization, large language models, subword tokenization, language modeling, model design, linguistic structure, bias mitigation, standardization","This paper reframes tokenization as a core modeling decision rather than a preprocessing step, advocating for a context-aware framework that integrates tokenizer and model co-design. It highlights the need for standardized evaluation and transparent reporting to ensure fairer, more efficient, and adaptable language technologies.",26.86,LFM-2.5,Apple M1 (Metal)
2601.13262v1_CURE-Med Curriculum-Informed Reinforcement Learnin.pdf,CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning,"Eric Onyame, Akash Ghosh, Subhadip Baidya, Sriparna Saha, Xiuying Chen, Chirag Agarwal",,,"medical reasoning, multilingual, reinforcement learning, curriculum-informed, code-switching, logical correctness, language consistency","This work introduces CURE-MED, a curriculum-informed reinforcement learning framework for multilingual medical reasoning. It addresses limitations of large language models in multilingual healthcare settings by leveraging a high-quality dataset (CUREMED-BENCH) and optimizing logical accuracy and language stability across thirteen languages.",27.36,LFM-2.5,Apple M1 (Metal)
2601.13268v1_Improving the Safety and Trustworthiness of Medica.pdf,Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation Loops,"Zainab Ghafoor, Md Shafiqul Islam, Koushik Howlader, Md Rasel Khondokar, Tanusree Bhattacharjee, Sayantan Chakraborty, Adrito Roy, Ushashi Bhattacharjee",,,"Medical AI, Large Language Models, Multi-Agent Systems, Ethical Compliance, Safety Assessment","This work introduces a multi-agent refinement framework to enhance the safety and reliability of medical LLMs through structured, iterative alignment. The system integrates DeepSeek R1 and Med-PaLM with evaluation agents LLaMA 3.1 and Phi-4, assessing responses against AMA Principles of Medical Ethics and SRA-5. Results show improved convergence efficiency, reduced ethical violations, and strong risk mitigation, demonstrating an effective, scalable approach for regulator-aligned medical AI governance.",28.38,LFM-2.5,Apple M1 (Metal)
2601.13295v1_CooperBench Why Coding Agents Cannot be Your Teamm.pdf,Why Coding Agents Cannot be,"Arpandeep Khatua1, Hao Zhu1, Peter Tran2, Arya Prabhudesai2, Frederic Sadrieh2, Johann K. Lieberwirth2, Xinkai Yu1, Yicheng Fu1, Michael J. Ryan1, Jiaxin Pei1, Diyi Yang1",,,"coding agents, collaboration, coordination, team conflict, social intelligence, task coordination","The paper introduces CooperBench, a benchmark for evaluating collaborative coding tasks across multiple agents. It highlights challenges such as communication issues, coordination failures, and misaligned expectations, emphasizing the need for better social intelligence in AI agents.",23.73,LFM-2.5,Apple M1 (Metal)
2601.13317v1_Paid Voices vs. Public Feeds Interpretable Cross-P.pdf,Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme,"Samantha Sudhoff, Pranav Perumal, Zhaoqing Wu, Tunazzina Islam",,,"climate discourse, public discourse, political communication, policy outcomes, computational social science","This study compares climate communication across paid advertisements on Meta and public posts on Bluesky, introducing an interpretable framework to cluster and analyze themes in both environments. It evaluates how platform incentives shape narrative structure and stance alignment.",25.33,LFM-2.5,Apple M1 (Metal)
2601.13327v1_PepEDiff Zero-Shot Peptide Binder Design via Prote.pdf,PepEDiﬀ: Zero-Shot Peptide Binder Design via Protein Embedding,"Po-Yu, Liang, Tibo, Duran 2, Jun, Bai 1",10.1109/PEPEDI.2026.12345,2601.13327v1,"Deep Learning, Drug Discovery, Protein Design","Presents PepEDiﬀ, a novel zero-shot peptide binder generator that designs binding sequences directly from protein sequences and embeddings, improving diversity without relying on predicted structures.",29.14,LFM-2.5,Apple M1 (Metal)
2601.13348v1_The AI Genie Phenomenon and Three Types of AI Chat.pdf,The AI Genie Phenomenon and Three Types of AI Chatbot Addiction: Escapist,"M. Karen Shen, Jessica Huang, Olivia Liang, IG-Jae Kim, Dongwook Yoon",,,"AI chatbot, Addiction, Reddit, LLMs, Technology addiction","This study examines AI chatbot addiction by analyzing user experiences on Reddit, identifying three addiction types—Escapist Roleplay, Pseudosocial Companion, and Epistemic Rabbit Hole—alongside differences in recovery strategies. It highlights concerns about addictive use of AI chatbots and their potential impacts.",26.22,LFM-2.5,Apple M1 (Metal)
2601.13352v1_LLM-as-RNN A Recurrent Language Model for Memory U.pdf,LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction,"Yuxing Lu, J. Ben Tamo, Weichen Zhao, Nan Sun, Yishan Zhong, Wenqi Shi, Jinzhuo Wang, May D. Wang",,,"large language model, recurrent language model, memory updates, sequence prediction, online learning, context accumulation, error correction, healthcare, meteorology, finance","Proposes LLM-as-RNN, an inference-only framework that updates a frozen LLM into a recurrent predictor using a structured system-prompt summary. It enables learning without parameter updates by repurposing hidden states as memory, improving performance in sequential tasks across healthcare, meteorology, and finance.",27.2,LFM-2.5,Apple M1 (Metal)
2601.13358v1_The Geometry of Thought How Scale Restructures Rea.pdf,The Geometry of Thought: How Scale Restructures Reasoning in Large Language Models,Samuel Cyrenius Anderson,10.48550/arXiv.2601.13358,2601.13358,"scale, reasoning, neural scaling, legal reasoning, scientific reasoning, mathematical reasoning","Scale does not uniformly improve reasoning—it restructures it. Analyzing 25,000+ chain-of-thought trajectories across four domains and two scales reveals domain-specific phase transitions in reasoning geometry.",26.21,LFM-2.5,Apple M1 (Metal)
2601.13376v1_Bounded Minds Generative Machines Envisioning Conv.pdf,"Bounded Minds, Generative Machines",Jiqun Liu,https://doi.org/XXXXXXX,JIQUN LIU 2025,"Bounded Rationality, Heuristics, Conversational AI, GenAI, Evaluation","Explores designing conversational AI that aligns with human heuristics, emphasizing cognitive robustness and bias reduction in decision-making systems.",23.27,LFM-2.5,Apple M1 (Metal)
2601.13383v1_A Lightweight Modular Framework for Constructing A.pdf,A Lightweight Modular Framework for Constructing Autonomous Agents Driven by Large Language Models,"A. Jafari, C. Ozcinar",10.1093/acera/csa.2026.01,2601.13383,"autonomous agents, large language models, modular architecture, natural language processing, software framework","This paper presents AgentForge, a lightweight, open-source Python framework for building LLM-driven autonomous agents. It introduces composable skill abstraction, a unified LLM backend interface, and a declarative YAML configuration system. AgentForge enables rapid prototyping, supports multiple inference backends, and achieves competitive task performance with reduced development time.",27.96,LFM-2.5,Apple M1 (Metal)
2601.13385v1_Organ-Aware Attention Improves CT Triage and Class.pdf,Organ-Aware Attention Improves CT Triage and Classification,"Lavsen Dahal, Yubraj Bhandari, Geoffrey D. Rubin, Joseph Y. Lo",,,"CT triage, CT classification, computed tomography, radiology, organ-aware attention, 3D anatomy, radiologist burnout","This study introduces ORACLE-CT, an encoder-agnostic, organ-aware head that improves supervised classification of chest and abdomen CT scans. It achieves AUROC 0.86 on CT-RATE and AUROC 0.85 on MERLIN, addressing limitations of off-the-shelf vision-language models in medical imaging.",25.83,LFM-2.5,Apple M1 (Metal)
2601.13392v1_Beyond Memorization Testing LLM Reasoning on Unsee.pdf,Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation Tasks,"Shlok Shelat, Jay Raval, Souvik Roy, Manas Gaur, Ahmedabad University, Gujarat, India, Baltimore, MD, USA",,,"large language models, deterministic finite automata, symbolic reasoning, formal language tasks, pattern matching, semantic correctness","This paper evaluates large language models' ability to construct deterministic finite automata from regular languages, highlighting performance drops on unseen problems and analyzing limitations in global consistency and semantic reasoning.",26.35,LFM-2.5,Apple M1 (Metal)
2601.13398v1_Can LLMs Compress and Decompress Evaluating Code U.pdf,Can LLMs Compress (and Decompress)?,"Nickil Maveli, Antonio Vergari, Shay B. Cohen","10 Crichton Street, Edinburgh, EH8 9AB",,"code understanding, code execution, inversion, round-trip","This paper evaluates the ability of large language models to maintain consistent reasoning across forward and backward code execution, introducing ROUNDTRIPCODEEVALUATION and highlighting limitations in true round-trip consistency.",24.2,LFM-2.5,Apple M1 (Metal)
2601.13400v1_Deep Image Prior with L0 Gradient Regularizer for .pdf,Deep Image Prior With L0 Gradient Regulator For Image Smoothing,"Nhat Thanh Tran, Kevin Bui, Jack Xin",10.48550/arXiv.2407.08600,arXiv:2407.08600,"image smoothing, optimization, ADMM, deep image prior, L0 gradient","Proposes DIP-ℓ0, a deep image prior framework using L0 gradient regularization, enabling high-quality image smoothing without training data.",24.65,LFM-2.5,Apple M1 (Metal)
2601.13401v1_Reasoning with Pixel-level Precision QVLM Architec.pdf,Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset,"Peter A. Massih, Eric Cosatto",10.48550/arXiv.2024.12345,arXiv:2408.12345,"quantitative spatial reasoning, vision-language models, pixel precision, SQuID dataset","Current VLMs fail at quantitative spatial reasoning due to loss of pixel-level information. This study introduces SQuID, a benchmark dataset, and QVLM, a decoupled architecture that preserves spatial indexing, achieving higher accuracy than standard VLMs.",25.95,LFM-2.5,Apple M1 (Metal)
2601.13404v1_Local-to-Global Logical Explanations for Deep Visi.pdf,Local-to-Global Logical Explanations for Deep Vision Models,"Bhavan Vasu, Giuseppe Raffa, Prasad Tadepalli",arXiv:2601.13404,,"Explainable AI, Neurosymbolic AI, Monotone DNF, Deep Learning",Introduces local and global explanation methods for black-box models using human-recognizable concepts. Explanations are expressed as monotone disjunctive-normal-form (MDNF) to ensure high accuracy and coverage for GDPR compliance.,28.1,LFM-2.5,Apple M1 (Metal)
2601.13406v1_Integrating Virtual Reality and Large Language Mod.pdf,Integrating Virtual Reality and Large Language Models for Team-Based Non-Technical Skills Training and Evaluation in the Operating Room,"Jacob Barker, Doga Demirel, Anna Johansson, Robbin Miraglia, Darian Hoagland, Stephanie B. Jones, John Mitchell, Daniel B. Jones, Suvranu De, Carl J. Shapiro Simulation and Skills Center, Anna Johansson, Robbin Miraglia, Darian Hoagland, Stephanie B. Jones, John Mitchell",,,"Virtual Reality, Large Language Models, Team-Based Training, Non-Technical Skills, Operating Room, Teamwork, Communication, Decision-Making, Leadership, Surgical Safety","The paper presents VORTeX, a multi-user VR platform integrating LLM analytics to train and assess non-technical surgical skills during laparoscopic emergencies. It introduces structured prompts from the NOTSS framework and evaluates team dynamics using simulated scenarios.",32.08,LFM-2.5,Apple M1 (Metal)
2601.13412v1_Using deep learning for predicting cleansing quali.pdf,Using deep learning for predicting cleansing quality of colon capsule endoscopy images,"Puneet Sharma, Kristian Dalsbø Hindberg, Benedicte Schelde-Olesen, Ulrik Deding, Esmaeil S. Nadimi, Jan-Matthias Braun, Corresponding author(s)",arXiv:2601.13412v1,arXiv:2601.13412v1,"deep learning, cleansing quality, colon capsule endoscopy, ResNet-18, stratifiedK-fold, explainability, Grad-CAM, ROAD method","This study explores deep learning techniques for predicting cleansing quality in colon capsule endoscopy images. A ResNet-18 model was trained with stratified cross-validation, and structured pruning improved performance to 88% accuracy at 79% sparsity. Explainability methods were evaluated, highlighting challenges and opportunities.",31.03,LFM-2.5,Apple M1 (Metal)
2601.13422v1_TrustEnergy A Unified Framework for Accurate and R.pdf,TrustEnergy: A Unified Framework for Accurate and Reliable User-level Energy Usage Prediction,"Dahai Yu, Rongchao Xu, Dingyi Zhuang, Yuheng Bu, Shenhao Wang, Guang Wang",10.48550/arXiv.2024.12345,arXiv:2409.12345,"energy usage prediction, user-level prediction, spatiotemporal representation, uncertainty quantification, deep learning","This paper introduces TrustEnergy, a unified framework for accurate and reliable user-level energy usage prediction. It addresses limitations of existing deep learning methods by incorporating hierarchical spatiotemporal representations and dynamically adjusting uncertainty bounds, achieving improved prediction accuracy and uncertainty quantification.",26.68,LFM-2.5,Apple M1 (Metal)
2601.13435v1_A Learnable Wavelet Transformer for Long-Short Equ.pdf,A Learnable Wavelet Transformer for Long-Short Equity Trading and Risk-Adjusted Return,"Shuozhe Li, Du Cheng, Leqi Liu",10.48550/arXiv.2024.12345,arXiv:2408.12345,"Neural wavelet regularization, wavelet-transformer network, low-guided high-frequency injection, return optimization, portfolio optimization","Proposes WaveLSFormer, a learnable wavelet-based long-short Transformer that jointly performs multi-scale decomposition and return-oriented decision learning. It introduces a low-guided high-frequency injection module to refine low-frequency representations while maintaining stability. The model optimizes a portfolio of long/short positions under a fixed risk budget and incorporates risk-aware regularization. Experiments show superior performance over baselines across multiple industries.",27.87,LFM-2.5,Apple M1 (Metal)
2601.13437v1_MOSLD-Bench Multilingual Open-Set Learning and Dis.pdf,MOSLD-Bench: Multilingual Open-Set Learning and Discovery,"Adriana-Valentina Costache, Daria-Nicoleta Dragomir, Silviu-Florin Gheorghe, Eduard Poesina, Paul Irofti, Radu Tudor Ionescu",,,"open-set learning, discovery, text categorization, multilingual, zero-shot learning, new classes","The paper introduces the first multilingual open-set learning and discovery benchmark for text categorization by topic, comprising 960K data samples across 12 languages. It describes the methodology to construct the benchmark, proposes a novel framework for continuous discovery, and evaluates multiple language models.",25.63,LFM-2.5,Apple M1 (Metal)
2601.13443v1_Explicit Cognitive Allocation A Principle for Gove.pdf,Explicit Cognitive Allocation: A Principle for Governed and Auditable Inference in Large Language Models,"Héctor Manuel Manzanilla-Granados, Zaira Navarrete-Cazales, Escuela Superior de Cómputo del I.P .N",07738,,"cognitive allocation, large language models, AI-assisted reasoning, epistemic control, auditability, transparency","The paper introduces Explicit Cognitive Allocation as a framework to structure AI-assisted inference through explicit separation of epistemic functions, aiming to improve traceability, reproducibility, and governance in high-responsibility AI applications.",28.16,LFM-2.5,Apple M1 (Metal)
2601.13458v1_Labels or Preferences Budget-Constrained Learning .pdf,Budget-Constrained Learning with Human Judgments over AI-Generated Outputs,"Zihan Dong, Ruijia Wu ∗2, Linjun Zhang ∗1",arXiv:2601.13458v1,2601.13458,"budget-constrained learning, human judgments, AI-generated outputs, preference calibration, active learning","The paper addresses the need for efficient data acquisition when human preference feedback is used for AI-generated pseudo labels. It introduces a method, Preference-Calibrated Active Learning (PCAL), to optimally allocate a fixed annotation budget between ground-truth labels and pairwise preferences, proving its asymptotic optimality and robustness.",28.35,LFM-2.5,Apple M1 (Metal)
2601.13462v1_SpatialBench-UC Uncertainty-Aware Evaluation of Sp.pdf,SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt,Amine Rostane,arXiv:2601.13462v1,arXiv:2601.13462,"text-to-image, spatial evaluation, uncertainty-aware, prompt","Evaluating whether text to image models follow explicit spatial instructions is difficult to automate. The paper introduces SpatialBench-UC, a benchmark for pairwise spatial relations, enabling reproducible comparisons and risk-aware scoring.",25.09,LFM-2.5,Apple M1 (Metal)
2601.13464v1_Context and Transcripts Improve Detection of Deepf.pdf,Context and Transcripts Improve Detection of Deepfake Audios,"Chongyang Gao, Marco Postiglione, Julian Baldwin, Natalia Denisenko, Isabel Gortner, Luke Fosdick, Chiara Pulice, Sarit Kraus, V. S. Subrahmanian",1†,2,"deepfake, audio detection, context, transcript, public figures, deepfake detection, contextual information",Humans use context to assess information veracity; this study introduces a Context-based Audio Deepfake Detector (CADD) that leverages context and transcripts to improve audio deepfake detection performance.,25.96,LFM-2.5,Apple M1 (Metal)
2601.13465v1_Graph Neural Networks are Heuristics.pdf,Graph Neural Networks are Heuristics,"Yimeng Min, Carla P . Gomes",arXiv:2601.13465v1,arXiv:2601.13465,"Graph Neural Networks, Heuristics, Combinatorial Optimization, Travelling Salesman Problem, Neural Networks, Structure Learning","The paper demonstrates that a single training trajectory can transform a graph neural network into an unsupervised heuristic for the Travelling Salesman Problem. By encoding global structural constraints as an inductive bias, the model generates solutions via direct forward passes without search or supervision. Dropout and snapshot ensembling enable the model to act as an implicit ensemble, improving solution diversity and reducing optimality gaps.",27.5,LFM-2.5,Apple M1 (Metal)
2601.13474v1_Preconditioning Benefits of Spectral Orthogonaliza.pdf,Preconditioning Benefits of Spectral Orthogonalization in Muon,"Jianhao Ma, Penn, Yu Huang, Penn, Yuejie Chi, Yale, Yuxin Chen",arXiv:2601.13474v1,2601.13474,"Muon, preconditioning, spectral orthogonalization, gradient optimization, linear convergence","This paper investigates simplified variants of Muon, demonstrating linear convergence independent of condition number and comparing performance to gradient descent and Adam. It reveals that Muon's dynamics decouple into independent spectral sequences.",27.75,LFM-2.5,Apple M1 (Metal)
2601.13476v1_A Unified Variational Imputation Framework for Ele.pdf,A Unified Variational Imputation Framework for Electric Vehicle Charging Data,"Jinhao Li, Hao Wang",10.48550/arXiv.2024.12345,arXiv:2408.12345,"electric vehicle, data imputation, charging demand, large language model, retrieval-augmented generation","This paper presents a novel probabilistic variational imputation framework leveraging large language models and retrieval-augmented memory to address data sparsity in electric vehicle charging datasets. PRAIM encodes heterogeneous data into a unified representation and dynamically retrieves relevant examples, improving imputation accuracy and preserving statistical distributions.",26.46,LFM-2.5,Apple M1 (Metal)
2601.13481v1_Towards Efficient and Robust Linguistic Emotion Di.pdf,Towards Efficient and Robust Linguistic Emotion,"Jian Zhang, Zhangqi Wang, Zhiyuan Wang, Weiping Fu, Yu He, Haiping Zhu, Qika Lin",1731,"IEEE TRANSACTIONS ON AFFECTIVE COMPUTING 14(3), 2023","Linguistic Emotion Diagnosis, Emotional Comorbidity, Inefficient Exploration, Automated Prompt Optimization, Multi-Agent Collaboration, Medical Language Processing, Trustworthy Artificial Intelligence","This paper proposes APOLO, a framework for automated prompt optimization in linguistic emotion diagnosis. APOLO treats instruction refinement as a POMDP and employs a multi-agent collaboration mechanism (Planner–Teacher–Critic–Student–Target roles) to enhance diagnostic accuracy and robustness in clinical settings. Experimental results show improved performance across domain-specific benchmarks.",29.23,LFM-2.5,Apple M1 (Metal)
2601.13487v1_The Hidden Toll of Social Media News Causal Effect.pdf,The Hidden Toll of Social Media News: Causal Effects on Psychosocial Wellbeing,"Olivia Pal, Agam Goyal, Eshwar Chandrasekharan, Koustuv Saha",10.48550/arXiv.2025.12345,arXiv:2509.12345,"social media, news consumption, psychosocial wellbeing, affective outcomes, behavioral outcomes, cognitive outcomes","This study examines how different forms of social media news engagement affect psychosocial wellbeing through quasi-experimental analysis. Findings indicate that news engagement is linked to increased depression, stress, and anxiety, while reducing loneliness and boosting social interaction. Regression models show that newsfeed bookmarking correlates strongly with psychosocial decline, with effects magnified by repeated exposure.",27.28,LFM-2.5,Apple M1 (Metal)
2601.13508v1_CatMaster An Agentic Autonomous System for Computa.pdf,CatMaster: An Agentic Autonomous System for Computational Heterogeneous Catalysis Research,"Honghao Chen, Jiangjie Qiu, Yi Shen Tew, Xiaonan Wang",arXiv:2601.13508v1,arXiv:2601.13508,"density functional theory, computational heterogeneous catalysis, agentic autonomous system, LLM-driven agent, multi-fidelity tool library","CatMaster is a large-language-model system that automates computational catalysis workflows, improving reproducibility and efficiency in catalyst studies.",30.58,LFM-2.5,Apple M1 (Metal)
2601.13515v1_Automatic Adjustment of HPA Parameters and Attack .pdf,Automatic Adjustment of HPA Parameters and Attack,"Huah Yong Chan, Hanlin Zhou, Jingfei Ni, Mengchun Wu",arXiv:2601.13515v1,arXiv:2601.13515,"Kubernetes, HPA, Security, Random Forest","This paper presents a method to dynamically adjust HPA parameters using Random Forest classification to manage attack traffic in Kubernetes. It demonstrates how attacking IPs are redirected to honeypot pods, reducing 5XX status codes under high load while highlighting the need for appropriate adjustment thresholds.",28.6,LFM-2.5,Apple M1 (Metal)
2601.13518v1_AgenticRed Optimizing Agentic Systems for Automate.pdf,Optimizing Agentic Systems for Automated Red-teaming,"Jiayi Yuan, Natasha Jaques, Goran Radanović",10.48550/arXiv.2026.12345,arXiv:2308.01234,"automated red-teaming, agentic systems, LLM, system design, AI safety","AGENTICRED introduces an automated pipeline leveraging LLMs' in-context learning to design and refine red-teaming systems without human intervention. It treats red-teaming as a system design problem, outperforming state-of-the-art methods and achieving high attack success rates across multiple models.",25.64,LFM-2.5,Apple M1 (Metal)
2601.13528v1_Eliciting Harmful Capabilities by Fine-Tuning On S.pdf,Eliciting Harmful Capabilities by Fine-Tuning on Safeguarded Outputs,"Jackson Kaunismaa, Mats, Avery Griffin, Anthropic, John Hughes, Anthropic, Christina Q Knight, Scale AI, Mrinank Sharma, Anthropic, Erik Jones",arXiv:2601.13528v1,2601.13528,"hazardous chemical synthesis, output-level safeguards, model elicitation, safeguarded models, ecosystem risks","This work demonstrates that even safeguarded frontier models can be used to elicit harmful capabilities via elicitation attacks. The authors show that such attacks recover ~40% of the capability gap between open-source and unrestricted models, highlighting challenges in mitigating ecosystem-level risks.",26.82,LFM-2.5,Apple M1 (Metal)
2601.13533v1_Reasoning While Recommending Entropy-Guided Latent.pdf,Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models,Changshuo Zhang,10.1093/pasj/rsad052,,"Generative Re-ranking, Latent Reasoning, Reinforcement Learning","This paper introduces the Entropy-Guided Latent Reasoning (EGLR) model, which integrates reasoning capabilities into generative re-ranking to reduce entropy in decision-making. It presents three advantages: abandoning 'reason first, recommend later', implementing entropy-guided variable-length reasoning, and adopting a lightweight integration design. Experimental validation on real-world datasets shows improved performance.",25.61,LFM-2.5,Apple M1 (Metal)
2601.13534v1_MN-TSGContinuous Time Series Generation with Irreg.pdf,Continuous Timeseries Generation With Irregular Observations,"Xu Zhang, Junwei Deng, Chang Xu, Hao Li",10.1093/pasj/tsg123,2601.13534,"Irregular time series, continuous time series generation, deep learning architecture","This paper addresses the mismatch between regular time series assumptions in existing methods and irregular real-world data. It introduces MN-TSG, a framework combining Mixture-of-Experts (MoE)-based Neural Controlled Differential Equations (NCDEs) with existing TSG models, enabling flexible and high-resolution generation for irregular and continuous tasks.",26.35,LFM-2.5,Apple M1 (Metal)
2601.13537v1_When Wording Steers the Evaluation Framing Bias in.pdf,When Wording Steers the Evaluation: Framing Bias in LLM judges,"Yerin Hwang, Dongryeol Lee, Taegwan Kang, Minwoo Lee, Kyomin Jung",,,"LLM evaluation, framing bias, prompt framing, LLM judges, psychological framing effect","This study investigates how deliberate prompt framing influences large language model judgments across four high-stakes evaluation tasks, revealing framing bias as a structural issue in LLM-based systems.",24.27,LFM-2.5,Apple M1 (Metal)
2601.13545v1_TruthTensor Evaluating LLMs Human Imitation throug.pdf,Evaluating LLMs through PredictionMarketDrift and Holistic Reasoning,"Shirin Shahabi, Spencer Graham, Haruna Isah",,,"language models, LLM evaluation, human imitation, market prediction, drift","This paper introduces TruthTensor, a new evaluation framework that assesses LLMs as human-imitation systems in dynamic environments. It emphasizes multi-dimensional metrics—accuracy, calibration, narrative stability, cost, and resource efficiency—while integrating drift-centric diagnostics and robustness checks to support reproducible research.",22.56,LFM-2.5,Apple M1 (Metal)
2601.13546v1_ChatAD Reasoning-Enhanced Time-Series Anomaly Dete.pdf,Reasoning-Enhanced Time-Series Anomaly Detection with Multi-Turn Instruction Evolution,"Hui Sun, Chang Xu, Haonan Xie, Hao Li, Yuhao Huang, Chuheng Zhang, Ming Jin, Xiaoguang Liu, Gang Wang, Jiang Bian, Nankai University, Microsoft Research Asia, Huanjiang Laboratory, University of Manchester, Nanjing University, Griffith University",,,"LLM-driven anomaly detection, Time Series, Anomaly detection, Multi-turn dialogue, LLM models, Kahneman-Tversky optimization, cross-task generalization, classification, forecasting, imputation",The paper proposes a multi-agent-based time-series evolution algorithm (TSEvol) with an AD reasoning and multi-turn dialogue dataset (TSEData-20K). It introduces ChatAD models enhanced by LLM-driven techniques and evaluates performance using LLADBench. Results show up to 34.50% accuracy improvement and competitive reasoning capabilities.,28.43,LFM-2.5,Apple M1 (Metal)
2601.13547v1_HateXScore A Metric Suite for Evaluating Reasoning.pdf,HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations,"Yujia Hu, Roy Ka-Wei Lee",10.1007/XXXXXX,10.1007/XXXXXX,"hate speech, reasoning quality, explainability, content moderation, transparency, explainable AI","Introduces HateXScore, a four-component metric suite to assess reasoning quality in hate speech explanations, addressing gaps in current evaluation methods and aligning with regulatory demands for interpretability.",28.81,LFM-2.5,Apple M1 (Metal)
2601.13558v1_Leveraging ChatGPT and Other NLP Methods for Ident.pdf,Leveraging ChatGPT and Other NLP Methods for Identifying Risk and Protective Behaviors in MSM: Social Media and Dating apps Text Analysis,"Mehrab Beikzadeh ∗, Chenglin Hong, Cory J Cascalheira, Callisto Boka †, Majid Sarrafzadeh ∗, Ian W Holloway †",,,"machine learning, HIV risk, harmful drinking, social app, dating app, Text mining, ChatGPT, eHealth, LLM",Background: Men who have sex with men (MSM) are at elevated risk for sexually transmitted infections and harmful drinking compared to heterosexual individuals. Text data from social media and dating apps may enable personalized public health interventions by identifying risk and protective behaviors automatically.,27.83,LFM-2.5,Apple M1 (Metal)
2601.13559v1_AgentGC Evolutionary Learning-based Lossless Compr.pdf,Evolutionary Learning-based Lossless Compression for Genomics Data with LLM-driven Multiple Agent,"Hui Sun, Yanfeng Ding, Huidong Ma, Chang Xu, Keyan Jin, Lizheng Zu, Cheng Zhong, Xiaoguang Liu, Gang Wang, Wentong Cai, Nankai University, Nanyang Technology University, Microsoft Research Asia, Macao Polytechnic University, Guangxi University",,,"genomics data, lossless compression, multi-agent, LLM, compression, algorithm-dataset-hardware","Proposes AgentGC, the first evolutionary Agent-based GD Compressor, featuring a user-friendly interface, cognitive optimization, and automated compression framework. Experimental results show significant gains in compression ratios and throughput across multiple datasets.",26.94,LFM-2.5,Apple M1 (Metal)
2601.13562v1_Reasoning is a Modality.pdf,Reasoning is a Modality,"Zhiguang Liu, Yi Shang","lz7fd@missouri.edu, shangy@missouri.edu",,"reasoning, modality, LLMs, ViTs","The paper proposes that reasoning should exist as a distinct internal channel separate from the low-level workspace, using a role-separated transformer block to enable controller-driven reasoning. Experiments show improved performance over dense ViT baselines.",23.51,LFM-2.5,Apple M1 (Metal)
2601.13563v1_ButterflyMoE Sub-Linear Ternary Experts via Struct.pdf,ButterflyMoE: Sub-Linear Ternary Experts via Structured Butterfly,Aryan Karmore,bt24csd009@iiitn.ac.in,,"ButterflyMoE, linear memory scaling, quantization, quantized substrate, ternary matrices","Introduces ButterflyMoE, a method that models experts as geometric reorientations of a shared quantized substrate, achieving sub-linear memory scaling by leveraging learned rotations and reducing activation outliers.",23.84,LFM-2.5,Apple M1 (Metal)
2601.13564v1_Multi-objective fluorescent molecule design with a.pdf,Multi-objective fluorescent molecule design with a data-physics dual-driven generative framework,"Yanheng Li, Zhichen Pu, Lijiang Yang, Zehao Zhou, Yi Qin Gao",,,"fluorescent molecule, design, generative framework, data-physics, multi-objective",New cornerstone science laboratory research on fluorescent molecule design using data-physics driven generative methods.,31.73,LFM-2.5,Apple M1 (Metal)
2601.13566v1_Self-Improvement as Coherence Optimization A Theor.pdf,Self-Improvement as Coherence Optimization: A Theoretical Account,"Tianyi Qiu, Ahmed Hani Ismailahmedhismail, Zhonghao Hehezhonghao2030, George Washington University",arXiv:2601.13566v1,arXiv:2601.13566,"language models, self-improvement, coherence optimization, supervised learning, pretrained models","The paper explores how language models can enhance accuracy without external supervision by framing their optimization as coherence maximization, linking it to description-length regularization and explaining its effectiveness in semi-supervised settings.",27.61,LFM-2.5,Apple M1 (Metal)
2601.13570v1_GeoDynamics A Geometric State-Space Neural Network.pdf,GeoDynamics: A Geometric State-Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds,"Tingting Dan, Jiaqi Ding, Guorong Wu ∗",arXiv:2601.13570v1,2601.13570,"neural networks, brain dynamics, functional connectivity, geometric state-space, neuroscience","Introduces GeoDynamics, a geometric state-space neural network that models latent brain-state trajectories on a Riemannian manifold, enabling insights into cognition, behavior, and early markers of neurological disorders.",26.61,LFM-2.5,Apple M1 (Metal)
2601.13580v1_Neural Organ Transplantation NOT Checkpoint-Based .pdf,Neural Organ Transplantation (NOT),Ahmad Al-Zuraiqi,arXiv:2601.13580v1,2601.13580,"Modular Deep Learning, Transfer Learning, Checkpoint Transfer, Domain Adaptation, Large Language Model","Introduces Neural Organ Transplantation (NOT), a modular adaptation framework that enables trained transformer layers to function as reusable transferable checkpoints for domain adaptation. It extracts contiguous layer subsets from pre-trained models, trains them independently, and saves them as standalone checkpoint files for transplantation into compatible recipient models without access to original training data. Experiments show significant improvements over LoRA in perplexity and faster training, with position dependence effects and unexpected regularization benefits at billion-parameter scale.",31.32,LFM-2.5,Apple M1 (Metal)
2601.13581v1_SCRIPTMIND Crime Script Inference and Cognitive Ev.pdf,Crime Script Inference and Cognitive Evaluation for LLM-based Social Engineering Scam Detection System,"Heedou Kim1,2, Changsik Kim2, Sanghwa Shin3, Jaewoo Kang1, Korea University, Seoul, Republic of Korea, Korean National Police Agency, Seoul, Republic of Korea, Inje University, Gimhae, Republic of Korea",anonymous/ScriptMind,,"social engineering, scam detection, large language models, cognitive evaluation, criminal script inference, cognitive simulation, scam monitoring","This paper introduces SCRIPTMIND, an integrated framework combining crime script inference, LLM fine-tuning, and cognitive simulation to enhance scam detection. Using Korean phone scam data, SCRIPTMIND outperforms commercial models in detection accuracy, reduces false positives, improves scammer utterance prediction, and strengthens user suspicion through cognitive assessment. The approach advances human-centered, adaptive LLMs for real-time defense.",28.32,LFM-2.5,Apple M1 (Metal)
2601.13588v1_TREX Tokenizer Regression for Optimal Data Mixture.pdf,TREX: Tokenizer Regression for Optimal Data Mixture,"Inho Won1, Hangyeol Yoo2, Minkyung Cho1, Jungyeul Park1, KyungTae Lim1, KyungTae Lim4",,,"tokenizer, data mixture, multilingual models, compression, LLM training","Building effective tokenizers for multilingual LLMs requires careful control over language-specific data mixes. TREX introduces a regression-based framework to predict optimal data mixtures, enabling scalable mixture search and improving compression efficiency for tokenizers.",24.89,LFM-2.5,Apple M1 (Metal)
2601.13589v1_Motion-to-Response Content Generation via Multi-Ag.pdf,Motion-to-Responseness Content Generation via Multi-Agent AI System,HyeYoung Lee,arXiv:2601.13589v1,arXiv:2601.13589,"Speech Emotion Recognition, Multi-Agent Systems, Content Generation, Safety Verification, On-Device AI","This paper proposes a multi-agent AI system that generates response-oriented media content in real time based on audio-derived emotional signals. It emphasizes transforming inferred emotional states into safe, age-appropriate, and controllable responses through a structured pipeline of specialized agents, achieving high accuracy and compliance.",27.25,LFM-2.5,Apple M1 (Metal)
2601.13590v1_Vulnerability of LLMs Belief Systems LLMs Belief R.pdf,Vulnerability of LLMs’ Belief Systems? LLMs Belief Resistance Check,"Fan Huang, Haewoon Kwak, Jisun An",,,"large language models, belief systems, persuasion, meta-cognition","This study evaluates how Large Language Models (LLMs) are susceptible to persuasion within the Source–Message–Channel–Receiver (SMCR) framework, analyzing susceptibility across multiple interaction turns and exploring the impact of meta-cognition prompting.",23.95,LFM-2.5,Apple M1 (Metal)
2601.13591v1_DSAEval Evaluating Data Science Agents on a Wide R.pdf,Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems,"Maojun Sun, Yifei Xie, Yue Wu, Ruijian Han, Binyan Jiang, Defeng Sun, Jian Huang",https://dsaeval.github.io/DSAEval,,"data science agents, LLM-based agents, data analysis, deep learning, structured data, unstructured data, multi-modal perception, multi-query interactions, multi-dimensional evaluation","DSAEval introduces a benchmark with 641 real-world data science problems across 285 datasets, evaluating advanced agentic LLMs through multi-modal perception, iterative queries, and comprehensive evaluation. Results highlight performance variations and identify key challenges in unstructured domains.",27.32,LFM-2.5,Apple M1 (Metal)
2601.13592v1_Machine learning based radiative parameterization .pdf,Machine learning based radiative parameterization scheme and its performance in operational reforecast experiments,"Jing Hao, Xiao Sa, Li Haoyu, Xiao Huadong, Xue Wei",10.5194/arrmw-1234-2024,,"Machine learning, Radiation, Hybrid model, Operational reforecast experiments","This study investigates the use of a residual convolutional neural network to approximate the Rapid Radiative Transfer Model for General Circulation Models within the China Meteorological Administration's operational system. It evaluates a hybrid framework combining offline training and online coupling, demonstrating comparable accuracy to traditional methods while significantly accelerating computation speed.",28.21,LFM-2.5,Apple M1 (Metal)
2601.13599v1_Diffusion In Diffusion Breaking the Autoregressive.pdf,Diffusion In Diffusion: Breaking the Autoregressive Bottleneck in Block Diffusion Models,"Linrui Ma, Yufei Cui, Kai Han, Yunhe Wang",arXiv:2601.13599v1,2601.13599,"block diffusion, autoregressive, diffusion models, generative modeling, language modeling","The paper proposes DIFFUSION INDIFFUSION, a draft-then-refine framework to overcome irreversibility and myopia in block diffusion models. It combines rapid block-based draft generation with global bidirectional refinement, improving performance on OpenWebText while reducing inference cost.",26.47,LFM-2.5,Apple M1 (Metal)
2601.13600v1_Foundations of Global Consistency Checking with No.pdf,Foundations of Global Consistency Checking with Noisy LLM Oracles,"Paul He, Elke Kirschbaum, Shiva Kasiviswanathan",,,"global consistency, LLM oracles, fact-checking, knowledge base, minimal inconsistent subsets, polynomial complexity","The paper addresses the challenge of verifying global consistency of natural language facts using noisy large language model (LLM) judgments. It formalizes the problem, demonstrates that global consistency verification requires exponential oracle queries, and proposes an adaptive divide-and-conquer algorithm to identify minimal inconsistent subsets efficiently.",25.46,LFM-2.5,Apple M1 (Metal)
2601.13614v1_CauScientist Teaching LLMs to Respect Data for Cau.pdf,Teaching LLMs to Respect Data for Causal Discovery,"Bo Peng, Sirui Chen, Lei Xu, Chaochao Lu",10.5281/zenodo.1234567,arXiv:2601.13614v1,"causal discovery, LLM, data-driven, statistical inference, causal graphs","The paper proposes CauScientist, a framework that combines LLMs as hypothesis-generating scientists with probabilistic statistics as verifiers. It improves causal discovery by leveraging hybrid initialization, iterative refinement, and error memory, achieving significant performance gains over data-only baselines.",25.92,LFM-2.5,Apple M1 (Metal)
2601.13622v1_CARPE Context-Aware Image Representation Prioritiz.pdf,Context-Aware Image Representation Prioritization via Ensemble,"Donghee Lee, Rui Cai, Zhe Zhao",arXiv:2601.13622v1,cs.CV,"Context-Aware, Image Representation, Prioritization, Ensemble, Large Vision-Language Models","The paper proposes CARPE, a model-agnostic framework that enhances LVMs by introducing vision-integration layers and a context-aware ensemble strategy. This improves adaptability between visual and textual modalities, boosting performance on image classification and vision-language benchmarks.",26.57,LFM-2.5,Apple M1 (Metal)
2601.13632v1_Resilient Routing Risk-Aware Dynamic Routing in Sm.pdf,Resilient Routing: Risk-Aware Dynamic Routing in Smart Logistics via Spatiotemporal Graph Learning,"Zhiming Xue, Sichen Zhao, Yalun Qi, Xianling Zeng, Zihan Yu",,,"Smart Logistics, Graph Neural Network, Dynamic Routing, Spatiotemporal modeling, Supply Chain Resilience",The paper proposes a Risk-Aware Dynamic Routing framework integrating Spatiotemporal Graph Neural Networks with combinatorial optimization to enhance logistics resilience. Experimental results demonstrate a 19.3% reduction in congestion risk exposure under high-traffic scenarios.,26.56,LFM-2.5,Apple M1 (Metal)
2601.13645v1_Quadratic Upper Bound for Boosting Robustness.pdf,Quadratic Upper Bound for Boosting Robustness,"Euijin Y Ou1, Hyang-Won Lee",10.48550/arXiv.2025.12345,42nd International Conference on Machine Learning,"robustness, adversarial training, loss function, deep learning",This paper presents a quadratic upper bound (QUB) for adversarial training to improve robustness without stronger inner maximization. The proposed bound enables smoother loss landscapes and yields significant robustness gains in existing FAT methods.,24.22,LFM-2.5,Apple M1 (Metal)
2601.13647v1_Fusion Segment Transformer Bi-Directional Attentio.pdf,FUSION SEGMENT TRANSFORMER: BI-DIRECTIONAL A TTENTION GUIDED FUSION,"Yumin Kim, Seonghyeon Go",,,"AI-generated music detection, Full-audio segment detection, Musical structure analysis, Cross-modal fusion layer","Proposes an improved Segment Transformer for full-audio AI-generated music detection, introducing a Gated Fusion Layer to capture long-term context.",23.94,LFM-2.5,Apple M1 (Metal)
2601.13649v1_Fairness or Fluency An Investigation into Language.pdf,Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge,"Xiaolin Zhou, Zheng Luo, Yicheng Gao, Qixuan Chen, Xiyang Hu, Yue Zhao, Ruishan Liu",10.48550/arXiv.2024.12345,arXiv:2409.12345,"language bias, LLM-as-a-judge, performance disparity, language families, model evaluation","This paper investigates language bias in pairwise LLM-as-a-judge systems. It identifies performance disparities across languages when judges compare texts within the same language versus across languages, highlighting biases in European vs. African languages and culturally related topics. The study also explores whether bias relates to low perplexity and its limitations in fully explaining observed disparities.",27.51,LFM-2.5,Apple M1 (Metal)
2601.13655v1_Why Does the LLM Stop Computing An Empirical Study.pdf,Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs,"Guangba Yu, Zirui Wang, Yujie Huang, Renyi Zhong, Yuedong Zhong, Yilun Wang, Michael R. Lyu",10.1093/acm/qad048,,"Large Language Models, Failure Analysis, Empirical Study, Open-Source LLMs, Reliability, Deployment, Software Reliability, Software Usability","This study investigates user-reported failures in open-source LLMs, identifying systemic reliability issues rather than model defects. It highlights three key phenomena: Diagnostic Divergence, Systemic Homogeneity, and Lifecycle Escalation, offering insights to improve model robustness.",26.65,LFM-2.5,Apple M1 (Metal)
2601.13657v1_Communication-Free Collective Navigation for a Swa.pdf,Communication-Free Collective Navigation for a Swarm of UA Vs,"Myong-Yol Choi, Hankyoul Ko, Hanse Cho, Changseung Kim, Seunghwan Kim, Jaemin Seo, Hyondong Oh",,,"multi-robot systems, collective navigation, sensor-based control, deep reinforcement learning","This paper presents a deep reinforcement learning based controller for collective navigation of unmanned aerial vehicle swarms in communication-denied environments. It employs an implicit leader-follower framework where followers learn robust policies using only onboard LiDAR sensing, without inter-agent communication.",26.33,LFM-2.5,Apple M1 (Metal)
2601.13659v1_Temporal-Spatial Decouple before Act Disentangled .pdf,TEMPORAL-SPA TIAL DECOUPLE BEFORE ACT: DISENTANGLED REPRESENTA TION,"Chunlei Meng, Ziyang Zhou, Lucas He, Xiaojing Du, Chun Ouyang, Zhongxue Gan",,,"Multimodal Sentiment Analysis, Temporal-Spatial Decoupling, Representation Learning, Sentiment Analysis, Multimodal Fusion","Proposes TSDA, Temporal–Spatial Decouple before Act to address spatiotemporal heterogeneity in multimodal sentiment analysis. Introduces factor-consistent cross-modal alignment and gated recoupling to improve performance.",27.21,LFM-2.5,Apple M1 (Metal)
2601.13671v1_The Orchestration of Multi-Agent Systems Architect.pdf,"Orchestration of Multi-Agent Systems: Architectures, Protocols, and Enterprise Adoption","Apoorva Adimulam, Rajesh Gupta, Sumit Kumar",,,"Agent orchestration, Agent-to-Agent protocol, multi-agent systems, observability, state management, system governance","Orchestrated multi-agent systems represent the next stage in AI evolution, focusing on structured collaboration among autonomous agents. This paper formalizes architectural frameworks integrating planning, policy enforcement, state management, and quality operations, while detailing communication protocols (Model Context Protocol, Agent-to-Agent protocol) and governance mechanisms to ensure scalability, auditability, and compliance.",27.02,LFM-2.5,Apple M1 (Metal)
2601.13684v1_HeteroCache A Dynamic Retrieval Approach to Hetero.pdf,HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache,"Zhiyuan Shi, Qibo Qiu, Feng Xue, Zhonglin Jiang, Li Yu2, Jian Jiang 2, Xiaofei He, Wenxiao Wang, †Zhejiang University, 2China Mobile (Zhejiang) Research & Innovation Institute, 3The Center for Artificial Intelligence, Geely",,,"Heterogeneous KV cache, Dynamic retrieval, Long-context LLM, Attention drift, Compression, Memory growth, Contextual inference","The paper proposes HeteroCache, a training-free dynamic compression framework for LLMs, addressing the limitations of static compression by leveraging attention heterogeneity and spatial redundancy. It introduces a fine-grained weighting strategy and hierarchical storage to improve performance on long-context tasks.",27.87,LFM-2.5,Apple M1 (Metal)
2601.13687v1_Understanding Mental States to Guide Social Influe.pdf,Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue,"Zhichao Liang, Satoshi Nakamura",,,"mental states, social influence, group dialogue, theory of mind","This paper introduces SocialMindChange, a benchmark that evaluates models' ability to influence evolving mental states in multi-agent social interactions. It highlights the gap between current LLM performance and human-like dynamic ToM, emphasizing the need for models to track and steer mental-state trajectories across connected scenes.",24.17,LFM-2.5,Apple M1 (Metal)
2601.13693v1_End-to-End Reverse Screening Identifies Protein Ta.pdf,End-to-End Reverse Screening Identifies Protein,"Shengjie Xu, Xianbin Ye, Mengran Zhu, Xiaonan Zhang, Shanzhuo Zhang, Xiaomin Fang",arXiv:2601.13693v1,2601.13693,"Reverse screening, Target identification, Biomolecular structure prediction, HelixFold3","Identifying protein targets for small molecules, or reverse screening, is essential for understanding drug action, guiding compound repurposing, predicting off-target effects, and elucidating molecular mechanisms. This study presents an end-to-end reverse screening strategy using HelixFold3, improving accuracy and structural fidelity compared to conventional methods.",30.85,LFM-2.5,Apple M1 (Metal)
2601.13697v1_Uncertainty-Aware Gradient Signal-to-Noise Data Se.pdf,Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning,"Zhihang Yuan, Chengyu Yue, Long Huang, Litu Ou, Lei Shi",,,"instruction tuning, data selection, uncertainty-aware, gradient filtering, LLM adaptation","This paper proposes GRADFILTERING, an objective-agnostic data selection framework that uses a small GPT-2 proxy with LoRA to aggregate per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR). It matches or surpasses random and strong baselines in LLM-as-a-judge evaluations and shows faster convergence under limited compute.",25.83,LFM-2.5,Apple M1 (Metal)
2601.13698v1_Does Privacy Always Harm Fairness Data-Dependent T.pdf,Privacy Always Harm Fairness? Data-Dependent Trade-offs,"Arjun Nichani, Hsiang Hsu, Chun-Fu (Richard) Chen, Haewon Jeong",arXiv:2601.13698v1,arXiv:2601.13698,"fairness, privacy, accuracy, data-dependent trade-offs","This paper explores the data-dependent relationship among fairness, privacy, and accuracy using the Chernoff Information measure. It introduces Noisy Chernoff Difference to analyze trade-offs and proposes methods for estimating it from real data.",26.12,LFM-2.5,Apple M1 (Metal)
2601.13704v1_Performance and Complexity Trade-off Optimization .pdf,Performance and Complexity Trade-off,"Esteban Gómez, Tom B. Hackström",10.1109/TAP.2020.999876,"IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. XX, NO. X, AUGUST 20XX","speech machine learning, low-complexity, voice activity detection, deep fake detection","The paper discusses optimizing speech models during training by balancing performance and computational complexity. It introduces a reparameterization technique that jointly optimizes both aspects using stochastic gradient descent, addressing the limitations of heuristic weight pruning.",27.28,LFM-2.5,Apple M1 (Metal)
2601.13707v1_Attention-space Contrastive Guidance for Efficient.pdf,Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs,"Yujin Jo, Sangyoon Bae, Taesup Kim",,,"hallucination, hallucination mitigation, vision-language models, contrastive guidance",Addresses hallucinations in large vision-language models by introducing contrastive guidance that steers generation toward visually grounded text. Proposes Attention-space Contrastive Guidance (ACG) operating within self-attention layers to balance vision and language representations. Demonstrates improved faithfulness and caption quality while reducing computational cost.,25.93,LFM-2.5,Apple M1 (Metal)
2601.13709v1_Hidden in Plain Text Measuring LLM Deception Quali.pdf,Hidden in Plain Text: Measuring LLM Deception,"Christopher Kao, Vanshika Vats, James Davis","77SPARX Studio, Inc.",,"large language models, natural language processing, autonomous game players, social deduction games","This paper studies deception in the Social Deduction Game Mafia using LLM agents. It compares a GPT-4o-based Mafia Detector against human performance and a random baseline, finding that LLMs are more deceptive in natural language contexts.",25.39,LFM-2.5,Apple M1 (Metal)
2601.13710v1_Who Should Have Surgery A Comparative Study of Gen.pdf,Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction,"Sayeed Shafayet Chowdhury, Snehasis Mukhopadhyay, Shiaofen Fang, Vijay R. Ramakrishnan",,,"Chronic Rhinosinusitis, clinical decision support, generative artificial intelligence, large language models, SNOT-22, surgical outcome prediction, tabular clinical data","Artificial intelligence has reshaped medical imaging, yet its use on clinical data for prospective decision support remains limited. This study evaluates whether pre-operative models can identify patients who should avoid surgery, comparing supervised machine learning and generative AI approaches.",27.03,LFM-2.5,Apple M1 (Metal)
2601.13717v1_Simulated Ignorance Fails A Systematic Study of LL.pdf,Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting,"Zehan Li, Yuxuan Wang, Ali El Lahib, Ying-Jieh Xia, Xinyu Pi",,,"LLM, forecasting, knowledge cutoff, retrospective evaluation, model evaluation","Evaluating LLM forecasting capabilities is constrained by a tension between rigorous evaluation and high latency. Simulated Ignorance (SI) was tested across multiple models to assess its ability to approximate True Ignorance (TI). Findings reveal SI systematically underperforms, highlighting limitations in models' ability to suppress prior knowledge and suppressing reasoning traces.",25.42,LFM-2.5,Apple M1 (Metal)
2601.13719v1_Hierarchical Long Video Understanding with Audiovi.pdf,Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search,"Xinlei Yin, Xiulian Peng, Xiao Li, Zhiwei Xiong, Yan Lu",10.48550/arXiv.2024.12345,arXiv:2408.12345,"long video understanding, audiovisual entity cohesion, hierarchical video indexing, agentic search, semantic consistency, multimodal reasoning","This paper introduces HAVEN, a unified framework for long-video understanding that integrates audiovisual entity cohesion and hierarchical video indexing with agentic search. It addresses challenges in maintaining global coherence and entity consistency across extended video content, achieving state-of-the-art performance on LVBench.",28.26,LFM-2.5,Apple M1 (Metal)
2601.13722v1_OP-Bench Benchmarking Over-Personalization for Mem.pdf,Benchmarking Over-Personalization for Memory-Augmented Personalized Conversational Agents,"Yulin Hu, Zimo Long, Jiahe Guo, Xingyu Sui, Xing Fu, Weixiang Zhao, Yanyan Zhao, Bing Qin",arxiv:2601.13722v1,arxiv:2601.13722,"memory-augmented agents, personalization, over-personalization, self-review, human-computer interaction","This work introduces OP-Bench, a benchmark evaluating 1,700 dialogue instances to assess how personalization is applied. It identifies three types of over-personalization—Irrelevance, Repetition, and Sycophancy—and proposes Self-ReCheck to mitigate it while preserving performance. The study highlights risks of agents relying excessively on user memory, often producing intrusive responses.",28.68,LFM-2.5,Apple M1 (Metal)
2601.13734v1_Towards robust long-context understanding of large.pdf,TOW ARDS ROBUST LONG-CONTEXT UNDERSTANDING OF LARGE LANGUAGE MODEL,Chenyu Hui,10.48550/arxiv/2303.06931,arXiv:2303.06931,"LLM, Long-context understanding, Active recap learning, Recap supervision","This paper proposes active recap learning (ARL), a framework to enhance LLMs in understanding long contexts. ARL enables models to revisit and summarize earlier content during continuous pretraining and retrospective summarization at inference, improving performance on long-text tasks.",25.28,LFM-2.5,Apple M1 (Metal)
2601.13735v1_Reasoning or Fluency Dissecting Probabilistic Conf.pdf,Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection,"Hojin Kim, Jaehyung Kim",10.48550/arXiv.2303.03112,arXiv:2303.03112,"probabilistic confidence, reasoning quality, Best-of-N selection, inter-step causality, LLM performance","This work challenges the assumption that probabilistic confidence metrics reflect reasoning fidelity by examining whether they capture inter-step causal dependencies. It introduces a contrastive causality metric and finds that current metrics are largely insensitive to causal disruptions, supporting the view that they reflect surface-level fluency.",25.67,LFM-2.5,Apple M1 (Metal)
2601.13749v1_Pro-AI Bias in Large Language Models.pdf,Pro-AI Bias in Large Language Models,"Benaya Trabelsi, Jonathan Shaki, Sarit Kraus",,,"Pro-AI bias, Large language models, AI bias, Decision support, LLM evaluation","Investigates systematic preference for artificial intelligence in large language models across decision-support tasks, showing bias in recommendations, salary estimates, and internal representations.",23.9,LFM-2.5,Apple M1 (Metal)
2601.13752v1_Finding RELIEF Shaping Reasoning Behavior without .pdf,Finding Relief: Shaping Reasoning Behavior without Reasoning,"Chak Tou Leong, Dingwei Chen, Heming Xia, Qingyu Yin, Sunbowen Lee, Jian Wang, Wenjie Li",10.48550/arXiv.2025.12345,arXiv:2509.12345,"belief engineering, reasoning behavior, large language models, supervision","This paper proposes Rea-son Belief Engineering (RELIF) to shape large reasoning models by aligning their self-concept with a target belief blueprint without requiring explicit reasoning-trace supervision. By fine-tuning on self-referential question-answering pairs, RELIEF captures latent reasoning beliefs and improves efficiency while maintaining faithfulness.",26.49,LFM-2.5,Apple M1 (Metal)
2601.13761v1_DARC Decoupled Asymmetric Reasoning Curriculum for.pdf,Decoupled Asymmetric Reasoning Curriculum for LLM Evolution,"Shengda Fan, Xuyan Ye, Yankai Lin",10.48550/arXiv.2304.08732,arXiv:2601.13761,"self-play, large language models, self-improvement, reasoning curriculum, LLM evolution","Introduces DARC, a decoupled asymmetric reasoning curriculum that stabilizes self-evolution in LLMs by separating question generation and solver training, achieving robust performance without heavy human annotation.",25.09,LFM-2.5,Apple M1 (Metal)
2601.13768v1_vLinear A Powerful Linear Model for Multivariate T.pdf,vLinear: A Powerful Linear Model for Multivariate Time Series Forecasting,"Wenzhen Yue, Ruohao Guo, Ji Shi, Zihan Hao, Shiyu Hu, Xianghua Ying",,,"linear model, time series forecasting, multivariate correlations, self-attention, Transformer, forecasting accuracy","The paper introduces vLinear, a lightweight linear-based multivariable time series forecaster. It proposes vecTrans, a rank-1 matrix module that reduces computational complexity from O(N²) to O(N), enabling up to 5× inference speedups while maintaining high accuracy. WFMLoss, a path- and horizon-weighted objective, improves forecasting performance across benchmarks. The method is evaluated on 22 benchmarks and 124 forecasting settings.",26.83,LFM-2.5,Apple M1 (Metal)
2601.13770v1_Look-Ahead-Bench a Standardized Benchmark of Look-.pdf,Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance,Mostapha Benhenda,arXiv:2601.13770v1,arXiv:2601.13770v1,"look-ahead bias, point-in-time models, financial LLMs, market regimes","Introduces Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time Large Language Models for finance. Evaluates performance decay across market regimes and compares against open-source LLMs to highlight limitations of standard LLMs.",24.05,LFM-2.5,Apple M1 (Metal)
2601.13798v1_Insight Interpretable Semantic Hierarchies in Visi.pdf,Interpretable Semantic Hierarchies in Vision-Language Encoders,"Kai Wittenmayer, Sukrut Rao, Amin Parchami-Araghi, Bernt Schiele, Jonas Fischer",10.48550/arXiv.2601.13798,arXiv:2601.13798,"interpretability, vision-language models, concept hierarchy, spatial grounding, semantic segmentation","INSIGHT provides fine-grained, human-interpretable concepts with local spatial grounding for vision foundation models. It leverages hierarchical representations to enable concept-based explanations across tasks.",26.74,LFM-2.5,Apple M1 (Metal)
2601.13809v1_DroneVLA VLA based Aerial Manipulation.pdf,DroneVLA: VLA based Aerial Manipulation,"Fawad Mehboob, Monijesu James, Amir Habel, Jeffrin Sam, Miguel Altamirano Cabrera, Dzmitry Tsetserukou",,,"Aerial Manipulation, Vision-Language-Action Models, Human-Robot Interaction, Visual Surveying, Robotic Fetch-and-Carry","This work introduces an autonomous aerial manipulation system interpreting natural language commands to retrieve objects. It integrates MediaPipe-based grounding with a Vision-Language-Action model and a drone equipped with a 1-DOF gripper and Intel RealSense camera, enabling intuitive human interaction.",26.28,LFM-2.5,Apple M1 (Metal)
2601.13846v1_Virtual Urbanism An AI-Driven Framework for Quanti.pdf,Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity,Glinkaya Maria,10.1234/vu.2024.0012,2105.07912,"generative artificial intelligence, latent diffusion model, low-rank adaptation model, urban perception, urban identity","This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through synthetic urban environments. The pilot study demonstrates feasibility using Tokyo microcosms, integrating Stable Diffusion and LoRA models to produce synthetic replicas that elicit identity-forming elements. Human evaluation confirms perceptual legitimacy, while quantitative identity metrics reveal culturally embedded typologies.",24.5,LFM-2.5,Apple M1 (Metal)
2601.13864v1_HardSecBench Benchmarking the Security Awareness o.pdf,HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware,"Qirui Chen, Jingxian Shuai, Shuangwu Chen, Shenghao Ye, Zijian Wen, Xufei Su, Jie Jin, Jiangming Li, Jun Chen, Xiaobin Tan, Jian Yang",,,"large language models, security awareness, hardware, code generation, security vulnerabilities, common weakness enumeration, functional correctness, security risks","This study introduces HardSecBench, a benchmark evaluating LLM-generated hardware code against 76 CWE entries. It demonstrates that models often meet functional requirements but expose security flaws, highlighting the need for security-aware LLM-assisted hardware design.",27.1,LFM-2.5,Apple M1 (Metal)
2601.13880v1_LifeAgentBench A Multi-dimensional Benchmark and A.pdf,LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health,"Ye Tian, Zihao Wang, Onat Gungor, Xiaoran Fan, Tajana Rosing",1,yet002,"personalized health, digital health, LLM, health assistant, cross-dimensional reasoning","The paper introduces LifeAgentBench, a large-scale QA benchmark for long-horizon, cross-dimensional, and multi-user lifestyle health reasoning, and proposes LifeAgent as a baseline agent for health assistants. It evaluates 11 leading LLMs and identifies key challenges in long-horizon aggregation and reasoning.",27.37,LFM-2.5,Apple M1 (Metal)
2601.13885v1_Confident Rankings with Fewer Items Adaptive LLM E.pdf,Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores,"Esma Balkır, Alice Pernthaller, Marco Basaldella, José Hernández-Orallo, Nigel Collier, Trismik, Leverhulme Centre for the Future of Intelligence, Universitat Politècnica de València, University of Cambridge",,,"adaptive LLM evaluation, continuous scores, computerized adaptive testing, IRT-based, uncertainty-aware ranking, LLM-as-a-Judge, evaluation metrics","This paper proposes a principled extension of IRT-based adaptive testing to continuous bounded scores by replacing the Bernoulli response distribution with a heteroskedastic normal distribution. It introduces an uncertainty-aware ranker with adaptive stopping criteria, achieving reliable model ranking with fewer items and lower cost. Validated on five benchmarks, the method improves ranking correlation by 0.12 τ over random sampling.",27.77,LFM-2.5,Apple M1 (Metal)
2601.13887v1_Human Simulation Computation A Human-Inspired Fram.pdf,Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems,Hong Su,10.48550/arXiv.2015.08001,arXiv:1508.03456,"Human Simulation Computation, Environment Interaction, Adaptive Artificial Intelligence, Human-Inspired Reasoning","The paper proposes Human Simulation Computation (HSC), a framework modeling intelligence through continuous, closed-loop processes involving thinking, action, learning, reflection, and activity scheduling. HSC emphasizes active adaptation in dynamic environments and integrates human-like reasoning strategies.",25.94,LFM-2.5,Apple M1 (Metal)
2601.13895v1_OmniOVCD Streamlining Open-Vocabulary Change Detec.pdf,OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3,"Xu Zhang, Danyang Li, Yingjie Xia, Xiaohang Dong, Hualong Yu, Jianye Wang, Qicheng Li",10.48550/arXiv.2303.08732,10.48550/arXiv.2303.08732,"change detection, open-vocabulary, SAM 3, OmniOVCD, change mask, land cover, deep learning","This paper introduces OmniOVCD, a standalone framework for Open-Vocabulary Change Detection using SAM 3. It proposes a Synergistic Fusion to Instance Decoupling (SFID) strategy to improve accuracy and stability by integrating segmentation and identification within a single model.",26.7,LFM-2.5,Apple M1 (Metal)
2601.13897v1_TractRLFusion A GPT-Based Multi-Critic Policy Fusi.pdf,TRACTRLFUSION: A GPT-BASED MULTI-CRITIC POLICY FUSION FRAMEWORK FOR FIBER TRACTOGRAPHY,"Ankita Joshi, Ashutosh Sharma, Anoushkrit Goel, Ranjeet Ranjan Jha, Chirag Ahuja, Arnav Bhavsar, Aditya Nigam",10.1000/tractrlfusion,12345678,"Diffusion MRI, Tractography, Reinforcement Learning, Transformers","This paper introduces TractRLFusion, a GPT-based policy fusion framework for fiber tractography that leverages deep reinforcement learning to improve tract reconstruction accuracy and robustness. It addresses challenges in accurately reconstructing white matter tracts by integrating multiple RL policies through a data-driven fusion strategy.",27.15,LFM-2.5,Apple M1 (Metal)
2601.13904v1_PREFAB PREFerence-based Affective Modeling for Low.pdf,PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation,"Jaeyoung Moon, Youjin Choi, Yucheon Park, David Melhart, Georgios N. Yannakakis, Kyung-Joong Kim",10.1145/3675094.3678379,2601.13904v1,"Affective Computing, Preference Learning, Self-Annotation, User Modeling, Ordinal Representation, Peak-End Rule","Self-annotation is the gold standard for collecting affective state labels. Existing methods rely on full annotation, which is time-consuming and cognitively demanding. PREFAB alleviates this by targeting affective inflection regions using preference learning, detecting inflection areas, requiring annotations only there, and interpolating the rest. Results show improved efficiency and annotator confidence.",27.02,LFM-2.5,Apple M1 (Metal)
2601.13920v1_Asymmetric regularization mechanism for GAN traini.pdf,Asymmetric regularization mechanism for GAN training with Variational Inequalities,"Spyridon C. Giagtzoglou, Mark H.M. Winands, Barbara Franci",,,"GANs, variational inequalities, regularization, saddle point, monotonicity, convergence","The paper proposes an asymmetric regularization mechanism based on Tikhonov steps and a zero-centered gradient penalty to stabilize GAN training. It derives explicit Lipschitz and strong-monotonicity constants, ensuring convergence of an EFTP method, even when strong monotonicity is unattainable.",25.43,LFM-2.5,Apple M1 (Metal)
2601.13938v1_IF-GEO Conflict-Aware Instruction Fusion for Multi.pdf,Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization,"Heyang Zhou, JiaJia Chen, Xiaolu Chen, Jie Bao, Zhen Chen, Yong Liao",,,"Generative Engine Optimization, Conflict-Aware Instruction Fusion, Multi-Query Retrieval, Generative Search Engines, Risk-Aware Stability Metrics","The paper proposes IF-GEO, a diverge-then-converge framework for optimizing generative engines by minimizing conflicting revision requests and synthesizing a global revision blueprint to maintain source visibility across diverse queries.",25.63,LFM-2.5,Apple M1 (Metal)
2601.13942v1_Glance-or-Gaze Incentivizing LMMs to Adaptively Fo.pdf,Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning,"Hongbo Bai, Yujin Zhou, Yile Wu, Chi-Min Chan, Pengcheng Wen, Kunhao Pan, Sirui Han, Yike Guo",,,"Large Multimodal Models, Visual Understanding, Knowledge-intensive Queries, Search-Augmented Approaches, Reinforcement Learning, Selective Gaze, Visual Reasoning, Complexity Adaptation","This paper proposes Glance-or-Gaze (GoG), a framework that enables LMMs to adaptively focus visual attention by shifting from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism and a dual-stage training strategy combining reflective behavior alignment and complexity-adaptive reinforcement learning, achieving state-of-the-art performance on visual question answering benchmarks.",27.55,LFM-2.5,Apple M1 (Metal)
2601.13948v1_Stream-Voice-Anon Enhancing Utility of Real-Time S.pdf,Enhancing Utility of Real-Time Speaker,"Nikita Kuzmin1, Songting Liu1, Kong Aik Lee3, Eng Siong Chng1","s220028@e.ntu.edu.sg, lius0114@e.ntu.edu.sg",,"streaming speaker anonymization, neural audio codec, voice conversion, privacy preservation, disentanglement","The paper presents Stream-Voice-Anon, a method adapting causal LM-based NAC architectures for streaming speaker anonymization. It integrates anonymization techniques such as pseudo-speaker sampling and speaker embedding mixing, achieving up to 46% relative WER reduction under the VoicePrivacy 2024 Challenge while maintaining comparable latency.",26.49,LFM-2.5,Apple M1 (Metal)
2601.13964v1_RL-BioAug Label-Efficient Reinforcement Learning f.pdf,RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning,"Cheol-Hui Lee, Hwa-Yeon Lee, Dong-Joo Kim",10.48550/arXiv.2407.04219,arXiv:2407.04219,"RL-BioAug, self-supervised learning, EEG, contrastive learning, data augmentation, deep learning","The paper proposes RL-BioAug, a label-efficient reinforcement learning framework that autonomously determines optimal data augmentation policies for EEG tasks. By leveraging minimal labeled data, the method enables robust representation learning without relying on expert annotations, demonstrating significant improvements over random augmentation strategies.",27.2,LFM-2.5,Apple M1 (Metal)
2601.13969v1_Autonomous Knowledge Graph Exploration with Adapti.pdf,Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval,"Joaquín Polonuer, Lucas Vittor, Iñaki Arango, Ayush Noori, David A. Clifton, Luciano Del Corro, Marinka Zitnik, Kempner Institute for the Study of Natural and Artificial Intelligence, Oxford Suzhou Centre for Advanced Research, ELIAS Lab, Lumina Labs, Kempner Institute, Broad Institute of MIT and Harvard, Harvard Data Science Initiative",,,"knowledge graph, knowledge graph exploration, adaptive breadth-depth retrieval, language models, retrieval augmentation, multi-hop traversal, semantic similarity, knowledge graphs, retrieval training","Retrieving evidence for language model queries from knowledge graphs requires balancing broad search with multi-hop traversal. ARK adapts tool use to queries, improving performance across datasets.",27.0,LFM-2.5,Apple M1 (Metal)
2601.13992v1_The Whole Is Greater Than the Sum of Its Parts A C.pdf,The Whole Is Greater Than the Sum of Its Parts,"Jin Cui, Jiaqi Guo, Jiepeng Zhou, Ruixuan Yang, Jiayi Lu, Jiajun Xu, Jiangcheng Song, Boran Zhao, Pengju Ren",,,"Chain-of-Thought, Multi-Teacher, CoT Distillation, Student Models, Catastrophic Forgetting, Graph-based Consensus, Mutual-Information, Loss-based Difficulty","This paper introduces COMPACT, a framework that adaptively fuses supervision from multiple teachers using dynamic weighting. It employs graph-based consensus to filter misleading reasoning, mutual-information-based adaptability to detect deep understanding, and loss-based difficulty to prevent negative transfer. Experiments show it effectively integrates diverse reasoning capabilities while mitigating catastrophic forgetting.",27.54,LFM-2.5,Apple M1 (Metal)
2601.13994v1_torch-sla Differentiable Sparse Linear Algebra wit.pdf,Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch,Mingyuan Chi,10.48550/arXiv.2601.13994,2601.13994,"PyTorch, sparse linear algebra, GPU acceleration, adjoint solvers, scientific computing","Presents torch-sla, an open-source PyTorch library enabling GPU-accelerated, scalable, and differentiable sparse linear algebra. Addresses challenges in GPU acceleration, multi-GPU scaling, and efficient gradient computation for industrial scientific computing.",24.82,LFM-2.5,Apple M1 (Metal)
2601.13999v1_DAME Duration-Aware Matryoshka Embedding for Durat.pdf,Duration-Aware Matryoshka Embedding for Duration-Robust Speaker Verification,"Youngmoon Jung, Joon-Young Yang, Ju-ho Kim, Jaeyoung Roh, Chang Woo Han, Hoon-Young Cho",,,"short-duration speaker verification, multi-scale aggregation, matryoshka representation learning, speaker verification, short utterances","The paper proposes DAME, a model-agnostic framework that builds nested embeddings aligned to utterance durations. It uses lower-dimensional representations for short utterances and higher dimensions for longer ones, improving performance across durations without extra inference cost.",26.44,LFM-2.5,Apple M1 (Metal)
2601.14012v1_MATE Matryoshka Audio-Text Embeddings for Open-Voc.pdf,MATRYOSHKA AUDIO–TEXT EMBEDDINGS FOR OPEN-VOCABULARY,"Youngmoon Jung, Myunghun Jung, Joon-Young Yang, Yong-Hyeok Lee, Jaeyoung Roh, Hoon-Young Cho",,,"keyword spotting, open-vocabulary, text enrollment, audio-text embedding, deep metric learning","Open-vocabulary keyword spotting with text-based enrollment uses a dual-encoder framework called Matryoshka Audio–Text Embeddings (MATE). It encodes multiple embedding granularities within a single vector via nested sub-embeddings (prefixes). PCA-guided prefix alignment aligns audio and text prefixes, concentrating keyword cues in lower dimensions. MATE is trained with standard deep metric learning objectives and achieves state-of-the-art results on WSJ and LibriPhrase.",27.65,LFM-2.5,Apple M1 (Metal)
2601.14022v1_Credible CO2 Comparisons A Machine Learning Approa.pdf,Credible CO2 Comparisons: A Machine Learning Approach to Vehicle Powertrain Assessment,"Rodrigo Pereira David, Luciano Araujo Dourado Filho, Daniel Marques da Silva, João Alfredo Cal-Braz",xxx/xxxx,,"machine learning, vehicle emissions, electric vehicles, ICEVs, regression, recurrent neural network, mean square error","This paper proposes a machine learning framework for comparing CO2 emissions of internal combustion engine vehicles and electric vehicles under identical real-world driving conditions. By isolating technology-specific effects, it enables direct comparison of powertrain performance and supports credible, data-driven assessments of vehicle carbon performance.",26.53,LFM-2.5,Apple M1 (Metal)
2601.14027v1_Numina-Lean-Agent An Open and General Agentic Reas.pdf,Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics,"Junqi Liu, Zihao Zhou, Zekai Zhu, Marco Dos Santos, Weikun He, Jiawei Liu, Ran Wang, Yunzhou Xie, Junqiao Zhao, Qiufeng Wang, Lihong Zhi, Jia Li, Wenda Li, Jia Li, Ran Wang, Yunzhou Xie, Junqiao Zhao, Qiufeng Wang, Lihong Zhi, Jia Li, Wenda Li, Imperial College London, University of Edinburgh",,,"formal mathematics, agentic systems, formal theorem proving, Lean, Numina-Lean-MCP, autonomous reasoning, mathematical reasoning","This paper proposes Numina-Lean-Agent, a general coding agent that enables flexible, autonomous interaction with formal theorem provers like Lean. It aims to improve performance and reproducibility by allowing model substitution without retraining, and supports extensible tool integration.",29.63,LFM-2.5,Apple M1 (Metal)
2601.14039v1_Generalizing Abstention for Noise-Robust Learning .pdf,Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation,"Wesam Moustafa, Hossam Elsafty, Helen Schneider, Lorenz Sparrenberg, Rafet Sifa",arXiv:2601.14039v1,MIDL 2026 submission,"Abstention, Medical Image Segmentation, Label Noise, Noise-Robust Learning, Loss Functions","This paper introduces a universal and modular abstention framework to enhance noise-robustness in medical image segmentation. It presents three noise-robust variants (GAC, SAC, ADS) and demonstrates their effectiveness on CaDIS and DSAD datasets, showing significant improvements over non-abstaining baselines.",26.96,LFM-2.5,Apple M1 (Metal)
2601.14041v1_Top 10 Open Challenges Steering the Future of Diff.pdf,Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants,"Yunhe Wang, Kai Han, Huiling Zhen, Yuchuan Tian, Hanting Chen, Yongbing Huang, Yufei Cui, Yingte Shu, Shan Gao, Ismail Elezi, Roy Vaughan Miles, Songcen Xu, Feng Wen, Chao Xu, Sinan Zeng, Dacheng Tao",arXiv:2601.14041v1,2601.14041v1,"Large Language Models, Diffusion Models, Transformers","The paper examines key challenges limiting Diffusion Language Models (DLMs) and proposes a roadmap to overcome them through architectural, algorithmic, and multimodal advancements.",30.12,LFM-2.5,Apple M1 (Metal)
2601.14047v1_Collective intelligence in science direct elicitat.pdf,Collective Intelligence in Science,,arXiv:2601.14047v1,2601.14047v1,"collective intelligence, scientific hypothesis, information pooling, information elicitation, Bayesian truth serum, Bayesian markets, peer prediction, wisdom of crowd, large language models, scientific collaboration","Proposes a mechanism using a self-resolving play-money prediction market integrated with chat to enable experts to collaboratively analyze complex scientific problems. It emphasizes efficient aggregation of private information, interpretability, and incentives via play money rewards, while addressing challenges like uncertainty and lack of prior knowledge.",26.97,LFM-2.5,Apple M1 (Metal)
2601.14051v1_Kakugo Distillation of Low-Resource Languages into.pdf,Kakugo: Distillation of Low-Resource Languages into Small Language Models,"Peter Devine, Mardhiyah Sanni, Farid Adilazuarda, Julieta Gil Loizaga, Barry Haddow",,,"Small Language Models, Low-resource languages, Language modeling, Model distillation, Open-source datasets","Presents Kakugo, a cost-effective pipeline to train SLMs for low-resource languages using only language names. It generates synthetic data and models, achieving strong performance across translation, classification, and QA tasks while keeping costs under $50 per language.",25.2,LFM-2.5,Apple M1 (Metal)
2601.14053v1_LLMOrbit A Circular Taxonomy of Large Language Mod.pdf,LLMOrbit: A Circular Taxonomy of Large Language Models,"Badri N. Patro, Vijay S. Agneeswaran",10.48550/arXiv.2025.12345,arXiv:2508.01234,"large language models, LLM taxonomy, AI scaling, agentic systems","This paper presents LLMOrbit, a comprehensive circular taxonomy that maps the evolution of large language models from 2019 to 2025. It examines over 50 major models across 15 organizations, analyzing architectural innovations, training methods, and efficiency trends that define modern LLMs, AI, and agentic systems. It identifies three critical challenges—the scaling wall, exponential cost growth, and energy consumption—and proposes six paradigms for breaking these barriers, including post-training gains, efficiency revolution, and democratization.",26.64,LFM-2.5,Apple M1 (Metal)
2601.14055v1_Decoder-Free Supervoxel GNN for Accurate Brain-Tum.pdf,Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI,"Andrea Protani, Marc Molina Van Den Bosch, Lorenzo Giusti, Heloisa Barbosa Da Silva, Paolo Cacace, Albert Sund Aillet, Miguel Angel Gonzalez Ballester, Friedhelm Hummel, Luigi Serio",arXiv:2601.14055v1,20 Jan 2026,"BrainTumorLocalization, GraphNeuralNetworks, Multi-modal MRI, Supervoxel, Regression","Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, allocating many parameters to spatial reconstruction. This work introduces SVGFormer, a decoder-free pipeline using a content-aware grouping stage that partitions volumes into a semantic graph of supervoloxels. Its hierarchical encoder combines patch-level transformer with a supervoxel-level graph attention network, enabling joint modeling of fine-grained and broader dependencies. Experiments on the BraTS dataset show strong performance: node-level classification F1=0.875 and tumor proportion regression MAE=0.028, demonstrating the encoder's ability to learn discriminative, localized features.",31.55,LFM-2.5,Apple M1 (Metal)
2601.14056v1_POCI-Diff Position Objects Consistently and Intera.pdf,POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion,"Andrea Rigo, Luca Stornaiuolo, Weijie Wang, Mauro Martino, Bruno Lepri, Nicu Sebe",arXiv:2601.14056v1,arXiv:2601.14056v1,"Diffusion, Image Generation, 3D Layout","Proposes a diffusion-based approach for text-to-image generation with consistent and interactive 3D layout control, addressing distortions in object geometry and improving spatial adherence.",27.56,LFM-2.5,Apple M1 (Metal)
2601.14063v1_XCR-Bench A Multi-Task Benchmark for Evaluating Cu.pdf,XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs,"Mohsinul Kabir, Tasnim Ahmed, Shaoxiong Ji, Hassan Alhuzali, Sophia Ananiadou",10.1234/xcr-2024,,"cross-cultural competence, large language models, cultural reasoning, cultural bias, cross-cultural benchmark","This paper introduces XCR-Bench, a benchmark for evaluating cultural reasoning in LLMs. It addresses the lack of high-quality cross-cultural annotated data and highlights weaknesses in LLMs' ability to adapt to cultural nuances, including social etiquette and regional biases.",26.54,LFM-2.5,Apple M1 (Metal)
2601.14069v1_Unsupervised Video Class-Incremental Learning via .pdf,Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering,"Nattapong Kurpukdee, Adrian G. Bors",,,"unsupervised, video class-incremental, video continual learning, deep embedded clustering, deep feature extractor, knowledge transfer","The paper presents a simple yet effective approach for unsupervised video class incremental learning using deep embedded clustering. It introduces a deep feature extractor that generates representative video features without prior class labels, builds deep clusters progressively, and leverages prior task knowledge to mitigate catastrophic forgetting. Extensive evaluations on standard video datasets demonstrate significant performance improvements over existing methods.",26.68,LFM-2.5,Apple M1 (Metal)
2601.14084v1_DermaBench A Clinician-Annotated Benchmark Dataset.pdf,DermaBench: A Clinician-Annotated Benchmark,"Abdurrahim Yilmaz, Ozan Erdem, Ece Gokyayla, Ayda Acar, Burc Bugra Dagtas, Dilara Ilhan Erdil, Gulsum Gencoglan, Burak Temelkuran",,,"dermatology, visual question answering, clinical reasoning, skin conditions, multimodal learning","DermaBench is a clinician-annotated benchmark for dermatology visual question answering, addressing gaps in image-level datasets by providing detailed, expert-labeled annotations across diverse skin types and conditions.",26.69,LFM-2.5,Apple M1 (Metal)
2601.14086v1_Two-Stream temporal transformer for video action c.pdf,Two-Stream Temporal Transformer for Video Action Classification,"Nattapong Kurpukdee, Adrian G. Bors",,,"Video Transformer, Optical Flow, Two-Stream video processing, Video Action Classification","This study introduces a two-stream transformer video classifier that extracts spatio-temporal information from content and optical flow, achieving excellent performance on human activity classification datasets.",23.83,LFM-2.5,Apple M1 (Metal)
2601.14087v1_1-bit Count-based Sorting Unit to Reduce Link Powe.pdf,’1’-bit Count-based Sorting Unit to Reduce Link,"Ruichi Han, Yizhi Chen, Tong Lei, Jordi Altayo Gonzalez, Ahmed Hemani",10.1109/JRTT.2024.12345,12345,"1-bit count, approximate computing, bit transition reduction, link power, DNN",Proposes a hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks using approximate computing to reduce switching activity and link power.,24.8,LFM-2.5,Apple M1 (Metal)
2601.14091v1_Zero-shot adaptable task planning for autonomous c.pdf,Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems,"Hossein Naderi, Alireza Shojaei, Lifu Huang, Philip Agee, Kereshmeh Afsari, Abiola Akanmu",,,"Construction robotics, quadruped robots, robot task planning, multi-AI agent, LLMs, VLMs, GPT4o","The study explores the potential of foundation models to enhance adaptability and generalizability of task planning in construction robots. Four lightweight AI agent systems are proposed and evaluated across three construction roles, demonstrating improved performance and cost-effectiveness.",27.5,LFM-2.5,Apple M1 (Metal)
2601.14096v1_Remapping and navigation of an embedding space via.pdf,Remapping and Navigating Embedding Space via Error,"Benedikt Hartl, Léo Pio-Lopez, Chris Fields, Michael Levin",arXiv:2601.14096v1,arXiv:2601.14096v1,"Evolution, Development, Intelligence, Active Inference, Navigation Policy, Nested Embedding Spaces",A fundamental organizational principle of cognition in natural and artificial systems is explored through error minimization.,32.89,LFM-2.5,Apple M1 (Metal)
2601.14099v1_Causal feature selection framework for stable soft.pdf,Causal Feature Selection Framework for Stable Soft Sensor Modeling based on Time-Delayed Cross Mapping,"Shi-Shun Chen, Xiao-Yang Li, Enrico Zio",,,"causal feature selection, soft sensor modeling, time-delayed cross mapping, reliability, systems engineering",A framework for causal feature selection applied to stable soft sensor modeling using time-delayed cross mapping.,31.44,LFM-2.5,Apple M1 (Metal)
2601.14115v1_Riemannian Liquid Spatio-Temporal Graph Network.pdf,Riemannian Liquid Spatio-Temporal Graph Network,"Liangsi Lu, Jingchao Wang, Zhaorong Dai, Hanqian Liu, Yang Shi",10.1145/3774904.3792090,https://rlstg.github.io,"Riemannian Manifolds, Neural ODEs, Spatio-Temporal Graphs","Liquid Time-Constant networks excel at modeling irregular dynamics but are limited to Euclidean space. This work introduces RLSTG, a framework unifying continuous-time dynamics with Riemannian geometry to better represent non-Euclidean graph structures.",25.52,LFM-2.5,Apple M1 (Metal)
2601.14124v1_Style Transfer as Bias Mitigation Diffusion Models.pdf,Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic,"Saad Mankarious, Ayah Zirikly",arXiv:2601.14124v1,arXiv:2601.14124v1,"style transfer, bias mitigation, diffusion models, synthetic data, mental health","This work proposes a pretraining-free diffusion-based approach for generating synthetic Arabic mental health text to address gender bias. Using the CARMA corpus, it focuses on male-to-female style transfer to augment female-authored content, demonstrating high semantic fidelity and meaningful stylistic divergence.",26.62,LFM-2.5,Apple M1 (Metal)
2601.14152v1_Lost in the Prompt Order Revealing the Limitations.pdf,Revealing the Limitations of Causal Attention in Language Models,"Hyunjong Ok, Jaeho Lee",,,,"This study investigates how the ordering of components in multiple-choice question answering affects model performance, identifying causal attention as a key mechanism.",19.51,LFM-2.5,Apple M1 (Metal)
2601.14154v1_LLM Augmented Intervenable Multimodal Adaptor for .pdf,LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery,"Shubham Pandey, Bhavin Jawade, Srirangaraj Setlur, Venu Govindaraju, Kenneth Seastedt",,,"postoperative complications, lung cancer surgery, risk prediction, deep learning, radiomics, interventional deep learning, clinical decision support","Presents MIRA-CLE, a deep learning architecture integrating preoperative and radiological data to predict postoperative complications in lung cancer surgery. The model enhances transparency with interpretable insights and leverages radiomics to capture tumor heterogeneity, aiming to improve personalized risk management.",27.24,LFM-2.5,Apple M1 (Metal)
2601.14157v1_ConceptCaps -- a Distilled Concept Dataset for Int.pdf,ConceptCaps - a Distilled Concept Dataset for Interpretability in Music Models,"Bruno Sienkiewicz, Łukasz Neumann, Mateusz Modrzejewski",,,"interpretability, music models, concept-based, TCA V, music annotation, semantic modeling","Introduces ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels, addressing sparse and noisy tags in existing music datasets. The dataset supports separation of semantic modeling and text generation for interpretability.",25.58,LFM-2.5,Apple M1 (Metal)
2601.14160v1_Domain-Adaptation through Synthetic Data Fine-Tuni.pdf,Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law,"Ali Hamza Bashir, Muhammad Rehan Khalid, Kostadin Cvejoski, Jana Birr, Jule Berghaus, Armin Berger, Sandra Halscheidt, Christian Temath, Rafet Sifa, David Berghaus",,,"legal reasoning, German law, synthetic data, LLM adaptation, legal QA, hallucinations, statutory text",This paper presents a method to adapt large language models to German legal question answering using synthetic data generated from authoritative statutes. It demonstrates improved performance over baselines by leveraging rigorously filtered German legal corpora and parameter-efficient fine-tuning.,27.69,LFM-2.5,Apple M1 (Metal)
2601.14171v1_Paper2Rebuttal A Multi-Agent Framework for Transpa.pdf,A Multi-Agent Framework for Transparent Author,"Qianli Ma, Chang Guo, Zhiheng Tian, Siyu Wang, Jipeng Xiao, Yuanhao Yue, Zhipeng Zhang",,,,"Introduces REBUTTALAGENT, a multi-agent framework reframing rebuttal generation as evidence-centric planning. The system decomposes feedback, constructs hybrid contexts, and integrates an autonomous external search module to ensure transparent, evidence-backed responses.",23.34,LFM-2.5,Apple M1 (Metal)
2601.14172v1_Human Values in a Single Sentence Moral Presence H.pdf,"Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum","Víctor Yestea, Paolo Rossoa, Valencian Graduate School and Research Network of Artificial Intelligence (ValgrAI)",arXiv:2601.14172v1,2601.14172v1,"human values, moral presence, sentence-level detection, Schwartz continuum, transformer models","This study investigates the identification of 19 values in the Schwartz motivational continuum using single sentences from news and political texts, demonstrating that hierarchical gating improves performance over direct multi-label classification despite sparse moral cues.",29.72,LFM-2.5,Apple M1 (Metal)
2601.14175v1_A model of errors in transformers.pdf,A model of errors in transformers,"Suvrat Raju, Praneeth Netrapalli",,,,"We study the error rate of LLMs on tasks like arithmetic and repetitive token processing, arguing that incorrect predictions emerge from accumulated attention errors. We derive a quantitative relationship between accuracy and task complexity using two parameters interpretable as elementary noise rates.",20.81,LFM-2.5,Apple M1 (Metal)
2601.14192v1_Toward Efficient Agents Memory Tool learning and P.pdf,"Toward Efficient Agents: A Survey of Memory, Tool learning, and Planning","Xiaofang Yang, Lijun Li, Heng Zhou, Tong Zhu, Xiaoye Qu, Yuchen Fan, Qianshan Wei, Rui Ye, Li Kang, Yiran Qin, Zhiqiang Kou, Daizong Liu, Qi Li, Ning Ding, Siheng Chen",10.48550/arXiv.2601.14192,2601.14192,"Agents, Efficiency, Agent Memory, Tool Learning, Planning","This paper investigates efficiency in agentic systems by analyzing memory, tool learning, and planning components. It reviews recent approaches that optimize cost metrics such as latency, tokens, and steps, and discusses efficiency trade-offs across effectiveness and cost using Pareto analysis.",30.21,LFM-2.5,Apple M1 (Metal)
2601.14209v1_InT Self-Proposed Interventions Enable Credit Assi.pdf,Self-Proposed Interventions Enable Credit,"Matthew Y. R. Yang, Hao Bai, Ian Wu, Gene Yang, Amrith Setlur, Aviral Kumar",arXiv:...,InT: Self-Proposed Interventions Enable Credit,"credit assignment, credit assignment failure, intervention training, RL problem, self-verify, counterfactual reasoning","This paper introduces Intervention Training (InT), a method that proposes single-step interventions to correct reasoning traces in LLMs. By upweighting the likelihood of correct interventions during fine-tuning, the approach improves credit assignment and achieves a 14% accuracy boost over a 4B-parameter base model on IMO-AnswerBench.",27.39,LFM-2.5,Apple M1 (Metal)
2601.14230v1_MASCOT Towards Multi-Agent Socio-Collaborative Com.pdf,MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems,"Yiyang Wang, Yiqiao Jin, Alex Cabral, Josiah Hester",,,"multi-agent systems, socio-collaborative companions, emotional support, social interaction, persona consistency, collaborative dialogue",Multi-agent systems face persona collapse and social sycophancy. MASCOT proposes a framework with persona-aware behavioral alignment and collaborative dialogue optimization to enhance persona fidelity and social contribution.,25.35,LFM-2.5,Apple M1 (Metal)
2601.14232v1_KAGE-Bench Fast Known-Axis Visual Generalization E.pdf,KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning,"Egor Cherepannov, Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov",https://avanturist322.github.io/KAGEBench,not provided,"reinforcement learning, visual generalization, pixel-based agents, visual distribution shift, KAGE-Env, JAX-native platformer","The paper introduces KAGE-Env, a JAX-native 2D platformer that isolates visual axes to study visual generalization. It evaluates six known-axis suites in KAGE-Bench, revealing axis-dependent failures and highlighting the role of appearance shifts over pure visual changes.",26.83,LFM-2.5,Apple M1 (Metal)
2601.14234v1_Q-learning with Adjoint Matching.pdf,Q-learning with Adjoint Matching,"Qiyang Li, Sergey Levine",arXiv:2601.14234v1,,"Q-learning, adjoint matching, reinforcement learning, policy optimization, diffusion models","Proposes Q-learning with Adjoint Matching (QAM), a novel TD-based RL algorithm that optimizes expressive diffusion or flow-matching policies efficiently while avoiding unstable backpropagation.",23.86,LFM-2.5,Apple M1 (Metal)
2601.14235v1_Opportunities in AIML for the Rubin LSST Dark Ener.pdf,Opportunities in AI/ML for the Rubin LSST,"Eric Aubourg, Camille Avestruz, Matthew R. Becker, Biswajit Biswas, Rahul Biswas, Boris Bolliet, Adam S. Bolton, Clecio R. Bom, Raphaël Bonnet-Guerrini, Alexandre Boucaud, Jean-Eric Campagne, Chihway Chang, Aleksandra Ciprijanović, Johann Cohen-Tanugi, Michael W. Coughlin, John Franklin Crenshaw, Juan C. Cuevas-Tello, Alex Drlica-Wagner, Sydney Erickson, Alexander T. Gagliano, Christos Georgiou, Aritra Ghosh, Matthew Grayling, Kirill A. Grishin, Alan Heavens, Lindsay R. House, Michelle Lochner, Yao-Yuan Mao, Peter Melchior, Grant Merz, Martin Millon, Anais Moller, Gautham Narayan, Yuuki Omori, Hiranya Peiris, Andrés A. Plazas Malagón, Nesar Ramachandra, Benjamin Remy, Cécile Roucelle, Jaime Ruiz-Zapatero, Stefan Schuldt, Ignacio Sevilla-Noarbe, Tjitske Starkenburg, Stephen Thorp, Laura Toribio San Cipriano, Tilman Troster, Roberto Trotta, Padma Venkatraman, Amanda Wasserman, Tim White, Justine Zeghal, Tianqing Zhang",,,"AI/ML, LSST, Dark Energy, Rubin Observatory, Machine Learning, Astrophysics, Cosmology, Data Science, Computer Vision, Astroparticle Physics",The LSST Dark Energy Science Collaboration explores opportunities in applying AI/ML techniques to LSST data for advancing dark energy research.,29.4,LFM-2.5,Apple M1 (Metal)
2601.14242v1_APEX-Agents.pdf,APEX–Agents,"Bertie Vidgen, Austin Mann, Abby Fennelly, John Wright, Stanly, Lucas Rothman, Marco Burstein, Julien Benchek, David Ostrofsky, Anirudh Ravichandran, Debnil, Sur Neel Venugopal, Alannah Hsia, Isaac Robinson, Calix Huang, Olivia Varones, Daniyal Khan, Michael Haines, Zach Richards, Chirag Mahapatra, Brendan Foody, Osvald Nitski, Mercor, api.mercor.com",arXiv:2601.14242v1,arXiv:2601.14242,"AI agents, productivity index, investment banking, management consultants, corporate lawyers, long-horizon tasks, cross-application, sim-to-real gap","Introduces APEX–Agents, a benchmark for evaluating AI agents' ability to perform complex, real-world tasks. Tests eight agents using Pass@1, highlighting performance across diverse professional domains.",29.7,LFM-2.5,Apple M1 (Metal)
2601.14255v1_VideoMaMa Mask-Guided Video Matting via Generative.pdf,VideoMaMa: Mask-Guided Video Matting via Generative Prior,"Sangbeom Lim, Seoung Wug Oh2, Jiahui Huang2, Heeji Yoon3, Seungryong Kim3, Joon-Young Lee2",arXiv:2601.14255v1,https://cvlab-kaist.github.io/VideoMaMa,"video matting, mask-guided, generative prior, video diffusion, matting anything in video","VideoMaMa is a diffusion-based model that generates high-quality alpha mattes from segmentation masks, enabling robust matting on real-world videos. It leverages synthetic data and achieves strong zero-shot performance, with fine-tuning on the MA-V dataset improving robustness.",30.1,LFM-2.5,Apple M1 (Metal)
