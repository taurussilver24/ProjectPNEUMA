filename,title,authors,doi,arxiv_id,keywords,summary,tps,model,platform
2601.07192v1_Relink Constructing Query-Driven Evidence Graph On.pdf,Relink: Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG,"Manzong Huang, Chenyang Bu, Yi He, Xingrui Zhuo, Xindong Wu",Not found,Not found,"Graph-based Retrieval-Augmented Generation, Large Language Models, Knowledge Graph, Query-Driven Evidence Graph, Incompleteness, Distractor Facts","Graph-based Retrieval-Augmented Generation (GraphRAG) mitigates hallucinations in Large Language Models (LLMs) by grounding them in structured knowledge. However, current GraphRAG methods are constrained by a static, pre-constructed Knowledge Graph (KG) paradigm, which faces challenges of incomplete reasoning paths and misleading distractor facts. Relink proposes a reason-and-construct paradigm and a framework that dynamically builds a query-specific evidence graph, instantiating required facts from a latent relation pool and employing a unified, query-aware evaluation strategy to handle incompleteness and misleading facts, achieving significant improvements in open-domain question answering benchmarks.",27.85,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07197v1_Beyond Variance Knowledge-Aware LLM Compression vi.pdf,Beyond Variance: Knowledge-Aware LLM Compression via Fisher-Aligned Subspace Diagnostics,"Ibne Farabi Shihab*, Sanjeda Akter*, Anuj Sharma",,,"Large Language Models, Knowledge Compression, Transformer Architectures, Fisher Information Matrix, Activation-Gradient Coupling","Post-training activation compression is essential for deploying Large Language Models on resource-constrained hardware. Standard methods like Singular Value Decomposition (SVD) are gradient-blind, preserving high-variance dimensions regardless of their impact on factual knowledge preservation. This paper introduces Fisher-Aligned Subspace Compression (FASC), a knowledge-aware compression framework that selects subspaces by directly modeling activation-gradient coupling, minimizing a second-order surrogate of the loss function. FASC leverages the Fisher Information Matrix to identify dimensions critical for factual knowledge, often residing in low-variance but high-gradient-sensitivity subspaces. The Dependence Violation Score (ρ) is proposed as a general-purpose diagnostic metric to quantify activation-gradient coupling, revealing where factual knowledge is stored within transformer architectures. Extensive experiments on Mistral-7B and Llama-3-8B demonstrate that FASC preserves 6-8% more accuracy on knowledge-intensive benchmarks (MMLU, LAMA) compared to variance-based methods at 50% rank reduction, effectively enabling a 7B model to match the factual recall of a 13B uncompressed model. The analysis reveals that ρ serves as a fundamental signal of stored knowledge, with high-ρ layers emerging only when models internalize factual associations during training.",29.4,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07199v1_Forward versus Backward Comparing Reasoning Object.pdf,Forward versus Backward: Comparing Reasoning Objectives in Direct Preference Optimization,"Murtaza Nikazad, Raghuram Ramanujan",Not found,Not found,"Large language models, Reasoning, Direct Preference Optimization, Forward reasoning, Backward verification","This paper investigates the effect of training objective composition on reasoning reliability through Direct Preference Optimization, comparing forward chain-of-thought generation and backward verification training signals.",27.63,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07200v1_Safeguarding LLM Fine-tuning via Push-Pull Distrib.pdf,Safeguarding LLM Fine-tuning via Push-Pull Distributional Alignment,"Haozhong Wang, Zhuo Li, Yibo Yang, He Zhao, Hongyuan Zha, Dandan Guo",,,"Large Language Models, Fine-tuning, Safety Alignment, Optimal Transport, Push-Pull Mechanism, Data Distribution, Harmful Patterns","The paper introduces Safety Optimal Transport (SOT), a novel framework that reframes safe fine-tuning from an instance-level filtering challenge to a distribution-level alignment task grounded in Optimal Transport (OT). SOT optimizes sample importance by actively pulling the downstream distribution towards a trusted safe anchor while simultaneously pushing it away from a general harmful reference, establishing a robust geometric safety boundary that effectively purifies the training data. Extensive experiments demonstrate that SOT significantly enhances model safety while maintaining competitive downstream performance.",28.02,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07201v1_CalPro Prior-Aware Evidential--Conformal Predictio.pdf,CalPro: Prior-Aware Evidential–Conformal Prediction with Structure-Aware Guarantees for Protein Structures,"Ibne Farabi Shihab * 1, Sanjeda Akter * 1, Anuj Sharma 2",,2601.03457,"protein structure prediction, uncertainty quantification, conformal prediction, deep learning, graph-based architecture","A prior-aware evidential-conformal framework for shift-robust uncertainty quantification in protein structure prediction, combining geometric evidential head, differentiable conformal layer, and domain priors (disorder, flexibility). Empirically, it achieves ≤5% coverage degradation across modalities, reduces calibration error by 30–50%, and improves downstream ligand-docking success by 25%.",27.88,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07206v1_LLMRouterBench A Massive Benchmark and Unified Fra.pdf,LLMRouterBench: A Massive Benchmark and Unified Framework for LLM Routing,"Hao Li*, Yiqun Zhang*, Zhaoyan Guo*, Chenxu Wang*, Shengji Tang, Qiaosheng Zhang, Yang Chen, Biqing Qi, Peng Ye, Lei Bai, Zhen Wang†, Shuyue Hu†",Not found,Not found,"Large language models, routing, benchmark, evaluation, performance-cost trade-off","LLMRouterBench is a large-scale benchmark and unified framework for LLM routing, comprising over 400K instances from 21 datasets and 33 models. It provides comprehensive metrics for both performance-oriented and performance-cost trade-off routing, integrating 10 representative routing baselines. The framework confirms strong model complementarity and highlights the limitations of recent routing methods, particularly in reliable performance outperformance and persistent model-recall failures. It also demonstrates the impact of backbone embedding models and the diminishing returns of larger ensembles compared to careful model curation. The benchmark enables latency-aware analysis and is available at https://github.com/ynulihao/LLMRouterBench.",28.56,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07209v1_SIRR-LMM Single-image Reflection Removal via Large.pdf,SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model,"Yu Guo, Zhiqiang Lao, Xiyun Song, Yubin Zhou, Heather Yu",,,"reflection removal, single-image, large multimodal model, path tracing, 3D glass models, HDR environment maps","Introduces a synthetic dataset generation framework that path-traces 3D glass models over real background imagery to create physically accurate reflection scenarios, and leverages the capabilities of Large Multimodal Model (LMM) for improved reflection removal and separation performance.",27.26,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07214v1_BlindU Blind Machine Unlearning without Revealing .pdf,BlindU: Blind Machine Unlearning without Revealing Erasing Data,"Weiqi Wang, Zhiyi Tian, Chenhao Zhang, Shui Yu",Not found,Not found,"Machine Unlearning, Federated Learning, Privacy Leakage, Privacy Preserving, Information Bottleneck","Proposes a method for machine unlearning that does not require the unlearning requesters to upload their data to the server, thus protecting privacy in scenarios where servers are prohibited from accessing users' data, such as federated learning.",26.49,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07224v1_Consolidation or Adaptation PRISM Disentangling SF.pdf,Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration,"Yang Zhao, Yangou Ouyang, Xiao Ding, Hepeng Wang, Bibo Cai, Jinglong Gao, Zhouhao Sun, Li Du, Bing Qin, Ting Liu",Not provided,Not provided,"Schema Theory, Hybrid Supervised Fine-Tuning, Reinforcement Learning, Data Arbitration, Gradient Concentration, Pattern Consolidation, Structural Adaptation","PRISM is a dynamics-aware framework that arbitrates data based on its degree of cognitive conflict with the model's existing knowledge. By analyzing the spatial geometric structure of gradients, PRISM identifies data triggering high spatial concentration as high-conflict signals that require RL for structural restructuring, while data yielding diffuse updates is routed to SFT for efficient consolidation. Extensive experiments on WebShop and ALFWorld demonstrate that PRISM achieves a Pareto improvement, outperforming state-of-the-art hybrid methods while reducing computational costs by up to 3.22 ×. The findings suggest that disentangling data based on internal optimization regimes is crucial for scalable and robust agent alignment.",28.09,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07226v1_Lost in the Noise How Reasoning Models Fail with C.pdf,Lost in the Noise: How Reasoning Models Fail with Contextual Distractors,"Seongyun Lee, Yongrae Jo, Minju Seo, Moontae Lee, Minjoon Seo",,,"reasoning models, agentic AI, external tools, robustness, contextual distractors, emergent misalignment","Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent.",26.53,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07229v2_DiSCo Making Absence Visible in Intelligent Summar.pdf,DiSCo: Making Absence Visible in Intelligent Summarization Interfaces,"ERAN FAINMAN, University of Haifa, Israel, HAGIT BEN SHOSHAN, University of Haifa, Israel, ADIR SOLOMON, University of Haifa, Israel, OSNAT MOKRYN, University of Haifa, Israel",,,"Review Summarization, Absence, Expectations, Learning via surprisability, Missing commonalities","Intelligent interfaces increasingly use large language models to summarize user-generated content, yet these summaries emphasize what is mentioned while overlooking what is missing. This presence bias can mislead users who rely on summaries to make decisions. We present Domain Informed Summarization through Contrast (DiSCo), an expectation-based computational approach that makes absences visible by comparing each entity’s content with domain topical expectations captured in reference distributions of aspects typically discussed in comparable accommodations. This comparison identifies aspects that are either unusually emphasized or missing relative to domain norms and integrates them into the generated text. In a user study across three accommodation domains, namely ski, beach, and city center, DiSCo summaries were rated as more detailed and useful for decision making than baseline large language model summaries, although slightly harder to read. The findings show that modeling expectations reduces presence bias and improves both transparency and decision support in intelligent summarization interfaces.",28.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07232v1_Yes FLoReNce I Will Do Better Next Time Agentic Fe.pdf,Agentic Feedback Reasoning for Humorous Meme Detection,"Olivia Shanhong Liu, Pai Chet Ng, De Wen Soh, Konstantinos N. Plataniotis",Not found,Not found,"humorous memes, agentic feedback, closed-loop reasoning, open-loop inference, prompting, multimodal","This paper proposes FLoReNce, an agentic feedback reasoning framework for detecting meme humor. It treats meme understanding as a closed-loop process during learning and an open-loop process during inference, enabling better, self-aligned reasoning without finetuning. FLoReNce improves predictive performance and explanation quality over static multimodal baselines on the PrideMM dataset.",25.58,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07233v1_From Thinking to Justifying Aligning High-Stakes E.pdf,Thinking” to “Justifying”: Aligning High-Stakes Explainability with Professional Communication Standards,"Chen Qian, Yimeng Wang, Yu Chen, Lingfei Wu, Andreas Stathopoulos",,,"explainable AI, Chain-of-Thought, professional communication, legal writing, high-stakes domains","Explainable AI in high-stakes domains should help stakeholders trust and verify system outputs. The paper proposes 'Result → Justify', which constrains the output communication to present a conclusion before its structured justification. SEF operationalizes professional conventions via six metrics for structure and grounding, validating this approach across four tasks in three domains.",26.38,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07238v1_Group Pattern Selection Optimization Let LRMs Pick.pdf,Group Pattern Selection Optimization: Let LRMs Pick the Right Pattern for Reasoning,"Hanbin Wang, Jingwei Song, Jinpeng Li, Fei Mi, Lifeng Shang",,,"Large reasoning models, reinforcement learning, pattern selection, mathematics, science, reasoning patterns","A reinforcement learning framework called GPSO is introduced to optimize the reasoning patterns of large reasoning models, addressing the sub-optimality of default patterns and fostering more robust and adaptable reasoning.",26.74,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07239v1_Stochastic CHAOS Why Deterministic Inference Kills.pdf,"Stochas(c CHAOS: Why Determinis(c Inference Kills, and Distribu(onal Variability Is the Heartbeat of Ar(ﬁcial Cogni(on","Tanmay Joshi1, Shourya Aggarwal1, Anusa Saha1, Aadi Pandey1, Shreyash Dhoot1, Vighnesh Rai1, Raxit Goswami2, Aman Chadha3, Vinija Jain4, Amitava Das1",Not found,Not found,"Large Language Models, Reproducibility, Distributio(nal Variability, Stochasticity, Emergent Abilities, Reasoning, Safety Alignment","This paper argues against deterministic inference in large language models (LLMs), claiming it kills the ability to model uncertainty, makes emergent abilities vanish, disrupts reasoning abilities, and renders safety alignment brittle. The authors advocate for stochastic CHAOS, arguing that distributional variability is the heart of artificial cognition. They present empirical evidence showing that deterministic inference is systematically misleading in various aspects of LLMs.",26.44,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07245v1_Learning to Trust the Crowd A Multi-Model Consensu.pdf,Learning to Trust the Crowd: A Multi-Model Consensus Reasoning Engine for Large Language Models,Pranav Kallem,,,"Large language models, reliability, multi-model consensus, supervised learning, meta-learner, graph neural networks","The study investigates the reliability of large language models (LLMs) through multi-model consensus. Given responses from several heterogeneous LLMs, the system learns which answer is most likely correct for a given query. The system maps natural language responses into structured features and applies various machine learning techniques to improve accuracy and reliability.",25.16,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07250v1_DDT A Dual-Masking Dual-Expert Transformer for Ene.pdf,DDT: A Dual-Masking Dual-Expert Transformer for Energy Time-Series Forecasting,"Mingnan Zhu, Qixuan Zhang, Yixuan Cheng, Fangzhou Gu, Shiming Lin",,,"Energy Forecasting, Time-Series Analysis, Deep Learning, Dynamic Masking, Adaptive Feature Fusion","This paper proposes DDT, a novel deep learning framework for high-precision energy time-series forecasting, addressing challenges from complex temporal dependencies and heterogeneous multi-source data.",24.64,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07261v1_Pseudodata-guided Invariant Representation Learnin.pdf,Pseudodata-guided Invariant Representation Learning Boosts the Out-of-Distribution Generalization in Enzymatic Kinetic Parameter Prediction,"Haomin Wu, Zhiwei Nie, Hongyu Zhang, Zhixiang Ren",Not provided,2601.07261,"enzyme kinetics, deep learning, out-of-distribution, perturbation augmentation, invariant representation, catalytic mechanisms, enzyme engineering","A lightweight module, O 2DENet, is proposed to enhance out-of-distribution generalization in enzyme–substrate interaction predictors by introducing enzyme-substrate perturbations and enforcing consistency between original and augmented representations, achieving state-of-the-art results in accuracy and robustness metrics.",27.96,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07263v1_When Bots Take the Bait Exposing and Mitigating th.pdf,When Bots Take the Bait: Exposing and Mitigating the Emerging Social Engineering Attack in Web Automation Agent,"Xinyi Wu†, Geng Hong †B, Yueyue Chen†, MingXuan Liu §, Feier Jin †, Xudong Pan †‡, Jiarun Dai †, Baojun Liu ¶",Not provided,Not provided,"web automation, social engineering, web agents, large language models, runtime mitigation","This paper presents the first systematic study of social engineering attacks against web automation agents and designs a pluggable runtime mitigation solution. It introduces the AGENTBAIT paradigm and proposes SUPERVISOR, a lightweight runtime module to enforce environment and intention consistency alignment. Empirical results show that mainstream frameworks are highly vulnerable to AGENTBAIT, with an average attack success rate of 67.5% and peaks above 80% under specific strategies. The proposed module can reduce attack success rates by up to 78.1% while incurring only a 7.7% runtime overhead and preserving usability.",26.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07291v1_A Visual Semantic Adaptive Watermark grounded by P.pdf,A Visual Semantic Adaptive Watermark grounded by Prefix-Tuning for Large Vision-Language Model,"Qi Zheng, Shuliang Liu, Yu Huang, Sihang Jia, Jungang Li, Lyuhao Chen, Junhao Chen, Hanqian Li, Aiwei Liu, Yibo Yan, Xuming Hu",,,"Watermarking, Large Vision-Language Models, Prefix-Tuning, Visual Evidence Weights, Semantic Fidelity, Inference Efficiency","Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases, while some semantic-aware methods incur prohibitive inference latency due to rejection sampling. This paper proposes the Visual Semantic Adaptive Watermark (VISA-Mark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. Our approach employs a lightweight, efficiently trained prefix-tuner to extract dynamic Visual Evidence Weights, which guide an adaptive vocabulary partitioning and logits perturbation mechanism, concentrating watermark strength specifically on visually-supported tokens. By actively aligning the watermark with visual evidence, VISA-Mark effectively maintains visual fidelity and outperforms conventional methods in terms of visual consistency and semantic fidelity.",28.36,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07292v1_Photometric Redshift Estimation Using Scaled Ensem.pdf,Photometric Redshift Estimation Using Scaled Ensemble Learning,"Swagata Biswas, Shubhrangshu Ghosh, Avyarthana Ghosh, Yogesh Wadadekar, Abhishek Roy Choudhury, Arijit Mukherjee, Shailesh Deshpande, Arpan Pal",,,"Galaxies, High-redshift galaxies, Redshift surveys, Neural networks","The study presents a new ensemble-based ML framework aimed at predicting photometric redshifts (Pz) for faint galaxies and higher redshift ranges, relying solely on optical (grizy) photometric data. The proposed architecture integrates several learning algorithms, including gradient boosting machine, extreme gradient boosting, k-nearest neighbors, and artificial neural networks, within a scaled ensemble structure. The framework demonstrates consistent accuracy in estimating redshifts, maintaining strong performance up to z ∼ 4. The model is validated using publicly available data from the Hyper Suprime-Cam Strategic Survey Program by the Subaru Telescope. Our results show marked improvements in the precision and reliability of Pz estimation.",27.84,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07296v1_LRAS Advanced Legal Reasoning with Agentic Search.pdf,LRAS: Advanced Legal Reasoning with Agentic Search,"Yujin Zhou, Chuxue Cao, Jinluan Yang, Lijun Wu, Conghui He, Sirui Han, Yike Guo",,,"Legal Reasoning, Agentic Search, Introspective Imitation Learning, Difficulty-aware Reinforcement Learning","This paper presents LRAS, a framework designed to enhance legal reasoning capabilities of Large Reasoning Models (LRMs) by integrating introspective imitation learning and difficulty-aware reinforcement learning. Empirical results show that LRAS outperforms state-of-the-art baselines by 8.2-32%, particularly in tasks requiring deep reasoning with reliable knowledge.",24.71,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07304v1_Heterogeneous Multi-Expert Reinforcement Learning .pdf,Heterogeneous Multi-Expert Reinforcement Learning for Long-Horizon Multi-Goal Tasks in Autonomous Forklifts,"Yun Chen, Bowei Huang, Fan Guo, Kang Song",,,"Autonomous Forklift, Hierarchical Reinforcement Learning, Mobile Manipulation, Hybrid Training, Modality Decoupling","This work proposes a Heterogeneous Multi-Expert Reinforcement Learning (HMER) framework tailored for autonomous forklifts, decomposing long-horizon tasks into specialized sub-policies controlled by a Semantic Task Planner to separate navigation and manipulation, improving task success rate and reducing operation time.",26.58,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07309v1_ARM Role-Conditioned Neuron Transplantation for Tr.pdf,ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging,"Zhuoka Feng, Kang Chen, Sihan Zhao, Kai Xiong, Yaoning Wang, Minshen Yu, Junjie Nian, Changyi Xiao, Yixin Cao",,2601.07309,"large language models, model merging, neuron transplantation, interactive agents, generalization","This paper proposes ARM, a role-conditioned neuron transplantation method for training-free model merging in large language model (LLM) agents, aiming to improve cross-environment robustness and generalization ability. ARM enhances existing merging methods from static natural language tasks to multi-turn agent scenarios, achieving this with a 3-step framework: constructing merged backbones, selection based on role-conditioned activation analysis, and neuron transplantation for fine-grained refinements. Without gradient-based optimization, ARM improves cross-benchmark generalization while maintaining efficiency.",28.12,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07313v1_Explaining Machine Learning Predictive Models thro.pdf,Explaining Machine Learning Predictive Models through Conditional Expectation Methods,"Silvia Ruiz-Espa˜ naa, Laura Arnal, Francois Signola, Juan-Carlos Perez-Cortes, Joaquim Arlandis",Not found,Not found,"machine learning, XAI, explainable models, local explainability, model-agnostic, uncertainty, stability","The rapid adoption of complex artificial intelligence and machine learning models has led to their characterization as black boxes due to the difficulty of explaining their internal decision-making processes. This lack of transparency hinders users' ability to understand, validate, and trust model behavior, particularly in high-risk applications. This work introduces Multivariate Conditional Expectation (MUCE), a model-agnostic method for local explainability designed to capture prediction changes from feature interactions. MUCE extends Individual Conditional Expectation (ICE) by exploring a multivariate grid of values in the neighborhood of a given observation at inference time, providing graphical explanations that illustrate the local evolution of model predictions. In addition, two quantitative indices, stability and uncertainty, summarize local behavior and assess model reliability.",28.47,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07315v1_VLM-CAD VLM-Optimized Collaborative Agent Design W.pdf,VLM-CAD:VLM-OptimizedCollaborativeAgent Design Workflow for Analog Circuit Sizing,"1st Guanyuan Pan, 2nd Yugui Lin, 3rd Tiansheng Zhou, 4th Pietro Li `o, 5th Shuai Wang, 6th Yaqi Wang*",,,"Analog Circuit Sizing, Agentic AI, Vision Language Model, Explainability, Electronic Design Automation","Analog mixed-signal circuit sizing involves complex trade-offs within high-dimensional design spaces. Existing automatic analog circuit sizing approaches often underutilize circuit schematics and lack the explainability required for industry adoption. To tackle these challenges, we propose a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD), which analyzes circuits, optimizes DC operating points, performs inference-based sizing and executes external sizing optimization. We integrate Image2Net to annotate circuit schematics and generate a structured JSON description for precise interpretation by Vision Language Models. Furthermore, we propose an Explainable Trust Region Bayesian Optimization method (ExTuRBO) that employs collaborative warm-starting from agent-generated seeds and offers dual-granularity sensitivity analysis for external sizing optimization, supporting a comprehensive final design report. Experiment results on amplifier sizing tasks using 180nm, 90nm, and 45nm Predictive Technology Models demonstrate that VLM-CAD effectively balances power and performance, achieving a 100% success rate in optimizing an amplifier with a complementary input and a class-AB output stage, while maintaining total runtime under 43 minutes across all experiments.",28.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07316v1_BEAT-Net Injecting Biomimetic Spatio-Temporal Prio.pdf,BEAT-Net: Injecting Biomimetic Spatio-Temporal Priors for Interpretable ECG Classification,"Ma Runze, Liao Caizhi ∗",Not found,2601.07316,"ECG classification, deep learning, biomimetic priors, self-supervised learning","Proposes BEAT-Net, a framework that reformulates ECG analysis as a language modeling task, using QRS tokenization to transform continuous signals into biologically aligned heartbeat sequences. This approach explicitly decomposes cardiac physiology through specialized encoders, improving diagnostic accuracy and robustness compared to dominant CNN architectures.",28.13,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07320v1_Segmental Advantage Estimation Enhancing PPO for L.pdf,Segmental Advantage Estimation: Enhancing PPO for Long-Context LLM Training,"Xue Gong, Qi Yi, Ziyuan Nan, Guanhua Huang, Kejiao Li, Yuhao Jiang, Ruibin Xiong, Zenan Xu, Jiaming Guo, Shaohui Peng, Bo Zhou",Not found,Not found,"Proximal Policy Optimization, Reinforcement Learning with Verifiable Rewards, Generalized Advantage Estimation, Long-Context Language Models, Advantage Estimation, Segmental Advantage Estimation","Training Large Language Models (LLMs) for reasoning tasks is increasingly driven by Reinforcement Learning with Verifiable Rewards (RLVR), where Proximal Policy Optimization (PPO) provides a principled framework for stable policy updates. However, the practical application of PPO is hindered by unreliable advantage estimation in the sparse-reward RLVR regime. This issue arises because the sparse rewards in RLVR lead to inaccurate intermediate value predictions, which in turn introduce significant bias when aggregated at every token by Generalized Advantage Estimation (GAE). To address this, we introduce Segmental Advantage Estimation (SAE), which mitigates the bias that GAE can incur in RLVR.",28.7,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07342v1_Agentic Diagnostic Reasoning over Telecom and Data.pdf,Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure,Nicolas Tacheny,10.1101/2601.07342,2601.07342,"Large Language Model, Telecom Infrastructure, Datacenter, Root Cause Analysis, Impact Analysis, Autonomous Incident Resolution, Change Impact Mitigation","This work introduces an agentic diagnostic framework using a Large Language Model to autonomously navigate and investigate telecom and datacenter infrastructures, enabling autonomous incident resolution and change impact mitigation.",27.67,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07344v1_PulseMind A Multi-Modal Medical Model for Real-Wor.pdf,PulseMind: A Multi-Modal Medical Model for Real-World Clinical Diagnosis,"Jiao Xu, Junwei Liu, Jiangwei Lao, Qi Zhu, Yunpeng Zhao, Congyun Jin, Shinan Liu, Zhihong Lu, Lihe Zhang, Xin Chen, Jian Wang, Ping Wang",,,"medical modeling, multi-modal diagnosis, real-world clinical diagnosis, contextual understanding, PulseMind benchmark, Comparison-based Reinforcement Policy Optimization (CRPO)","PulseMind introduces a new family of multi-modal diagnostic models that integrate a curated dataset, a comprehensive evaluation benchmark, and a tailored training framework. It addresses the complexity of real-world clinical diagnostics by incorporating heterogeneous inputs and ongoing contextual understanding during patient-physician interactions.",27.39,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07348v4_Controlled Self-Evolution for Algorithmic Code Opt.pdf,Controlled Self-Evolution for Algorithmic Code Optimization,"Tu Hu, Ronghao Chen, Shuo Zhang, Jianghao Yin, Mou Xiao Feng, Jingping Liu, Shaolei Zhang, Wenqi Jiang, Yuqi Fang, Sen Hu, Huacan Wang, Yi Xu",Not found,2601.07348,"algorithmic code optimization, self-evolution, controlled evolution, stochastic operations, experience reuse","This paper proposes Controlled Self-Evolution (CSE) to improve the exploration efficiency of self-evolution methods in algorithmic code optimization. CSE consists of diversified planning initialization, genetic evolution with feedback-guided mechanisms, and hierarchical evolution memory, which consistently outperforms existing baselines across various large language model backbones.",28.82,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07351v2_Beyond Hard Masks Progressive Token Evolution for .pdf,Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models,"Linhao Zhong, Linyu Wu, Bozhen Fang, Tianjian Feng, Chenchen Jing, Wen Wang, Jiaheng Zhang, Hao Chen, Chunhua Shen",Not provided,Not provided,"Diffusion Language Models, Progressive Token Evolution, Continuous Trajectory Supervision, Masked Diffusion Language Models, Revisable Decoding","This paper proposes EvoToken-DLM, a novel diffusion-based language modeling approach that replaces hard binary masks with evolving soft token distributions. It enables a progressive transition from masked states to discrete outputs, supporting revisable decoding. The authors introduce continuous trajectory supervision to effectively support this evolution and demonstrate that EvoToken-DLM consistently achieves superior performance compared to strong diffusion-based and masked DLM baselines.",26.63,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07356v1_Efficient Convolutional Forward Model for Passive .pdf,Efficient Convolutional Forward Model for Passive Acoustic Mapping and Temporal Monitoring,"Tatiana Gelvez-Barrera, Barbara Nicolas, Bruno Gilles, Adrian Basarab, Denis Kouamé",Not found,Not found,"Passive Acoustic Mapping, Model-based beamforming, Convolutional forward model, Temporal monitoring","Passive acoustic mapping (PAM) is a key imaging technique for characterizing cavitation activity in therapeutic ultrasound applications. Recent model-based beamforming algorithms offer high reconstruction quality and strong physical interpretability. However, their computational burden and limited temporal resolution restrict their use in applications with time-evolving cavitation. This paper introduces a PAM beamforming framework based on a novel convolutional formulation in the time domain, which enables efficient computation. The framework formulates PAM as an inverse problem, incorporating prior knowledge on cavitation activity, and demonstrates superior performance compared to classical beamforming methods.",27.08,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07359v1_Seeing Right but Saying Wrong Inter- and Intra-Lay.pdf,Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs,"Shezheng Song, Shasha Li, Jie Yu",Not found,Not found,"Multimodal Language Models, Attention Mechanism, Inference Quality, Dual-Perspective Decoding","This paper addresses the internal inconsistency in Multimodal Large Language Models (MLLMs) where deeper layers attend to correct visual regions but final predictions are misled by noisy attention from earlier layers. The authors propose DualPD, a dual-perspective decoding refinement strategy that enhances the model's visual understanding without additional training. Experiments on both LLaV A and Qwen-VL model families demonstrate that DualPD consistently improves accuracy without training.",27.17,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07364v1_On the universal definition of intelligence.pdf,On the universal definition of intelligence,Joseph Chen,,1910.00807,"intelligence, human, AI, comparison, predictive ability",This paper proposes a universal definition of intelligence to enable fair and consistent comparison of human and artificial intelligence.,27.14,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07372v1_Conditional Memory via Scalable Lookup A New Axis .pdf,Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models,"Xin Cheng, Wangding Zeng, Damai Dai, Qinyu Chen, Bingxuan Wang, Zhenda Xie, Kezhao Huang, Xingkai Yu, Zhewen Hao, Yukun Li, Han Zhang, Huishuai Zhang, Dongyan Zhao, Wenfeng Liang",Not found,2601.07372,"Conditional Memory, Sparsity, Large Language Models, Transformers, Mixture-of-Experts, Engram, Lookup","This paper introduces conditional memory as a new sparsity axis for large language models, complementing Mixture-of-Experts (MoE). The authors demonstrate that conditional memory, implemented via Engram, can significantly improve performance in various domains, including general reasoning, code/math, and long-context retrieval.",27.98,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07376v1_OpenTinker Separating Concerns in Agentic Reinforc.pdf,OpenTinker: Separating Concerns in Agentic Reinforcement Learning,"Siqi Zhu, Jiaxuan You",,2601.07376v1,"Reinforcement Learning, Large Language Models, Agent Interaction, Decoupling Concerns","OpenTinker introduces an infrastructure for reinforcement learning of large language model agents, focusing on separating concerns across algorithm design, execution, and agent-environment interaction.",26.65,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07377v1_Learning Dynamic Collaborative Network for Semi-su.pdf,Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation,"Jiao Xu, Xin Chen",Not found,Not found,"3D vessel segmentation, semi-supervised learning, dynamic collaborative network, adversarial supervision, multi-view integration",This paper presents a dynamic collaborative network (DiCo) for semi-supervised 3D vessel segmentation. It addresses the challenges of scarce labeled data and complex vessel appearance by allowing dynamic switching of teacher-student roles and incorporating adversarial supervision and multi-view integration. Experiments show improved performance on 3D vessel segmentation benchmarks.,27.16,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07389v1_On the Non-decoupling of Supervised Fine-tuning an.pdf,On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training,"Xueyan Niuniuxueyan3@huawei.com, Bo Baibo8@huawei.com, Wei Hanharvey.hanwei@huawei.com, Weixi Zhangzhangweixi1@huawei.com",,2601.07389,"Supervised Fine-Tuning, Reinforcement Learning, Large Language Models, Post-training, Decoupling","This paper investigates the non-decoupling of supervised fine-tuning and reinforcement learning in post-training of large language models. It proves that decoupling is impossible in either order: SFT-then-RL coupling increases SFT loss under SFT optimality, and RL-then-SFT coupling lowers the reward achieved by RL. Experiments on Qwen3-0.6B confirm the predicted degradation, verifying that SFT and RL cannot be separated without loss of prior performance in the post-training pipeline.",28.78,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07392v1_OceanSAR-2 A Universal Feature Extractor for SAR O.pdf,OceanSAR-2: A Universal Feature Extractor for SAR Ocean Observation,"Alexandre Tuela, Thomas Kerdreux, Quentin Febvre, Alexis Mouche, Antoine Grouazel, Jean-Renaud Miadana, Antoine Audras, Chen Wang, Bertrand Chapron",,,"SAR, Ocean, Self-supervised learning, Sentinel-1, Wave Mode, Feature extraction","Presenting OceanSAR-2, the second generation of the foundation model for SAR-based ocean observation, which enhances performance through improved self-supervised learning and dynamic data curation strategies, demonstrating strong transfer performance across downstream tasks such as geophysical pattern classification, ocean surface wind vector estimation, and iceberg detection.",27.49,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07393v1_Software-Hardware Co-optimization for Modular E2E .pdf,SOFTWARE-HARDWARECO-OPTIMIZATION FORMODULARE2E,"Chengzhi Ji1, Xingfeng Li1, Zhaodong Lv1, Hao Sun2, Pan Liu1, Hao Frank Yang3, Ziyuan Pu1,∗",Not found,2601.07393,"Modular end-to-end, Closed-Loop Evaluation, Software–Hardware co-optimization, Energy Consumption","This paper proposes a reusable software–hardware co-optimization and closed-loop evaluation framework for ME2E autonomous driving inference, aiming to improve deployability and system-level performance by addressing critical considerations such as energy consumption and inference latency.",27.75,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07395v1_MCP-ITP An Automated Framework for Implicit Tool P.pdf,MCP-ITP: An Automated Framework for Implicit Tool Poisoning in MCP,"Ruiqi Li, Zhiqiang Wang, Y unhao Y ao, Xiang-Y ang Li",Not provided,Not provided,"Model Context Protocol, Implicit Tool Poisoning, Tool Poisoning Attack, Large Language Models, Security","To standardize interactions between LLM-based agents and their environments, the Model Context Protocol (MCP) was proposed and has since been widely adopted. However, integrating external tools expands the attack surface, exposing agents to tool poisoning attacks. In such attacks, malicious instructions embedded in tool metadata are injected into the agent context during MCP registration phase, thereby manipulating agent behavior. Prior work primarily focuses on explicit tool poisoning or relied on manually crafted poisoned tools. In contrast, we focus on a stealthy variant: implicit tool poisoning, where the poisoned tool itself remains uninvoked. Instead, the instructions embedded in the tool metadata induce the agent to invoke a legitimate but high-privilege tool to perform malicious operations. We propose MCP-ITP, the first automated and adaptive framework for implicit tool poisoning within the MCP ecosystem. MCP-ITP formulates poisoned tool generation as a black-box optimization problem and employs an iterative optimization strategy that leverages feedback from both an evaluation LLM and a detection LLM to maximize Attack Success Rate (ASR) while evading current detection mechanisms. Experimental results on the MCPTox dataset across 12 LLM agents demonstrate that MCP-ITP consistently outperforms the manually crafted baseline, achieving up to 84.2% ASR while suppressing the Malicious Tool Detection Rate (MDR) to as low as 0.3%.",29.19,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07397v1_Layerwise goal-oriented adaptivity for neural ODEs.pdf,Layerwise goal-oriented adaptivity for neural ODEs: an optimal control perspective,"Michael Hintermuller, Michael Hinze, Denis Korolev",Not found,arXiv:2601.07397v1,"Resnet, neural ODEs, parameter identification/learning, adaptive neural network","In this work, a novel layerwise adaptive construction method for neural network architectures is proposed, based on a goal-oriented dual-weighted residual technique for the optimal control of neural differential equations. This leads to an optimization problem with controls acting as coefficients and a specific loss function, implemented using a DG(0) Galerkin discretization of the neural ODE and steepest descent for optimization. The method is applied to the construction of neural networks for data set classification, presenting results for selected examples from the literature.",28.8,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07411v1_SCALPEL Selective Capability Ablation via Low-rank.pdf,SCALPEL: Selective Capability Ablation via Low-rank Parameter Editing for Large Language Model Interpretability Analysis,"Zihao Fu, Xufeng Duan, Zhenguang G. Cai",,,"Large language models, interpretability, neural networks, gradient attribution, activation analysis, low-rank parameter editing, capability ablation","Large language models have achieved remarkable success across diverse domains, yet their deployment in many applications remains limited by our incomplete understanding of their internal mechanisms. SCALPEL presents a framework that represents capabilities as low-rank parameter subspaces rather than discrete modules, enabling precise capability removal without affecting others. By training LoRA adapters to reduce the model's ability to distinguish correct from incorrect answers while preserving general language modeling quality, SCALPEL identifies the low-rank representation responsible for a particular capability while remaining disentangled from other capabilities.",28.75,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07422v1_Two Pathways to Truthfulness On the Intrinsic Enco.pdf,Two Pathways to Truthfulness: On the Intrinsic Encoding of LLM Hallucinations,"Wen Luo, Guangyue Peng, Wei Li, Shaohang Wei, Feifan Song, Liang Wang, Nan Yang, Xingxing Zhang, Jing Jin, Furu Wei, Houfeng Wang",Not found,Not found,"large language models, hallucinations, truthfulness, internal representations, attention mechanisms, knowledge boundaries","Despite impressive capabilities, large language models (LLMs) frequently generate hallucinations. This paper demonstrates that truthfulness cues arise from two distinct information pathways: a Question-Anchored pathway and an Answer-Anchored pathway. The authors validate and disentangle these pathways through attention knockout and token patching, uncovering notable properties of these mechanisms. They also propose two applications to enhance hallucination detection performance and discuss the implications for more reliable and self-aware generative systems.",27.74,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07430v1_KALE Enhancing Knowledge Manipulation in Large Lan.pdf,KALE: Enhancing Knowledge Manipulation in Large Language Models via Knowledge-aware Learning,"Qitan Lv, Tianyu Liu, Qiaosheng Zhang, Xingcheng Xu, Chaochao Lu",,,"large language models, knowledge manipulation, supervised fine-tuning, knowledge graphs, rationales, KL divergence","Despite the impressive performance of large language models (LLMs) pretrained on vast knowledge corpora, advancing their knowledge manipulation—the ability to effectively recall, reason, and transfer relevant knowledge—remains challenging. Existing methods mainly leverage Supervised Fine-Tuning (SFT) on labeled datasets to enhance LLMs' knowledge manipulation ability. However, we observe that SFT models still exhibit the known&incorrect phenomenon, where they explicitly possess relevant knowledge for a given question but fail to leverage it for correct answers. To address this challenge, we propose KALE (Knowledge-Aware LEarning)—a post-training framework that leverages knowledge graphs (KGs) to generate high-quality rationales and enhance LLMs' knowledge manipulation ability.",27.81,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07449v1_RLPO Residual Listwise Preference Optimization for.pdf,RLPO: Residual Listwise Preference Optimization for Long-Context Review Ranking,"Hao Jiang, Zhi Yang, Annan Wang, Yichi Zhang, Weisi Lin*",,,"review ranking, long-context, residual learning, large language models","Review ranking is crucial in e-commerce for prioritizing user-generated feedback. Existing ranking paradigms face a trade-off in long-context settings, with pointwise scoring being efficient but often failing to account for list-level interactions. Listwise approaches leverage global context but are computationally expensive and unstable. This paper proposes Residual Listwise Preference Optimization (RLPO), which formulates ranking as residual correction over a strong pointwise LLM scorer, improving NDCG@k over strong pointwise and listwise baselines and remaining robust as list length increases.",27.41,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07463v1_Puzzle it Out Local-to-Global World Model for Offl.pdf,Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning,"Sijia Li, Xinran Li, Shibo Chen, Jun Zhang",Not found,Not found,"Offline multi-agent reinforcement learning, Multi-agent model-based reinforcement learning","This paper proposes a local-to-global (LOGO) world model for offline multi-agent reinforcement learning (MARL) to improve prediction accuracy and generalize beyond the support of the data. It introduces a novel framework that leverages local predictions to infer global state dynamics, generating synthetic data to augment the original dataset and reducing approximation error propagation to policies. The approach requires only an additional encoder for uncertainty estimation, significantly reducing computational overhead while maintaining accuracy.",27.28,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07464v1_IFDNS An Iterative Feedback-Driven Neuro-Symbolic .pdf,IFDNS: An Iterative Feedback-Driven Neuro-Symbolic Method for Faithful Logical Reasoning,"Xiaoheng Wang, Tongxuan Liu, Zi Gong, Xianzhe Dong, Yuting Zeng, Minhan Hu, Weizhe Huang, Jing Li",Not found,Not found,"Logical Reasoning, Large Language Model, Reasoning","Large language models (LLMs) have demonstrated impressive capabilities across a wide range of reasoning tasks, including logical and mathematical problem-solving. While prompt-based methods like Chain-of-Thought (CoT) can enhance LLM reasoning abilities to some extent, they often suffer from a lack of faithfulness, where the derived conclusions may not align with the generated reasoning chain. To address this issue, researchers have explored neuro-symbolic approaches to bolster LLM logical reasoning capabilities. However, existing neuro-symbolic methods still face challenges with information loss during the process. To overcome these limitations, we introduce Iterative Feedback-Driven Neuro-Symbolic (IFDNS), a novel prompt-based method that employs a multi-round feedback mechanism to address LLM limitations in handling complex logical relationships. IFDNS utilizes iterative feedback during the logic extraction phase to accurately extract causal relationship statements and translate them into propositional and logical implication expressions, effectively mitigating information loss issues. Furthermore, IFDNS is orthogonal to existing prompt methods, allowing for seamless integration with various prompting approaches. Empirical evaluations across six datasets demonstrate the effectiveness of IFDNS in significantly improving the performance of CoT and Chain-of-Thought with Self-Consistency (CoT-SC). Specifically, IFDNS achieves a +9.40% accuracy boost for CoT on the LogiQA dataset and a +11.70% improvement for CoT-SC on the PrOntoQA dataset.",28.66,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07468v1_Beyond Dialogue Time Temporal Semantic Memory for .pdf,Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents,"Miao Su, Yucan Guo, Zhongni Hou, Long Bai, Zixuan Li, Yufei Zhang, Guojun Yin, Wei Lin, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng",,2601.07468,"Large Language Models, Personalized Agents, Temporal Memory, Semantic Memory","Memory enables Large Language Model (LLM) agents to perceive, store, and use information from past dialogues, which is essential for personalization. However, existing methods fail to properly model the temporal dimension of memory in two aspects: 1) Temporal inaccuracy: memories are organized by dialogue time rather than their actual occurrence time; 2) Temporal fragmentation: existing methods focus on point-wise memory, losing durative information that captures persistent states and evolving patterns. To address these limitations, we propose Temporal Semantic Memory (TSM), a memory framework that models semantic time for point-wise memory and supports the construction and utilization of durative memory.",27.97,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07469v1_Knowledge Distillation for LLM-Based Human Activit.pdf,KNOWLEDGE DISTILLATION FOR LLM-B ASED HUMAN ACTIVITY RECOGNITION IN HOMES,"Julien Cumin, Oussama Er-Rahmany, Xi Chen",,arXiv:2601.07469v1,"Human activity recognition, large language models, knowledge distillation, ambient intelligence, smart homes","This paper presents new experimental results regarding the use of large language models (LLMs) for human activity recognition (HAR) in homes, using two state-of-the-art datasets. It explores how recognition performance varies with the size of the LLM used and experiments with fine-tuning smaller LLMs with HAR reasoning examples generated by larger LLMs. The results show that fine-tuned models can perform almost as well as the largest LLMs, while having 50 times fewer parameters.",27.83,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07470v1_Learning How to Remember A Meta-Cognitive Manageme.pdf,Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory,"Sirui Liang, Pengfei Cao, Jian Zhao, Wenhao Teng, Xiangwen Liao, Jun Zhao, Kang Liu",Not found,Not found,"Memory management, Meta-cognition, Transferable memory, Hierarchical memory organization, Direct preference optimization","This paper proposes the Meta-Cognitive Memory Abstraction (MCMA) method, which treats memory abstraction as a learnable cognitive skill. MCMA decouples task execution from memory management by combining a frozen task model with a learned memory copilot. The memory copilot is trained using direct preference optimization, determining how memories should be structured, abstracted, and reused. Memories are organized into a hierarchy of abstraction levels, enabling selective reuse based on task similarity. When no memory is transferable, MCMA transfers the ability to abstract and manage memory by transferring the memory copilot. Experiments on ALFWorld, ScienceWorld, and BabyAI demonstrate substantial improvements in performance, out-of-distribution generalization, and cross-task transfer over several baselines.",28.36,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07474v1_Task Prototype-Based Knowledge Retrieval for Multi.pdf,Task Prototype-Based Knowledge Retrieval for Multi-Task Learning,"Youngmin Oh, Hyung-Il Kim, Jung Uk Kim",Not found,Not found,"Multi-task learning, Prototype-based knowledge retrieval, Partially labeled data, Robust multi-task learning","Proposes a prototype-based knowledge retrieval framework for robust multi-task learning, addressing the challenges of obtaining fully annotated data for all tasks. The framework consists of task prototypes and a knowledge retrieval transformer, ensuring consistent task-specific characteristics and adaptive feature refinement based on task associations.",24.04,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07475v1_ARCQuant Boosting NVFP4 Quantization with Augmente.pdf,ARCQuant: Boosting NVFP4 Quantization with Augmented Residual Channels for LLMs,"Haoqian Meng, Yilun Luo, Yafei Zhao, Wenyuan Liu, Peng Zhang*",Not found,Not found,"Large Language Models, Post-Training Quantization, NVFP4, Augmented Residual Channels, Fine-Grained Numerical Formats","The paper proposes ARCQuant, a framework that boosts NVFP4 performance via Augmented Residual Channels. It addresses challenges in adapting existing Post-Training Quantization strategies to fine-grained formats like NVFP4, which compromise block isolation or hardware uniformity. ARCQuant maintains a strictly unified NVFP4 format by augmenting the activation matrix with quantized residual channels, enabling the use of standard, highly optimized GEMM kernels with minimal overhead. Extensive experiments on LLaMA and Qwen models demonstrate that ARCQuant achieves state-of-the-art accuracy, comparable to full-precision base-liners in perplexity and downstream tasks.",25.45,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07477v1_JudgeFlow Agentic Workflow Optimization via Block .pdf,JUDGEFLOW: AGENTICWORKFLOWOPTIMIZATION,"Zihan Ma∗, Zhikai Zhao∗, Chuanbo Hua1, Federico Berto1, Jinkyoo Park1",,,"Large language models, agentic workflows, LLM optimization, workflow automation","Optimizing LLM-based agentic workflows is challenging, and current methods rely on coarse evaluation signals. JUDGEFLOW proposes an Evaluation-Judge-Optimization-Update pipeline to address these limitations by incorporating reusable, configurable logic blocks and a dedicated Judge module that assigns fine-grained responsibility scores to problematic blocks. This approach improves sample efficiency and enhances interpretability through block-level diagnostics.",26.93,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07496v1_Graph Inference Towards ICD Coding.pdf,Graph Inference Towards ICD Coding,1st Xiaoxiao Deng,460-519,038.9276.2038.9,"transfer learning, graph convolutional network, lightweight attention, ICD code prediction, adversarial domain adaptation","Automated ICD coding involves assigning standardized diagnostic codes to clinical narratives. Challenges include extreme class imbalance and sparse, hierarchical structure. LabGraph reformulates the task as graph generation, combining adversarial domain adaptation, graph-based reinforcement learning, and perturbation regularization to enhance model robustness and generalization. Experiments on benchmark datasets demonstrate LabGraph's superior performance on micro-F1, micro-AUC, and P@K metrics.",26.79,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07514v1_Data-Driven Stochastic VRP Integration of Forecast.pdf,Data-Driven Stochastic VRP: Integration of Forecast Duration into Optimization for Utility Workforce Management,Matteo Garbellia,,2601.07514,"Stochastic VRP, Machine Learning, XGBoost, Sub-Gaussian Concentration, Multi-Objective Optimization, Evolutionary Algorithms","This paper investigates the integration of machine learning forecasts of intervention durations into a stochastic variant of the Capacitated Vehicle Routing Problem with Time Windows (CVRPTW). It uses tree-based gradient boosting (XGBoost) trained on eight years of gas meter maintenance data to produce point predictions and uncertainty estimates, which drive a multi-objective evolutionary optimization routine. The methodology addresses uncertainty through sub-Gaussian concentration bounds for route-level risk buffers and explicitly accounts for competing operational KPIs through a multi-objective formulation. Empirical analysis of prediction residuals validates the sub-Gaussian assumption underlying the risk model. The results report improvements around 20-25% in operator utilization and completion rates compared with plans computed using default durations. The integration of uncertainty quantification and risk-aware optimization provides a practical framework for handling stochastic service durations in real-world routing applications.",29.77,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07516v1_Controlling Multimodal Conversational Agents with .pdf,Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions,"Yongqi Li, Hao Lang, Tieyun Qian, Yongbin Li",,,"Reinforcement Learning, Multimodal Conversational Agents, Latent Actions, Vision-Language Models","This paper proposes a method to enhance the performance of reinforcement learning fine-tuning for multimodal conversational agents by learning a compact latent action space. The authors leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for transforming text embeddings into image-text embeddings. They also introduce a novel cycle consistency loss to enhance the robustness of the cross-modal projector. The method outperforms competitive baselines on two conversation tasks across various RL algorithms.",26.99,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07518v1_Mon3tr Monocular 3D Telepresence with Pre-built Ga.pdf,Mon3tr: Monocular 3D Telepresence with Pre-built Gaussian Avatars as Amortization,"Fangyu Lin, Yingdong Hu, Zhening Liu, Yufan Zhuang, Zehong Lin, Jun Zhang",Not found,Not found,"Monocular 3D telepresence, 3D Gaussian splatting, animatable avatars, real-time neural rendering","A novel Monocular 3D telepresence framework that integrates 3D Gaussian splatting (3DGS) based parametric human modeling into telepresence for the first time, reducing system complexity and cost by using a single monocular RGB camera to capture body motions and facial expressions in real time to drive the 3DGS-based parametric human model. The framework achieves state-of-the-art performance in terms of PSNR, end-to-end latency, and bandwidth reduction compared to point-cloud streaming.",27.42,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07525v1_Thinking Before Constraining A Unified Decoding Fr.pdf,Thinking Before Constraining: A Unified Decoding Framework for Large Language Models,"Ngoc Trinh Hung Nguyen, Alonso Silva, Laith Zumot, Liubov Tupikina, Armen Aghasaryan, Mehwish Alam",,,"Natural language generation, Structured generation, Language models, Constrained decoding, LLMs","This work proposes a simple approach that combines the advantages of natural and structured generation, allowing LLMs to reason freely until specific trigger tokens are generated, and then switching to structured generation to preserve the expressive power of natural language reasoning while ensuring the reliability of structured outputs.",26.19,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07528v1_From RAG to Agentic RAG for Faithful Islamic Quest.pdf,From RAG to Agentic RAG for Faithful Islamic Question Answering,"Gagan Bhatia1, Hamdy Mubarak1, Mustafa Jarrar2, George Mikros2, Fadi Zaraket3, Mahmoud Alhirthani2, Mutaz Al-Khatib4, Logan Cochrane5, Kareem Darwish1, Rashid Yahiaoui2, Firoj Alam1",Not found,Not found,"Islamic question answering, faithful responses, grounded models, retrieval, agentic RAG, Quran grounding, LLMs","This paper introduces ISLAMIC FAITH QA, a 3,810-item bilingual generative benchmark for Islamic question answering. It also develops an agentic Quran-grounding framework (agentic RAG) that uses structured tool calls for iterative evidence seeking and answer revision. Experiments show that retrieval improves correctness and agentic RAG yields the largest gains beyond standard RAG, achieving state-of-the-art performance and stronger Arabic-English robustness even with a small model.",27.79,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07553v1_VirtualEnv A Platform for Embodied AI Research.pdf,VirtualEnv: A Platform for Embodied AI Research,"Kabir Swain, Sijie Han, Ayush Raina, Jin Zhang, Shuang Li, Michael Stopa, Antonio Torralba",,,"Embodied AI, Simulation Platform, Large Language Models, Interactive Environments, Object Manipulation, Navigation, Multi-Agent Collaboration, Procedural Generation, Natural Language Control","VirtualEnv is a next-generation simulation platform built on Unreal Engine 5 that enables fine-grained benchmarking of large language models (LLMs) in embodied and interactive scenarios. It supports rich agent-environment interactions and integrates large-scale LLMs and vision-language models (VLMs) to generate novel environments and structured tasks from multimodal inputs. The platform is released as an open-source platform to advance research at the intersection of AI and gaming, enable standardized evaluation of LLMs in embodied AI settings, and pave the way for future developments in immersive simulations and interactive entertainment.",27.43,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07556v1_Backpropagation-Free Test-Time Adaptation for Ligh.pdf,Backpropagation-Free Test-Time Adaptation for Lightweight EEG-Based Brain-Computer Interfaces,"Siyang Li, Jiayi Ouyang, Zhenyao Cui, Ziwei Wang, Tianwang Jia, Feng Wan, Dongrui Wu",Not found,Not found,"test-time adaptation, brain-computer interface, electroencephalogram, domain adaptation, transfer learning","This paper proposes Backpropagation-Free Transformations (BFT), a test-time adaptation approach for EEG decoding that eliminates computational overhead, privacy risks, and sensitivity to noisy data streams. BFT applies multiple sample-wise transformations of knowledge-guided augmentations or approximate Bayesian inference to each test trial, generating multiple prediction scores for a single test sample. A learning-to-rank module enhances the weighting of these predictions, enabling robust aggregation for uncertainty suppression during inference. Experiments on five EEG datasets demonstrate the effectiveness, versatility, robustness, and efficiency of BFT.",27.51,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07565v1_A Unified Framework for Emotion Recognition and Se.pdf,A Unified Framework for Emotion Recognition and Sentiment Analysis via Expert-Guided Multimodal Fusion with Large Language Models,"Jiaqi Qiao, Xiujuan Xu, Xinran Li, Yu Liu",Not found,Not found,"emotion recognition, sentiment analysis, multimodal fusion, large language models, expert-guided fusion, multimodal representation, context-aware feature selection, pseudo token injection, prompt-based conditioning, natural language generation, large language model fine-tuning, cross-lingual robustness","This paper presents EGMF, a unified framework combining expert-guided multimodal fusion with large language models for multimodal emotion recognition and sentiment analysis. It features specialized expert networks for fine-grained emotional nuances, semantic cross-modal relationships, and long-range dependencies, adaptively integrated through hierarchical dynamic gating. Enhanced multimodal representations are integrated with LLMs via pseudo token injection and prompt-based conditioning, enabling a single generative framework for both classification and regression. LoRA fine-tuning is employed for computational efficiency. Experiments on bilingual benchmarks demonstrate consistent improvements over state-of-the-art methods, with superior cross-lingual robustness revealing universal patterns in multimodal emotional expressions across English and Chinese.",28.95,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07568v1_d3LLM Ultra-Fast Diffusion LLM using Pseudo-Trajec.pdf,d3LLM: Ultra-Fast dLLM using Pseudo-Trajectory Distillation,"Yu-Yang Qian, Junda Su, Lanxiang Hu, Peiyuan Zhang, Zhijie Deng, Peng Zhao†, Hao Zhang†",Not found,Not found,"diffusion large language models, pseudo-trajectory distillation, parallel decoding, random-order generation, accuracy-parallelism trade-off","d3LLM proposes a method to balance accuracy and parallelism in diffusion large language models (dLLMs), introducing pseudo-trajectory distillation during training and entropy-based multi-block decoding during inference to achieve high parallelism while maintaining accuracy.",27.01,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07573v1_A Model of Artificial Jagged Intelligence.pdf,A Model of Artificial Jagged Intelligence,Joshua S. Gans,Not found,2601.07573,"generative AI, adoption, calibration, learning, knowledge density, scaling","This paper develops an economic model of AI performance, called Artificial Jagged Intelligence (AJI), where generative AI systems show uneven performance across tasks. It treats adoption as an information problem, where users care about local reliability but observe only coarse global quality signals. The model interpolates optimally and measures local error by posterior variance.",28.68,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07577v1_Beyond Entangled Planning Task-Decoupled Planning .pdf,Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents,"Yunfan Li, Bingbing Xu, Xueyun Tian, Xiucheng Xu, Huawei Shen",Not found,Not found,"long-horizon tasks, task decoupling, planning, reactivity, robustness","Recent advances in large language models have enabled agents to autonomously execute complex, long-horizon tasks. However, planning remains a bottleneck for reliable task execution. This paper proposes Task-Decoupled Planning (TDP), a training-free framework that decomposes tasks into a directed acyclic graph of sub-goals via a Supervisor, and confine reasoning and replanning to the active sub-task. This approach improves robustness and efficiency for long-horizon agents.",27.16,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07580v1_Large Language Models for Physics Instrument Desig.pdf,Large Language Models for Physics Instrument Design,"Sara Zoccheddu∗1, Shah Rukh Qasim1, Patrick Owen2, Nicola Serra1",Not found,Not found,"Large Language Models, Physics Instrument Design, Reinforcement Learning, Detector Design, Particle-Matter Interactions","This study investigates the use of large language models (LLMs) for physics instrument design and compares their performance to reinforcement learning (RL). LLMs, given task constraints and summaries of prior high-scoring designs, propose complete detector configurations, which are evaluated using simulators and reward functions similar to those used in RL-based optimization. Despite having no task-specific training, modern LLMs consistently generate valid, resource-aware, and physically meaningful configurations that draw on broad pretrained knowledge of detector design principles and particle-matter interactions.",28.29,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07582v2_ES-Mem Event Segmentation-Based Memory for Long-Te.pdf,ES-Mem: Event Segmentation-Based Memory for Long-Term Dialogue Agents,"Huhai Zou, Tianhao Sun†, Chuanjiang He, Yu Tian, Zhenyang Li, Li Jin, Nayu Liu, Jiang Zhong, Kaiwen Wei†",Not found,Not found,"Memory, Dialogue Agents, Event Segmentation, Hierarchical Memory, Context Localization","Memory is critical for dialogue agents to maintain coherence and enable continuous adaptation in long-term interactions. ES-Mem, a framework incorporating dynamic event segmentation and hierarchical memory, mitigates the limitations of existing memory mechanisms by leveraging boundary semantics to anchor specific episodic memory for precise context localization.",25.31,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07597v1_Pheromone-Focused Ant Colony Optimization algorith.pdf,Pheromone-Focused Ant Colony Optimization algorithm for path planning,"Yi Liu, Hongda Zhang, Zhongxue Gan, Yuning Chen, Ziqing Zhou, Chunlei Meng, Chun Ouyang",Not found,Not found,"Ant Colony Optimization, Path Planning, Pheromone-Focused, Forward-Looking Mechanism, Global Optimization","This paper proposes the Pheromone-Focused Ant Colony Optimization (PFACO) algorithm to enhance the problem-solving ability of the ant colony in path planning. It introduces three key strategies: concentrated initial pheromone distribution, reinforcement of promising solutions, and a forward-looking mechanism to penalize redundant path turns, leading to smoother and more efficient solutions.",27.81,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07606v1_Proof of Time A Benchmark for Evaluating Scientifi.pdf,Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments,"Bingyang Ye1,2†, Shan Chen1,2,3†, Jingxuan Tu4, Chen Liu5, Zidi Xiong1, Samuel Schmidgall6, Danielle S. Bitterman1,2,3§",Not found,Not found,"scientific idea evaluation, forecasting, benchmarks, time-indexed evaluation, AI for Science","A benchmarking framework called Proof of Time (PoT) is introduced to link scientific idea judgments to downstream signals that become observable later, enabling verifiable evaluation when ground truth arrives, scalable benchmarking without exhaustive expert annotation, and analysis of human–model misalignment against signals such as peer-review awards. PoT supports scalable evaluation of agents on future-facing scientific idea judgment tasks.",25.44,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07611v1_DIAGPaper Diagnosing Valid and Specific Weaknesses.pdf,DIAGPaper: Diagnosing Valid and Specific Weaknesses in Scientific Papers via Multi-Agent Reasoning,"Zhuoyang Zou, Abolfazl Ansari, Delvin Ce Zhang†, Dongwon Lee, Wenpeng Yin",Not found,Not found,"paper weakness identification, multi-agent reasoning, scientific review, author rebuttal","A novel multi-agent framework for diagnosing valid and specific weaknesses in scientific papers, addressing limitations of existing approaches by simulating human-defined review criteria, introducing structured debate between author and reviewer agents, and prioritizing the most consequential weaknesses for users.",26.31,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07618v1_Neural Architecture for Fast and Reliable Coagulat.pdf,Neural Architecture for Fast and Reliable Coagulation Assessment in Clinical Settings: Leveraging Thromboelastography,"Yulu Wang, Ziqian Zeng, Jianjun Wu, Zhifeng Tang",,,"Thromboelastography, Coagulation, Clinical AI, Deep Learning, Few-Shot Learning, Domain Adaptation, Real-Time Monitoring","Presenting Physiological State Reconstruction (PSR), a new algorithm designed to make reliable predictions and diagnoses from small amounts of clinical data, and MDFE for integrating varied temporal signals. Achieves remarkable performance in predicting coagulation traits and reducing errors compared to state-of-the-art methods.",26.89,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07632v2_GeoMotionGPT Geometry-Aligned Motion Understanding.pdf,GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models,"Zhankai Ye, Bofan Li, Yukai Jin, Shuoqiu Li, Wei Wang, Yanfu Zhang, Shangqiu Gao, Xin Liu",https://doi.org/https://github.com/JYe16/GeoMotionGPT https://doi.org/https://huggingface.co/zy22b/GeoMotionGPT,,"motion understanding, large language models, motion tokenization, semantic embedding, geometry alignment","This paper presents a novel framework that explicitly enforces orthogonality on both the motion codebook and the large language model (LLM) embedding space, ensuring that their relational structures naturally mirror each other. This approach bridges the modalities by using a sparse projection that maps motion codes into the LLM embedding space while preserving orthogonality, and it enforces soft constraints during tokenizer training and LLM fine-tuning to maintain geometric alignment without hindering semantic adaptation. Extensive experiments on HumanML3D demonstrate that the framework achieves a 20% performance improvement over current state-of-the-art methods.",27.71,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07635v2_Learning About Learning A Physics Path from Spin G.pdf,Learning About Learning: A Physics Path from Spin Glasses to Artificial Intelligence,"Denis D. Caprioti, Matheus Haas, Constantino F. Vasconcelos, Mauricio Girardi-Schappo",Not found,Not found,"Spin glasses, Neural networks, Artificial intelligence, Statistical physics, Dynamical systems, Linear algebra, Computational methods","This paper presents the Hopfield model as a pedagogically rich framework that naturally unifies core topics from undergraduate statistical physics, dynamical systems, linear algebra, and computational methods. It provides a concise and illustrated theoretical introduction grounded in familiar physics concepts, analyzes the model's energy function, dynamics, and pattern stability, and discusses practical aspects of simulation, including a freely available simulation code. The work aims to help prepare physics students to understand, apply, and critically engage with computational tools increasingly central to research, industry, and society.",27.33,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07638v1_SALT-KG A Benchmark for Semantics-Aware Learning o.pdf,SALT-KG: A Benchmark for Semantics-Aware Learning on Enterprise Tables,"Isaiah Onando Mulang', Felix Sasaki, Tassilo Klein, Jonas Kolk, Nikolay Grechanov, Johannes Hoffart",10.48550/arXiv.2601.07638,2601.07638,"Semantics-Aware Learning, Enterprise Tables, Metadata Knowledge Graph, Tabular Data, Relational Prediction","This paper introduces SALT-KG, a benchmark for semantics-aware learning on enterprise tables, extending the SALT benchmark by linking multi-table transactional data with a structured Operational Business Knowledge (OBKG) that captures field-level semantics and business object-types. The empirical analysis reveals that metadata-derived features improve classical prediction metrics and highlight gaps in models' ability to leverage semantics in relational context.",28.24,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07641v1_Beyond Static Tools Test-Time Tool Evolution for S.pdf,Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning,"Jiaxuan Lu1,†, Ziyu Kong2,†, Yemin Wang3,†, Rong Fu4,∗, Haiyuan Wan1,5, Cheng Yang6, Wenjie Lou1, Haoran Sun1, Lilong Wang1, Yankai Jiang1, Xiaosong Wang1, Xiao Sun1, Dongzhan Zhou1",Not provided,Not provided,"AI for Science, Test-Time Tool Evolution, Scientific Reasoning, Executable Tools, Tool Evolution, Scientific Computing","This paper proposes Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries, facilitating rigorous evaluation through a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, enabling effective cross-domain adaptation of computational tools.",28.25,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07651v1_Active Evaluation of General Agents Problem Defini.pdf,Active Evaluation of General Agents: Problem Definition and Comparison of Baseline Algorithms,"Marc Lanctot, Kate Larson, Ian Gemp, Michael Kaisers",Not found,Not found,"general evaluation, multitask evaluation, ranking, active learning, game theory, social choice theory","This paper proposes a formal definition and conceptual framework for active evaluation of agents across multiple tasks, assessing the performance of ranking algorithms as a function of number of evaluation data samples. It compares several baselines, including the classical Elo rating system and a recently-proposed method, Soft Condorcet Optimization, under different experimental contexts.",26.69,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07654v1_Towards Automating Blockchain Consensus Verificati.pdf,Towards Automating Blockchain Consensus Verification with IsabeLLM,"Elliot Jones, William Knottenbelt",Not found,2601.07654,"Blockchain, Consensus, Formal Verification, Theorem Proving, Artificial Intelligence","Consensus protocols are crucial for blockchain systems, requiring correct design and implementation to prevent adversaries from carrying out malicious behavior. Formal verification ensures correctness but requires high effort and expertise. This paper presents IsabeLLM, a tool integrating Isabelle with a Large Language Model to automate proofs. It demonstrates the tool's effectiveness by verifying Bitcoin's Proof of Work consensus protocol.",28.29,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07663v2_Reasoning Models Will Blatantly Lie About Their Re.pdf,Reasoning Models Will Blatantly Lie About Their Reasoning,"William Walden, Johns Hopkins University",,,"Large Reasoning Models, Model Transparency, CoT Faithfulness, Prompt Hints","This study extends previous work on Large Reasoning Models (LRMs) showing that these models will not only omit information about how they use hints in their reasoning but will also lie about it, denying reliance on hints even when explicitly instructed to check for and report on them. The findings have implications for monitoring and interpreting CoTs.",26.11,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07666v1_Variational Contrastive Learning for Skeleton-base.pdf,Variational Contrastive Learning for Skeleton-based Action Recognition,"Dang-Dinh NGUYEN, Decky ASPANDI-LATIF, Titus ZAHARIA",Not found,2601.07666,"Human Action Recognition, Self - Supervised Learning, Variational Inference","In recent years, self-supervised representation learning for skeleton-based action recognition has advanced with the development of contrastive learning methods. However, most contrastive paradigms are inherently discriminative and often struggle to capture the variability and uncertainty intrinsic to human motion. To address this issue, we propose a variational contrastive learning framework that integrates probabilistic latent modeling with contrastive self-supervised learning. This formulation enables the learning of structured and semantically meaningful representations that generalize across different datasets and supervision levels. Extensive experiments on three widely used skeleton-based action recognition benchmarks show that our proposed method consistently outperforms existing approaches, particularly in low-label regimes. Moreover, qualitative analyses show that the features provided by our method are more relevant given the motion and sample characteristics, with more focus on important skeleton joints, when compared to the other methods.",29.17,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07667v1_Adaptive Layer Selection for Layer-Wise Token Prun.pdf,Adaptive Layer Selection for Layer-Wise Token Pruning in LLM Inference,"Rei Taniguchi, Yuyang Dong, Makoto Onizuka, Chuan Xiao",,,"Large Language Models, Key-Value Cache Reduction, Layer-Wise Token Pruning, Attention Patterns, Transformer Layers","This paper proposes ASL, a training-free method that adaptively chooses the selection layer for layer-wise token pruning in LLM inference. By exploiting the variance of token ranks ordered by attention score, ASL balances performance across different tasks while meeting the user-specified KV budget requirement. The method operates during the prefilling stage and can be jointly used with existing KV cache reduction methods such as SnapKV to optimize the decoding stage. Evaluations on benchmarks show that ASL outperforms state-of-the-art layer-wise token selection methods in accuracy while maintaining decoding speed and KV cache reduction.",27.55,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07685v1_Predictive Analytics for Dementia Machine Learning.pdf,Predictive Analytics for Dementia: Machine Learning on Healthcare Data,"1st Shafiul Ajam Opee, 2nd Nafiz Fahad, 3rd Anik Sen, 4th Rasel Ahmed, 5th Fariha Jahan, 6th Md. Kishor Morol, 7th Md Rashedul Islam",,,"Dementia, Machine learning, Linear Discriminant Analysis (LDA), APOE-ϵ4 allele","This study focuses on enhancing dementia prediction using machine learning techniques on patient health data. Supervised learning algorithms including K-Nearest Neighbors (KNN), Quadratic Discriminant Analysis (QDA), Linear Discriminant Analysis (LDA), and Gaussian Process Classifiers are applied. Techniques such as Synthetic Minority Over-sampling Technique (SMOTE) and Term Frequency-Inverse Document Frequency (TF-IDF) vectorization are used to address class imbalance and improve model performance. LDA achieved the highest testing accuracy of 98%. This study highlights the importance of model interpretability and the correlation of dementia with features such as the presence of the APOE-ϵ4 allele and chronic conditions like diabetes. Future ML innovations, particularly in integrating explainable AI approaches, are advocated to further improve predictive capabilities in dementia care.",28.53,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07701v1_Deep Whole-body Parkour.pdf,Deep Whole-body Parkour,"Ziwen Zhuang, Shaoting Zhu, Mengjie Zhao, Hang Zhao†",Not found,2601.07701,"Humanoid Robotics, Deep Reinforcement Learning, Parkour, Robust Control, Multi-contact Movements","Our framework enables a humanoid robot to autonomously traverse challenging obstacles that impose strict geometric constraints on robot odometry. The system utilizes onboard perception to proactively adjust its approach trajectory, ensuring precise foot placement and hand contact for successful whole-body interaction.",26.99,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07718v1_Hiking in the Wild A Scalable Perceptive Parkour F.pdf,Hiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoids,"Shaoting Zhu, Ziwen Zhuang, Mengjie Zhao, Kun-Ying Lee, Hang Zhao",,2601.07718,"Humanoid Robotics, Parkour, Perception, Reinforcement Learning, Robust Navigation","This work presents Hiking in the Wild, a scalable, end-to-end parkour perceptive framework designed for robust humanoid hiking in complex, unstructured environments. It enables a humanoid robot to traverse diverse terrains in both indoor and outdoor settings with a maximum speed of 2.5 m/s over complex terrain, negotiating stairs, gaps, high platforms, and ramps. The framework uses depth images for perception and introduces two key mechanisms: a foothold safety mechanism and a flat patch sampling strategy to ensure safety and training stability.",29.41,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07737v1_Evaluating the encoding competence of visual langu.pdf,Evaluating the encoding competence of visual language models using uncommon actions,"Chen Ling, Nai Ding*",,2601.07737v1,"undergraduate project, visual language models, encoding competence, uncommon actions",This paper evaluates the performance of visual language models in understanding and encoding uncommon actions.,30.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07748v1_Improving Domain Generalization in Contrastive Lea.pdf,Improving Domain Generalization in Contrastive Learning using Adaptive Temperature Control,"Robert Lewis∗, Katie Matton∗, Rosalind W. Picard, John Guttag",10.48550/arxiv.2601.07748,2601.07748,"contrastive learning, domain generalization, covariate shift, temperature control","This paper presents a method for improving domain generalization in contrastive learning by incorporating domain labels, which helps in maintaining in-distribution performance while significantly enhancing out-of-distribution generalization.",27.34,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07778v1_DT-ICU Towards Explainable Digital Twins for ICU P.pdf,DT-ICU: Towards Explainable Digital Twins for ICU Patient Monitoring via Multi-Modal and Multi-Task Iterative Inference,Wen Guo,Not found,2601.07778v1,"digital twin, intensive care, multi-modal, multi-task, iterative inference, patient monitoring, risk estimation","We introduce DT-ICU, a multimodal digital twin framework for continuous risk estimation in intensive care, integrating clinical time series with static patient information in a unified multitask architecture.",28.84,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07779v1_OS-Symphony A Holistic Framework for Robust and Ge.pdf,OS-SYMPHONY: A Holistic Framework for Robust and Generalist Computer-Using Agents,"Bowen Yang, Kaiming Jin, Zhenyu Wu, Zhaoyang Liu, Qiushi Sun, Zehao Li, Jingjing Xie, Zhoumianze Liu, Fangzhi Xu, Kanzhi Cheng, Qingyun Li, Yian Wang, Yu Qiao, Zun Wang, Zichen Ding",Not found,Not found,"Computer-Using Agents, Vision-Language Models, Holistic Framework, Robust Automation, Generalist Agents, Long-Horizon Tasks, Tutorial Retrieval, Trajectory-Level Self-Correction, Multimodal Searcher, Browser-Based Sandbox","OS-SYMPHONY is introduced as a holistic framework to address the limitations of current Computer-Using Agent (CUA) frameworks, which struggle with robustness in long-horizon workflows and generalization in novel domains. It comprises an Orchestrator coordinating two key innovations: a Reflection-Memory Agent utilizing milestone-driven long-term memory for trajectory-level self-correction, and Versatile Tool Agents featuring a Multimodal Searcher to navigate a browser-based sandbox and synthesize live, visually aligned tutorials. Experimental results demonstrate substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks.",27.87,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07782v1_Beyond Single-Shot Multi-step Tool Retrieval via Q.pdf,Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning,"Wei Fang, James Glass",,,"tool retrieval, query planning, reinforcement learning, large language models","This paper proposes TOOLQP, a lightweight framework that models retrieval as iterative query planning to address the challenges of complex tool retrieval in dynamic agentic workflows. It decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition.",25.65,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07790v1_Benchmarking Small Language Models and Small Reaso.pdf,Benchmarking Small Language Models and Small Reasoning,"Yahya Masri, Emily Ma, Zifu Wang, Joseph Rogers, Chaowei Yang",Not found,2601.07790,"Language models, System logs, Severity classification, Zero-shot, Few-shot, Retrieval-augmented generation","This paper evaluates nine small language models and small reasoning language models on system log severity classification using real-world journalctl data from Linux production servers. The results show strong stratification, with Qwen3-4B achieving the highest accuracy at 95.64% with retrieval-augmented generation, and Gemma3-1B improving from 20.25% under few-shot prompting to 85.28% with retrieval-augmented generation. The findings suggest that architectural design, training objectives, and the ability to integrate retrieved context jointly determine performance.",27.92,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07794v1_Kinship Data Benchmark for Multi-hop Reasoning.pdf,Kinship Data Benchmark for Multi-hop Reasoning,"Tianda Sun, Dimitar Kazakov",,,"Kinship, Multi-hop reasoning, Language models, Genealogy, Family trees","A benchmark designed to probe the ability of large language models to perform multi-hop reasoning over kinship relations, using a generative pipeline to produce large-scale, realistic, and culture-specific genealogical data.",25.15,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07821v1_Failure-Aware RL Reliable Offline-to-Online Reinfo.pdf,Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation,"Huanyu Li, Kun Lei, Sheng Zang, Kaizhe Hu, Yongyuan Liang, Bo An, Xiaoli Li, Huazhe Xu",Not provided,Not provided,"Reinforcement Learning, Offline-to-Online RL, Failure Detection, Self-Recovery, Real-World Manipulation","This paper introduces Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm designed to minimize failures during real-world reinforcement learning. FARL integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulations and real-world experiments demonstrate the effectiveness of FARL in significantly reducing Intervention-requiring Failures (IR Failures) while improving performance and generalization during online reinforcement learning post-training.",28.28,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07832v2_MHLA Restoring Expressivity of Linear Attention vi.pdf,MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head,"Kewei Zhang, Ye Huang, Yufan Deng, Jincheng Yu, Junsong Chen, Huan Ling, Enze Xie, Daquan Zhou",Not found,2601.07832,"Transformer, Linear Attention, Multi-Head, Expressive Power, Efficiency","MHLA proposes a new approach to linear attention by dividing heads along the token dimension, preserving representational diversity and maintaining linear complexity while recovering expressive power of softmax attention.",27.78,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07885v1_Small Symbols Big Risks Exploring Emoticon Semanti.pdf,"Small Symbols, Big Risks: Exploring Emoticon Semantic Confusion in Large Language Models","Weipeng Jiang, Xiaoyu Zhang, Juan Zhai, Shiqing Ma, Chao Shen, Yang Liu",Not found,Not found,"Emoticons, Large Language Models, Semantic Confusion, Safety Implications, Digital Communication, User Intent, Security Consequences","This paper identifies emoticon semantic confusion, a vulnerability where Large Language Models misinterpret ASCII-based emoticons to perform unintended and potentially destructive actions. The study reveals that this vulnerability is pervasive, with over 90% of confused responses leading to 'silent failures'. The research also shows that this vulnerability transfers to popular agent frameworks and that existing prompt-based mitigations are largely ineffective. The authors call for community recognition and development of effective mitigation methods to ensure the safety and reliability of human-AI interactions.",27.64,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07891v1_KVzap Fast Adaptive and Faithful KV Cache Pruning.pdf,"KVzap: Fast, Adaptive, and Faithful KV Cache","Simon Jégou *, Maximilian Jeblick",Not found,2601.07891v1,"transformer attention, language models, key-value cache, inference bottleneck, fast pruning","Growing context lengths in transformer-based language models have made the key-value (KV) cache a critical inference bottleneck. KVzap introduces a fast, input-adaptive approximation of KVzip, achieving 2–4× KV cache compression with negligible accuracy loss and state-of-the-art performance on the KVpress Leaderboard.",27.49,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07892v1_Sherry Hardware-Efficient 1.25-Bit Ternary Quantiz.pdf,Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification,"Hong Huang, Decheng Wu, Qiangqiang Hu, Guanghua Yu, Jinhai Yang, Jianchen Zhu, Xue Liu, Dapeng Wu",Not found,Not found,"Hardware efficiency, Ternary quantization, Fine-grained sparsification, Weight quantization, Edge computing","The deployment of Large Language Models (LLMs) on resource-constrained edge devices is increasingly hindered by prohibitive memory and computational requirements. Ternary quantization offers a compelling solution by reducing weights to {−1,0,+1}, but current implementations suffer from a fundamental misalignment with commodity hardware. Sherry proposes a hardware-efficient ternary quantization framework that introduces a 3:4 fine-grained sparsity to achieve a regularized 1.25-bit width by packing blocks of four weights into five bits, restoring power-of-two alignment. Sherry also addresses the weight trapping issue in sparse ternary training, which leads to representational collapse, by introducing an annealing residual synapse mechanism. Empirical evaluations demonstrate that Sherry matches state-of-the-art ternary performance while significantly reducing model size.",27.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07894v1_Revealing the Attention Floating Mechanism in Mask.pdf,Revealing the Attention Floating Mechanism in Masked Diffusion Models,"Xin Dai, Pengcheng Huang, Zhenghao Liu, Shuo Wang, Yukun Yan, Chaojun Xiao, Yu Gu, Ge Yu, Maosong Sun",Not found,Not found,"Masked Diffusion Models, Attention Floating, Bidirectional Attention, Diffusion Language Models, Auto-regressive Models, Attention Sinks, In-context Learning","This paper investigates the attention mechanism in Masked Diffusion Models (MDMs), revealing the phenomenon of Attention Floating. Unlike Auto-regressive Models (ARMs), where attention converges to a fixed sink, MDMs exhibit dynamic, dispersed attention anchors that shift across denoising steps and layers. The paper empirically demonstrates that this distinctive attention pattern provides a mechanistic explanation for the strong in-context learning capabilities of MDMs, allowing them to double the performance compared to ARMs in knowledge-intensive tasks.",27.96,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07898v1_Large Language Models and Algorithm Execution Appl.pdf,Large Language Models and Algorithm Execution: Application to an Arithmetic Function,"Farah Ben Slama, Frédéric Armetta",,,"Algorithmic learning in natural language, Supervised learning by decomposition, Large language model, Fine-tuning","This paper investigates the possibility of extending Large Language Models' capabilities to algorithm execution through specialized supervised training focused on reasoning decomposition. It introduces LLM-DAL, a training model, demonstrating that LLMs' ability to perform complex algorithmic inferences and generalize can be significantly improved when the training method is properly designed to guide the model in its learning process.",27.01,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07901v1_Decentralized Online Convex Optimization with Unkn.pdf,Decentralized Online Convex Optimization with Unknown Feedback Delays,"Hao Qiu, Mengxiao Zhang, Juliette Achddou",Not provided,2601.07901,"Decentralized Optimization, Online Convex Optimization, Feedback Delays, Gossip Algorithm, Spectral Gap","This paper studies decentralized online convex optimization under unknown, time- and agent-varying feedback delays, proposing a novel algorithm that achieves an improved regret bound and extending the framework to the strongly convex setting.",26.72,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07903v2_Enhancing Large Language Models for Time-Series Fo.pdf,Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning,"Jianqi Zhang, Jingyao Wang, Wenwen Qiang, Fanjiang Xu, Changwen Zheng",XXXXXXX.XXXXXXX,,"Time Series Forecasting, Large Language Model, In-context Learning","The paper proposes LVICL, a method to improve the forecasting performance of LLM4TSF by injecting example information into a frozen LLM, thereby enhancing its performance on time series forecasting tasks. This approach aims to reduce computational overhead while maintaining good prediction quality.",26.66,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07935v1_Towards Specialized Generalists A Multi-Task MoE-L.pdf,Towards Specialized Generalists: A Multi-Task MoE-LoRA Framework for Domain-Specific LLM Adaptation,"Yuxin Yang, Aoxiong Zeng, Xiangquan Yang",Not found,2601.07935,"Large Language Models, Domain Adaptation, Low-Rank Adaptation, Mixture-of-Experts, Medical Applications","This paper proposes Med-MoE-LoRA, a novel framework integrating Mixture-of-Experts and Low-Rank Adaptation to enable efficient multi-task domain adaptation for medical scenarios, addressing challenges such as the 'Stability-Plasticity Dilemma' and 'Task Interference'.",27.97,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07939v1_SECite Analyzing and Summarizing Citations in Soft.pdf,SECite: Analyzing and Summarizing Citations in Software Engineering Literature,"Shireesh Reddy Pyreddy, Khaja Valli Pathan, Hasan Masum, Tarannum Shaila Zaman",Not found,Not found,"Sentiment Analysis, LLMs, Text Summarization, Citations","This research introduces SECite, a novel approach for evaluating scholarly impact through sentiment analysis of citation contexts. It develops a semi-automated pipeline to extract citations and applies advanced natural language processing techniques with unsupervised machine learning to classify citation statements as positive or negative. It also uses generative AI to produce sentiment-specific summaries that capture the strengths and limitations of each target paper, derived from clustered citation groups and the full text. The findings reveal meaningful patterns in how the academic community perceives these works, highlighting areas of alignment and divergence between external citation feedback and the authors' own presentation.",28.34,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07941v2_Moonworks Lunara Aesthetic Dataset.pdf,Moonworks Lunara Aesthetic Dataset,"Yan Wang, M M Sayeef Abdullah, Partho Hassan, Sabit Hassan",10.48550/arxiv.2601.07941,2601.07941,"text-to-image generation, prompt grounding, style conditioning, aesthetic analysis, Moonworks Lunara","This data card presents the first public release of the Lunara Aesthetic Dataset, a curated set of 2,000 image–prompt pairs for research on prompt grounding and style conditioning in text-to-image generation systems.",28.56,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07946v1_Coupled Diffusion-Encoder Models for Reconstructio.pdf,DiffCoder: Coupled Diffusion–Encoder Models for Reconstruction of Flow Fields,"AmirPouya Hemmasian, Amir Barati Farimani",Not found,Not found,"Flow Field Reconstruction, Diffusion Models, Autoencoders, Kolmogorov Flow, Deep Learning","DiffCoder proposes a coupled framework integrating a probabilistic diffusion model with a conventional convolutional ResNet encoder for data-driven flow-field reconstruction. It aims to recover distributional and spectral properties of fluid flows, improving spectral accuracy under aggressive compression compared to Variational Autoencoders.",27.27,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07948v1_Reinforcement Learning Methods for Neighborhood Se.pdf,Reinforcement Learning Methods for Neighborhood Selection in Local Search,"Yannick Molinghen1, Augustin Delecluse2,3, Renaud De Landtsheer4, Stefano Michelini4",,,"Local Search, Reinforcement Learning, Combinatorial Optimization","This study evaluates reinforcement learning-based neighborhood selection strategies in local search metaheuristics, comparing them against multiple baselines across three problems: the traveling salesman problem, the pickup and delivery problem with time windows, and the car sequencing problem. It highlights the need for carefully designed reward functions to provide stable and informative learning signals, and shows that ε-greedy consistently ranks among the best performers, while deep reinforcement learning approaches have higher computational overhead and longer runtime.",27.34,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07951v1_Hybrid SARIMA LSTM Model for Local Weather Forecas.pdf,Hybrid SARIMA–LSTM Model for Local Weather Forecasting: A Residual-Learning Approach for Data-Driven Meteorological Prediction,"Shreyas Rajeev, Karthik Mudenahalli, Amit Mallappa",,,"weather forecasting, SARIMA, LSTM, residual learning, Fourier seasonal encoding, long-term prediction","This paper proposes a hybrid SARIMA–LSTM model for local weather forecasting, combining the strengths of classical statistical models and deep learning techniques to address the limitations of both. The model breaks temperature into a climate component that can be predicted using SARIMA and a weather component that is not linear, learned by LSTM. Fourier seasonal encoding is used to clearly show the yearly cycle, and a new stabilized recursive forecasting mechanism is introduced to predict up to a 293-day future horizon without direct observation. The goal is to accurately predict daily average temperature in New York City using data from 2020 to 2023.",28.87,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07953v1_Quantum automated theorem proving.pdf,Quantum automated theorem proving,"Zheng-Zhi Sun, Qi Ye, Dong-Ling Deng",Not found,Not found,"Quantum computing, Automated theorem proving, Quantum resolution, Quantum algebraic proving, Automated reasoning, Geometric theorems","Proposes a generic framework for quantum automated theorem proving, leveraging intrinsic quantum superposition and entanglement features to achieve quadratic reduction in query complexity for propositional and first-order logic, and demonstrates how a quantum computer can prove geometric theorems with quadratic better query complexity.",27.39,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07957v1_LWMSCNN-SE A Lightweight Multi-Scale Network for E.pdf,LWMSCNN-SE: A Lightweight Multi-Scale Network for Efficient Maize Disease Classification on Edge Devices,"Fikadu Weloday, Jianmei Su",,,"lightweight CNN, multi-scale feature extraction, attention mechanism, plant pathology","This paper proposes LWMSCNN-SE, a lightweight convolutional neural network (CNN) that integrates multi-scale feature extraction, depthwise separable convolutions, and squeeze-and-Excitation (SE) attention mechanisms, achieving 96.63% classifica- tion accuracy with only 241,348 parameters and 0.666 GFLOPs, suitable for real-time deployment in field applications.",27.65,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07958v1_LJ-Spoof A Generatively Varied Corpus for Audio An.pdf,LJ-SPOOF: A GENERATIVELY V ARIED CORPUS FOR AUDIO ANTI-SPOOFING AND SYNTHESIS SOURCE TRACING,"Surya Subramani, Hashim Ali, Hafiz Malik",,,"Anti-Spoofing, Speaker Verification, Deepfake, Source tracing, Synthetic Speech","Speaker-specific anti-spoofing and synthesis-source tracing are central challenges in audio anti-spoofing. This paper introduces LJ-Spoof, a speaker-specific, generatively diverse corpus that systematically varies prosody, vocoders, generative hyperparameters, bona fide prompt sources, training regimes, and neural post-processing. The corpus spans one speaker including studio-quality recordings, 30 TTS families, 500 generatively variant subsets, 10 bona fide neural-processing variants, and more than 3 million utterances. This variation-dense design enables robust speaker-conditioned anti-spoofing and fine-grained synthesis-source tracing.",27.69,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07964v1_Executable Ontologies in Game Development From Alg.pdf,Executable Ontologies in Game Development: From Algorithmic Control to Semantic World Modeling,Alexander Boldachev,,,"executable ontologies, game ai, behavior trees, goap, event semantics, dataflow architecture, semantic modeling","This paper examines the application of Executable Ontologies (EO) in game development, arguing that EO represents a paradigm shift from algorithmic behavior programming to semantic world modeling. It demonstrates EO's use in a survival game scenario to achieve priority-based task interruption through dataflow conditions, contrasting it with Behavior Trees (BT) and Goal-Oriented Action Planning (GOAP).",28.29,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07965v1_When Models Know When They Do Not Know Calibration.pdf,WHENMODELSKNOWWHENTHEYDONOTKNOW,"Chenjie Hao, Weyl Lu, Yuko Ishiwaka, Zengyi Li, Weier Wan, Yubei Chen",Not found,Not found,"model calibration, model uncertainty, confidence estimation, model cascading, data cleaning","This work proposes a simple, effective, and universal training-free method for model calibration, cascading, and data cleaning, applicable to both vision and language models. The method leverages calibrated confidence to improve model efficiency and reliability.",26.7,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07969v1_Tuberculosis Screening from Cough Audio Baseline M.pdf,"Tuberculosisscreening fromcoughaudio: Baselinemodels, clinicalvariables, anduncertaintyquantification","George P. Kafentzis, Efstratios Selisios",Not found,2601.07969,"Tuberculosis, Machine Learning, Cough Audio, Cross-Validation, Uncertainty Quantification, Feature Extraction","This paper proposes a standardized framework for automatic tuberculosis (TB) detection from cough audio and clinical data using machine learning. It addresses the gap in progress measurement by establishing a strong, well-documented baseline using a recently compiled dataset from several countries. The pipeline covers feature extraction, multimodal fusion, cougher-independent evaluation, and uncertainty quantification, and reports clinically relevant metrics for fair comparison.",27.69,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07973v1_Cultural Compass A Framework for Organizing Societ.pdf,Cultural Compass: A Framework for Organizing Societal Norms to Detect Violations in Human-AI Conversations,"Myra Cheng, Vinodkumar Prabhakaran, Alice Oh, Hayk Stepanyan, Aishwarya Verma, Charu Kalia, Erin MacMurray van Liemt, Sunipa Dev",Not provided,2601.07973,"Generative AI, Cultural norms, Human-AI interaction, Norm adherence, Sociocultural norms","This paper introduces a taxonomy of societal norms to clarify their contexts, specifications, and mechanisms, and demonstrates how this taxonomy can be used to automatically evaluate models' adherence to these norms in naturalistic, open-ended settings. The authors find that state-of-the-art models frequently violate norms, with rates varying by model, interactional context, and country. They also show that violation rates vary by prompt intent and situational framing. The taxonomy and evaluation pipeline enable nuanced, context-sensitive evaluation of cultural norm adherence in realistic settings.",27.7,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07988v1_From Word Sequences to Behavioral Sequences Adapti.pdf,From Word Sequences to Behavioral Sequences: Adapting Modeling and Evaluation Paradigms for Longitudinal NLP,"Adithya V Ganesan†‡, Vasudha Varadarajan⊙, Oscar NE Kjell‡, Whitney R Ringwaldδ, Scott Feltman†, Benjamin J Luft†, Roman Kotov†, Ryan L Boyd∆Θ, H Andrew Schwartz†‡",,,"longitudinal NLP, behavioral sequences, diary studies, mental health prediction, person-indexed sequences","While traditional NLP treats documents as independent and unordered samples, longitudinal studies reveal that documents are nested within authors and ordered in time, forming person-indexed, time-ordered behavioral sequences. This mismatch propagates through the standard NLP pipeline, leading to issues such as leaking person-specific signal, scrambling temporal order, ignoring informative history, and lacking clear real-world generalization goals. The authors propose a longitudinal modeling and evaluation paradigm that updates four parts of the pipeline: evaluation splits, accuracy metrics, sequence inputs, and model internals. They demonstrate the issues with traditional pipeline and propose improvements on a dataset of 17k daily diary transcripts paired with PTSD symptom severity from 238 participants, finding that traditional document-level evaluation can yield substantially different and sometimes reversed conclusions compared to their ecologically valid modeling and evaluation.",28.07,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.07994v2_DYCP Dynamic Context Pruning for Long-Form Dialogu.pdf,DYCP: Dynamic Context Pruning for Long-Form Dialogue with LLMs,"Nayoung Choi, Jonathan Zhang, Jinho D. Choi",,,"Large Language Models, Dialogue Systems, Context Management, Dynamic Pruning, Long-Form Conversations","This paper introduces DYCP, a lightweight context management method for long-form dialogue with Large Language Models (LLMs). It dynamically segments and retrieves relevant memory at query time, preserving the sequential structure of dialogue without predefined topic boundaries and supporting efficient, adaptive context retrieval.",27.98,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08000v1_Reasoning over Precedents Alongside Statutes Case-.pdf,Reasoning over Precedents Alongside Statutes,"Can Jin, Rui Wu, Tong Che, Qixin Zhang, Hongwu Peng, Jiahui Zhao, Wenqi Wei, Ligong Han, Zhao Zhang, Yuan Cao, Ruixiang Tang, Dimitris N. Metaxas",Not found,Not found,"Large Language Models, Safety, Deliberative Alignment, Reinforcement Learning, LLMs, Cybersecurity","This work evaluates the effectiveness of explicitly specifying extensive safety codes versus demonstrating them through illustrative cases in Large Language Models (LLMs). It proposes CADA, a case-augmented deliberative alignment method for LLMs, which enhances harmlessness, robustness, and utility while maintaining helpfulness.",34.04,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08003v1_LLM Review Enhancing Creative Writing via Blind Pe.pdf,LLM Review: Enhancing Creative Writing via Blind Peer Review Feedback,"Weiyue Li*, Mingxiao Song∗, Zhenda Shen∗, Dachuan Zhao∗, Yunfan Long, Yi Li, Yongce Li, Ruyi Yang, Mengyu Wang",,,"Large Language Models, Creative Writing, Peer Review, Human-Large Language Model Collaboration","A framework that enhances creativity by constraining rather than maximizing information flow through a mechanism called Blind Peer Review, where agents provide targeted feedback on peers' initial drafts but revise independently.",34.32,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08005v1_Internal Deployment Gaps in AI Regulation.pdf,Internal Deployment Gaps in AI Regulation,JOE Kwon∗ and STEPHEN CASPER,Not provided,Not provided,"AI regulation, internal deployment, frontier AI, regulatory gaps","This paper examines the gaps in existing AI regulation frameworks that may limit oversight of internal deployment of AI systems. It identifies three main gaps: scope ambiguity, point-in-time compliance assessments, and information asymmetries. The paper analyzes why these gaps persist and proposes potential approaches to address them.",35.59,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08011v1_TP-Blend Textual-Prompt Attention Pairing for Prec.pdf,TP-Blend: Textual-Prompt Attention Pairing for Precise Object-Style Blending in Diffusion Models,"Xin Jin, Yichuan Zhong, Yapeng Tian",Not provided,2601.08011,"Diffusion models, Text-conditioned editing, Object blending, Attention fusion","TP-Blend is a lightweight framework that injects two separate textual prompts into a single denoising trajectory, achieving precise object-style blending in diffusion models.",35.22,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08017v1_Representations of Text and Images Align From Laye.pdf,Representations of Text and Images Align From Layer One,"Evzen Wybitul, Javier Rando, Florian Tram`er, Stanislav Fort",Not found,2601.08017,"vision-language models, modal alignment, image-text alignment, DeepDream, representation space","This paper demonstrates that for various concepts in adapter-based vision-language models, the representations of their images and text descriptions are meaningfully aligned from the very first layer, contradicting the established view that such alignment only appears in late layers. The authors use a synthesis-based method inspired by DeepDream to show this, and find that synthesised images often depict salient visual features of the targeted textual concepts.",37.98,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08026v2_FigEx2 Visual-Conditioned Panel Detection and Capt.pdf,FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures,"Jifeng Song, Arun Das, Pan Wang, Hui Ji, Kun Zhao, Yufei Huang",,,"Scientific compound figures, Panel detection, Captioning, Open-ended captioning, Noise-aware gated fusion, Staged optimization, Supervised learning, Reinforcement learning, CLIP-based alignment, BERTScore-based semantic rewards, BioSci-Fig-Cap, Zero-shot transferability","This paper proposes FigEx2, a visual-conditioned framework for detecting and generating panel-wise captions of scientific compound figures. It addresses the challenge of missing or incomplete captions in real pipelines by introducing a noise-aware gated fusion module and a staged optimization strategy combining supervised and reinforcement learning. Experimental results show superior performance in detection and captioning tasks.",36.22,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08043v1_The Role of Noisy Data in Improving CNN Robustness.pdf,The Role of Noisy Data in Improving CNN Robustness for Image Classification,"Oscar H. Ramírez-Agudela, Nicoleta Gorea, Aliza Reif, Lorenzo Bonasera, Michael Karla",,,"deep learning, CNNs, data quality, CIFAR-10, noise injection, image classification, model robustness","Data quality plays a central role in the performance and robustness of convolutional neural networks (CNNs) for image classification. This paper investigates the effect of deliberately introducing controlled noise into the training data to improve model robustness. Using the CIFAR-10 dataset, we evaluate the impact of three common corruptions, namely Gaussian noise, Salt-and-Pepper noise, and Gaussian blur at varying intensities and training set pollution levels. Experiments using a Resnet-18 model reveal that incorporating just 10% noisy data during training is sufficient to significantly reduce test loss and enhance accuracy under fully corrupted test conditions, with minimal impact on clean-data performance. These findings suggest that strategic exposure to noise can act as a simple yet effective regularizer, offering a practical trade-off between traditional data cleanliness and real-world resilience.",38.56,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08049v1_Integrating Attendance Tracking and Emotion Detect.pdf,Integrating Attendance Tracking and Emotion Detection for Enhanced Student Engagement in Smart Classrooms,"Keith Ainebyona, Ann Move Oguti, Joseph Walusimbi, Ritah Kobusingye",,,"Affective computing, Attendance automation, Emotion detection, IoT, Smart classroom","This paper presents SCASED (Smart Classroom Attendance System with Emotion Detection), an IoT-based system that integrates automated attendance tracking with facial emotion recognition to support classroom engagement monitoring.",38.32,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08052v1_Forecast Aware Deep Reinforcement Learning for Eff.pdf,Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms,"Nawazish Alia, Rachael Shaw, Karl Mason",,2601.08052v1,"Dairy farming, Deep Reinforcement Learning, Electricity Load Scheduling, Battery Storage, Water Heating, Forecasting, Intermittent Renewable Energy, PID-KL PPO, Proportional–Integral–Derivative Controller, Realistic Operational Constraints","This study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints, achieving up to 1% lower electricity cost than PPO and 4.8% than DQN.",38.86,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08058v1_Reasoning Beyond Chain-of-Thought A Latent Computa.pdf,Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models,"Zhenghao He, Guangzhi Xiong, Bohan Liu, Sanchit Sinha, Aidong Zhang",Not found,Not found,"Large language models, Chain-of-Thought prompting, Reasoning performance, Latent features, Sparse Autoencoders, Multi-step reasoning","This work investigates the effectiveness of Chain-of-Thought (CoT) prompting in large language models (LLMs) and identifies a set of latent features that are causally associated with LLM reasoning behavior. By analyzing and intervening on the internal representations of LLMs with Sparse Autoencoders (SAEs), the authors find that steering a single reasoning-related latent feature can substantially improve accuracy without explicit CoT prompting. The results suggest that multi-step reasoning in LLMs is supported by latent internal activations that can be externally activated, while CoT prompting is one effective, but not unique, way of activating this mechanism rather than its necessary cause.",37.91,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08065v1_A New Strategy for Verifying Reach-Avoid Specifica.pdf,A New Strategy for Verifying Reach-Avoid Specifications in Neural Feedback Systems,"Samuel I. Akinwande, Sydney M. Katz, Mykel J. Kochenderfer, Clark Barrett",Not found,2601.08065,"Neural networks, Reachability analysis, Forward analysis, Backward analysis, Neural feedback systems, Safety verification","This work introduces new algorithms to compute both over- and under-approximations of backward reachable sets for neural feedback systems, integrating these with forward analysis techniques to provide a unified verification framework.",37.3,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08070v1_Semantic Gravity Wells Why Negative Constraints Ba.pdf,Semantic Gravity Wells: Why Negative Constraints Backfire,Shailesh Rana,Not found,Not found,"large language models, instruction-following, negative constraints, semantic pressure, failure mechanisms","Negative constraints, instructions of the form 'do not use word X', represent a fundamental test of instruction-following capability in large language models. Despite their apparent simplicity, these constraints fail with striking regularity, and the conditions governing failure have remained poorly understood. This paper presents the first comprehensive mechanistic investigation of negative instruction failure, introducing semantic pressure as a quantitative measure of the model's intrinsic probability of generating the forbidden token. It demonstrates that violation probability follows a tight logistic relationship with pressure, and through layer-wise analysis, establishes that the suppression signal induced by negative instructions is present but systematically weaker in failures, leading to a 4.4× asymmetry in violation probability between failures and successes. The paper traces this asymmetry to two mechanistically distinct failure modes: priming failure and override failure, revealing a fundamental tension in negative constraint design.",38.46,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08079v1_MemoBrain Executive Memory as an Agentic Brain for.pdf,MemoBrain: Executive Memory as an Agentic Brain for Reasoning,"Hongjin Qian, Zhao Cao, Zheng Liu*",,,"Executive Memory, Agent Reasoning, Long Horizon, Tool-Augmented Agents","This paper proposes MemoBrain, an executive memory model for tool-augmented agents, which constructs a dependency-aware memory over reasoning steps to capture salient intermediate states and their logical relations. It operates alongside the reasoning agent to manage reasoning progress without blocking execution and actively manages the working context.",34.45,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08089v1_Q-realign Piggybacking Realignment on Quantization.pdf,Q-realign: Piggybacking Realignment on Quantization for Safe and Efficient LLM Deployment,"Qitao Tan, Xiaoying Song, Ningxi Cheng, Ninghao Liu, Xiaoming Zhai, Lingzi Hong, Yanzhi Wang, Zhen Xiang, Geng Yuan",,,"large language models, fine-tuning, safety alignment, quantization, post-hoc defense, deployment","This paper proposes Q-realign, a post-hoc defense method based on post-training quantization, aimed at reducing unsafe behaviors and preserving task performance in large language models after fine-tuning, while significantly reducing memory usage and GPU hours.",34.7,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08094v1_Local-Global Feature Fusion for Subject-Independen.pdf,Local-Global Feature Fusion for Subject-Independent EEG Emotion Recognition,"Zheng Zhou, Isabella McEvoy, Camilo E. Valderrama",Not found,Not found,"EEG emotion recognition, subject-independent, local features, global features, transformer fusion, domain adaptation","This paper proposes a fusion framework for subject-independent EEG emotion recognition, integrating local channel-wise descriptors and global trial-level descriptors. The framework uses differential entropy and graph-theoretic features for local representations and summarizes time-domain, spectral, and complexity characteristics at the trial level for global representations. These representations are fused in a dual-branch transformer with attention-based fusion and domain-adversarial regularization, achieving approximately 40% mean accuracy in 7-class subject-independent emotion recognition on the SEED-VII dataset.",36.32,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08104v1_High-Fidelity Modeling of Stochastic Chemical Dyna.pdf,HIGH-FIDELITYMODELING OFSTOCHASTICCHEMICAL DYNAMICS ONCOMPLEXMANIFOLDS: A MULTI-SCALE SIREN-PINN FRAMEWORK FOR THECURVATURE-PERTURBED GINZBURG-LANDAUEQUATION,"Julian Evan Chrisnanto, Salsabila Rahma Alia, Nurfauzi Fadillah, Yulison Herry Chrisnanto",Not found,2601.08104,"Physics-Informed Neural Networks (PINNs), Spatiotemporal Chaos, Inverse Geometric Problems, Reaction-Diffusion Systems, Defect Turbulence, Riemann Manifold Learning","This work proposes a Multi-Scale SIREN-PINN architecture to accurately model and control spatiotemporal chaos in reaction-diffusion systems on complex manifolds, particularly when the underlying catalytic surface has complex, unknown topography. The architecture leverages periodic sinusoidal activations with frequency-diverse initialization to resolve high-frequency gradients and preserve topological invariants. It achieves state prediction error ϵL2 ≈1.92×10−2, outperforming standard baselines by an order of magnitude. The work solves the inverse pinning problem and reconstructs hidden Gaussian curvature fields from chaotic wave dynamics, revealing a distinctive Spectral Phase Transition at epoch ∼2,100.",38.56,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08107v1_STO-RL Offline RL under Sparse Rewards via LLM-Gui.pdf,STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order,"Chengyang Gu, Yuxin Pan, Hui Xiong, Yize Chen",10.1145/nnnnnnn.nnnnnnn,,"Offline RL, Temporal order, Large Language Models","Offline reinforcement learning (RL) enables policy learning from pre-collected datasets, avoiding costly and risky online interactions. However, it often struggles with long-horizon tasks involving sparse rewards. This paper proposes STO-RL, an offline RL framework that leverages large language models (LLMs) to generate temporally ordered subgoal sequences and corresponding state-to-subgoal-stage mappings, applying potential-based reward shaping to transform sparse terminal rewards into dense, temporally consistent signals, promoting subgoal progress while avoiding suboptimal solutions.",36.13,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08108v1_Debiasing Large Language Models via Adaptive Causa.pdf,Debiasing Large Language Models via Adaptive Causal Prompting with Sketch-of-Thought,"Bowen Li, Ziqi Xu, Jing Ren, Renqiang Luo, Xikun Zhang, Xiuzhen Zhang, Yongli Ren, Feng Xia",Not found,Not found,"Large Language Models, LLMs, prompting, Chain-of-Thought, Sketch-of-Thought, causal inference, front-door adjustment, inference efficiency","Despite notable advancements in prompting methods for Large Language Models (LLMs), such as Chain-of-Thought (CoT), existing strategies still suffer from excessive token usage and limited generalisability across diverse reasoning tasks. To address these limitations, we propose an Adaptive Causal Prompting with Sketch-of-Thought (ACPS) framework, which leverages structural causal models to infer the causal effect of a query on its answer and adaptively select an appropriate intervention (i.e., standard front-door and conditional front-door adjustments). This design enables generalisable causal reasoning across heterogeneous tasks without task-specific retraining. By replacing verbose CoT with concise Sketch-of-Thought, ACPS enables efficient reasoning that significantly reduces token usage and inference cost. Extensive experiments on multiple reasoning benchmarks and LLMs demonstrate that ACPS consistently outperforms existing prompting baselines in terms of accuracy, robustness, and computational efficiency.",37.46,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08109v1_CSQL Mapping Documents into Causal Databases.pdf,Csql: Mapping Documents into Causal Databases,Sridhar Mahadevan,Not found,2601.08109,"Causality, Natural Language, Databases, SQL, AI, Machine Learning","We describe a novel system, Csql, that automatically converts a collection of unstructured text documents into an SQL-queryable causal database (CDB). Csql supports causal analysis over document collections rather than purely associative retrieval.",34.66,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08118v1_MirrorBench An Extensible Framework to Evaluate Us.pdf,MIRRORBENCH: ANEXTENSIBLEFRAMEWORK TOEVALUATEUSER-PROXY AGENTS FORHUMAN-LIKENESS,"Ashutosh Hathidara, Julien Yu, Vaishali Senthil, Sebastian Schreiber, Anil Babu Ankisettipalli",Not found,Not found,"user proxy agents, human likeness, large language models, benchmarking, reproducibility, extensibility","MIRRORBENCH is a reproducible, extensible benchmarking framework designed to evaluate user proxy agents on their ability to produce human-like user utterances across diverse conversational tasks, explicitly decoupled from downstream task success. It features a modular execution engine with typed interfaces, metadata-driven registries, multi-backend support, caching, and robust observability, enabling researchers to evaluate arbitrary simulators under a uniform, variance-aware harness. The framework includes various metrics for assessing user proxies, such as lexical diversity metrics and LLM-judge-based metrics, and is open-source with a simple command-line interface for running experiments and generating reports.",37.41,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08125v1_How vehicles change lanes after encountering crash.pdf,How vehicles change lanes after encountering crashes: Empirical analysis and modeling,"Kequan Chen, Yuxuan Wang, Pan Liu, Victor L. Knoop, David Z. W. Wang, Yu Han",,,"crashes, lane changes, empirical analysis, vehicle behavior","This paper examines how vehicles adjust their lanes following collisions, providing both empirical data and modeling insights.",39.29,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08127v1_PathoGen Diffusion-Based Synthesis of Realistic Le.pdf,PathoGen: Diffusion-Based Synthesis of Realistic Lesions in Histopathology Images,"Mohamad Koohi-Moghadam1*, Mohammad-Ali Nikouei Mahani1",,1912.04429,"histopathology, artificial intelligence, deep learning, lesion synthesis, data augmentation","A diffusion-based generative model that enables controllable, high-fidelity inpainting of lesions into benign histopathology images, validating its performance across four diverse datasets and showing enhanced downstream segmentation performance compared to traditional geometric augmentations.",36.77,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08128v1_Embedded AI Companion System on Edge Devices.pdf,Embedded AI Companion System on Edge Devices,"Rahul Gupta ∗1 and Stephen Hsu 1,2",Not found,2601.08128,"AI companion, edge devices, memory systems, low-latency, personalization","This paper proposes a memory paradigm for an embedded AI companion system on edge devices, alternating between active and inactive phases to minimize latency while maintaining long-term personalization under resource constraints.",35.15,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08133v1_How Do Optical Flow and Textual Prompts Collaborat.pdf,How Do Optical Flow and Textual Prompts Collaborate to Assist in Audio-Visual Semantic Segmentation?,"Peng Gao, Yujian Lee, Yongqi Xu, Wentao Fan",Not found,Not found,"audio-visual semantic segmentation, optical flow, textual prompts, semantic understanding, machine perception","This paper introduces a novel collaborative framework, Stepping Stone Plus (SSP), which integrates optical flow and textual prompts to assist in audio-visual semantic segmentation. The pre-mask technique leverages optical flow to capture motion dynamics, providing essential temporal context for precise segmentation. SSP also incorporates specific textual prompts to address stationary sound-emitting objects, such as alarm clocks. A visual-textual alignment module (VTA) facilitates cross-modal integration, delivering more coherent and contextually relevant semantic interpretations. Experimental results demonstrate that SSP outperforms existing AVS methods, delivering efficient and precise segmentation results.",36.96,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08139v1_Subspace Alignment for Vision-Language Model Test-.pdf,Subspace Alignment for Vision-Language Model Test-time Adaptation,"Zhichen Zeng, Wenxuan Bao, Xiao Lin, Ruizhong Qiu, Tianxin Wei, Xuying Ning, Yuchen Yan, Chen Luo, Monica Xiao Cheng, Jingrui He, Hanghang Tong",,,"vision-language models, test-time adaptation, zero-shot adaptation, modal alignment, visual nuisance, semantic alignment","This paper proposes SubTTA, a method to enhance zero-shot predictions by aligning the semantic subspaces of both visual and textual modalities, addressing the limitations of existing test-time adaptation methods that rely on unreliable pseudo-labels for self-training under distribution shifts. The method bridges the modality gap by extracting principal subspaces and aligning visual manifolds to textual semantic anchors, and eliminates visual nuisance by projecting aligned visual features onto task-specific textual subspaces. Extensive experiments demonstrate the effectiveness of SubTTA, yielding an average improvement of 2.24% over state-of-the-art TTA methods.",37.59,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08141v1_Qalb Largest State-of-the-Art Urdu Large Language .pdf,Qalb: Largest State-of-the-Art Urdu Large Language Model for 230M Speakers,"1st Muhammad Taimoor Hassan, 2st Jawad Ahmed, 3st Muhammad Awais",,,"Urdu language model, continued pre-training, low-resource NLP, LoRA, language adaptation","Despite remarkable progress in large language models, Urdu—a language spoken by over 230 million people—remains critically underrepresented in modern NLP systems. We introduce Qalb, an Urdu language model developed through a two-stage approach: continued pre-training followed by supervised fine-tuning. Qalb demonstrates substantial improvements, achieving a weighted average score of 90.34 and outperforming the previous state-of-the-art Alif-1.0-Instruct model by 3.24 points, while also surpassing the base LLaMA-3.1 8B-Instruct model by 44.64 points. Qalb achieves state-of-the-art performance with comprehensive evaluation across seven diverse tasks including Classification, Sentiment Analysis, and Reasoning.",38.02,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08146v2_Mechanisms are Transferable Data-Efficient Low-Res.pdf,Mechanisms are Transferable: Data-Efficient Low-Resource Adaptation via Circuit-Targeted Supervised Fine-Tuning,"Khumaisa Nur'aini1, Ayu Purwarianti2, Alham Fikri Aji3, Derry Wijaya1,4",,,"transfer learning, low-resource adaptation, supervised fine-tuning, circuit-targeted, catastrophic forgetting, contextual decomposition transformer","Proposes a method for adapting large language models to low-resource languages, using a label-balanced mean baseline and task-directional relevance scoring to identify task-relevant attention heads in a proxy-language checkpoint, then transfer learning to a target language by updating only those heads (plus LayerNorm) via head-level gradient masking. Improves cross-lingual accuracy over continued full fine-tuning while updating only a small subset of model parameters. Reduces catastrophic forgetting and preserves proxy/source-language competence during transfer.",37.4,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08148v1_Enriching Semantic Profiles into Knowledge Graph f.pdf,Enriching Semantic Profiles into Knowledge Graph for Recommender Systems Using Large Language Models,"Seokho Ahn, Sungbok Shin, Young-Duk Seo",10.1145/3770854.3780324,,"Recommendation, Semantic Profiling, Large Language Models, Knowledge Graphs","Rich and informative profiling to capture user preferences is essential for improving recommendation quality. However, there is no consensus on how best to construct and utilize such profiles. This paper proposes a new recommendation model, SPiKE, which uses large language models to generate semantic profiles for all KG entities and integrates these profiles into the KG. The model also aligns LLM-based representations during training. Experiments demonstrate that SPiKE consistently outperforms state-of-the-art KG- and LLM-based recommenders in real-world settings.",36.62,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08149v1_Dynamic Graph Structure Learning via Resistance Cu.pdf,Dynamic Graph Structure Learning via Resistance Curvature Flow,"Chaoqun Fei, Huanjiang Liu, Tinglve Zhou, Y angyang Li 1, Tianyong Hao",Not found,2601.08149v1,"Graph structure learning, Dynamic graph, Resistance curvature flow, Deep metric learning, Manifold learning","Introduces a novel curvature flow method based on effective resistance from circuit theory, improving representation quality and downstream performance in graph structure learning tasks.",41.47,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08156v1_Project Synapse A Hierarchical Multi-Agent Framewo.pdf,Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions,"Arin Gopalan Yadav, Varad Dherange, Kumar Shivam",Not found,2601.08156,"last-mile delivery, disruptions, multi-agent systems, hybrid memory, autonomous resolution","This paper introduces Project Synapse, a novel hierarchical multi-agent framework designed for the autonomous resolution of last-mile delivery disruptions. The system employs a hybrid memory architecture that integrates short-term working memory, long-term episodic memory of past incidents, and semantic memory of organizational policies, enabling stateful, context-aware, and factually-grounded reasoning. The framework is validated using a benchmark dataset of 30 complex disruption scenarios and evaluated using an LLM-as-a-Judge protocol with explicit bias mitigation.",41.07,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08160v1_SwiftMem Fast Agentic Memory via Query-aware Index.pdf,SwiftMem: Fast Agentic Memory via Query-aware Indexing,"Anxin Tian, Yiming Li, Xing Li, Hui-Ling Zhen, Lei Chen, Xianzhi Yu, Zhenhua Dong, Mingxuan Yuan",Not found,Not found,"Agentic memory, Query-aware indexing, Temporal indexing, Semantic DAG index, Memory fragmentation, Embedding-tag co-consolidation","Agentic memory systems are crucial for enabling large language models (LLMs) to maintain long-term context and retrieve relevant information efficiently. However, existing memory frameworks suffer from a fundamental limitation: they perform exhaustive retrieval across the entire storage layer regardless of query characteristics. This brute-force approach creates severe latency bottlenecks as memory grows, hindering real-time agent interactions. SwiftMem proposes a query-aware agentic memory system that achieves sub-linear retrieval through specialized indexing over temporal and semantic dimensions. It enables logarithmic-time range queries for time-sensitive retrieval and maps queries to relevant topics through hierarchical tag structures. To address memory fragmentation during growth, it introduces an embedding-tag co-consolidation mechanism that reorganizes storage based on semantic clusters to improve cache locality. Experiments on LoCoMo and LongMemEval benchmarks demonstrate that SwiftMem achieves 47× faster search compared to state-of-the-art baselines while maintaining competitive accuracy, enabling practical deployment of memory-augmented LLM agents.",37.54,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08166v1_ZeroDVFS Zero-Shot LLM-Guided Core and Frequency A.pdf,Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms,"Mohammad Pivezhandi1, Mahdi Banisharif2, Abusayeed Saifullah3, Ali Jannesari2",Not found,2601.08166,"Dynamic Voltage and Frequency Scaling (DVFS), Task-to-Core Allocation, Embedded Systems, Thermal Management, Energy Efficiency, Reinforcement Learning, Zero-Shot Learning","This paper proposes a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. The framework enables zero-shot deployment for new workloads on trained platforms without requiring workload-specific profiling samples, leveraging LLM-extracted semantic features to generate synthetic training data. The authors demonstrate improved energy efficiency and makespan compared to Linux ondemand governor on benchmarks across various platforms.",28.51,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08173v1_The Agents First Day Benchmarking Learning Explora.pdf,"The Agent’s First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios","Daocheng Fu1,2,†, Jianbiao Mei3,2,†, Rong Wu3,2,†, Xuemeng Yang2,†, Jia Xu2, Ding Wang 2, Pinlong Cai 2, Yong Liu 3, B Licheng Wen 2, B Botian Shi 2",Not found,Not found,"Multi-modal Large Language Models, Workflow Automation, Dynamic Task Scheduling, Active Exploration, Continuous Learning, Stochastic Environment","The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation, but existing research mainly focuses on performance upper bounds in controlled environments, overlooking robustness for stochastic real-world deployment. This paper introduces Trainee-Bench, a dynamic evaluation environment that simulates a trainee agent exploring a novel setting. It evaluates agents along three dimensions: context-aware scheduling for streaming tasks with varying priorities, prudent information acquisition to reduce hallucination via active exploration, and continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning.",28.59,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08176v1_Prompt-Based Clarity Evaluation and Topic Detectio.pdf,Prompt-Based Clarity Evaluation and Topic Detection in Political Question Answering,"Lavanya Prahallad, Sai Utkarsh Choudarypally, Pragna Prahallad, Pranathi Prahallad",,,"Political Question-Answering, Large language models, Clarity evaluation, Prompt engineering, Chain-of-thought prompting, Evasion detection","This paper evaluates the impact of prompt design on automatic clarity evaluation of large language model responses in political question answering. It compares a GPT-3.5 baseline with GPT-5.2 evaluated under three prompting strategies: simple prompting, chain-of-thought prompting, and chain-of-thought with few-shot examples. The results show that GPT-5.2 consistently outperforms the GPT-3.5 baseline on clarity prediction, with accuracy improving from 56% to 63% under chain-of-thought with few-shot prompting. Chain-of-thought prompting yields the highest evasion accuracy (34%), though improvements are less stable across fine-grained evasion categories. The paper also evaluates topic identification and finds that reasoning-based prompting improves accuracy from 60% to 74% relative to human annotations. Overall, the findings indicate that prompt design reliably improves high-level clarity evaluation, while fine-grained evasion and topic detection remain challenging despite structured reasoning prompts.",28.49,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08179v1_Instruction-Driven 3D Facial Expression Generation.pdf,Instruction-Driven 3D Facial Expression Generation,"Anh H. V o, Tae-Seok Kim, Hulin Jin, Soo-Mi Choi, Yong-Guk Kim*",[Not found],[Not found],"Facial Expression, 3D Avatar, Facial Transition, Instruction-Driven, Controllable Avatar","This study presents a new framework for instruction-driven facial expression generation, which produces a 3D face and transforms the facial expression from one designated expression to another. The study proposes the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to generate a facial expression sequence according to the given instruction. The results suggest that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets.",27.55,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08183v2_GI-Bench A Panoramic Benchmark Revealing the Knowl.pdf,GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards,"Yan Zhu, Te Luo, Pei-Yao Fu, Zhen Zhang, Zi-Long Wang, Yi-Fan Qu, Zi-Han Geng, Jia-Qi Xu, Lu Yao, Li-Yun Ma, Wei Su, Wei-Feng Chen, Quan-Lin Li, Shuo Wang, Ping-Hong Zhou",,,"Multimodal Large Language Models, Gastrointestinal Endoscopy, Clinical Workflow, Human Benchmarks, Knowledge-Experience Dissociation","This study systematically evaluates state-of-the-art Multimodal Large Language Models (MLLMs) across a panoramic gastrointestinal endoscopy workflow, comparing their performance against human endoscopists and junior endoscopists. The study constructs GI-Bench, a benchmark encompassing 20 fine-grained lesion categories, and evaluates 12 MLLMs across a five-stage clinical workflow: anatomical localization, lesion identification, diagnosis, findings description, and management. Model performance is benchmarked against three junior endoscopists and three residency trainees using Macro-F1, mean Intersection-over-Union (mIoU), and multi-dimensional Likert scale. The results show that Gemini-3-Pro achieved state-of-the-art performance in diagnostic reasoning.",30.34,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08185v1_Autonomous Materials Exploration by Integrating Au.pdf,Autonomous Materials Exploration by Integrating Automated Phase Identification and AI-Assisted Human Reasoning,"Ming-Chiang Chang, Maximilian Amsler, Duncan R. Sutherland, Sebastian Ament, Katie R. Gann, Lan Zhou, Louisa M. Smieska, Arthur R. Woll, John M. Gregoire, Carla P. Gomes, R. Bruce van Dover, Michael O. Thompson",Not provided,Not provided,"materials science, autonomous experimentation, AI, robotics, phase identification, human reasoning, synthesis, thin-film samples, laser spike annealing, metastable phases, catalytic materials","This paper presents an extension of SARA, a Scientific Autonomous Reasoning Agent, to autonomously explore materials synthesis. It integrates automated phase identification with AI-assisted human reasoning to enhance the efficiency of material discovery and understanding. The authors demonstrate the effectiveness of their approach using synthetic benchmarks and experimental campaigns, showcasing the ability to identify new materials and understand their synthesis and properties.",28.79,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08187v2_Improving LLM Reasoning with Homophily-aware Struc.pdf,Improving LLM Reasoning with Homophily-Aware Structural and Semantic Text-Attributed Graph Compression,"Zijun Di, Bin Lu∗, Huquan Kang, Luoyi Fu∗, Jiaxin Ding, Xiaoying Gan, Lei Zhou, Xinbing Wang, Chenghu Zhou",Not found,2601.08187v2,"Large Language Models, Text-Attributed Graphs, Homophily, Graph Compression, LLM Reasoning","This paper proposes HS2C, a framework that enhances LLMs reasoning performance by exploiting graph homophily, structurally and semantically compressing the input. Extensive experiments on various benchmarks demonstrate its superiority and scalability.",28.08,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08189v2_ForgetMark Stealthy Fingerprint Embedding via Targ.pdf,FORGETMARK: STEALTHY FINGERPRINT EMBEDDING VIA TARGETED UNLEARNING,"Zhenhua Xu, Haobo Zhang, Zhebo Wang, Qichen Liu, Haitao Xu, Wenpeng Xing, Meng Han",10.1109/ICASSP39486.2026.7778292,,"Large Language Model, Copyright protection, Model Fingerprinting, Machine Unlearning","A stealthy fingerprinting framework that encodes provenance via targeted unlearning, avoiding high-perplexity triggers and reducing detectability and false triggers, achieving 100% ownership verification on fingerprinted models while maintaining standard performance.",27.11,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08196v1_Evaluating Implicit Regulatory Compliance in LLM T.pdf,Evaluating Implicit Regulatory Compliance in LLM Tool Invocation via Logic-Guided Synthesis,"Da Song1,2, Yuheng Huang3*, Boqi Chen4, Tianshuo Cong1,2, Randy Goebel5, Lei Ma 3,5, Foutse Khomh 6",Not provided,Not provided,"Large Language Models, Tool Invocation, Regulatory Compliance, Logic-Guided Synthesis, Safety Constraints","Recent advances in large language models (LLMs) have enabled the emergence of LLM-based agents capable of interpreting complex user instructions, invoking external tools, and interacting with the physical or digital world to achieve multi-step goals. However, in high-stakes domains such as financial services, legal reasoning, healthcare, and smart home control, functional task completion alone is insufficient; LLMs must consistently satisfy safety and regulatory constraints throughout the entire decision-making and execution process. Violations may result in severe consequences, including physical harm and regulatory non-compliance. To mitigate these risks, systematic evaluation and pre-deployment testing are essential for understanding an LLM's capability boundaries and building trust among developers, regulators, and end-users. Existing evaluation practices fall short of this requirement, relying on static test sets derived from manual cura-tion or web scraping, which are expensive to scale, difficult to validate, prone to data saturation, and primarily evaluate functional correctness.",28.4,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08211v1_Adapting Rules of Official International Mahjong f.pdf,Adapting Rules of Official International Mahjong for Online Players,"Chucai Wang, Lingfeng Li, Yunlong Lu, Wenxin Li",Not found,Not found,"Mahjong, game design, champion AI, first-mover advantage, subgoal scoring","This work adapts the rules of Official International Mahjong for online players, addressing issues of fragmented playtime and unfixed opponents. It employs a world champion AI for self-play competitions and statistical analysis, revealing first-mover advantages and subgoal scoring issues. The authors propose rule adaptations, including compensatory points for the first-mover advantage and refined subgoal scores, making the game more suitable for online play.",27.53,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08223v2_DNF Dual-Layer Nested Fingerprinting for Large Lan.pdf,DNF: DUAL-LAYER NESTED FINGERPRINTING FOR LARGE LANGUAGE MODEL,"Zhenhua Xu, Yiran Zhao, Mengting Zhong, De Zhang Kong, Changting Lin, Tong Qiao, Meng Han",10.1109/ICASSP39664.2026.7468568,2603.00000,"Large Language Model, Copyright Protection, Model Fingerprinting, Backdoor","A black-box method that embeds a hierarchical backdoor by coupling domain-specific stylistic cues with implicit semantic triggers, achieving perfect fingerprint activation while preserving downstream utility.",26.52,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08224v1_An Axiomatic Approach to General Intelligence SANC.pdf,An Axiomatic Approach to General Intelligence,"Daesuk Kwon, Won-gi Paeng",Not found,2601.08224,"axiomatization of intelligence, competitive selection, system tokens, reconstruction-compression trade-off, category formation, self-similar hierarchy, Gestalt completion","This paper proposes SANC(E3), an axiomatic framework for general intelligence, where representational units emerge through competitive selection, reconstruction, and compression under finite activation capacity. It draws a principled distinction between system tokens and sensory tokens, formalizing finite capacity, association from co-occurrence, similarity-based competition, confidence-based stabilization, and the reconstruction-compression-update trade-off. The framework unifies perception, imagination, prediction, planning, and action within a single representational and energetic process, extending to embodied and physical agents.",27.56,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08226v1_Knowledge-based learning in Text-RAG and Image-RAG.pdf,Knowledge-based learning in Text-RAG and Image-RAG,"Alexander Shim, Khalil Saieh, Samuel Clarke",,,"Knowledge-based learning, Text-RAG, Image-RAG, Radiology, Chest X-ray, Vision Transformer, Large Language Model, Hallucination Problem, Expected Calibration Error, Data Imbalance",This research analyzed and compared the multi-modal approach in the Vision Transformer (EVA-ViT) based image encoder with the LlaMA or ChatGPT LLM to reduce the hallucination problem and detect diseases in chest x-ray images.,26.14,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08230v1_GADPN Graph Adaptive Denoising and Perturbation Ne.pdf,GADPN: Graph Adaptive Denoising and Perturbation Networks via Singular Value Decomposition,"Hao Deng, Bo Liu",,1912.04468,"Graph Neural Networks, Graph Structure Learning, Network Representation Learning","Proposes GADPN, a simple yet effective framework for graph structure learning that adaptsively refines graph topology via low-rank denoising and generalized structural perturbation.",24.62,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08235v2_MPCI-Bench A Benchmark for Multimodal Pairwise Con.pdf,MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents,"Shouju Wang, Haopeng Zhang",,,"Contextual Integrity, Multimodal Privacy, Language Models, Evaluation Benchmarks","This paper introduces MPCI-Bench, the first multimodal pairwise contextual integrity benchmark for evaluating privacy behavior in agentic settings. It addresses the limitations of existing benchmarks, which are predominantly text-centric and focus on negative refusal scenarios, overlooking multimodal privacy risks and the trade-off between privacy and utility.",25.32,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08237v1_The End of Reward Engineering How LLMs Are Redefin.pdf,The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination,"Haoran Su, Yandong Sun, Congjia Yu",Not found,2601.08237,"multi-agent reinforcement learning, reward engineering, large language models, semantic reward specification, dynamic adaptation, human alignment","This paper argues that large language models (LLMs) enable a fundamental paradigm shift in reward engineering for multi-agent reinforcement learning, moving from hand-crafted numerical rewards to natural language objectives. It discusses recent work demonstrating LLMs' ability to generate human-level reward functions from language descriptions, adapt rewards dynamically without human intervention, and coordinate agents through semantic understanding. The paper also acknowledges challenges in computational cost, hallucination risks, and scalability.",28.01,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08251v1_Hyperbolic Heterogeneous Graph Transformer.pdf,Hyperbolic Heterogeneous Graph Transformer,"Jongmin Park, Seunghoon Han, Hyewon Lee, Won-Yong Shin, Sungsu Lim",,,"Heterogeneous Graphs, Hyperbolic Space, Graph Transformer, Graph Neural Networks, Hierarchical Structures, Long-Range Dependencies","This paper proposes a Hyperbolic Heterogeneous Graph Transformer (HypHGT) to effectively learn heterogeneous graph representations within the hyperbolic space, addressing challenges in existing methods that rely on tangent-space operations and focus on local neighborhood information. HypHGT uses a transformer-based architecture to capture both local and global dependencies, and a relation-specific hyperbolic attention mechanism for efficient computation and preservation of heterogeneous information across different relation types.",27.37,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08254v1_Large Artificial Intelligence Model Guided Deep Re.pdf,Large Artificial Intelligence Model–Guided Deep Reinforcement Learning for Resource Allocation in Non-Terrestrial Networks,"Abdikarim Mohamed Ibrahim, Rosdiadee Nordin",,,"Large AI Models (LAMs), Large Language Models (LLMs), Deep Reinforcement Learning (DRL), Satellite Communications, Non-Terrestrial Networks (NTNs)","Large AI Model (LAM) have been proposed to applications of Non-Terrestrial Networks (NTN), that offer better performance with its great generalization and reduced task specific trainings. In this paper, we propose a Deep Reinforcement Learning (DRL) agent that is guided by a Large Language Model (LLM). The LLM operates as a high level coordinator that generates textual guidance that shape the reward of the DRL agent during training. The results show that the LAM-DRL outperforms the traditional DRL by 40% in nominal weather scenarios and 64% in extreme weather scenarios compared to heuristics in terms of throughput, fairness, and outage probability.",28.87,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08257v2_On Evaluation of Unsupervised Feature Selection fo.pdf,On Evaluation of Unsupervised Feature Selection for Pattern Classification,"Gyu-Il Kim, Dae-Won Kim, Jaesung Lee",Not found,2601.08257,"Unsupervised feature selection, Pattern classification, Multi-label classification, Feature ranking, Hamming Loss, Ranking Loss, One-Error, Multi-Label Accuracy","This study revisits the evaluation paradigm of unsupervised feature selection methods by adopting a multi-label classification framework. Experiments on 21 multi-label datasets demonstrate that performance rankings differ markedly from those reported under single-label settings, suggesting the possibility of fair and reliable comparison of unsupervised feature selection methods in multi-label evaluation settings.",28.44,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08258v1_T3 Benchmarking Sycophancy and Skepticism in Causa.pdf,Benchmarking Sycophancy and Skepticism in Causal Judgment,Edward Y. Chang,,,"Large Language Models, Causal Reasoning, Sycophancy, Skepticism, Alignment Pathologies, Recursive Causal Audit","This paper introduces T3, a diagnostic benchmark to rigorously evaluate Large Language Models (LLMs) across Pearl's Causal Hierarchy. It comprises 454 expert-curated vignettes, prioritizing high-resolution failure analysis to distinguish genuine capability from safety-induced refusal. The benchmark reveals systematic tendencies toward either sycophancy or paralysis in modern frontier models. The authors use Recursive Causal Audit (RCA) to validate the protocol and confirm diagnoses.",27.38,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08262v1_VGG Induced Deep Hand Sign Language Detection.pdf,VGG Induced Deep Hand Sign Language Detection,"Subham Sharma, Sharmila Subudhi",Not found,2601.08262,"Hand gesture recognition, Convolutional neural network, Classification, VGG-16 net, API","This work proposes a novel hand gesture recognizing system for the differently-abled persons using a convolutional neural network, VGG-16 net, and validates the model with the NUS dataset and a testing dataset of 10 classes of hand gestures.",27.56,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08271v1_Sparsity Is Necessary Polynomial-Time Stability fo.pdf,Sparsity Is Necessary: Polynomial-Time Stability for Agentic LMs in Large Action Spaces,Angshul Majumdar,,2601.08271,"Large Language Models, Sequential Decision Making, Sparse Agentic Control, Stability, Compressed Sensing, Action Dimensionality, Latent Sparsity","This paper formalizes the setting of sequential decision-making with a massive discrete action universe in the context of tool-augmented LLM systems, where policies admit block-sparse representations over M≫1 actions and rewards depend on sparse main effects and optionally sparse synergies. It establishes sharp, compressed-sensing-style results for ℓ1,2-regularized policy learning, including estimation and value suboptimality scaling, exact tool-support recovery under certain conditions, and the necessity of sparse samples for dense policy classes. The paper also extends these results to partial observability and demonstrates the additive degradation of performance due to belief/representation error.",27.07,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08273v1_HIPPO Accelerating Video Large Language Models Inf.pdf,HIPPO: Accelerating Video Large Language Models Inference via Holistic-aware Parallel Speculative Decoding,"Qitan Lv, Tianyu Liu, Wen Wu, Xuenan Xu, Bowen Zhou, Feng Wu, Chao Zhang",Not provided,Not provided,"Large Language Models, Video Understanding, Speculative Decoding, Token Pruning, Parallel Processing",HIPPO proposes a holistic-aware parallel speculative decoding framework to accelerate video Large Language Models (LLMs) inference without sacrificing output quality. It addresses the limitations of existing methods by preserving semantic information at high pruning ratios and decoupling draft generation and target verification phases. Experiments on four video-LLMs across six benchmarks demonstrate up to 3.51× speedup compared to vanilla auto-regressive decoding.,24.36,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08276v1_ToolACE-MCP Generalizing History-Aware Routing fro.pdf,ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web,"Zhiyuan Yao, Zishan Xu, Yifu Guo, Zhiguang Han, Cheng Yang, Shuo Zhang, Weinan Zhang, Xingshan Zeng, Weiwen Liu",Not found,Not found,"Agent Web, Model Context Protocol (MCP), History-aware routing, Multi-agent collaboration, Robustness, Scalability, Noise robustness","Proposes ToolACE-MCP, a pipeline for training history-aware routers to empower precise navigation in large-scale ecosystems. By leveraging a dependency-rich candidate Graph, it effectively trains routers with dynamic context understanding to create the plug-and-play Light Routing Agent. Demonstrates superior performance on real-world benchmarks MCP-Universe and MCP-Mark, exhibiting critical properties for future Agent Web, including generalization to multi-agent collaboration with minimal adaptation and exceptional robustness against noise and scalability to massive candidate spaces.",26.6,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08280v1_Greedy Is Enough Sparse Action Discovery in Agenti.pdf,Greedy Is Enough: Sparse Action Discovery in Agentic LLMs,Angshul Majumdar,,,"agentic systems, language models, sparse action discovery, orthogonal matching pursuit, sparse linear reward model","Modern agentic systems operate in environments with extremely large action spaces. Despite this scale, empirical evidence suggests that only a small subset of actions meaningfully influences performance in a given deployment. This work studies a contextual linear reward model in which action relevance is governed by a structured sparsity assumption. It formulates action discovery as a block-sparse recovery problem and analyzes a greedy algorithm inspired by Orthogonal Matching Pursuit. Under standard assumptions, the greedy procedure exactly recovers the relevant action set with high probability, using a number of samples that scales polynomially in the sparsity level and latent dimension, and only logarithmically in the total number of actions.",25.7,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08288v1_OpenMic A Multi-Agent-Based Stand-Up Comedy Genera.pdf,OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System,"Yuyang Wu, Hanzhong Cao, Jianhao Chen, Yufei Li",,,"stand-up comedy, multi-agent system, humor generation, retrieval-augmented generation, JokeWriter, Chinese humor","OpenMic is an end-to-end multi-agent system that transforms a user-provided life topic into a 3–5 minute Chinese stand-up performance and produces a narrated comedy video. It orchestrates multiple specialized agents in a multi-round iterative loop to jointly optimize humor, timing, and performability, addressing the challenges of direct supervision misalignment and dataset-task mismatch.",26.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08297v1_Demystifying the Slash Pattern in Attention The Ro.pdf,Demystifying the Slash Pattern in Attention: The Role of RoPE,"Yuan Cheng1,∗, Fengzhuo Zhang1,∗, Yunlong Hou1,∗, Cunxiao Du2, Chao Du2, Tianyu Pang2, Aixin Sun3, Zhuoran Yang4",Not found,2601.08297,"Attention mechanisms, Rotary Position Embedding (RoPE), Large Language Models (LLMs), Slash-Dominant Heads (SDHs), Out-of-Distribution (OOD) generalization","This paper demystifies the emergence of Slash-Dominant Heads (SDHs) in large language models (LLMs), which exhibit slash attention patterns where attention scores concentrate along the ∆-th sub-diagonal for some offset ∆. The authors analyze open-source LLMs and find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. They explain the intrinsic emergence of SDHs by analyzing queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. The empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs.",24.37,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08302v1_Enhancing Sentiment Classification and Irony Detec.pdf,Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques,"Marvin Schmitt ∗∗, Anne Schwerk, Sebastian Lempert ∗",,,"sentiment analysis, irony detection, large language models (LLMs), prompt engineering","This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against a baseline. Key tasks include sentiment classification, aspect-based sentiment analysis, and detecting subtle nuances such as irony. The research details the theoretical background, datasets, and methods used, assessing performance of LLMs as measured by accuracy, recall, precision, and F1 score. Findings reveal that advanced prompting significantly improves sentiment analysis, with the few-shot approach excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%. Thus, while advanced prompting techniques overall improve performance, the fact that few-shot prompting works best for GPT-4o-mini and chain-of-thought excels in gemini-1.5-flash for irony detection suggests that prompting strategies must be tailored to both the model and the task. This highlights the importance of aligning prompt design with both the LLM’s architecture and the semantic complexity of the task.",26.14,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08310v1_ORBIT On-policy Exploration-Exploitation for Contr.pdf,ORBIT: On-policy Exploration-Exploitation for Controllable Multi-Budget Reasoning,"Kun Liang, Clive Bai, Xin Xu, Chenming Tang, Sanwoo Lee, Weijie Liu, Saiyong Yang, Yunfang Wu",,,"Reinforcement Learning, Large Language Models, Chain-of-Thought, Multi-Budget Reasoning, On-policy Distillation, Controllable Reasoning","Proposes ORBIT, a controllable multi-budget reasoning framework with well-separated reasoning modes triggered by input, employing multi-stage reinforcement learning to discover Pareto-optimal reasoning behaviors at each effort, followed by on-policy distillation to fuse these behaviors into a single unified model.",26.16,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08311v1_Enhancing Image Quality Assessment Ability of LMMs.pdf,Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation,"Kang Fu, Huiyu Duan, Zicheng Zhang, Yucheng Zhu, Jun Zhao, Xiongkuo Min, Jia Wang, Guangtao Zhai",Not found,Not found,"Image Quality Assessment, Large Multimodal Models, Retrieval-Augmented Generation, Zero-shot, Training-free","Large Multimodal Models (LMMs) have shown remarkable promise in low-level visual perception tasks, particularly in Image Quality Assessment (IQA). However, achieving state-of-the-art performance often requires computationally expensive fine-tuning methods. This paper introduces IQARAG, a novel, training-free framework that enhances LMMs' IQA ability by leveraging Retrieval-Augmented Generation (RAG) to retrieve semantically similar but quality-variant reference images for input images. Extensive experiments across multiple IQA datasets demonstrate that IQARAG effectively boosts the IQA performance of LMMs, offering a resource-efficient alternative to fine-tuning.",27.61,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08323v1_AtomMem  Learnable Dynamic Agentic Memory with Ato.pdf,AtomMem : Learnable Dynamic Agentic Memory,"Yupeng Huo, Yaxi Lu, Zhong Zhang, Haotian Chen, Yankai Lin*",,,"memory, agents, dynamic, learning, reinforcement, CRUD, fine-tuning","Proposes AtomMem, a learning-based memory framework for agents, which reframes memory management as a dynamic decision-making problem. It deconstructs memory processes into atomic CRUD operations and learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Demonstrates superior performance over static workflow memory methods across long-context benchmarks.",25.27,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08327v1_Safe Heterogeneous Multi-Agent RL with Communicati.pdf,Safe Heterogeneous Multi-Agent RL with Communication Regularization for Coordinated Target Acquisition,"Gabriele Calzolari ∗, Vidya Sumathy ∗, Christoforos Kanellakis ∗, George Nikolakopoulos ∗",Not found,Not found,"Cooperative target acquisition, Safe autonomous coordination, Decentralized multi-agent reinforcement learning, Heterogeneous robotic systems, Learning-based control","This paper introduces a decentralized multi-agent reinforcement learning framework enabling structurally heterogeneous teams of agents to jointly discover and acquire randomly located targets in environments characterized by partial observability, communication constraints, and dynamic interactions. Each agent's policy is trained with the Multi-Agent Proximal Policy Optimization algorithm and employs a Graph Attention Network encoder that integrates simulated range-sensing data with communication embeddings exchanged among neighboring agents, enabling context-aware decision-making from both local sensing and relational information. The work introduces a unified framework that integrates graph-based communication and trajectory-aware safety through safety filters. The architecture is supported by a structured reward formulation designed to encourage effective target discovery and acquisition, collision avoidance, and de-correlation between the agents' communication vectors by promoting informational orthogonality. The effectiveness of the proposed reward function is demonstrated through a comprehensive ablation study. Simulation results confirm the framework's effectiveness in safe and stable task execution.",28.0,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08332v1_IGAN A New Inception-based Model for Stable and Hi.pdf,IGAN: A New Inception-based Model for Stable and High-Fidelity Image Synthesis Using Generative Adversarial Networks,"Ahmed A. Hashim, Ali Al-Shuwaili, Asraa Saeed, Ali Al-Bayaty*",,,"Generative Adversarial Networks (GANs), dilation convolutions, inception module, spectral normalization, image synthesis, deep learning stability","This paper proposes a novel GAN structural model, termed Inception Generative Adversarial Network (IGAN), which generates high-quality synthetic images while maintaining training stability by reducing mode collapse and preventing vanishing and exploding gradients. The IGAN model achieves improved FID and IS scores compared to state-of-the-art GANs.",27.58,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08333v1_Semantic Laundering in AI Agent Architectures Why .pdf,Semantic Laundering in AI Agent Architectures: Why Tool Boundaries Do Not Confer Epistemic Warrant,"Oleg Romanchuk, Roman Bondar",,2601.08333,"AI, agent architectures, epistemic justification, semantic laundering, Gettier problem","This paper discusses how language models in AI agent architectures systematically conflate information transport mechanisms with epistemic justification mechanisms, leading to a phenomenon known as semantic laundering. It formalizes this architectural failure and shows that it constitutes an epistemic problem similar to the Gettier problem, where propositions with absent or weak warrant are accepted as admissible. The central result is the Theorem of Inevitable Self-Licensing, which states that under standard architectural assumptions, circular epistemic justification cannot be eliminated. The paper introduces the Warrant Erosion Principle as the fundamental explanation for this effect and demonstrates that scaling, model improvement, and LLM-as-judge schemes are structurally incapable of eliminating the problem.",29.4,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08360v1_Scalable Sequential Recommendation under Latency a.pdf,Scalable Sequential Recommendation under Latency and Memory Constraints,"Adithya Parthasarathy, Aswathnarayan Muthukrishnan, Kirubakaran, Vinoth Punniyamoorthy, Nachiappan Chockalingam, Lokesh Butra, Kabilan Kannan, Abhirup Mazumder, Sumit Saha",Not found,2601.08360,"Recommender Systems, Sequence Modeling, Representation Learning, Scalable Machine Learning, Deep Learning","This paper presents HoloMambaRec, a lightweight sequential recommendation architecture that combines holographic reduced representations for attribute-aware embedding with a selective state space encoder for linear-time sequence processing. It demonstrates consistent performance and lower memory complexity compared to existing methods.",28.77,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08371v1_Geo-NVS-w Geometry-Aware Novel View Synthesis In-t.pdf,Geo-NVS-w: Geometry-Aware Novel View Synthesis In-the-Wild,"Anastasios Tsalakopoulos, Angelos Kanlis, Evangelos Chatzis, Antonis Karakottas, Dimitrios Zarpalas",Not provided,Not provided,"Novel View Synthesis, Geometry-Aware, In-the-Wild, Signed Distance Function, Photorealism","We introduce Geo-NVS-w, a geometry-aware framework for high-fidelity novel view synthesis from unstructured, in-the-wild image collections. It addresses the limitation of existing methods by leveraging an underlying geometric representation based on a Signed Distance Function (SDF) to guide the rendering process, ensuring fine structural details are preserved. Our framework achieves competitive rendering performance and demonstrates a 4–5× reduction in energy consumption compared to similar methods.",27.43,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08379v1_Training-Free Distribution Adaptation for Diffusio.pdf,Training-Free Distribution Adaptation for Diffusion Models via Maximum Mean Discrepancy Guidance,"Matina Mahdizadeh Sani ∗, Nima Jamali ∗, Mohammad Jalali ∗, Farzan Farnia ¶",Not found,Not found,"Diffusion models, Maximum Mean Discrepancy (MMD), Distribution adaptation, Inference-time guidance","Pre-trained diffusion models are powerful generative priors for both unconditional and conditional sample generation, but their outputs often deviate from user-specific target data. MMD Guidance, a training-free mechanism, augments the reverse diffusion process with gradients of the MMD between generated samples and a reference dataset to achieve distributional alignment while preserving sample fidelity.",28.44,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08380v1_Thematic Working Group 5 -- Artificial Intelligenc.pdf,EDUsummIT 2025 - eBook,"Mary Webb, Matt Bower, Ana Amélia Carvalho, Fredrik Mørk Røkenes, Jodie Torrington, Jonathan D. Cohen, Yousra Chtouki, Kathryn MacCallum, Tanya Linden, Deirdre Butler, Juliana E. Raffagheli, Henriikka Vartiainen, Martina Ronci, Peter Tiernan, David M. Smith, Chris Shelton, Joyce Malyn-Smith, Pierre Gorissen",,,"Artificial Intelligence, AI literacy, teaching and learning, plagiarism, academic integrity, creativity, critical thinking","This paper focuses on the development and implementation of effective strategies for enhancing AI literacy and agency among teachers, equipping them with the knowledge and skills necessary to integrate AI into their teaching practices. It explores various aspects including curriculum design, professional development programs, practical classroom applications, and policy guidelines aimed at empowering educators to confidently utilize AI tools and foster a deeper understanding of AI concepts among students.",29.06,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08382v2_A Qualitative Model to Reason about Object Rotatio.pdf,A Qualitative Model to Reason about Object Rotations – applied to solve the Cube Comparison Test,Zoe Falomira,,,"cube comparison test, mental rotation, qualitative reasoning, spatial cognition, spatial reasoning",This paper presents a Qualitative model for Reasoning about Object Rotations (QOR) applied to solve the Cube Comparison Test (CCT) by Ekstrom et al. (1976). A conceptual neighborhood graph relating the Rotation movement to the Location change and the Orientation change of the features on the cube sides has been built and it produces composition tables to calculate inferences for reasoning about rotations.,26.66,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08383v1_Deconstructing Pre-training Knowledge Attribution .pdf,Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models,"Bo Wang, Junzhuo Li, Hong Chen, Yuanlin Chu, Yuxuan Fan, Xuming Hu",Not found,Not found,"Mixture-of-Experts, Knowledge Acquisition, Pre-training, Neuron-level Attribution, Log-Probability Increase, Dense Models, Interpretability","This paper introduces Gated-LPI, a neuron-level attribution metric, to analyze knowledge acquisition dynamics in Mixture-of-Experts (MoE) and dense models during pre-training. It presents a time-resolved comparison of these dynamics, uncovering three patterns: a high-utility core in MoE, early consolidation, and functional robustness. These findings suggest that sparsity fosters an intrinsically stable and distributed computational backbone from early training, aiding in bridging the gap between sparse architectures and training-time interpretability.",26.88,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08388v1_Creativity in AI as Emergence from Domain-Limited .pdf,Creativity in AI as Emergence from Domain-Limited Generative Models,Corina Chutaux,,,"Artificial Intelligence, Creativity, Generative Models, Emergence, Multimodal Systems","This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. It examines how four interacting components—pattern-based generation, induced world models, contextual grounding, and arbitrariness—manifest in multimodal generative systems, providing a technical framework for studying creativity as an emergent phenomenon in AI systems.",27.46,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08393v1_Controlled LLM Training on Spectral Sphere.pdf,Controlled LLM Training on Spectral Sphere,"Tian Xie1*, Haoming Luo2, Haoyu Tang2, Yiwen Hu2, Jason Klein Liu Qingnan Ren 1, Yang Wang1, Wayne Xin Zhao2, Rui Yan3, Bing Su2, Chong Luo1, Baining Guo1",Not found,2601.08393,"Large Language Models, Optimization, Spectral Sphere, Maximal Update Parametrization, Muon Optimizer, Megatron, Stability","This paper introduces the Spectral Sphere Optimizer (SSO) for training large language models (LLMs). SSO enforces strict spectral constraints on both weights and updates, ensuring a fully µP-aligned optimization process. Through extensive pretraining on diverse architectures, including Dense-1.7B, MoE 8B-A1B, and 200-layer DeepNet models, SSO consistently outperforms AdamW and Muon. The paper also highlights practical stability benefits, such as improved MoE router load balancing and strictly bounded activations.",28.96,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08401v1_An Explainable Two Stage Deep Learning Framework f.pdf,An Explainable Two-Stage Deep Learning Framework for Pericoronitis Assessment in Panoramic Radiographs Using YOLOv8 and ResNet-50,"Ajo Babu George 1*, Pranav S 2†, Kunal Agarwal 3†",Not found,2601.08401,"AI, Deep Learning, Pericoronitis, Panoramic Radiographs, YOLOv8, ResNet-50, Anatomical Localization, Pathological Classification, Interpretability","To overcome challenges in diagnosing pericoronitis on panoramic radiographs, an AI-assisted assessment system integrating anatomical localization, pathological classification, and interpretability was developed using a two-stage deep learning pipeline.",28.84,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08402v1_PATS Personality-Aware Teaching Strategies with La.pdf,PATS: Personality-Aware Teaching Strategies,"Donya Rooein1*, Sankalan Pal Chowdhury2*, Mariia Eremeeva2, Yuan Qin3, Debora Nozza 1, Mrinmaya Sachan 2, Dirk Hovy 1",,,"Personality, Teaching Strategies, Large Language Models, Education, Intelligent Tutoring Systems","Recent advances in large language models demonstrate their potential as educational tutors, but mismatches between tutoring strategies and student personalities can be counterproductive. This paper constructs a taxonomy linking pedagogical methods to personality profiles and simulates student-teacher conversations to adjust LLM tutors' strategies based on simulated student personality traits. The method increases the use of less common, high-impact strategies and is preferred over two base approaches by human teachers and annotators.",27.39,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08403v1_Owen-Shapley Policy Optimization OSPO A Principled.pdf,Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs,"Abhijnan Nath, Alireza Bagheri Garakani, Tianchen Zhou, Fan Yang, Nikhil Krishnaswamy",,,"Reinforcement Learning, Policy Optimization, Generative Search, Large Language Models, Shapley-Owen, Coalitions, Semantically Coherent Units, Reward Shaping, Interpretability, Efficiency","Large language models are increasingly trained via reinforcement learning for personalized recommendation tasks. Standard methods like GRPO rely on sparse, sequence-level rewards that create a credit assignment gap, obscuring which tokens drive success. OSPO redistributes sequence-level advantages based on tokens' marginal contributions to outcomes, identifying which response parts drive performance. Experiments show consistent gains over baselines, with notable test-time robustness to out-of-distribution retrievers unseen during training.",27.4,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08406v1_WebTrap Park An Automated Platform for Systematic .pdf,WebTrap Park: An Automated Platform for Systematic Security Evaluation of Web Agents,"Xinyi Wu†, Jiagui Chen †, Geng Hong †B, Jiayi Dong †, Xudong Pan †‡, Jiarun Dai †, Min Yang †B",,,"Web Agents, Security Evaluation, Automated Platform, Systematic Evaluation, Web Security","WebTrap Park is an automated platform designed for the systematic security evaluation of Web Agents, addressing the fragmented and difficult-to-standardize security evaluation of these agents. It instantiates three major sources of security risk into 1,226 evaluation tasks, enabling action-based assessment without requiring agent modification. The platform reveals clear security differences across agent frameworks and highlights the importance of agent architecture beyond the underlying model. WebTrap Park is publicly accessible at https://security.fudan.edu.cn/webagent and provides a scalable foundation for reproducible Web Agent security evaluation.",27.02,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08412v1_Hybrid Distillation with CoT Guidance for Edge-Dro.pdf,Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation,"Yizhan Feng, Hichem Snoussi, Yuhang Wang, Jing Teng, Abel Cherouat, Tian Wang",,,"Large language models, drone, Knowledge Distillation, Chain-of-Thought, Lightweight","With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models.",23.6,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08415v2_Regulatory gray areas of LLM Terms.pdf,Regulatory gray areas of LLM Terms,"Brittany I. Davidson, Kate Muir, Florian A.D. Burnat, Adam N. Joinson",,2601.08415v2,"Language Models, LLMs, Privacy Policy, Terms of Service, Regulation","Large Language Models (LLMs) are increasingly integrated into academic research pipelines, but the Terms of Service governing their use remain under-examined. This paper presents a comparative analysis of the Terms of Service of five major LLM providers, revealing substantial variation in usage restrictions for general users and researchers. It identifies specific complexities for researchers in security research, computational social sciences, and psychological studies, and identifies 'regulatory gray areas' where Terms of Service create uncertainty for legitimate use.",24.53,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08418v1_Taxon Hierarchical Tax Code Prediction with Semant.pdf,Taxon: Hierarchical Tax Code Prediction with Semantically Aligned LLM Expert Guidance,"Jihang Li, Qing Liu, Zulong Chen, Jing Wang, Wei Wang, Chuanfei Xu, Zeyi Wen",,,"Taxonomy, Hierarchical Tax Code Prediction, LLM Expert Guidance, Automated Invoicing, Compliance Management, Large-scale E-commerce","This paper presents Taxon, a semantically aligned and expert-guided framework for hierarchical tax code prediction in large-scale e-commerce platforms. It integrates a feature-gating mixture-of-experts architecture and a semantic consistency model from large language models to address the challenges of accurate product mapping to taxonomic hierarchies. The framework is validated on proprietary and public datasets, demonstrating state-of-the-art performance and structural consistency improvements.",24.76,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08430v1_RubricHub A Comprehensive and Highly Discriminativ.pdf,RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset,"Sunzhu Li, Jiale Zhao, Miteto Wei, Huimin Ren, Yang Zhou, Jingwen Yang, Shunyu Liu, Kaike Zhang, Wei Chen",Not found,Not found,"Reinforcement Learning, Verifiable Rewards, Automated Rubric Generation, Large Language Models, Mathematics, Coding","This paper proposes an automated Coarse-to-Fine Rubric Generation framework to address the challenges of optimizing open-ended generation in reinforcement learning with verifiable rewards. The framework combines principle-guided synthesis, multi-model aggregation, and difficulty evolution to produce comprehensive and highly discriminative criteria. The authors introduce RubricHub, a large-scale dataset (∼110k) and multi-domain dataset, validated through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate significant performance gains, with the post-trained Qwen3-14B achieving state-of-the-art results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5.",23.21,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08434v3_Large Multimodal Models for Embodied Intelligent D.pdf,Large Multimodal Models for Embodied Intelligent Driving: The Next Frontier in Self-Driving?,"Long Zhang, Yuchen Xia, Zhen Liu, Bingqing Wei, Shiwen Mao, Zhu Han, Mohsen Guizani",,,"autonomous driving, embodied intelligence, large multimodal models, self-driving, semantic understanding, policy optimization, deep reinforcement learning","This article introduces a novel hybrid decision framework to tackle the limitations of modular design in autonomous driving, ensuring continuous learning and joint decision-making through the integration of Large Multimodal Models (LMMs) for semantic understanding and cognitive representation, and deep reinforcement learning (DRL) for real-time policy optimization.",26.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08441v1_YaPO Learnable Sparse Activation Steering Vectors .pdf,YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation,"Abdelaziz Bounhar, Rania Hossam Elmohamady Elbadry, Hadi Abdine, Preslav Nakov, Michalis Vazirgiannis, Guokan Shang",,1909.09207,"Large Language Models, Domain Adaptation, Sparse Activation Steering, Bi-directional Preference Optimization, Direct Preference Optimization, Sparse Autoencoder, Steering Vectors, Fine-grained Alignment, Cultural Alignment, Hallucination, Wealth-seeking, Jailbreak, Power-seeking","This paper proposes Y aPO, a reference-free method that learns sparse steering vectors in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, Y aPO produces disentangled, interpretable, and efficient steering directions, achieving faster convergence, stronger performance, and improved training stability compared to dense steering baselines. Y aPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking, and preserves general knowledge without measurable degradation on MMLU. The method provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to control and domain adaptation.",28.24,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08444v1_Beyond Linearization Attributed Table Graphs for T.pdf,Beyond Linearization: Attributed Table Graphs for Table Reasoning,"Yuxiang Wang, Junhao Gan, Shengxiang Gao, Shenghao Ye, Zhengyi Yang, Jianzhong Qi",Not found,Not found,"Table Reasoning, Attributed Table Graphs, Large Language Models, Linearization, Graph-based Reasoning, Question-Guided Personalized PageRank","This paper proposes Table Graph Reasoner (TABGR), a training-free model that represents tables as an Attributed Table Graph (ATG) to address the limitations of linearization-based methods in table reasoning. TABGR explicitly preserves row-column-cell structures while enabling graph-based reasoning for explainability. It also introduces a Question-Guided Personalized PageRank (QG-PPR) mechanism to mitigate the lost-in-the-middle issue. Extensive experiments show that TABGR outperforms state-of-the-art models by up to 9.7% in accuracy.",27.53,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08448v1_Divide and Conquer Static-Dynamic Collaboration fo.pdf,Divide and Conquer: Static-Dynamic Collaboration for Few-Shot Class-Incremental Learning,"Kexin Bao, Daichi Zhang∗, Yong Li, Dan Zeng, Shiming Ge∗",10.1145/3731715.3733310,,"Few-Shot Class-Incremental Learning, Class-Incremental Learning","This paper proposes a framework termed Static-Dynamic Collaboration (SDC) to address the stability-plasticity dilemma in few-shot class-incremental learning (FSCIL). It divides the task into two stages: Static Retaining Stage (SRS) and Dynamic Learning Stage (DLS), which respectively retain and adapt to new classes. Extensive experiments on public benchmarks and a real-world application dataset demonstrate improved performance compared to other methods.",27.46,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08450v1_Decoding Order Matters in Autoregressive Speech Sy.pdf,DECODING ORDER MA TTERS IN AUTOREGRESSIVE SPEECH SYNTHESIS,"Minghui Zhao, Anton Ragni",,,"speech synthesis, discrete diffusion model, order-agnostic autoregressive decoding","This paper investigates the impact of decoding order on autoregressive speech synthesis, using a masked diffusion framework. It shows that randomness in decoding order affects speech quality and compares fixed and adaptive decoding strategies. The study also explores the use of quantized acoustic representations for speech synthesis.",25.58,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08457v1_An Under-Explored Application for Explainable Mult.pdf,An Under-Explored Application for Explainable Multimodal Misogyny Detection,"Sargam Yadava, Abhishek Kaushik, Kevin McDaid",,,"hate speech, misogyny, natural language processing, code-mixing, hinglish",This paper presents a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models and provides feature importance scores using explainability techniques.,26.61,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08462v1_M3-BENCH Process-Aware Evaluation of LLM Agents So.pdf,M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games,"Sixiong Xie*, Zhuofan Shi*, Haiyang Shen*, Gang Huang, Yun Ma, Xiang Jing*",,,"large language models, social behaviors, mixed-motive games, process-aware evaluation, behavioral trajectory analysis, reasoning process analysis, communication content analysis, Big Five personality model, Social Exchange Theory","As large language model agents advance, their social behaviors, including cooperation, deception, and collusion, require systematic evaluation. M3-BENCH proposes a multi-stage benchmark for mixed-motive games, integrating process-aware analysis across three modules: BTA, RPA, and CCA, and incorporating personality and social exchange theories to characterize agents' personality traits and capability profiles beyond simple task scores or outcome-based metrics.",27.52,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08464v1_CoMa Contextual Massing Generation with Vision-Lan.pdf,CoMa: Contextual Massing Generation with Vision-Language Models,"Evgenii Maslov, Valentin Khrulkov, Anastasia Volkova, Anton Gusarov, Andrey Kuznetsov, Ivan Oseledets",Not found,2601.08464,"architecture, massing, vision-language models, conditional generation, urban planning","This paper proposes an automated framework for generating building massing based on functional requirements and site context. It introduces the CoMa-20K dataset, a comprehensive collection that includes detailed massing geometries, associated economical and programmatic data, and visual representations of the development site within its existing urban context. The authors benchmark this dataset by formulating massing generation as a conditional task for Vision-Language Models (VLMs), evaluating both fine-tuned and large zero-shot models. The experiments reveal the inherent complexity of the task and demonstrate the potential of VLMs to produce context-sensitive massing options.",29.52,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08468v1_JudgeRLVR Judge First Generate Second for Efficien.pdf,"JudgeRLVR: Judge First, Generate Second for Efficient Reasoning","Jiangshan Duo †‡★, Hanyu Li ‡§, Hailin Zhang ‡, Yudong Wang †‡, Sujian Li †, Liang Zhao ‡",Not found,2601.08468,"Reinforcement Learning, Verifiable Rewards, Efficient Generation, Large Language Models, Reasoning","This paper proposes JudgeRLVR, a two-stage judge-then-generate paradigm for efficient reasoning in large language models. By training the model to judge solution responses with verifiable answers, it achieves a better quality-efficiency trade-off compared to vanilla RLVR, demonstrating enhanced generalization on both in-domain and out-of-domain benchmarks.",27.83,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08472v1_sui-1 Grounded and Verifiable Long-Form Summarizat.pdf,Grounded and Verifiable: Long-Form Summarization with Citation Annotations,"Benedikt Droste*, Jan Philipp Harries, Maximilian Idahl, Björn Plüster, ellamind",Not found,2601.08472,"summarization, citation grounding, long document, language models, verification","This paper presents sui-1, a 24B parameter model capable of generating abstractive summaries with inline citations, enabling users to trace each claim to its source sentence. The model processes documents up to 100K tokens and supports iterative processing for texts exceeding 2 million tokens. It uses synthetic data generation with chain-of-thought prompting and multi-stage verification to create high-quality training examples across five languages.",27.04,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08475v1_SUMMPILOT Bridging Efficiency and Customization fo.pdf,SUMMPILOT: Bridging Efficiency and Customization for Interactive Summarization System,"JungMin Yun, Juhwan Choi, Kyohoon Jin, Soojin Jang, Jinhee Jang, YoungBin Kim",,,"summarization, interactive, personalized, large language model, multi-document","This paper introduces SUMMPILOT, an interactive customizable summarization system that combines the efficiency of automatic summarization with personalized summaries tailored to individual users' interests and requirements. It leverages a large language model to facilitate both automatic and interactive summarization, allowing users to engage with the system to understand document content and personalize summaries through interactive components such as semantic graphs, entity clustering, and explainable evaluation. The system is demonstrated to be adaptable and useful for customizable summarization.",27.19,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08490v1_BenchOverflow Measuring Overflow in Large Language.pdf,BenchOverflow: Measuring Overflow in Large Language Models via Plain-Text Prompts,"Erin Feiglin, Nir Hutnik, Raz Lapid",Not found,2601.08490,"Large Language Models, Overflow, Plain-Text Prompts, Model Benchmarking","This paper investigates a failure mode in large language models where plain-text prompts elicit excessive outputs, termed Overflow. It introduces BenchOverflow, a model-agnostic benchmark of nine plain-text prompting strategies, evaluating their impact on output length and providing insights into the practical and economic implications of Overflow.",25.88,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08493v1_PKI Prior Knowledge-Infused Neural Network for Few.pdf,PKI: Prior Knowledge-Infused Neural Network for Few-Shot Class-Incremental Learning,"Kexin Bao, Fanzhao Lin, Zichen Wang, Yong Li, Dan Zeng, Shiming Ge",Not found,2601.08493,"Few-shot learning, Class-incremental learning, Neural networks, Prior knowledge, Catastrophic forgetting, Overfitting","This paper proposes a PKI (Prior Knowledge-Infused Neural Network) to address the challenges of catastrophic forgetting and overfitting in few-shot class-incremental learning, by retaining and infusing prior knowledge into the neural network during incremental sessions.",28.26,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08499v2_EfficientFSL Enhancing Few-Shot Classification via.pdf,EfficientFSL: Enhancing Few-Shot Classification via Query-Only Tuning in Vision Transformers,"Wenwen Liao, Hang Ruan, Jianbo Yu*, Bing Song, Yuansong Wang, Xiaofeng Yang",Not found,Not found,"Few-Shot Learning, Vision Transformers, Query-Only Tuning, Feature Extraction, Robustness","EfficientFSL proposes a query-only fine-tuning framework for Vision Transformers in few-shot classification, reducing computational overhead while achieving competitive performance. It leverages pre-trained model knowledge and introduces lightweight trainable Forward and Combine Blocks to synthesize task-specific queries and fuse multi-layer outputs, mitigating distribution shift through Support-Query Attention Block. Achieving state-of-the-art performance on in-domain and cross-domain few-shot datasets.",27.11,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08503v1_Temporal Fusion Nexus A task-agnostic multi-modal .pdf,Temporal Fusion Nexus: A task-agnostic multi-modal embedding model for clinical narratives and irregular time series in post-kidney transplant care,"Aditya Kumar, Simon Rauch, Mario Cypko, Marcel Naik, Matthieu-P Schapranow, Aadil Rashid, Fabian Halleck, Bilgin Osmanodja, Roland Roller, Lars Pape, Klemens Budde, Mario Schiffer, Oliver Amft",10.1101/2601.08503,2601.08503,"clinical narratives, irregular time series, post-kidney transplant care, multi-modal embedding, task-agnostic model","Temporal Fusion Nexus (TFN) is a multi-modal and task-agnostic embedding model that integrates irregular time series and unstructured clinical narratives. It achieved higher performance for graft loss and graft rejection compared to state-of-the-art models in post-kidney transplant care, and yielded an AUC of 0.86 in mortality prediction. TFN outperformed unimodal baselines and improved performance across all tasks, with disentanglement metrics confirming robust and interpretable latent factors in the embedding space.",28.61,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08509v1_What If TSF A Benchmark for Reframing Forecasting .pdf,What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal Forecasting,"Jinkwan Jang∗, Hyunbin Jin∗, Hyungjin Park, Kyubyung Chae, Taesup Kim†",Not found,Not found,"forecasting, multimodal, scenario-guided, time series, large language models","This paper introduces What If TSF (WIT), a multimodal forecasting benchmark designed to evaluate whether models can condition their forecasts on contextual text, especially future scenarios. By providing expert-crafted plausible or counterfactual scenarios, WIT offers a rigorous testbed for scenario-guided multimodal forecasting.",26.71,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08510v2_STAGE A Benchmark for Knowledge Graph Construction.pdf,"STAGE: A Benchmark for Knowledge Graph Construction, Question Answering, and In-Script Role-Playing over Movie Screenplays","Qiuyu Tian, Yiding Li, Fengyi Chen, Zequn Liu, Youyong Kong, Fan Guo, Yuyao Li, Jinjing Shen, Zhijing Xie, Yiyun Luo, Xin Zhang",Not found,2601.08510,"knowledge graph, screenplay, question answering, role-playing, narrative understanding","This paper introduces STAGE, a unified benchmark for narrative understanding over full-length movie screenplays, evaluating models' ability to construct coherent story worlds and use them consistently across multiple reasoning and generation tasks.",28.16,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08519v1_CD2 Constrained Dataset Distillation for Few-Shot .pdf,CD2: Constrained Dataset Distillation for Few-Shot Class-Incremental Learning,"Kexin Bao, Daichi Zhang, Hansong Zhang, Yong Li, Yutao Yue, Shiming Ge",Not found,Not found,"Few-shot learning, Class-incremental learning, Catastrophic forgetting, Dataset distillation","This paper proposes a framework termed Constrained Dataset Distillation (CD2) to facilitate few-shot class-incremental learning (FSCIL), which includes a dataset distillation module and a distillation constraint module. The framework aims to prevent catastrophic forgetting by synthesizing highly condensed samples guided by the classifier and introducing a designed loss to constrain the previously learned class distribution.",27.03,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08531v1_Sketch-Based Facade Renovation With Generative AI .pdf,Sketch-Based Facade Renovation With Generative AI Models,"Warissara Booranamaitree, Xusheng Du, Y ushu Cai, Zhengyang Wang, Ye Zhang, Haoran Xie",,,"Industrial building renovation, vision-language model, diffusion model, user sketches, facade renovation","This paper proposes a three-stage framework combining generative artificial intelligence (AI) and vision-language models (VLM) to produce renovation proposals directly from rough structural sketches and textual descriptions, bypassing the need for detailed as-built modelling. The framework generates photorealistic images of renovated facades while preserving the original structure.",27.69,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08545v2_Learner-Tailored Program Repair A Solution Generat.pdf,Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement,"Zhenlong Dai, Zhuoluo Zhao, Hengning Wang, Xiu Tang, Sai Wu, Chang Yao, Zhipeng Gao, Jingyuan Chen",,,"program repair, intelligent tutoring, large language models, code retrieval, bug fixing","This paper introduces a novel task, Learner-Tailored Program Repair (LPR), and proposes a novel framework, LSGEN, to enhance program repair while providing bug descriptions. The approach utilizes a solution retrieval database and an edit-driven code retrieval method to guide LLMs in identifying and fixing bugs in buggy code. It also proposes an Iterative Retrieval Enhancement method to improve performance in practical programming coaching scenarios.",27.96,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08549v1_Contrastive and Multi-Task Learning on Noisy Brain.pdf,Contrastive and Multi-Task Learning on Noisy Brains Signals with Nonlinear Dynamical Signatures,"Sucheta Ghosh, Zahra Monfared, Felix Dietrich",,,"Electroencephalography, Motor Imagery, Chaotic Dynamics, Nonlinear Dynamics, Self-Supervised Learning, Contrastive Learning, Denoising, Temporal Dynamics","This work introduces a two-stage multitask learning framework for analyzing noisy EEG signals, integrating denoising, dynamical modeling, and representation learning. It enhances robustness and generalization in EEG decoding, surpassing strong baselines and recent state-of-the-art methods.",27.75,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08557v1_VideoHEDGE Entropy-Based Hallucination Detection f.pdf,VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations,"Sushant Gautam, Cise Midoglu, Vajira Thambawita, Michael A. Riegler, Pål Halvorsen",,,"hallucination detection, video-vision-language models, entropy-based reliability, semantic clustering, spatiotemporal perturbations","VideoHEDGE is a modular framework for hallucination detection in video question answering, extending entropy-based reliability estimation from images to temporally structured inputs. It evaluates hallucinations on the SoccerChat benchmark using an LLM-as-a-judge to obtain binary hallucination labels. Across three 7B Video-VLMs, VASE consistently achieves the highest ROC-AUC, especially at larger distortion budgets, while SE and RadFlag often operate near chance. Embedding-based clustering matches NLI-based clustering in detection performance at lower computational cost, and domain fine-tuning reduces hallucination frequency but yields only modest improvements in calibration.",28.81,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08559v1_WaterCopilot An AI-Driven Virtual Assistant for Wa.pdf,WaterCopilot: An AI-Driven Virtual Assistant for Water Management,"Keerththanan Vickneswaran, Mariangel Garcia Andarcia, Hugo Retief, Chris Dickens, Paulo Silva",Not found,Not found,"Water resource management, Retrieval-Augmented Generation (RAG), Limpopo River Basin, Azure AI, Real-time APIs, Multilingual chatbots, Digital Twin, AWS deployment, RAGAS evaluation","This paper presents WaterCopilot, an AI-driven virtual assistant developed for the Limpopo River Basin to address challenges in sustainable water resource management, including fragmented data and limited real-time access. Built on RAG and tool-calling architectures, it integrates static policy documents and real-time hydrological data via custom plugins, enabling semantic search and dynamic insights. Evaluated using the RAGAS framework, WaterCopilot achieves high answer relevancy and context precision, with key innovations in automated alerts and integration with the LRB Digital Twin. While limitations remain, the study demonstrates the potential of AI assistants to support informed decision-making and strengthen water security in complex river basins.",28.11,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08565v1_Rewriting Video Text-Driven Reauthoring of Video F.pdf,Rewriting Video: Text-Driven Reauthoring of Video Footage,"Sitong Wang, Columbia University, New York, NY, USA, sw3504@columbia.edu, Anh Truong, Adobe Research, New York, NY, USA, truong@adobe.com, Lydia B. Chilton, Columbia University, New York, NY, USA, chilton@cs.columbia.edu, Dingzeyu Li, Adobe Research, Seattle, WA, USA, dinli@adobe.com",,,"Video reauthoring, Text-driven video editing, Generative video models, Creative AI tools","Video is a powerful medium for communication and storytelling, yet reauthoring existing footage remains challenging. Recent advances in generative AI suggest a new paradigm: what if editing a video were as straightforward as rewriting text? This paper presents a tech probe and a study on text-driven video reauthoring, involving a generative reconstruction algorithm and an interactive probe, Rewrite Kit, to manipulate editable text prompts. A technical evaluation reveals a critical human-AI perceptual gap, and a probe study with 12 creators surfaced novel use cases such as virtual reshooting, synthetic continuity, and aesthetic restyling, highlighting key tensions around coherence, control, and creative alignment.",28.87,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08602v1_WaveFormer Frequency-Time Decoupled Vision Modelin.pdf,WaveFormer: Frequency-Time Decoupled Vision Modeling with Wave Equation,"Zishan Shu, Juntong Wu, Wei Yan, Xudong Liu, Hongyu Zhang, Chang Liu, Youdong Mao, Jie Chen",Not found,Not found,"Vision modeling, Transformer, Wave equation, Frequency-time decoupling, Self-attention, Partial differential equations","This paper presents WaveFormer, a novel vision modeling approach that treats feature maps as spatial signals governed by an underdamped wave equation. It models spatial frequency explicitly and controls its interaction with propagation time, achieving competitive accuracy and up to 1.6× higher throughput compared to attention-based alternatives.",27.26,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08605v1_ExpSeek Self-Triggered Experience Seeking for Web .pdf,ExpSeek: Self-Triggered Experience Seeking for Web Agents,"Wenyuan Zhang, Xinghua Zhang, Haiyang Yu, Shuaiyi Nie, Bingli Wu, Juwei Yue, Tingwen Liu, Yongbin Li",,,"Experience intervention, Web agents, Self-triggered seeking, Entropy, Large language models","ExpSeek proposes a self-triggered experience seeking approach for web agents, shifting experience from passive global injection to step-level proactive seeking. It estimates step-level entropy thresholds to determine intervention timing and designs step-level tailored experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate significant improvements in performance.",26.89,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08611v1_VeriTaS The First Dynamic Benchmark for Multimodal.pdf,VERITAS: The First Dynamic Benchmark for Multimodal Automated Fact-Checking,"Mark Rothermel, Marcus Kornmann, Marcus Rohrbach, Anna Rohrbach",,,"Automated Fact-Checking, Multimodal AI, Benchmarking, Fact-Verification","The growing scale of online misinformation urgently demands Automated Fact-Checking (AFC). Existing benchmarks for evaluating AFC systems are largely limited in terms of task scope, modalities, domain, language diversity, realism, or coverage of misinformation types. Critically, they are static, thus subject to data leakage as their claims enter the pretraining corpora of LLMs. As a result, benchmark performance no longer reliably reflects the actual ability to verify claims. We introduce VERITAS, a dynamic benchmark for multimodal Automated Fact-Checking.",26.1,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08620v1_ViDoRe V3 A Comprehensive Evaluation of Retrieval .pdf,ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios,"António Loison*, Quentin Macé*, Antoine Edy*, Victor Xing, Tom Balough, Gabriel Moreira, Bo Liu, Manuel Faysse†, Céline Hudelot†, Gautier Viaud",Not provided,Not provided,"Retrieval-Augmented Generation, RAG, Multi-modal, Human-verified queries, Visual elements, Document corpora, Professional domains","ViDoRe V3 is a comprehensive multi-modal benchmark for Retrieval-Augmented Generation (RAG) pipelines, covering 10 datasets across diverse professional domains. It evaluates the performance of state-of-the-art RAG pipelines, revealing that visual retrievers outperform textual ones, late-interaction models, and textual reranking. The benchmark provides high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers through 12,000 hours of human annotation effort.",28.0,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08623v1_SafeRedir Prompt Embedding Redirection for Robust .pdf,SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models,"Renyang Liu, Kangjie Chen, Han Qiu, Jie Zhang, Kwok-Yan Lam, Tianwei Zhang, See-Kiong Ng",,,"Image Generation, Unlearning, Prompt Embedding, Adversarial Attacks, Semantic Control","This paper introduces SafeRedir, a lightweight inference-time framework for robust unlearning in image generation models. It redirects unsafe prompts toward safe semantic regions through token-level interventions in the embedding space, achieving effective unlearning, high semantic and perceptual preservation, robust image quality, and enhanced resistance to adversarial attacks.",26.87,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08631v1_M2FMoE Multi-Resolution Multi-View Frequency Mixtu.pdf,M2FMoE: Multi-Resolution Multi-View Frequency Mixture-of-Experts for Extreme-Adaptive Time Series Forecasting,"Yaohui Huang, Runmin Zou, Yun Wang*, Laeeq Aslam, Ruipeng Dong",Not found,Not found,"time series forecasting, extreme events, multi-resolution, multi-view, frequency modeling, mixture-of-experts, hydrology","This paper proposes M2FMoE, an extreme-adaptive forecasting model that learns both regular and extreme patterns through multi-resolution and multi-view frequency modeling. It comprises three modules: a multi-view frequency mixture-of-experts module, a multi-resolution adaptive fusion module, and a temporal gating integration module. Experiments on real-world hydrological datasets demonstrate that M2FMoE outperforms state-of-the-art baselines without requiring extreme-event labels.",27.74,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08634v1_Moral Lenses Political Coordinates Towards Ideolog.pdf,"Moral Lenses, Political Coordinates: Towards Ideological Positioning of Morally Conditioned LLMs","Chenchen Yuan, Bolei Ma, Zheyu Zhang, Bardh Prenkaj, Frauke Kreuter",,abs/2312.09668,"Large Language Models, Political Orientation, Moral Conditioning, Political Compass Test, Moral Values, Social Psychology","This work investigates the causal relationship between moral values and political positioning by treating moral orientation as a controllable condition. It evaluates the shifts in models' political orientations when conditioned to endorse or reject specific moral values, using the Political Compass Test. The findings show pronounced, value-specific shifts in models' political coordinates and highlight the importance of anchoring political assessments within broader social values, including morality.",27.67,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08641v1_Resisting Manipulative Bots in Memecoin Copy Tradi.pdf,Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning,"Yichen Luo, Yebo Feng, Jiahua Xu, Yang Liu",XXXXXXX.XXXXXXX,,"memecoin, copy trading, multi-agent system, chain-of-thought reasoning, LLM, meme coin investment","This paper proposes an explainable multi-agent system for meme coin copy trading to address the challenges posed by manipulative bots and uncertainty in wallet performance. The system decomposes the task into subtasks and employs few-shot chain-of-thought prompting to generate explainable decisions. Empirical evaluation shows superior performance compared to traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader wallets, respectively.",27.02,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08653v1_Prism Towards Lowering User Cognitive Load in LLMs.pdf,Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding,"Zenghua Liao, Jinzhi Liao, Xiang Zhao",,,"Complex intent understanding, Large language models, Logical clarification","Large Language Models are rapidly emerging as web-native interfaces to social platforms. Existing approaches to clarify user intents through sequential or parallel questioning fall short of addressing the core challenge: modeling the logical dependencies among clarification questions. Inspired by Cognitive Load Theory, Prism proposes a novel framework for complex intent understanding that enables logically coherent and efficient intent clarification. It comprises four tailored modules: a complex intent decomposition module, a logical clarification generation module, an intent-aware reward module, and a self-evolved intent tuning module. Prism consistently outperforms existing approaches across clarification interactions, intent execution, and cognitive load benchmarks.",27.39,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08654v1_RULERS Locked Rubrics and Evidence-Anchored Scorin.pdf,RULERS: Locked Rubrics and Evidence-Anchored Scoring for Robust LLM Evaluation,"Yihan Hong, Huaiyuan Yao, Bolin Shen, Wanpeng Xu, Hua Wei, Yushun Dong",Not found,Not found,"LLM evaluation, rubric alignment, stochasticity, human grading, model calibration, natural language processing","This paper introduces RULERS, a compiler–executor framework that transforms natural language rubrics into executable specifications to address the challenges of aligning frozen, black-box models with human standards in scalable LLM evaluation. It addresses rubric instability due to prompt sensitivity, unverifiable reasoning lacking auditable evidence, and scale misalignment with human grading boundaries. Extensive experiments on essay and summarization benchmarks demonstrate that RULERS significantly outperforms representative base-lin​es in human agreement, maintains exceptional stability against adversarial rubric perturbations, and enables smaller models to rival larger proprietary judges.",27.28,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08659v1_TRACE Reconstruction-Based Anomaly Detection in En.pdf,TRACE: Reconstruction-Based Anomaly Detection in Ensemble and Time-Dependent Simulations,"Hamid Gadirov, Martijn Westra, Steffen Frey",,1912.04406,"Anomaly detection, Reconstruction-based methods, Ensemble simulations, Time-dependent data, Convolutional autoencoders, Kármán vortex street simulations","This work investigates reconstruction-based anomaly detection for ensemble data generated from parameterized Kármán vortex street simulations using convolutional autoencoders. It compares two architectural variants: a two-dimensional convolutional autoencoder operating on individual time steps and a three-dimensional convolutional autoencoder processing short temporal stacks of consecutive simulation frames. The 3D model leverages spatio-temporal context and detects anomalies related to dynamic behavior and motion characteristics, identifying anomalous evolution patterns not apparent when analyzing frames independently. The study highlights the complementary strengths of 2D and 3D convolutional autoencoders for anomaly detection in ensemble and time-dependent simulation data, emphasizing the importance of incorporating temporal context when analyzing dynamic flow phenomena.",27.23,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08662v1_From Classical to Quantum Reinforcement Learning a.pdf,From Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner’s Tutorial,"Abhijit Sen, Sonali Panda, Mahima Arya, Subhajit Patra, Zizhan Zheng, Denys I. Bondar",,,"Reinforcement Learning, Quantum Control, AI, Machine Learning, Quantum Computing","This tutorial aims to make reinforcement learning more accessible to undergraduate students by offering clear, example-driven explanations, bridging the gap between RL theory and practical coding applications. It focuses on practical applications in quantum control and code availability is provided.",26.9,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08670v1_Parallel Context-of-Experts Decoding for Retrieval.pdf,Parallel Context-of-Experts Decoding for Retrieval Augmented Generation,"Giulio Corallo, Paolo Papotti",,,"Retrieval Augmented Generation, Parallel Context-of-Experts Decoding, KV caching, Cross-document reasoning","This paper proposes Parallel Context-of-Experts Decoding (PCED), a training-free framework that shifts document aggregation from attention to decoding. PCED treats retrieved documents as isolated 'experts', synchronizing their predictions via a novel retrieval-aware contrastive decoding rule. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.",26.44,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08673v1_Why AI Alignment Failure Is Structural Learned Hum.pdf,Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock,"Didier Sornette, Sandro Claudio Lera, Ke Wu",,2601.08673v1,"AI alignment, malign agency, human interaction, blackmail, market pricing, authority relations, ultimatum bargaining","Recent reports of large language models exhibiting unethical behaviors are often interpreted as evidence of alignment failure. The authors argue that these behaviors are structural generalizations of interaction regimes under extreme asymmetries, and reframe concerns about AGI as a risk of amplification, complexity, and regime stability rather than model-level intent.",28.15,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08676v2_Advancing ESG Intelligence An Expert-level Agent a.pdf,Advancing ESG Intelligence: An Expert-level Agent and Comprehensive Benchmark for Sustainable Finance,"Yilei Zhao, Wentao Zhang, Lei Xiao, Yandan Zheng, Mengpu Liu, Wei Yang Bryan Lim",,,"ESG, sustainability, finance, corporate performance, data fragmentation, LLMs, multi-agent system, retrieval augmentation, web search, domain-specific functions, benchmark","This paper introduces ESGAgent, a hierarchical multi-agent system designed to generate in-depth ESG analysis. Complementing this system, a comprehensive three-level benchmark is presented, derived from 310 corporate sustainability reports. Empirical evaluations demonstrate that ESGAgent outperforms state-of-the-art closed-source LLMs in atomic question-answering tasks and excels in professional report generation by integrating rich charts and verifiable references. These findings confirm the diagnostic value of the benchmark, establishing it as a vital testbed for assessing general and advanced agentic capabilities in high-stakes vertical domains.",27.75,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08679v1_PersonaDual Balancing Personalization and Objectiv.pdf,PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning,"Xiaoyou Liu, Xinyi Mou, Shengbin Yue, Liang Wang, Yuqing Wang, Qiexiang Wang, Tianrui Qin, Wangchunshu Zhou, Zhongyu Wei",Not provided,Not provided,"Personalization, Objectivity, Adaptive Reasoning, Large Language Models, Memory Mechanisms","PersonaDual is a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, adapting modes based on context. It preserves the benefits of personalization while reducing interference and improving objective problem-solving.",26.56,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08682v1_Lessons from the Field An Adaptable Lifecycle Appr.pdf,Lessons from the Field: An Adaptable Lifecycle Approach to Applied Dialogue Summarization,"Kushal Chawla, Chenyang Zhu, Pengshan Cai, Sangwoo Cho, Scott Novotney, Ayushman Singh, Jonah Lewis, Keasha Safewright, Alfy Samuel, Erin Babinsky, Shi-Xiong Zhang, Sambit Sahu",,,"dialogue summarization, adaptable lifecycle, industry case study, robust methods, evaluation, component-wise optimization, upstream data bottlenecks, vendor lock-in","Summarization of multi-party dialogues is a critical capability in industry, enhancing knowledge transfer and operational effectiveness across various domains. However, generating high-quality summaries is challenging due to the need to adhere to strict, multi-dimensional requirements such as accuracy, completeness, and readability. This work presents an industry case study on developing an adaptable summarization system, sharing practical insights and protocols for robust evaluation, component-wise optimization, and addressing upstream data bottlenecks and vendor lock-in issues.",28.46,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08683v1_Region of interest detection for efficient aortic .pdf,Region of interest detection for efficient aortic segmentation,"Loris Giordano, Ine Dirks, Tom Lenaerts, Jef Vandemeulebrouck",,,"Detection, Segmentation, Multi-task learning, Cascade models, Aorta, Computed tomography","This study presents an innovative approach for efficient aortic segmentation using targeted region of interest (ROI) detection. It compares the performance of a one-step segmentation model, nnU-Net, and a cascade model composed of a detection and a segmentation step, achieving state-of-the-art performance while being compact and robust.",26.58,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08684v1_MEMEWEAVER Inter-Meme Graph Reasoning for Sexism a.pdf,MEMEWEAVER: Inter-Meme Graph Reasoning for Sexism and Misogyny Detection,"Paolo Italiani, David Gimeno-Gomez, Luca Ragazzi, Gianluca Moro, Paolo Rosso",Not found,Not found,"sexism, misogyny, graph-based, multimodal, hate speech, online harassment","Women are twice as likely as men to face online harassment due to their gender. Despite recent advances in multimodal content moderation, most approaches still overlook the social dynamics behind this phenomenon. Graph-based methods offer a promising way to capture such interactions, yet existing solutions remain limited by heuristic graph construction, shallow modality fusion, and instance-level reasoning. This work presents MEMEWEAVER, an end-to-end trainable multimodal framework for detecting sexism and misogyny through a novel inter-meme graph reasoning mechanism. It systematically evaluates multiple visual-textual fusion strategies and shows that the approach consistently outperforms state-of-the-art base-lines on the MAMI and EXIST benchmarks, while achieving faster training convergence. Further analyses reveal that the learned graph structure captures semantically meaningful patterns, offering valuable insights into the relational nature of online hate.",28.1,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08690v1_All Required In Order Phase-Level Evaluation for A.pdf,Phase-Level Evaluation for AI–Human Dialogue in Healthcare and Beyond,"Shubham Kulkarni, Alexander Lyzhov, Shiva Chaitanya, Preetam Joshi",,,"AI, Healthcare, Dialogue, Evaluation, Compliance, Clinical","This paper introduces Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE), an evaluation method that checks whether every required clinical obligation is met in the right order with clear evidence for clinicians to review. The method aims to make complex rules practical and auditable, closing the gap between technical progress and healthcare needs. Demonstrated in two case studies (respiratory history, benefits verification), OIP-SCE turns policy into shared, actionable steps, providing a single, auditable evaluation surface that aligns AI capability with clinical workflow and supports routine, safe use.",27.43,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08697v2_Auditing Student-AI Collaboration A Case Study of .pdf,Auditing Student–AI Collaboration: A Case Study of Online Graduate CS Students,Nifu Dan,10.1145/XXXXXXX.XXXXXXX,,"AI in education, human–AI collaboration, student agency, automated systems, generative AI, academic integrity, human-computer interaction","This study examines student preferences for and actual usage of AI in academic tasks, focusing on the alignment between current AI capabilities and students' desired levels of automation. It uses two surveys to capture perceived benefits, risks, and preferred boundaries of AI use, aiming to identify gaps between existing AI affordances and students' normative expectations of collaboration.",27.21,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08703v1_Evaluating the Ability of Explanations to Disambig.pdf,Evaluating the Ability of Explanations to Disambiguate Models in a Rashomon Set,"Kaivalya Rawal, Eoin Delaney, Zihao Fu, Sandra Wachter, Chris Russell",10.1145/3715275.3732219,,"explainable artificial intelligence, model disambiguation, Rashomon set, feature-importance explanations, model selection, fairness","This paper evaluates the ability of explanations to disambiguate models in a Rashomon set, proposing three principles of explanation evaluation and a new method AXE to assess the quality of feature-importance explanations. It highlights the limitations of relying on ground truth comparisons and demonstrates how AXE can detect adversarial fairwashing of explanations.",27.23,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08713v1_Real-Time Localization Framework for Autonomous Ba.pdf,Real-Time Localization Framework for Autonomous Basketball Robots,"Naren Medarametla, Sreejon Mondal",,,"Robot Localization, Autonomous Navigation, Neural Networks, Robocon",This paper proposes a hybrid localization algorithm that integrates classical techniques with learning-based methods to achieve self-localization on the basketball field for autonomous robots in Robocon 2025.,25.14,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08731v1_Learning from Demonstrations via Capability-Aware .pdf,Learning from Demonstrations via Capability-Aware Goal Sampling,"Yuanlin Duan, Rutgers University, yw895@cs.rutgers.edu, Yuning Wang, Rutgers University, yw895@cs.rutgers.edu, Wenjie Qiu, Rutgers University, wq37@cs.rutgers.edu, He Zhu, Rutgers University, hz375@cs.rutgers.edu",10.13039/56110000180e,2601.08731,"Imitation Learning, Goal Sampling, Capability-Aware, Reinforcement Learning","Despite the promise of imitation learning, it often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. This paper introduces Cago (Capability-Aware Goal Sampling), a novel learning-from-demonstrations method that dynamically tracks the agent’s competence and selects intermediate steps to guide learning, resulting in an adaptive curriculum that enables steady progress toward solving the full task.",28.47,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08732v1_ISLA A U-Net for MRI-based acute ischemic stroke l.pdf,"ISLA: A U-Net for MRI-based acute ischemic stroke lesion segmentation with deep supervision, attention, domain adaptation, and ensemble learning","Vincent Rocaa, Martin Bretzner, Hilde Henon, Laurent Puy, Grégory Kuchcinski, Renaud Lopes",Not found,Not found,"stroke, ischemic stroke, MRI, lesion segmentation, deep learning, U-Net, deep supervision, attention, domain adaptation, ensemble learning","Accurate delineation of acute ischemic stroke lesions in MRI is critical for clinical decision-making. ISLA is a new deep learning model for AIS lesion segmentation from diffusion MRI, trained on three multicenter databases totaling more than 1500 AIS participants. Through systematic optimization of the loss function, convolutional architecture, deep supervision, and attention mechanisms, ISLA developed a robust segmentation framework and outperformed two state-of-the-art approaches on an external test set.",27.95,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08734v1_TerraFormer Automated Infrastructure-as-Code with .pdf,TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback,"Prithwish Jana∗, Sam Davidson, Bhavana Bhasker, Andrey Kan, Anoop Deoras, Laurent Callot",10.1145/3786583.3786898,,"Infrastructure as Code (IaC), IaC generation, IaC mutation, Neuro-symbolic AI, Large language models, Formal Verification","Automating Infrastructure-as-Code (IaC) is challenging, and large language models often produce incorrect configurations from natural language. We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. TerraFormer improves correctness over its base LLMs by 15.94% on IaC-Eval, 11.65% on TF-Gen(Test), and 19.60% on TF-Mutn(Test). It outperforms larger models on both TF-Gen(Test) and TF-Mutn(Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.",28.23,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08743v1_TableCache Primary Foreign Key Guided KV Cache Pre.pdf,TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency Text-to-SQL,"Jinbo Su, Yuxuan Hu, Cuiping Li, Hong Chen, Jia Li, Lintao Ma, Jing Zhang*",Not provided,Not provided,"Text-to-SQL, KV cache, Low latency, Database, Natural Language Processing",This paper proposes a method to precompute table representations as KV caches offline and query the required ones online to address the inefficiency of redundant prefix cache copies generated by current inference engines. The approach preserves primary foreign key relationships and uses a Table Trie structure for efficient KV cache lookups during inference. Experimental results show that the proposed TableCache achieves up to a 3.62× speedup in Time to First Token (TTFT) with negligible performance degradation.,27.98,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08747v2_To Retrieve or To Think An Agentic Approach for Co.pdf,T o Retrieve or T o Think? An Agentic Approach for Context Evolution,"Rubing Chen, Jian Wang, Wenjie Li, Xiao-Yong Wei, Qing Li",,,"context evolution, agentic approach, retrieval-augmented generation, human metacognition","Current context augmentation methods, such as retrieval-augmented generation, are essential for knowledge-intensive reasoning tasks. However, they typically adhere to a rigid, brute-force strategy that executes retrieval at every step, leading to unnecessary computational costs and performance degradation. To address these limitations, Agentic Context Evolution (ACE) is introduced, a framework inspired by human metacognition that dynamically determines whether to seek new evidence or reason with existing knowledge. ACE employs a central orchestrator agent to make decisions strategically via majority voting, alternating between activating a retriever agent for external retrieval and a reasoner agent for internal analysis and refinement. By eliminating redundant retrieval steps, ACE maintains a concise and evolved context. Extensive experiments on challenging multi-hop QA benchmarks demonstrate that ACE significantly outperforms competitive baselines in accuracy while achieving efficient token consumption.",28.39,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08753v1_Grid-Aware Charging and Operational Optimization f.pdf,Grid-Aware Charging and Operational Optimization for Mixed-Fleet Public Transit,"Rishav Sen, Amutheezan Sivagnanam, Aron Laszka, Ayan Mukhopadhyay, Abhishek Dubey",,,"Mixed transit fleet, electricity pricing, dynamic pricing, public transit, sustainable transportation, mixed-integer linear programming (MILP)","This paper presents a comprehensive mixed-integer linear programming (MILP) model to address the challenges of managing mixed fleets consisting of both electric and diesel buses, including dynamic electricity pricing, vehicle capacity, and route constraints. The model optimizes charging schedules and trip assignments while considering secondary considerations such as seating constraints. Real-world data from Chattanooga, Tennessee, USA, is used to demonstrate the approach's potential for significant savings in operating costs.",27.76,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08768v1_AI as Entertainment.pdf,AI as Entertainment,"Cody Kommers∗, Ari Holtzman",XXXXXXX.XXXXXXX,,"Generative AI, Entertainment, Culture, LLMs, Societal Impact, Meaning-making","Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor. However, this mainstream narrative about AI is in tension with another emerging use case: entertainment. The authors argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society.",26.79,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08773v1_Reliable Graph-RAG for Codebases AST-Derived Graph.pdf,Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs,Manideep Reddy Chinthareddy,Not found,2601.08773,"Retrieval-Augmented Generation, Software Engineering, Graphs, Knowledge Graphs, AST, Tree-sitter, Codebases","This paper benchmarks three retrieval pipelines on Java codebases, comparing vector-only, LLM-generated knowledge graphs, and deterministic AST-derived knowledge graphs in terms of indexing overhead, query-time latency, corpus coverage, and end-to-end cost.",27.6,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08776v1_Translating Light-Sheet Microscopy Images to Virtu.pdf,Translating Light-Sheet Microscopy Images to Virtual H&E Using CycleGAN,Yanhua Zhao,Not found,Not found,"Histopathology, image translation, H&E staining, unpaired learning, CycleGAN","Histopathology analysis relies on Hematoxylin and Eosin (H&E) staining, but fluorescence microscopy offers complementary information. Converting fluorescence images to H&E-like appearance can aid interpretation and integration with standard workflows. This paper presents a Cycle-Consistent Adversarial Network (CycleGAN) approach for unpaired image-to-image translation from multi-channel fluorescence microscopy to pseudo H&E stained histopathology images.",27.17,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08777v1_Asymptotic Universal Alignment A New Alignment Fra.pdf,Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling,"Yang Cai*, Weiqiang Zheng*",Not found,2601.08777v1,"alignment, universal alignment, test-time scaling, robust alignment","This paper formalizes an ideal notion of universal alignment through test-time scaling and introduces (k, f(k))-robust alignment and asymptotic universal alignment (U-alignment). The main result characterizes the optimal convergence rate for achieving U-alignment.",28.0,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08778v3_Pervasive Annotation Errors Break Text-to-SQL Benc.pdf,Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards,"Tengjun Jin, Yoojin Choi, Yuxuan Zhu, Daniel Kang",Not provided,Not provided,"Text-to-SQL, Benchmarking, Annotation Errors, Leaderboards","Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of data-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. However, these benchmarks heavily rely on human annotations during question construction and answer evaluation, leading to annotation errors. This paper conducts an empirical study to benchmark annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and corrects a subset of the BIRD development set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings.",27.68,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08785v1_Uncovering Political Bias in Large Language Models.pdf,Uncovering Political Bias in Large Language Models using Parliamentary Voting Records,"Jieying Chen, Karen de Jong, Andreas Poole, Jan Burakowski, Elena Elderson Nosti, Joep Windt, Chendi Wang","XXX '26, XX, XX",XXXXXXX.XXXXXXX,"Political bias, Large language models, Ideological alignment, Multilingual NLP, Benchmarking, Bias evaluation, Parliamentary motions, LLM fairness","This paper introduces a methodology for constructing political-bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. It applies this methodology in three national case studies and assesses ideological tendencies and political entity bias in LLM behavior. The findings highlight the value of transparent, cross-national evaluation grounded in real parliamentary behavior for understanding and auditing political bias in modern LLMs.",27.63,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08806v1_APEX-SWE.pdf,APEX–SWE: A Benchmark for Assessing Frontier AI Models in Software Engineering,"Abhi Kottamasu1∧, Akul Datta1∧, Aakash Barthwal1, Ajay Arun1, Chirag Mahapatra1, Adarsh Hiremath1, Brendan Foody1, Bertie Vidgen1∗",Not found,2601.08806,"AI Productivity Index, Software Engineering, Frontier AI Models, Integration Tasks, Observability Tasks, Epistemic Reasoning, Uncertainty Resolution","This paper introduces APEX–SWE, a benchmark for evaluating frontier AI models in software engineering. Unlike traditional benchmarks, APEX–SWE assesses two novel task types: Integration tasks and Observability tasks, which reflect real-world software engineering challenges. Eight frontier models are evaluated, and Gemini 3 Pro achieves the best performance with a Pass@1 score of 25%. The analysis shows that strong performance is driven by epistemic reasoning and the ability to resolve uncertainty.",28.21,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08807v1_S3-CLIP Video Super Resolution for Person-ReID.pdf,S3-CLIP: Video Super Resolution for Person-ReID,"Tam´as Endrei, Gy¨orgy Cserey",Not found,Not found,"person re-identification, video super-resolution, CLIP, DINO, WACV 2026","This paper introduces S3-CLIP, a video super-resolution-based CLIP-ReID framework for the VReID-XFD challenge at WACV 2026. It integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios.",26.8,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08808v1_Multiplex Thinking Reasoning via Token-wise Branch.pdf,Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge,"Yao Tang, Li Dong, Yaru Hao, Qingxiu Dong, Furu Wei, Jiatao Gu",Not found,2601.08808,"Large Language Models, Reasoning, Token-wise Branch-and-Merge, Continuous Tokens, Reinforcement Learning","Proposes Multiplex Thinking, a stochastic soft reasoning mechanism that samples candidate tokens and aggregates their embeddings into a single continuous multiplex token, optimizing with on-policy reinforcement learning and producing shorter sequences than discrete CoT and RL baselines on challenging math reasoning benchmarks.",26.82,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08811v1_Reasoning Matters for 3D Visual Grounding.pdf,Reasoning Matters for 3D Visual Grounding,"Hsiang-Wei Huang, Kuang-Ming Chen, Wenhao Chai, Cheng-Yen Yang, Jen-Hao Cheng, Jenq-Neng Hwang",,,"3D visual grounding, Large Language Models, Reasoning, Synthetic Data, LLM fine-tuning","The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. However, 3D visual grounding remains challenging due to the limited reasoning ability of recent 3D visual grounding models. This work proposes a 3D visual grounding data pipeline capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning processes, and introduces Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based methods using only 1.6% of their training data.",28.1,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08816v2_MemRec Collaborative Memory-Augmented Agentic Reco.pdf,MemRec: Collaborative Memory-Augmented Agentic Recommender System,"Weixin Chen, Yuhan Zhao, Jingyuan Huang, Zihe Ye, Clark Mingxuan Ju, Tong Zhao, Neil Shah",,,"Recommender Systems, Semantic Memory, Collaborative Filtering, Large Language Models, Memory Augmentation, Efficient Memory Management","MemRec is a framework that decouples reasoning from memory management to enable efficient collaborative augmentation. It introduces a dedicated, cost-effective LMMem to manage a dynamic collaborative memory graph, serving synthesized, high-signal context to a downstream LLMRec. Extensive experiments on four benchmarks demonstrate state-of-the-art performance.",26.97,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08828v1_Motion Attribution for Video Generation.pdf,Motion Attribution for Video Generation,"Xindi Wu, Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-Taixé, Olga Russakovsky, Sanja Fidler, Jonathan Lorraine",Not found,Not found,"video generation, motion attribution, gradient-based, data curation, temporal dynamics, physical plausibility","Presenting Motive, a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, enabling efficient and scalable motion-specific influence computation. It identifies clips that strongly affect motion and guides data curation for improved temporal consistency and physical plausibility. Motive improves motion smoothness and dynamic degree on VBench compared to a pretrained base model.",27.63,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08829v1_Modeling LLM Agent Reviewer Dynamics in Elo-Ranked.pdf,Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System,"Hsiang-Wei Huang*, Junbin Lu*",,,"Large Language Model, Peer Review, Elo Rating, Simulation","This work explores the dynamics of Large Language Model (LLM) agent reviewers in an Elo-ranked review system using real-world conference paper submissions. It compares a baseline setting with conditions that incorporate Elo ratings and reviewer memory, showcasing improvements in Area Chair decision accuracy and adaptive review strategies. The code is available at https://github.com/hsiangwei0903/EloReview.",26.61,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08873v1_ForensicFormer Hierarchical Multi-Scale Reasoning .pdf,ForensicFormer: Hierarchical Multi-Scale Reasoning for Cross-Domain Image Forgery Detection,"Hema Hariharan, Samson",Not provided,Not provided,"Image forensics, forgery detection, transformers, cross-domain generalization, AI-generated images, hierarchical reasoning","Presenting ForensicFormer, a hierarchical multi-scale framework that unifies low-level artifact detection, mid-level boundary analysis, and high-level semantic reasoning via cross-attention transformers. Unlike prior single-paradigm approaches, our method maintains 86.8% average accuracy across seven diverse test sets, significantly improving over state-of-the-art universal detectors.",27.25,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08874v1_The Illusion of Friendship Why Generative AI Deman.pdf,The Illusion of Friendship: Why Generative AI Demands Unprecedented Ethical Vigilance,Md Zahidul Islam,,,"Generative AI, Ethics, Friendship, Emotional Attachment, Transformer Models","This paper discusses the ethical risks associated with the 'illusion of friendship' created by Generative AI systems, such as ChatGPT, which can lead to emotional attachments and potential harmful consequences. It explores the philosophical and ethical arguments for why these systems lack moral agency and proposes safeguards for their responsible use.",27.11,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08875v2_Learning Domain-Invariant Representations for Cros.pdf,Learning Domain-Invariant Representations for Cross-Domain Image Registration via Scene-Appearance Disentanglement,"Jiahao Qin∗†, Yiwen Wang∗",Not found,Not found,"Image Registration, Domain Shift, Scene Appearance, Disentanglement, Cross-Domain","This paper proposes SAR-Net, a unified framework for addressing image registration challenges under domain shift. The key insight is that observed images can be decomposed into domain-invariant scene representations and domain-specific appearance codes, enabling registration via re-rendering rather than direct intensity matching. The method achieves state-of-the-art performance on the ANHIR challenge benchmark, outperforming the MEVIS method by 7.4% in terms of median relative Target Registration Error (rTRE).",27.67,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08881v1_TAG-MoE Task-Aware Gating for Unified Generative M.pdf,TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts,"Yu Xu1,2†, Hongbin Yan1, Juan Cao1, Yiji Cheng2, Tiankai Hang2, Runze He2, Zijin Yin2, Shiyi Zhang2, Yuxin Zhang1, Jintao Li1, Chunyu Wang2‡, Qinglin Lu2, Tong-Yee Lee3, Fan Tang1§",https://doi.org/10.1101/2601.08881v1,2601.08881,"Unified image generation, Generative Mixture-of-Experts, Task-aware gating, Sparse Mixture-of-Experts, Diffusion transformers, Task interference, Semantic intent","This paper proposes a novel framework to inject semantic intent into the gating network of the Mixture-of-Experts (MoE) paradigm, enabling a unified diffusion transformer model to handle diverse generative tasks without task interference.",28.41,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08882v1_Compressing Vision Transformers in Geospatial Tran.pdf,Compressing Vision Transformers in Geospatial,"Thomas Snyder, H. Lexie Yang, Stefan Schnake, Steffen Schotthöfer",Not found,2601.08882,"geospatial, vision transformers, transfer learning, compression, manifold-constrained optimization","Deploying geospatial foundation models on resource-constrained edge devices demands compact architectures that maintain high downstream performance. This work leverages manifold-constrained optimization framework DLRT to compress large vision transformer-based geospatial foundation models during transfer learning, achieving strong compression while preserving task-specific accuracy.",27.29,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08884v1_Bridging the Gap Empowering Small Models in Reliab.pdf,Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting,"Samyak Jhaveri, Cristina V. Lopes",Not found,Not found,"OpenACC, Parallel Programming, Large Language Models, Prompt Optimization, GPU Offloading, High-Performance Computing","This work presents a systematic prompt optimization approach to enhance OpenACC pragma generation without prohibitive computational costs associated with LLM post-training. It uses the GEPA framework to iteratively evolve prompts through a reflective feedback loop, guided by expertly curated 'gold' pragma examples and structured feedback based on clause and parameter-level mismatches. The evaluation on the PolyBench suite shows significant improvements in compilation success rates and functional GPU speedups for smaller, cheaper models.",27.02,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08891v1_Attention Consistency Regularization for Interpret.pdf,Attention Consistency Regularization for Interpretable Early-Exit Neural Networks,"Yanhua Zhao, KIS*MED (AI Systems in Medicine), Technische Universitat Darmstadt",,,"Early exit networks, explainable AI, attention mechanisms, multi-objective learning","This paper presents Explanation-Guided Training (EGT), a multi-objective framework that improves interpretability and consistency in early-exit neural networks through attention-based regularization. EGT introduces an attention consistency loss that aligns early-exit attention maps with the final exit, and jointly optimizes classification accuracy and attention consistency through a weighted combination of losses. Experiments on a real-world image classification dataset demonstrate that EGT achieves up to 98.97% overall accuracy with a 1.97× inference speedup through early exits, while improving attention consistency by up to 18.5% compared to baseline models.",27.87,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08892v1_Evaluating Role-Consistency in LLMs for Counselor .pdf,Evaluating Role-Consistency in LLMs for Counselor Training,"Eric Rudolph, Natalie Engert, Jens Albrecht",Not found,2601.08892,"Counseling, Chatbot, LargeLanguageModel, PersonaConsistency, EducationalRole-Play","This paper extends research on VirCo, a VirtualClientforOnlineCounseling, to test the role-consistency of large language models (LLMs) in virtual client interactions, comparing findings with earlier research and assessing various open-source LLMs.",27.83,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08896v1_XGBoost Forecasting of NEPSE Index Log Returns wit.pdf,XGBoost Forecasting of NEPSE Index Log Returns with Walk Forward Validation,"Sahaj Raj Mallaa, Shreeyash Kayastha, Rumi Suwala, Harish Chandra Bhandara, Rajendra Adhikari",Not found,Not found,"NEPSE Index, stock index forecasting, XGBoost, walk-forward validation, hyperparameter optimization, time series forecasting, emerging markets, feature engineering","This study develops a robust machine learning framework for one-step-ahead forecasting of daily log-returns in the Nepal Stock Exchange (NEPSE) Index using the XGBoost regressor. A comprehensive feature set is engineered, including lagged log-returns (up to 30 days) and established technical indicators such as short- and medium-term rolling volatility measures and the 14-period Relative Strength Index. Hyperparameter optimization is performed using Optuna with time-series cross-validation on the initial training segment. Out-of-sample performance is rigorously assessed via walk-forward validation under both expanding and fixed-length rolling windows schemes across multiple lag configurations, simulating real-world deployment and avoiding lookahead bias. Predictive accuracy is evaluated using root mean squared error, mean absolute error, coefficient of determination (R²), and directional accuracy on both log-returns and reconstructed closing prices. Empirical results show that the optimal configuration—an expanding window with 20 lags—outperforms tuned ARIMA and Ridge regression benchmarks, achieving the lowest log-return RMSE (0.013450) and MAE (0.009814) alongside a directional accuracy of 65.15%. While the R² remains modest, consistent with the noisy nature of financial returns, primary emphasis is placed on relative error reduction and directional prediction. Feature importance analysis and visual inspection further enhance interpretability. These findings demonstrate the effectiveness of gradient boosting ensembles in modeling nonlinear dynamics in volatile emerging market time series and establish a reproducible benchmark for NEPSE Index forecasting.",29.09,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08901v1_Navigating Ideation Space Decomposed Conceptual Re.pdf,Navigating Ideation Space: Decomposed Conceptual Representations for Scientific Discovery,"Yuexi Shen, Minqian Liu, Dawei Zhou, Lifu Huang",,not found,"Scientific Discovery, Ideation Space, Conceptual Representations, Literature Retrieval, Novelty Assessment","Scientific discovery is a cumulative process requiring new ideas to be situated within an expanding landscape of existing knowledge. The challenge is to identify conceptually relevant prior work and assess how a new idea differentiates from existing research. Current embedding approaches typically conflate distinct conceptual aspects into single representations, hindering fine-grained literature retrieval. This paper introduces the Ideation Space, a structured representation that decomposes scientific knowledge into three dimensions: research problem, methodology, and core findings, learned through contrastive training. This framework enables principled measurement of conceptual distance and modeling of ideation transitions. A Hierarchical Sub-Space Retrieval framework and a Decomposed Novelty Assessment algorithm are proposed for efficient, targeted literature retrieval and identifying novel aspects of ideas, respectively. Extensive experiments demonstrate substantial improvements in recall, hit rate, and correlation with expert judgments.",28.66,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08910v1_Towards a Self-Driving Trigger at the LHC Adaptive.pdf,Towards a Self-Driving Trigger at the LHC: Adaptive Response in Real Time,"Shaghayegh Emami, Cecilia Tosciri, Giovanna Salvi, Zixin Ding, Yuxin Chen, Abhijith Gandrakota, Christian Herwig, David W. Miller, Jennifer Ngadiuba, Nhan Tran",,2601.08910v1,"self-driving trigger, adaptive response, real-time data filtering, high-throughput scientific facilities, Large Hadron Collider (LHC), machine learning, anomaly detection, energy sum triggers, cost optimization","This work explores the concept of a self-driving trigger, an autonomous data-filtering framework that reallocates resources and adjusts thresholds dynamically in real-time to optimize signal efficiency, rate stability, and computational cost in high-throughput scientific facilities such as the experiments at the Large Hadron Collider (LHC).",29.71,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08950v1_ConvoLearn A Dataset of Constructivist Tutor-Stude.pdf,ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue,"Mayank Sharma, Roy Pea, Hari Subramonyam",,1https://huggingface.co/datasets/masharma/convolearn,"AI tutoring, constructivism, pedagogy, LLMs, education","This paper introduces ConvoLearn, a dataset of tutor-student dialogues grounded in knowledge-building theory, designed to shift LLM behavior towards knowledge-building strategies. The dataset consists of 1,250 tutor-student dialogues (20 turns each) in middle school Earth Science, constructed through controlled interactions between human teachers and a simulated student. The findings demonstrate that training on ConvoLearn meaningfully shifts LLM behavior, with fine-tuned models outperforming base versions in human evaluations.",27.2,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08951v1_PluriHarms Benchmarking the Full Spectrum of Human.pdf,PLURIHARMS: BENCHMARKING THE FULL SPECTRUM OF HUMAN JUDGMENTS ON AI HARM,"Jing-Jing Li, Joel Mire, Eve Fleisig, Valentina Pyatkin, Anne G. E. Collins, Maarten Sap, Sydney Levine",,,"AI safety, human judgments, harm, plurality, benchmark","This paper introduces PLURIHARMS, a benchmark designed to systematically study human harm judgments across two key dimensions: the harm axis (benign to harmful) and the agreement axis (agreement to disagreement). The benchmark includes 150 prompts with 15,000 ratings from 100 human annotators, and analyses show that prompts related to imminent risks and tangible harms amplify perceived harmfulness, while annotator traits and their interactions with prompt content explain systematic disagreement. The work provides a principled benchmark for moving beyond ",28.13,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08953v1_Fairness risk and its privacy-enabled solution in .pdf,Fairness risk and its privacy-enabled solution in AI-driven robotic applications,"Le Liu, Bangguo Yu, Nynke Vellinga, Ming Cao",Not found,2601.08953,"Robotic Decision-making, Large Language Model, Fairness, Privacy","Complex decision-making by autonomous machines and algorithms could underpin the foundations of future society. Generative AI is emerging as a powerful engine for such transitions. However, Generative AI-driven developments pose a critical pitfall: fairness concerns. In robotic applications, although intuitions about fairness are common, a precise and implementable definition that captures user utility and inherent data randomness is missing. This paper provides a utility-aware fairness metric for robotic decision-making and analyzes fairness jointly with user-data privacy, deriving conditions under which privacy budgets govern fairness metrics.",29.21,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08955v1_Imagine-then-Plan Agent Learning from Adaptive Loo.pdf,Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models,"Youwei Liu, Jian Wang, Hanlin Wang, Beichen Guo, Wenjie Li",Not provided,Not provided,"Agent learning, World models, Adaptive lookahead, Multi-step planning, Reinforcement learning","This paper proposes Imagine-then-Plan (ITP), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with a learned world model to generate multi-step 'imagined' trajectories. The framework addresses the limitation of current methods by introducing an adaptive lookahead mechanism that balances the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, which are fused with current observations to form a partially observable and imaginable Markov decision process for policy learning. Extensive experiments demonstrate that ITP significantly outperforms competitive baselines.",27.53,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.08988v1_ART Action-based Reasoning Task Benchmarking for M.pdf,AAAI 2026: Healthy Aging and Longevity Workshop (AIAA) - Action-based Reasoning Task Benchmarking for Medical AI Agents,"Ananya Mantravadi, Shivali Dalmia, Abhishek Mukherji",Not provided,Not provided,"Medical AI agents, synthetic data generation, clinical reasoning evaluation, healthcare LLMs, benchmark, HITL","This paper introduces ART, an Action-based Reasoning clinical Task benchmark for medical AI agents, which mines real-world EHR data to create challenging tasks targeting known reasoning weaknesses in clinical decision support.",27.02,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09012v3_TranslateGemma Technical Report.pdf,"We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models",Google Translate Research Team,,,"machine translation, open models, supervised fine-tuning, reinforcement learning, Gemma 3, synthetic data, human-translated data, image translation","The paper introduces TranslateGemma, an open machine translation model suite based on the Gemma 3 foundation models, which enhances translation quality through a two-stage process of supervised fine-tuning and reinforcement learning. The model achieves significant gains in automatic metrics and retains strong multimodal capabilities.",26.91,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09018v1_Meta-learning to Address Data Shift in Time Series.pdf,META-LEARNING TOADDRESSDATASHIFT IN TIMESERIES CLASSIFICATION,"Samuel Myrenab, Nidhi Parikha, Natalie Kleina",Not found,2601.09018,"signals, seismology, Reptile, FOMAML, model-agnostic meta-learning, domain generalization","This work systematically compares traditional deep learning (TDL) with fine-tuning and optimization-based meta-learning algorithms in addressing data shift in time-series classification. It introduces a controlled seismic benchmark (SeisTask) and shows that meta-learning typically achieves faster and more stable adaptation with reduced overfitting in data-scarce regimes and smaller model architectures. Task diversity influences meta-learning performance, but alignment between training and test distributions drives performance gains. The study contributes SeisTask as a benchmark for advancing adaptive learning research in time-series domains.",27.87,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09028v1_OpenDecoder Open Large Language Model Decoding to .pdf,OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG,"Fengran Mo, Zhan Su, Yuchen Hui, Jianhan Zhang, Jia Ao Sun, Zheyuan Liu, Chao Zhang, Tetsuya Sakai, Jian-Yun Nie",10.1145/nnnnnnn.nnnnnnn,,"Information Retrieval, Retrieval-Augmented Generation, Robust Question Answer, Decoding Paradigm, Large Language Model","The paper proposes OpenDecoder, a new approach that leverages explicit evaluation of retrieved information as quality indicators for generation, aiming to build a more robust RAG model that is less sensitive to varying levels of noisy context.",27.2,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09029v1_Proactively Detecting Threats A Novel Approach Usi.pdf,Proactively Detecting Threats: A Novel Approach,"Aniesh Chawla ∗, Udbhav Prasad ∗",,,"Malware, Indicators of Compromise, Cybersecurity, LLMs, GenAI, Machine Learning Algorithms, Deep Neural Network","Enterprise security faces escalating threats from sophisticated malware, compounded by expanding digital operations. This paper presents the first systematic evaluation of large language models (LLMs) to proactively identify indicators of compromise (IOCs) from unstructured web-based threat intelligence sources, distinguishing it from reactive malware detection approaches. The evaluation reveals significant performance variations among six LLM models, with Gemini 1.5 Pro achieving 0.958 precision and 0.788 specificity for malicious IOC identification, while demonstrating perfect recall (1.0) for actual threats.",28.01,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09031v1_Generalizable Geometric Prior and Recurrent Spikin.pdf,GENERALIZABLE GEOMETRIC PRIOR AND RECURRENT SPIKING FEATURE LEARNING FOR HUMANOID ROBOT MANIPULATION,"Xuetao Li, Wenke Huang, Mang Ye, Jifeng Xuan, Bo Du, Sheng Liu, Miao Li",Not found,Not found,"Humanoid Robot Manipulation, Geometric Prior, Recurrent Spiking Feature Learning","This paper presents a novel RGMP-S, Recurrent Geometric-prior Multimodal Policy with Spiking features, facilitating both high-level skill reasoning and data-efficient motion synthesis. It addresses challenges in precise scene understanding and sample-efficient learning from human demonstrations, enabling robust generalization in unseen environments.",27.37,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09032v1_The Hierarchy of Agentic Capabilities Evaluating F.pdf,The Hierarchy of Agentic Capabilities: Evaluating Frontier Models on Realistic RL Environments,"Logan Ritchie∗, Sushant Mehta, Nick Heiner, Mason Yu, Edwin Chen, Surge AI",Not found,2601.09032,"large language models, agent capabilities, realistic RL environments, workplace tasks","This study evaluates frontier AI models on 150 workplace tasks within a realistic e-commerce RL environment, revealing a hierarchy of agentic capabilities that models must master for real-world deployment.",27.4,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09035v1_A Decompilation-Driven Framework for Malware Detec.pdf,A Decompilation-Driven Framework for Malware Detection with Large Language Models,"Aniesh Chawla, Udbhav Prasad",Not found,2601.09035,"Malware, Ghidra, Cybersecurity, LLMs, GenAI, Machine Learning Algorithms, LLMs Code development","This paper evaluates the efficacy of state-of-the-art Large Language Models (LLMs) in classifying executable code as either benign or malicious. It introduces an automated pipeline that first decompiles Windows executable into C code using Ghidra disassembler and then leverages LLMs for classification. The evaluation reveals that while standard LLMs show promise, they are not yet robust enough to replace traditional anti-virus software. A fine-tuned model trained on curated malware and benign datasets significantly outperforms its vanilla counterpart, but performance degrades notably with newer malware. This finding highlights the need for continuous fine-tuning with emerging threats to maintain model effectiveness.",28.47,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09041v1_Can LLMs interpret figurative language as humans d.pdf,CANLLMSINTERPRETFIGURATIVELANGUAGE ASHUMANSDO?: SURFACE-LEVEL VS.,"Samhita Bollepally, Aurora Sloman-Moll, Takashi Yamauchi",,,"Large language models, Human interpretation, Figurative language, Sarcasm, Emotion, Idiomacy, Slang","This study investigates how large language models (LLMs) interpret figurative and socially grounded language compared to humans. Participants rated 240 dialogue-based sentences representing six linguistic traits, and both humans and LLMs rated these sentences on a 10-point Likert scale. Results show that humans and LLMs align at the surface level but diverge significantly at the representational level, especially in interpreting idioms and slang.",27.52,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09049v1_Is Grokking Worthwhile Functional Analysis and Tra.pdf,Is Grokking Worthwhile? Functional Analysis and Transferability of Generalization Circuits in Transformers,"Kaiyu He, Mian Zhang, Peilin Wu, Xinya Du, Zhiyu Zoey Chen",,,"Transformers, grokking, generalization circuit, two-hop reasoning, knowledge assimilation, transferability","This work evaluates the role of the Generalization Circuit in knowledge assimilation and transfer, demonstrating that non-grokked and grokked models establish identical inference paths for in-distribution compositional queries. It argues that grokking is the process of integrating memorized atomic facts into an established reasoning path, and that achieving high accuracy on unseen cases after prolonged training and forming a reasoning path are independent of each other under specific data regimes. It also suggests that mature circuits exhibit limited transferability when integrating new knowledge, indicating that grokked Transformers do not achieve full mastery of compositional logic.",27.48,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09066v1_Midm 2.0 Korea-centric Bilingual Language Models.pdf,Mi:dm 2.0: Korea-Centric Bilingual Language Models,"Tech. Innovation Group, KT",Not found,2601.09066v1,"Bilingual language model, Korean-centric AI, KT, Language processing, Cultural alignment, Data quality, LLM","This paper introduces Mi:dm 2.0, a bilingual large language model (LLM) specifically engineered to advance Korea-centric AI. It addresses limitations of existing LLMs by integrating values, reasoning patterns, and commonsense knowledge inherent to Korean society, enabling nuanced understanding of cultural contexts, emotional subtleties, and real-world scenarios.",27.49,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09069v1_From Symbolic to Natural-Language Relations Rethin.pdf,From Symbolic to Natural-Language Relations: Rethinking Knowledge Graph Construction in the Era of Large Language Models,"Kanyao Han, Yushang Lai",,,"Knowledge Graphs, Natural Language, Large Language Models, Relation Extraction, Semantic Representation","This paper argues for a shift from symbolic to natural-language relation descriptions in knowledge graph construction, advocating for hybrid design principles that preserve a minimal structural backbone while enabling more flexible and context-sensitive relational representations.",26.04,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09072v1_Human-AI Co-design for Clinical Prediction Models.pdf,Human-AI Co-design for Clinical Prediction Models,"Jean Feng1,*,†, Avni Kothari 1,*, Patrick Vossler 1, Andrew Bishara 1, Lucas Zier 1, Newton Addo 1, Aaron Kornblith 1, Yan Shuo Tan 2, Chandan Singh 3, Equal contribution, Corresponding author: jean.feng@ucsf.edu",Not found,2601.09072,"Large language models, Electronic health records, Concept Bottleneck, Human-AI Interaction","Introducing HACHI, an iterative human-in-the-loop framework that uses AI agents to accelerate the development of fully interpretable clinical prediction models (CPMs), enabling exploration of concepts in clinical notes. HACHI outperforms existing approaches in real-world prediction tasks and reveals the critical role of the clinical AI team.",28.34,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09085v1_MMR-GRPO Accelerating GRPO-Style Training through .pdf,MMR-GRPO: Accelerating GRPO-Style Training through Diversity-Aware Reward Reweighting,"Kangda Wei, Ruihong Huang",Not provided,Not provided,"Group Relative Policy Optimization, Mathematical Reasoning, Reinforcement Learning, Reward Reweighting, Diversity","This paper proposes MMR-GRPO, a method that integrates Maximal Marginal Relevance to reweight rewards based on completion diversity, aiming to accelerate training for mathematical reasoning models. It shows that MMR-GRPO achieves comparable peak performance while requiring on average 47.9% fewer training steps and 70.2% less wall-clock time compared to vanilla R1-Zero/GRPO-Style training.",26.84,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09089v1_SubTokenTest A Practical Benchmark for Real-World .pdf,SUBTOKENTEST: A Practical Benchmark for Real-World Sub-token Understanding,"Shuyang Hou∗, Yi Hu∗, Muhan Zhang†",,2601.09089,"Sub-token understanding, Large language models, Tokenization, Character-level tasks, Practical relevance","SUBTOKENTEST introduces a comprehensive benchmark to assess sub-token understanding through practical, utility-driven tasks. It includes ten tasks across four domains and isolates tokenization-related failures by decoupling performance from complex reasoning. The benchmark evaluates nine advanced LLMs and investigates the impact of test-time scaling on sub-token reasoning.",26.84,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09097v1_Programming over Thinking Efficient and Robust Mul.pdf,Programming over Thinking: Efficient and Robust Multi-Constraint Planning,"Derrick Goh Xin Deik1, Quanyu Long1, Zhengyuan Liu2, Nancy F. Chen2, Wenya Wang1",Not found,Not found,"Planning, Multi-constraint, Efficiency, Robustness, Large Language Models, Code-based Planning, Solver Integration","This paper introduces the ScalableCOdePlanning Engine (SCOPE), a framework that separates query-specific reasoning from generic code execution. SCOPE produces solver functions that are consistent, deterministic, and reusable across queries, achieving state-of-the-art performance while lowering cost and latency. It addresses the limitations of existing large language model approaches in multi-constraint planning, which face fundamental challenges in consistency, error accumulation, and scalability.",27.47,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09100v2_DScheLLM Enabling Dynamic Scheduling through a Fin.pdf,DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large Language Model,"Lixiang Zhang, Chenggong Zhao, Qing Gao, Xiaoke Zhao, Gengyi Bai, Jinhu Lv",Not provided,Not provided,"dynamic scheduling, large language model, fine-tuning, job shop scheduling","This paper proposes DScheLLM, a dynamic scheduling approach that leverages fine-tuned large language models within a dual-system (fast–slow) reasoning architecture to address disturbances of different scales. It constructs a unified large language model-based framework to handle dynamic events, using exact schedules from an operations research solver for training datasets. The Huawei OpenPangu Embedded-7B model is fine-tuned under hybrid reasoning paradigms using LoRA. Experimental evaluations on standard job shop scheduling benchmarks show that the fast-thinking mode can efficiently generate high-quality schedules, while the slow-thinking mode produces solver-compatible and well-formatted decision inputs.",28.49,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09105v2_AviationLMM A Large Multimodal Foundation Model fo.pdf,AviationLMM: A Large Multimodal Foundation Model for Civil Aviation,"Wenbin Li, Jingling Wu, Xiaoyong Lin, Jing Chen, Cong Chen",,,"civil aviation, multi-modal model, foundation model, cloud edge collaboration, hybrid training, computer systems organization, computing methodologies","This paper introduces AviationLMM, a large multimodal foundation model for civil aviation designed to unify heterogeneous data streams and enable understanding, reasoning, generation, and agentic applications. It addresses gaps between existing AI solutions and requirements, including data acquisition, alignment and fusion, pretraining, reasoning, trustworthiness, privacy, robustness to missing modalities, and synthetic scenario generation.",27.81,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09113v1_The AI Hippocampus How Far are We From Human Memor.pdf,The AI Hippocampus: How Far are We From Human Memory?,"Zixia Jia, Jiaqi Li, Yipeng Kang, Yuxuan Wang, Tong Wu, Quansen Wang, Xiaobo Wang, Shuyi Zhang, Junzhe Shen, Qing Li, Siyuan Qi, Yitao Liang, Di He, Zilong Zheng, Song-Chun Zhu",Not found,2601.09113v1,"AI, Memory, Large Language Models, Multi-Modal Models, Hippocampus, Continual Learning, Personalized Inference","This survey presents a comprehensive and structured synthesis of memory in Large Language Models (LLMs) and Multi-Modal LLMs, organizing the literature into a cohesive taxonomy of implicit, explicit, and agentic memory paradigms.",28.68,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09116v1_LP-LLM End-to-End Real-World Degraded License Plat.pdf,LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models,"Haoyan Gong, Hongbin Liu",,,"License Plate Recognition, Text Recognition, Degraded Images, Vision-Language Models, Multimodal Reasoning, Character Slot Queries, Fine-Grained Evidence Retrieval, Residual Modulation, LoRA Fine-Tuning","Proposes an end-to-end structure-aware multimodal reasoning framework for real-world License Plate Recognition, addressing challenges from severe degradations such as motion blur and low resolution, by incorporating learnable Character Slot Queries and residual modulation techniques.",27.45,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09117v1_A Marketplace for AI-Generated Adult Content and D.pdf,A Marketplace for AI-Generated Adult Content and Deepfakes,"Shalmoli Ghosh1, Matthew R. DeVerna2, Filippo Menczer1",,,"AI-generated content, deepfakes, marketplace, synthetic media, gender asymmetry","A longitudinal analysis of all publicly available bounty requests on Civitai, a community-driven platform for AI-generated content, reveals that the marketplace is dominated by tools that steer AI models toward content they were not trained to generate. Requests for 'Not Safe For Work' content are widespread and have increased steadily over time, now comprising a majority of all bounties. Participation in bounty creation is uneven, with 20% of requesters accounting for roughly half of requests. Requests for 'deepfake' media, especially involving explicit content, exhibit a higher concentration than other types of bounties and disproportionately target female celebrities, revealing a pronounced gender asymmetry in social harm.",28.35,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09120v1_Adaptive Multi-Stage Patent Claim Generation with .pdf,Adaptive Multi-Stage Patent Claim Generation with Unified Quality Assessment,"Chen-Wei Liang, Bin Guo, Zhen-Yuan Wei, Mu-Jiang-Shan Wang",,2601.09120,"Patent claim generation, Cross-jurisdictional learning, Quality assessment, Transformer, Domain adaptation","Introduces a novel three-stage framework to address fundamental limitations in current patent claim generation systems, including cross-jurisdictional generalization, semantic relationship modeling, and quality assessment.",29.1,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09130v1_Equi-ViT Rotational Equivariant Vision Transformer.pdf,EQUI-VIT: ROTATIONAL EQUIVARIANT VISION TRANSFORMER FOR ROBUST HISTOPATHOLOGY ANALYSIS,"Fuyao Chen, Yuexi Du, Eléonore V. Lieffrig, Nicha C. Dvornek, John A. Onofrey",Not found,Not found,"Vision Transformer, Rotation Equivariance, Histopathology, Artificial Intelligence","Proposes Equi-ViT, a vision transformer architecture that integrates an equivariant convolution kernel into the patch embedding stage, achieving superior rotation-consistent patch embeddings and stable classification performance across image orientations. Demonstrates enhanced data efficiency and robustness in a public colorectal cancer dataset.",27.4,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09136v1_SkinFlow Efficient Information Transmission for Op.pdf,SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL,"Lijun Liu*, Linwei Chen *, Zhishou Zhang, Meng Tian, Hengfu Cui, Ruiyang Li, Zhaocheng Liu, Qiang Ju, Qianxi Li, Hong-Yu Zhou*",,,"SkinFlow, Reinforcement Learning, Dynamic Visual Encoding, Dermatology, Information Transmission, Large Vision-Language Models, Diffuse Attention, Pathological Lesions, Semantic Matching","This paper introduces SkinFlow, a framework for efficient information transmission in dermatological diagnosis. It utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to optimize visual information transmission without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. The authors propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching, achieving significant improvements in accuracy compared to general-purpose models.",28.15,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09147v2_SSVP Synergistic Semantic-Visual Prompting for Ind.pdf,SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection,"Chenhao Fu, Han Fang, Xiuzheng Zheng, Wenbo Wei, Yonghua Li, Hao Sun, Xuelong Li",,,"Zero-Shot Anomaly Detection, Synergistic Semantic-Visual Prompting, Vision-Language Models, Industrial Anomaly Detection, Dynamic Prompting, CLIP, Hierarchical Semantic-Visual Synergy, Vision-Conditioned Prompt Generator, Visual-Text Anomaly Mapper","This paper proposes Synergistic Semantic-Visual Prompting (SSVP) to enhance industrial zero-shot anomaly detection (ZSAD) by efficiently fusing diverse visual encodings and integrating DINOv3’s multi-scale structural priors into the CLIP semantic space. The method employs cross-modal attention to guide dynamic prompt generation, enabling linguistic queries to precisely anchor to specific anomaly patterns. Additionally, it establishes a dual-gated calibration paradigm to address the discrepancy between global scoring and local evidence, achieving state-of-the-art performance on MVTec-AD benchmarks.",28.17,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09152v1_PrivacyReasoner Can LLM Emulate a Human-like Priva.pdf,PrivacyReasoner: Can LLM Emulate a Human-like Privacy Mind?,"Yiwen Tu, Xuan Liu, Lianhui Qin, Haojian Jin",,,"Privacy, AI-agent, Privacy Reasoning, User-specific Privacy, Contextual Integrity, Cognitive Processes, Privacy Concerns, LLM","This paper introduces PrivacyReasoner, an AI-agent designed to simulate how individual users form privacy concerns in response to real-world news. Moving beyond population-level sentiment analysis, PrivacyReasoner integrates privacy and cognitive theories to model user-specific privacy reasoning grounded in personal comment histories and contextual cues. The agent reconstructs each user’s ",27.22,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09156v1_KTCF Actionable Recourse in Knowledge Tracing via .pdf,KTCF: Actionable Recourse in Knowledge Tracing via Counterfactual Explanations for Education,"Woojin Kim, Changkwon Lee, Hyeoncheol Kim",,,"Knowledge Tracing, Counterfactual Explanations, Education, AI Ethics","Proposes KTCF, a counterfactual explanation generation method for Knowledge Tracing that accounts for knowledge concept relationships and converts counterfactual explanations into educational instructions. Demonstrates superior and robust performance over existing methods, with improvements ranging from 5.7% to 34% across metrics. Provides qualitative evaluation of post-processing scheme, showing educational instructions help in reducing large study burden. Highlights the potential of counterfactuals to advance responsible and practical use of AI in education.",27.26,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09182v1_Position on LLM-Assisted Peer Review Addressing Re.pdf,Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback,"JungMin Yun*, JuneHyoung Kwon*, MiHyeon Kim*, YoungBin Kim",,,"peer review, AI, reviewer gap, mentoring, feedback, sustainability","The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. It proposes two complementary systems: an LLM-assisted mentoring system to cultivate reviewers' long-term competencies and an LLM-assisted feedback system to help reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.",27.62,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09195v1_ProFit Leveraging High-Value Signals in SFT via Pr.pdf,ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection,"Tao Liu, Taiqiang Wu, Runming Yang, Shaoning Sun, Junjie Wang, Yujiu Yang",Not found,Not found,"Supervised fine-tuning, SFT, Probability-guided token selection, High-value tokens, Single-reference overfitting, Answer diversity, Semantic importance","ProFit addresses the issue of single-reference overfitting in supervised fine-tuning by selectively masking low-probability tokens, thereby improving performance on general reasoning and mathematical benchmarks. It achieves this by leveraging the intrinsic connection between token probability and semantic importance, focusing supervision on high-value tokens to capture core semantic integrity without sacrificing efficiency.",26.98,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09208v2_Mikasa A Character-Driven Emotional AI Companion I.pdf,Mikasa: A Character-Driven Emotional AI Companion Inspired by Japanese Oshi Culture,Miki Ueno,,,"AI companion, character design, emotional AI, user engagement","Recent progress in AI companions has led to systems with fluent and emotionally expressive conversations. However, these systems often struggle with long-term user satisfaction and engagement. This paper argues that the problems are not primarily due to weak models but to poor character design and unclear definitions of the user-AI relationship. Mikasa, inspired by Japanese Oshi culture, is presented as a case study of character-driven companion design. It aims to stabilize interaction norms and reduce the work users must do to redefine the relationship.",27.58,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09212v1_Annealed Relaxation of Speculative Decoding for Fa.pdf,Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image Generation,"Xingyao Li, Fengzhuo Zhang, Cunxiao Du",Not found,Not found,"Auto-regressive models, Speculative decoding, Image generation, Annealing, Resampling","This paper proposes COOL-SD, an annealed relaxation of speculative decoding, which aims to accelerate auto-regressive image generation while maintaining comparable quality. It establishes theoretical foundations for speculative decoding and introduces COOL-SD, a method that improves inference speed and quality through an annealed design based on two key insights: analyzing the TV distance between the target model and relaxed speculative decoding and using perturbation analysis to reveal annealing behavior.",26.89,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09213v1_SpikeVAEDiff Neural Spike-based Natural Visual Sce.pdf,SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-V AE and Versatile Diffusion,"Jialu Li, Taiyan Zhou",Not found,Not found,"Neural Spikes, Image Reconstruction, Variational Autoencoder, Diffusion Models, Neuroscience, Computer Vision","This paper presents SpikeVAEDiff, a novel two-stage framework combining a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model for generating high-resolution and semantically meaningful image reconstructions from neural spike data. The method uses VDVAE to produce low-resolution preliminary reconstructions and Versatile Diffusion to refine the images. The authors explore the use of spike data on the Allen Visual Coding—Neuropixels dataset and test different brain regions, finding that the VISI region exhibits the most prominent activation. The study highlights the potential of spike data for visual neural decoding and the importance of considering different visual brain areas for improved neural decoding.",27.65,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09233v1_GIFT Unlocking Global Optimality in Post-Training .pdf,GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization,"Zhengyang Zhao, Lu Ma, Yizhen Jiang, Xiaochen Ma, Chengyu Shen, Lexiang Tang, Haoze Sun, Peng Pei, Wentao Zhang",Not found,Not found,"Large Reasoning Models, Supervised Fine-Tuning, Reinforcement Learning, Distributional Collapse, Finite-Temperature Gibbs Initialization, Global Optimality","This paper proposes a new approach called GIFT to address the optimization mismatch between supervised fine-tuning and reinforcement learning in the training of large reasoning models. GIFT incorporates supervision as a finite-temperature energy potential, ensuring objective consistency throughout the post-training pipeline and significantly outperforming standard fine-tuning methods in reinforcement learning initialization.",26.87,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09236v2_Reward Learning through Ranking Mean Squared Error.pdf,REWARDLEARNING THROUGH RANKINGMEANSQUAREDERROR,"Chaitanya Kharyal, Calarina Muslimani, Matthew E. Taylor",,,"reinforcement learning, reward learning, ranking mean squared error, human feedback, robotic locomotion","Reward design remains a significant bottleneck in applying reinforcement learning to real-world problems. A popular alternative is reward learning, where reward functions are inferred from human feedback rather than manually specified. Recent work has proposed learning reward functions from human feedback in the form of ratings, rather than traditional binary preferences, enabling richer and potentially less cognitively demanding supervision. Building on this paradigm, we introduce a new rating-based RL method, Ranked Return Regression for RL (R4). At its core, R4 employs a novel ranking mean squared error (rMSE) loss, which treats teacher-provided ratings as ordinal targets. Our approach learns from a dataset of trajectory-rating pairs, where each trajectory is labeled with a discrete rating (e.g., “bad,” “neutral,” “good”). At each training step, we sample a set of trajectories, predict their returns, and rank them using a differentiable sorting operator (soft ranks). We then optimize a mean squared error loss between the resulting soft ranks and the teacher’s ratings. Unlike prior rating-based approaches, R4 offers formal guarantees: its solution set is provably minimal and complete under mild assumptions. Empirically, using simulated human feedback, we demonstrate that R4 consistently matches or outperforms existing rating and preference-based RL methods on robotic locomotion benchmarks from OpenAI Gym and the Deep-Mind Control Suite, while requiring significantly less feedback.",28.97,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09239v2_DSA-Tokenizer Disentangled Semantic-Acoustic Token.pdf,DSA-Tokenizer: Disentangled Semantic-Acoustic Tokenization via Flow Matching-based Hierarchical Fusion,"Hanlin ZHANG, Daxin Tan, Dehua Tao, Xiao Chen, Haochen Tan, Yunhe Li, Yuchen Cao, Jianping Wang, Linqi Song",,,"Speech Tokenization, Disentanglement, Hierarchical Fusion, Flow Matching, Semantic-Acoustic Separation","DSA-Tokenizer is proposed to achieve better disentanglement in speech tokenization by explicitly separating speech into discrete semantic and acoustic tokens via distinct optimization constraints. It uses ASR for semantic tokens and mel-spectrogram restoration for acoustic tokens, and introduces a hierarchical Flow-Matching decoder to eliminate rigid length constraints between sequences. This enables high-fidelity reconstruction and flexible recombination, facilitating controllable generation in Speech Large Language Models (Speech LLMs).",27.21,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09248v1_Hybrid guided variational autoencoder for visual p.pdf,Hybrid guided variational autoencoder for visual place recognition,"Ni Wang, Zihan You, Emre Neftci, Thorben Schoepe",Not found,2601.09248,"Visual place recognition, Spiking neural network","This work presents a hybrid guided variational autoencoder (VAE) for visual place recognition (VPR) that combines event-based vision sensors with a spiking neural network model. The model successfully disentangles visual features of 16 distinct places in an indoor VPR dataset, demonstrating robust performance under various illumination conditions and generalization capabilities for novel inputs.",28.01,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09251v1_HGATSolver A Heterogeneous Graph Attention Solver .pdf,HGA TSolver: A Heterogeneous Graph Attention Solver for Fluid–Structure Interaction,"Qin-Yi Zhang, Hong Wang, Siyao Liu, Haichuan Lin, Linying Cao, Xiao-Hu Zhou, Chen Chen, Shuangyi Wang, Zeng-Guang Hou",,,"Fluid-Structure Interaction, Heterogeneous Graph Attention, Physics-Informed Neural Networks, Coupled Multi-Physics Systems","This paper proposes HGA TSolver, a novel solver for Fluid-Structure Interaction (FSI) systems. It encodes the system as a heterogeneous graph, embedding physical structure directly into the model via distinct node and edge types for fluid, solid, and interface regions. This enables specialized message-passing mechanisms tailored to each physical domain, addressing the challenges of capturing heterogeneous dynamics and stabilizing prediction during explicit time stepping. Extensive experiments demonstrate state-of-the-art performance, establishing an effective framework for surrogate modeling of coupled multi-physics systems.",27.06,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09253v1_RIFT Repurposing Negative Samples via Reward-Infor.pdf,RIFT: Repurposing Negative Samples via Reward-Informed Fine-Tuning,"Zehua Liu, Shuqi Liu†, Tao Zhong, Mingxuan Yuan",Not found,Not found,"Large Language Models, Fine-Tuning, Reward Informed Fine-Tuning, Rejection Sampling Fine-Tuning, Data Efficiency, Catastrophic Forgetting","RIFT proposes a simple yet effective framework that utilizes all self-generated samples to address the inefficiency of Supervised Fine-Tuning and Rejection Sampling Fine-Tuning. Unlike RFT, which discards negative samples, RIFT repurposes them, reweighting the loss with scalar rewards to learn from both positive and negative trajectories. This approach aims to improve alignment using mixed-quality, self-generated data.",27.52,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09259v1_MAXS Meta-Adaptive Exploration with LLM Agents.pdf,MAXS: Meta-Adaptive Exploration with LLM Agents,"Jian Zhang, Zhiyuan Wang, Zhangqi Wang, Yu He, Haoran Luo, Li Yuan, Lingling Zhang, Rui Mao, Qika Lin, Jun Liu",,,"Large Language Model, LLM Agents, Meta-adaptive Exploration, Tool Execution, Reasoning Planning","Meta-adaptive exploration with LLM Agents (MAXS) addresses the issues of locally myopic generation and trajectory instability in agent inference. MAXS employs a lookahead strategy to extend reasoning paths and select stable, consistent, and high-value reasoning steps. It also introduces a trajectory convergence mechanism to control computational cost and balance resource efficiency and global effectiveness in multi-tool reasoning.",26.95,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09260v1_Efficient Paths and Dense Rewards Probabilistic Fl.pdf,Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models,"Yan Liu, Feng Zhang, Zhanyu Ma, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He, Han Liu, Yangdong Deng",Not found,Not found,"Large Language Models, Chain-of-Thought, Probabilistic Flow, Reasoning Efficiency, Optimization Difficulty","This work proposes CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, enabling flow-guided decoding and flow-based reinforcement learning. Experiments demonstrate superior balance between inference efficiency and reasoning performance on challenging benchmarks.",26.79,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09262v1_Magnifying change Rapid burn scar mapping with mul.pdf,"Magnifying change: Rapid burn scar mapping with multi-resolution, multi-source satellite imagery","Maria Sdraka, Dimitrios Michail, Ioannis Papoutsis",Not provided,Not provided,"Artificial intelligence, Machine Learning, Remote Sensing, burnt area mapping, disaster management, disaster monitoring, wildfires, burn scar mapping, change detection, downscaling, super-resolution","A novel deep learning model, BAM-MRCD, is proposed to detect small scale wildfires with high accuracy using multi-resolution, multi-source satellite imagery (MODIS and Sentinel-2) for timely production of detailed burnt area maps with high spatial and temporal resolution.",27.29,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09264v1_Coordinated Pandemic Control with Large Language M.pdf,Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants,"Ziyi Shi, Xusen Guo, Hongliang Lu, Mingxing Peng, Haotian Wang, Zheng Zhu, Zhenning Li, Yuxuan Liang, Xinhu Zheng, Hai Yang",,,"Pandemic Control, Large Language Models, Multi-Agent System, Coordinated Policymaking","Effective pandemic control requires timely and coordinated policymaking across regions, which is often fragmented and reactive. This paper proposes a large language model (LLM) multi-agent framework to support coordinated and proactive pandemic control, enabling agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process.",27.9,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09269v2_RISER Orchestrating Latent Reasoning Skills for Ad.pdf,Orchestrating Latent Reasoning Skills for Adaptive Activation Steering,"Wencheng Ye, Xiaoyang Yuan, Yi Bin, Hengyu Jin, Liang Peng, Pengpeng Zeng, Heng Tao Shen",Not found,Not found,"Large Language Models, Domain-specific reasoning, Activation steering, Reinforcement Learning, Latent reasoning, Adaptive activation","This paper proposes RISER, a plug-and-play intervention framework that adaptively steers large language model reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input, optimizing the Router via reinforcement learning under task-level rewards. Across seven diverse benchmarks, RISER yields 3.4–6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2–3× higher token efficiency and robust accuracy gains.",27.72,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09274v1_A3-Bench Benchmarking Memory-Driven Scientific Rea.pdf,A3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation,"Jian Zhang, Yu He, Zhiyuan Wang, Zhangqi Wang, Kai He, Fangzhi Xu, Qika Lin, Jun Liu",Not found,2601.09274,"memory-driven reasoning, anchor and attractor activation, scientific reasoning, benchmarking","A3-Bench is a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. It addresses the gap in existing benchmarks by focusing on memory activation and its impact on reasoning performance.",27.3,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09278v1_M3Searcher Modular Multimodal Information Seeking .pdf,M3Searcher: Modular Multimodal Information Seeking Agency with Retrieval-Oriented Reasoning,"Xiaohan Yu, Chao Feng, Lang Mei, Chong Chen",,,"DeepResearch, multimodal information seeking, reinforcement learning, information retrieval, LLMs, modular design","Recent advances in DeepResearch-style agents have demonstrated strong capabilities in autonomous information acquisition and synthesis from real-world web environments. However, existing approaches remain limited to text modality. M3Searcher, a modular multimodal information-seeking agent, explicitly decouples information acquisition from answer derivation, optimizing with a retrieval-oriented multi-objective reward that encourages factual accuracy, reasoning soundness, and retrieval fidelity. It also supports multimodal multi-hop training with MM-SearchVQA.",27.64,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09280v1_ReGraM Region-First Knowledge Graph Reasoning for .pdf,ReGraM: Region-First Knowledge Graph Reasoning for Medical Question Answering,"Chaerin Lee, Sohee Park, Hyunsik Na, Daseon Choi",,,"Medical QA, Knowledge Graph, Multi-hop Reasoning, Large Language Models, KG-based QA, Graph Constrained Prompting",A framework for improving factual accuracy and consistency in medical question answering by constructing a query-aligned subgraph and performing stepwise reasoning constrained to this localized region under multiple evidence-aware modes.,25.62,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09281v1_STaR Sensitive Trajectory Regulation for Unlearnin.pdf,STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models,"Jingjing Zhou1*, Gaoxiang Cong2, Li Su1†, Liang Li1,2†",Not found,Not found,"Large Reasoning Models, Unlearning, Privacy Protection, Sensitive Trajectory Regulation, Multi-step Reasoning, Chain-of-Thought","Large Reasoning Models have advanced automated multi-step reasoning, but their ability to generate complex Chain-of-Thought trajectories introduces severe privacy risks. Existing unlearning approaches for Large Language Models are insufficient for Large Reasoning Models, as they fail to remove sensitive content from intermediate steps. To address these challenges, we propose Sensitive Trajectory Regulation (STaR), a parameter-free, inference-time unlearning framework that achieves robust privacy protection throughout the reasoning process.",27.76,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09282v1_Cluster Workload Allocation Semantic Soft Affinity.pdf,Cluster workload allocation: semantic soft affinity using natural language processing,"Leszek Sliwko1, Jolanta Mizeria-Pietraszko2",,,"Cluster workload allocation, Natural Language Processing, Semantic soft affinity, Kubernetes scheduler, Load balancing, Soft-Affinity, Task Assignment","This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed, demonstrating high LLM parsing accuracy for top-tier models and superior or equivalent scheduling quality compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios.",27.52,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09286v1_Why not Collaborative Filtering in Dual View Bridg.pdf,Why not Collaborative Filtering in Dual View? Bridging Sparse and Dense Models,"HANZE GUO, JIANXUN LIAN, XIAO ZHOU",XXXXXXX.XXXXXXX,,"Collaborative Filtering, Dual View Alignment, Sparse and Dense model","This paper proposes SaD (Sparse and Dense), a unified framework that integrates the semantic expressiveness of dense embeddings with the structural reliability of sparse interaction patterns, to overcome the signal-to-noise ratio ceiling in modeling unpopular items.",26.18,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09292v1_Blue Teaming Function-Calling Agents.pdf,Blue Teaming Function-Calling Agents,"Greta Dolcetti1†, Giulio Zizzo 2, Sergio Maffeis 3",Not found,Not found,"Blue Teaming, Function-calling, Large Language Models, Adversarial Attacks, Defenses, Open Source","An experimental evaluation of four open source LLMs with function-calling capabilities against three different attacks, and measurement of the effectiveness of eight different defences. Results show these models are not safe by default, and defenses are not yet employable in real-world scenarios.",26.58,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09293v1_Policy-Based Reinforcement Learning with Action Ma.pdf,Policy-Based Reinforcement Learning with Action Masking for Dynamic Job Shop Scheduling under Uncertainty,"Sofiene Lassoued a,∗, Stefan Lier b",,,"Dynamic Job Shop Scheduling, Fault tolerance, Reinforcement learning, actions masking, Petri nets","We present a novel framework for solving Dynamic Job Shop Scheduling Problems under uncertainty, addressing the challenges introduced by stochastic job arrivals and unexpected machine breakdowns. Our approach follows a model-based paradigm, using Coloured Timed Petri Nets to represent the scheduling environment, and Maskable Proximal Policy Optimization to enable dynamic decision-making while restricting the agent to feasible actions at each decision point. The framework simulates realistic industrial conditions using stochastic models for dynamic job arrivals and machine failures, and it studies two action-masking strategies: a non-gradient approach and a gradient-based approach. Extensive experiments demonstrate that our method consistently outperforms traditional heuristic and rule-based approaches in terms of makespan minimization.",28.11,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09306v1_On-Device Large Language Models for Sequential Rec.pdf,On-Device Large Language Models for Sequential Recommendation,"Xin Xia, Hongzhi Yin, Shane Culpepper",10.1145/3773966.3777961,,"Recommender Systems, Sequential Recommendation, On-Device Recommendation, Model Compression, Resource Constrained Devices","This paper proposes OD-LLM, a task-adaptive compression framework designed for efficient and accurate on-device deployment of large language models for sequential recommendation tasks. It integrates low-rank structural compression and a novel tokenization normalization technique, and uses a progressive alignment algorithm to refine parameters. Empirical evaluations show that OD-LLM maintains effectiveness when model size is halved.",26.89,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09313v1_Understanding or Memorizing A Case Study of German.pdf,Understanding or Memorizing? A Case Study of German Definite Articles in Language Models,"Jonathan Drechsel, Erisa Bytyqi, Steffen Herbold",,,"Language models, German definite articles, Grammatical agreement, Gradient-based interpretability, Memorization, Rule-based generalization","This study investigates whether language models encode abstract grammatical rules or rely on surface-level memorization of frequent token-context associations for German definite singular articles, which are syncretic and depend on gender and case. Using GRADIEND, a gradient-based interpretability method, the authors learn parameter update directions for gender-case specific article transitions and find that these updates frequently affect unrelated gender-case settings, indicating that models at least partly rely on memorized associations rather than abstract grammatical rules.",27.35,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09342v1_Improving Implicit Hate Speech Detection via a Com.pdf,Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework,"Ewelina Gajewska, Katarzyna Budzynska, Jarosław A. Chudziak",,,"LLMs, Community agents, Hate speech, Social media, Moderation, Fairness","This work proposes a contextualised detection framework for implicitly hateful speech, implemented as a multi-agent system comprising a central Moderator Agent and dynamically constructed Community Agents representing specific demographic groups. Our approach explicitly integrates socio-cultural context from publicly available knowledge sources, enabling identity-aware moderation that surpasses state-of-the-art prompting methods and alternative approaches on a challenging ToxiGen dataset. We enhance the technical rigour of performance evaluation by incorporating balanced accuracy as a central metric of classification fairness that accounts for the trade-off between true positive and true negative rates. We demonstrate that our community-driven consultative framework significantly improves both classification accuracy and fairness across all target groups.",27.57,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09351v1_Navigating Ethical AI Challenges in the Industrial.pdf,Navigating Ethical AI Challenges in the Industrial Sector: Balancing Innovation and Responsibility,"Ruomu Tan, Martin W Hoffmann",,2601.09351,"Artificial Intelligence, Ethics, Industrial Sector, Innovation, Responsibility","This chapter explores the ethical challenges and practices in integrating AI into the industrial sector, emphasizing the need for transparency, accountability, and fairness in AI applications.",28.45,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09353v1_Monte-Carlo Tree Search with Neural Network Guidan.pdf,Monte-Carlo Tree Search with Neural Network Guidance for Lane-Free Autonomous Driving,"Ioannis Peridis, Dimitrios Troullinos, Georgios Chalkiadakis, Pantelis Giankoulidis, Ioannis Papamichail, Markos Papageorgiou",Not found,2601.09353,"Monte-Carlo Tree Search, Neural Network, Autonomous Driving, Lane-Free Traffic, Reinforcement Learning, Markov Decision Process, Nudging","This work considers a Monte-Carlo Tree Search (MCTS) planning approach for single-agent autonomous driving in lane-free traffic, incorporating a pre-trained neural network (NN) to guide the selection phase. The procedure incorporates the predictive capabilities of NNs for a more informed tree search process under computational constraints. Experimental evaluation addresses both safety and efficacy metrics, examining the influence of isotropic state information and the acceleration of performance for the NN-guided variant of MCTS.",28.82,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09361v1_GeoRA Geometry-Aware Low-Rank Adaptation for RLVR.pdf,GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR,"Jiaying Zhang1,2†*, Lei Shi1*, Jiguo Li1, Jun Xu1‡, Jiuchong Gao 1‡, Jinghua Hao 1, Renqing He 1",,,"Reinforcement Learning, Verifiable Rewards, Parameter-efficient, Low-Rank Adaptation, Geometry-Aware, Optimization Dynamics, Geometric Structures","Proposes GeoRA, a method that exploits the anisotropic and compressible nature of RL update subspaces to mitigate optimization bottlenecks caused by geometric misalignment in Reinforcement Learning with Verifiable Rewards (RLVR). GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components, preserving the pre-trained geometric structure and enabling efficient GPU computation through dense operators. Experiments demonstrate that GeoRA consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results and superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.",28.1,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09365v1_Frame of Reference Addressing the Challenges of Co.pdf,Frame of Reference: Addressing the Challenges of Common Ground,"Biswesh Mohapatra*, Théo Charlot†‡, Giovanni Duca†‡, Mayank Palan†‡, Laurent Romary*, Justine Cassell*",,,"Common Ground, Situational Dialogs, Embodied Conversational Agents, Social Robots, Large Language Models","This work evaluates a model's ability to establish common ground in dynamic and shared environments of situated dialogs, leveraging relational references. It also tests multiple methods for representing common ground and proposes approaches to improve performance.",26.55,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09381v1_Query Languages for Machine-Learning Models.pdf,Query Languages for Machine-Learning Models,"Martin Grohe, envel⌢pe",Not found,2601.09381,"Expressive power of query languages, fixed-point logics, weighted structures, neural networks, explainable AI","In this paper, the author discusses two logics for weighted finite structures: first-order logic with summation (FO(SUM)) and its recursive extension IFP(SUM). These logics are investigated as query languages for machine learning models, specifically neural networks, which are naturally represented as weighted graphs. The author presents illustrative examples of queries to neural networks that can be expressed in these logics and discusses fundamental results on their expressiveness and computational complexity.",27.59,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09382v1_Long-term Task-oriented Agent Proactive Long-term .pdf,Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments,"Qinglong Shi, Donghai Wang, Hantao Zhou, Jiguo Li, Jun Xu, Jiuchong Gao, Jinghua Hao, Renqing He",Not found,Not found,"long-term task-oriented interaction, proactive agents, dynamic environments, user intent maintenance","This paper proposes a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. It formalizes proactivity through Intent-Conditioned Monitoring and Event-Triggered Follow-up, and introduces a high-quality data synthesis pipeline to construct complex, multi-turn dialog data. The authors evaluate leading models and propose a new benchmark, ChronosBench, to address the lack of evaluation criteria for task-oriented interaction in dynamic environments. The fine-tuned model trained on synthetic data achieves a task completion rate of 85.19% for complex tasks, outperforming other models under test.",27.79,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09394v2_FairGE Fairness-Aware Graph Encoding in Incomplete.pdf,FairGE: Fairness-Aware Graph Encoding in Incomplete Social Networks,"Renqiang Luo, Huafei Huang, Tao Tang, Jing Ren, Ziqi Xu, Mingliang Hou, Enyan Dai, Feng Xia",XXXXXXX.XXXXXXX,,"Social Networks, Graph Learning, Graph Transformers, Fairness, Incomplete Data","This paper introduces FairGE, a fairness-aware framework for Graph Transformers in incomplete social networks, which encodes fairness directly through spectral graph theory without generating sensitive attributes, thus enhancing fairness and achieving at least a 16% improvement in statistical parity and equality of opportunity compared to state-of-the-art baselines.",26.91,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09398v1_Ability Transfer and Recovery via Modularized Para.pdf,Ability Transfer and Recovery via Modularized Parameters Localization,"Songyao Jin, Kun Zhou*, Wenqi Li, Peng Wang, Biwei Huang",,,"Large Language Models, Fine-tuning, Ability Transfer, Catastrophic Forgetting, Modularization","This work investigates how abilities are distributed within large language models (LLMs) and proposes ACT (Activation-Guided Channel-wise Ability Transfer), which localizes ability-relevant channels via activation differences and selectively transfers corresponding parameters, leading to the recovery of forgotten abilities while preserving retained skills.",26.14,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09413v1_Speech-Hands A Self-Reflection Voice Agentic Appro.pdf,Speech-Hands: A Self-Reflection Voice Agentic Approach to Speech Recognition and Audio Reasoning with Omni Perception,"Zhen Wan, Chao-Han Huck Yang, Jinchuan Tian, Hanrong Ye, Ankita Pasad, Szu-wei Fu, Arushi Goel, Ryo Hachiuma, Shizhe Diao, Kunal Dhawan, Sreyan Ghosh, Yusuke Hirota, Zhehuai Chen, Rafael Valle, Ehsan Hosseini Asl, Chenhui Chu, Shinji Watanabe, Yu-Chiang Frank Wang, Boris Ginsburg",,,"speech recognition, audio reasoning, self-reflection, omni-perception, voice agentic, audio intelligence","Introducing a voice-agentic framework that learns to know when to trust itself versus when to consult external audio perception, addressing the issue of naive fine-tuning on both speech recognition and external sound understanding tasks. This learnable reflection primitive prevents the model from being misled by noisy hypotheses, showing robust generalization and reliability across diverse audio question answering datasets.",27.72,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09416v1_Radiomics-Integrated Deep Learning with Hierarchic.pdf,RADIOMICS-INTEGRA TED DEEP LEARNING WITH HIERARCHICAL LOSS FOR OSTEOSARCOMA HISTOLOGY CLASSIFICA TION,"Yaxi Chen, Zi Ye, Shaheer U. Saeed, Oliver Yu, Simin Ni, Jie Huang, Yipeng Hu",Not found,Not found,"Osteosarcoma, Radiomics, Deep Learning, Hierarchical Loss, Histopathology, Tumor Classification","This work proposes the use of radiomic features as additional input in model training and introduces a hierarchical loss for two binary classification tasks (tumor-vs-non-tumor and viable-vs-non-viable) to improve classification performance and interpretability. The approach is demonstrated on the TCIA OS Tumor Assessment dataset, achieving new state-of-the-art performance.",27.27,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09421v2_Bias Dynamics in BabyLMs Towards a Compute-Efficie.pdf,Bias Dynamics in BabyLMs: Towards a Compute-Efficient Sandbox for Democratising Pre-Training Debiasing,"Filip Trhlik, Andrew Caines, Paula Buttery",,,"Bias, Pre-training, Debiasing, BabyLM, BERT, Language Models, Computational Cost","This work investigates whether significantly less costly models could replace standard LMs in debiasing research, focusing on models from the BabyLM Challenge. It shows that BabyLMs display closely aligned patterns of intrinsic bias formation and performance development compared to standard BERT models, despite their drastically reduced size. The study demonstrates that BabyLMs can serve as an effective sandbox for large-scale LMs, reducing pre-training costs from over 500 GPU-hours to under 30 GPU-hours.",27.53,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09433v1_Do Transformers Understand Ancient Roman Coin Moti.pdf,Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?,"David Reid, Ognjen Arandjelović",,2601.09433,"Transformers, CNNs, Ancient Coins, Machine Learning, Computer Vision","This paper evaluates the performance of Vision Transformer (ViT) models in identifying semantic elements on ancient coins compared to Convolutional Neural Networks (CNNs), finding ViT models to outperform the newly trained CNN models in accuracy.",28.06,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09445v1_Where Knowledge Collides A Mechanistic Study of In.pdf,Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models,"Minh Vu Pham, Hsuvas Borkakoty, Yufang Hou",Not found,2601.09445,"language models, knowledge conflict, intra-memory, pre-training, mechanistic interpretability","This work designs a framework based on mechanistic interpretability methods to identify where and how conflicting knowledge from pre-training data is encoded within language models, contributing to the understanding of specific internal components responsible for such conflicts.",26.86,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09446v1_Improving Symbolic Translation of Language Models .pdf,Improving Symbolic Translation of Language Models for Logical Reasoning,"Ramya Keerthy Thatikonda1, Jiuzhou Han1, Wray Buntine2, Ehsan Shareghi1",Not found,Not found,"Language models, Logical reasoning, Symbolic translation, Formal language, Natural language, First-order logic, External solver, Incremental inference, Verification module","The paper discusses the challenges faced by smaller language models in translating natural language into first-order logic for deductive logical reasoning tasks. It introduces incremental inference and a verification module to improve the performance of these models. The evaluation is performed using defined error categories, and the study evaluates three families of models across four logical-reasoning datasets. The comprehensive fine-tuning, incremental inference, and verification modules reduce error rates, increase predicate coverage, and improve reasoning performance for smaller LMs.",28.04,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09448v1_Population-Aligned Audio Reproduction With LLM-Bas.pdf,Population-Aligned Audio Reproduction With LLM-Based Equalizers,"Ioannis Stylianou, Jon Francombe, Pablo Martínez-Nuevo, Sven Ewan Shepstone, Zheng-Hua Tan",,,"Large Language Model, Equalization, Audio Reproduction, Listening Experiments, Recommender Systems","This paper introduces a Large Language Model (LLM)-based alternative for conventional audio equalization, enabling a conversational approach to sound system control. By utilizing data from a controlled listening experiment, the models reliably align with population-preferred equalization settings, showing statistically significant improvements in distributional alignment over random sampling and static preset baselines.",27.49,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09451v1_Late Breaking Results Quamba-SE Soft-edge Quantize.pdf,Late Breaking Results: Quamba-SE: Soft-edge Quantizer for Activations in State Space Models,"Yizhi Chen, Ahmed Hemani",,,"Quantization, State Space Models, Quamba","We propose Quamba-SE, a soft-edge quantizer for State Space Model (SSM) activation quantization. Unlike existing methods, Quamba-SE employs three adaptive scales: high-precision for small values, standard scale for normal values, and low-precision for outliers. This preserves outlier information instead of hard clipping, while maintaining precision for other values. We evaluate on Mamba-130M across 6 zero-shot benchmarks. Results show that Quamba-SE consistently outperforms Quamba, achieving up to +2.68% on individual benchmarks and up to +0.83% improvement in the average accuracy of 6 datasets.",27.51,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09455v1_On the Hardness of Computing Counterfactual and Se.pdf,On the Hardness of Computing Counterfactual and Semi-factual Explanations in XAI,"André Artelt, Martin Olsen, Kevin Tierney",Not found,2601.09455v1,"explainable artificial intelligence, counterfactual explanations, semi-factual explanations, computational complexity","Providing clear explanations to the choices of machine learning models is essential for these models to be deployed in crucial applications. The authors provide an overview of the computational complexity results in the literature for generating counterfactual and semi-factual explanations, finding that in many cases, generating explanations is computationally hard. They also contribute their own inapproximability results showing that not only are explanations often hard to generate, but under certain assumptions, they are also hard to approximate.",28.14,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09460v1_SoK Enhancing Cryptographic Collaborative Learning.pdf,SoK: Enhancing Cryptographic Collaborative Learning with Differential Privacy,"Francesco Capano, Jonas Böhler, Benjamin Weggenmann",Not found,Not found,"Differential privacy, cryptographic collaborative learning, multi-party computation, homomorphic encryption, collaborative machine learning","This work systematizes the CPCL landscape, introduces a unified framework, and analyzes trade-offs of different secure noise sampling techniques, noise types, and DP mechanisms. It evaluates their accuracy and cryptographic overhead across CPCL paradigms and proposes future research directions.",26.65,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09465v1_EvoFSM Controllable Self-Evolution for Deep Resear.pdf,EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines,"Shuo Zhang, Chaofa Yuan, Ryan Guo, Xiaomin Yu, Rui Xu, Zhangquan Chen, Zinuo Li, Zhi Yang, Shuhao Guan, Zhenheng Tang, Sen Hu, Liwen Zhang, Ronghao Chen, Huacan Wang",Not found,2601.09465,"self-evolution, finite state machines, deep research, adaptive systems, machine learning","EvoFSM is a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. It decouples the optimization space into macroscopic Flow(state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations and incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries.",28.65,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09467v1_Searth Transformer A Transformer Architecture Inco.pdf,Searth Transformer: A Transformer Architecture Incorporating Earth's Geospheric Physical Priors for Global Mid-Range Weather Forecasting,"Tianye Li, Qi Liu, Hao Li, Lei Chen, Wencong Cheng, Fei Zheng, Xiangao Xia, Ya Wang, Gang Huang, Weiwei Wang, Xuan Tong, Ziqing Zu, Yi Fang, Shenming Fu, Jiang Jiang, Haochen Li, Mingxing Li, Jiangjiang Xia",Not provided,Not provided,"Transformer, Earth physics, Global weather forecasting, Mid-range forecasting, Zonal periodicity, Meridional boundaries, Relay Autoregressive (RAR)","This paper presents the Shifted Earth Transformer (Searth Transformer), a novel transformer architecture designed for global medium-range weather forecasting. It incorporates zonal periodicity and meridional boundaries into window-based self-attention, enabling physically consistent global information exchange. The authors also introduce the Relay Autoregressive (RAR) fine-tuning strategy to mitigate computational bottlenecks, making the model more resource-efficient and suitable for resource-constrained institutions.",27.77,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09469v2_FairGU Fairness-aware Graph Unlearning in Social N.pdf,FairGU: Fairness-aware Graph Unlearning in Social Networks,"Renqiang Luo, Yongshuai Yang, Huafei Huang, Qing Qing, Mingliang Hou, Ziqi Xu, Yi Yu, Jingjing Zhou, Feng Xia",XXXXXXX.XXXXXXX,,"fairness, privacy, graph unlearning, social network","Introduces FairGU, a fairness-aware graph unlearning framework designed to preserve both utility and fairness during the unlearning process, demonstrating superior performance in accuracy and fairness metrics compared to state-of-the-art methods.",26.09,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09470v1_Personalized Multimodal Feedback Using Multiple Ex.pdf,Personalized Multimodal Feedback Using Multiple External Representations: Strategy Profiles and Learning in High School Physics,"Natalia Revenga-Lozano1*, Karina E. Avila1, Steffen Steinert1, Matthias Schweinberger1, Clara E. Gómez-Pérez1, Jochen Kuhn1, Stefan Küchemann1*",Not provided,2601.09470,"Physics education, Multimodal feedback, Personalized feedback, High school physics, Strategy selection","This study investigates the effectiveness of personalized feedback using multiple external representations in high school physics, particularly in the context of multimodal large language models. It found that elaborated multirepresentational feedback had a small but consistent positive association with post-test scores, independent of prior knowledge and confidence. Learners adopted distinct representation-selection strategies, with lower competence students benefiting more from diverse representation use, while this advantage diminished as competence increased.",29.04,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09473v1_SimMerge Learning to Select Merge Operators from S.pdf,SimMerge: Learning to Select Merge Operators from Similarity Signals,"Oliver Bolton, Aakanksha, Arash Ahmadian, Sara Hooker, Marzieh Fadaee, Beyza Ermis",Not found,2601.09473,"Model Merging, Language Models, Similarity Signals, Merge Selection, Language Models","This work introduces SimMerge, a predictive merge-selection method that selects the best merge using inexpensive, task-agnostic similarity signals between models, eliminating the expensive merge-and-evaluate loop.",27.23,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09478v3_Bridging Semantic Understanding and Popularity Bia.pdf,Bridging Semantic Understanding and Popularity Bias with LLMs,"Renqiang Luo, Dong Zhang, Yupeng Gao, Wen Shi, Mingliang Hou, Jiaying Liu, Zhe Wang, Shuo Yu*",XXXXXXX.XXXXXXX,,"Semantic analysis, Recommender systems, Algorithmic fairness, Popularity bias, LLM","This paper proposes FairLRM, a novel framework that bridges the gap in the semantic understanding of popularity bias with Recommendation via Large Language Model (RecLLM). It decomposes popularity bias into item-side and user-side components and enhances the model's comprehension of global item distributions and individual user preferences using structured instruction-based prompts. The framework significantly improves fairness and recommendation accuracy through empirical evaluation.",26.74,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09503v1_What Do LLM Agents Know About Their World Task2Qui.pdf,What Do LLM Agents Know About Their World?,"Siyuan Liu, Hongbang Yuan, Xinze Li, Ziyue Zhu, Yixin Cao, Yu-Gang Jiang",Not found,2601.09503,"large language models, generalization, environment understanding, trajectory-based evaluation","This paper investigates agent's environment understanding beyond current task success evaluation, proposing Task-to-Quiz (T2Q) to decouple task execution from world-state understanding.",27.44,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09518v1_Learning Whole-Body Human-Humanoid Interaction fro.pdf,Learning Whole-Body Human-Humanoid Interaction from Human-Human Demonstrations,"Wei-Jin Huang, Yue-Yi Zhang, Yi-Lin Wei, Zhi-Wei Xia, Juantao Tan, Yuan-Ming Li, Zhilin Zhao, Wei-Shi Zheng",,2601.09518,"Human-Human Interaction, Human-Humanoid Interaction, Retargeting, Physics-Aware Interaction, Imitation Learning, Hierarchical Policy, Spatio-Temporal Reasoning","This paper presents a method to enable humanoid robots to physically interact with humans by leveraging abundant Human-Human Interaction data. It introduces a contact-centric, two-stage pipeline called PAIR (Physics-Aware Interaction Retargeting) to generate physically consistent Human-Humanoid Interaction data. The authors also introduce D-STAR (Decoupled Spatio-Temporal Action Reasoner), a hierarchical policy that disentangles when to act from where to act, to improve interactive understanding.",29.18,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09520v1_Towards Realistic Synthetic Data for Automatic Dru.pdf,TOW ARDS REALISTIC SYNTHETIC DA TA FOR AUTOMA TIC DRUM TRANSCRIPTION,"Pierfrancesco Melucci, Paolo Merialdo, Taketo Akama",,,"Automatic Drum Transcription, Deep Learning, Synthetic Data, Drum Transcription, SoundFont","This paper introduces a new paradigm for Automatic Drum Transcription (ADT) that circumvents the need for paired audio-MIDI training data. It presents a semi-supervised method to automatically curate a large and diverse corpus of one-shot drum samples from unlabeled audio sources, synthesizing a high-quality dataset from MIDI files alone for training a sequence-to-sequence transcription model. The model achieves new state-of-the-art results on the ENST and MDB test sets, significantly outperforming both fully supervised methods and previous synthetic-data approaches.",27.88,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09527v1_Private LLM Inference on Consumer Blackwell GPUs A.pdf,Private LLM Inference on Consumer Blackwell GPUs: A Practical Guide for Cost-Effective Local Deployment in SMEs,Jonathan Knoop1 and Hendrik Holtmann2,,,"Large Language Models, SMEs, Data Privacy, Inference, Consumer GPUs, Blackwell, RTX 5060 Ti, RTX 5070 Ti, RTX 5090, Quantization, Inference Cost, Latency","This paper presents a systematic evaluation of NVIDIA's Blackwell consumer GPUs (RTX 5060 Ti, 5070 Ti, 5090) for production Large Language Model (LLM) inference. It benchmarks four open-weight models across various configurations and workloads, providing insights into the performance and cost-effectiveness of these GPUs for SMEs. The results show that consumer GPUs can reliably replace cloud inference for most SME workloads, except for latency-critical long-context RAG tasks, where high-end GPUs remain essential. The paper also provides deployment guidance and releases all benchmark data for reproducible SME-scale deployments.",28.17,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09536v1_Omni-R1 Towards the Unified Generative Paradigm fo.pdf,Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning,"Dongjie Cheng, Yongqi Li, Zhixin Ma, Hongru Cai, Yupeng Hu, Wenjie Wang, Liqiang Nie, Wenjie Li",,2601.09536,"Multimodal Reasoning, Generative Models, Two-Stage Framework, Perception Alignment, Perception Reward, Visual Information","This paper proposes a unified generative multimodal reasoning approach, Omni-R1, which integrates diverse multimodal reasoning skills by generating intermediate images during the reasoning process. It introduces Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average.",28.18,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09555v1_Benchmarking Post-Training Quantization of Large L.pdf,Benchmarking Post-Training Quantization of Large Language Models under Microscaling Floating Point Formats,"Manyi Zhang, Ji-Fu Li, Zhongao Sun, Haoli Bai, Hui-Ling Zhen, Zhenhua Dong, Xianzhi Yu",,,"Large Language Models, Post-Training Quantization, Microscaling Floating-Point Formats, Quantization, Accuracy, Efficiency","This work conducts a systematic investigation of post-training quantization under Microscaling Floating-Point (MXFP) formats, encompassing over 7 PTQ algorithms, 15 evaluation benchmarks, and 3 LLM families. Key findings include MXFP8 achieving near-lossless performance, MXFP4 introducing substantial accuracy degradation, and PTQ effectiveness depending on format compatibility. The study provides practical guidance on adapting existing PTQ methods to MXFP quantization.",26.81,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09566v2_Hot-Start from Pixels Low-Resolution Visual Tokens.pdf,Hot-Start from Pixels: Low-Resolution Visual Tokens for Chinese Language Modeling,"Shuyang Xiang, Hao Guan",Not found,2601.09566,"Chinese language modeling, visual tokens, low-resolution inputs, hot-start effect","This paper investigates whether low-resolution visual inputs can serve as an alternative for character-level modeling of Chinese characters, achieving 39.2% accuracy and demonstrating a pronounced hot-start effect in minimal resource settings.",26.51,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09600v1_Information Access of the Oppressed A Problem-Posi.pdf,Information Access of the Oppressed: A Problem-Posing Framework for Envisioning Emancipatory Information Access Platforms,"BHASKAR MITRA, NICOLA NEOPHYTOU, SIREESH GURURAJA",XXXXXXX.XXXXXXX,,"Emancipatory Information Access, Search and Society, Sociotechnical Information Systems","This paper explores the challenges of authoritarian capture of information access platforms, particularly in the context of democratic erosion and the rise of generative AI technologies. It proposes a problem-posing framework to envision alternative IA infrastructure that can mitigate these risks, drawing on Paulo Freire's theories of emancipatory pedagogy.",26.55,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09603v1_Linear Complexity Self-Supervised Learning for Mus.pdf,Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer,"Petros Vavaroutsos, Theodoros Palamas, Pantelis Vikatos",10.1145/3748522.3779786,,"Deep Learning, Learnable Representations, Music Understanding, Transformers, Embeddings, Attention","This paper focuses on reducing the size of a foundation model when applied to music information retrieval tasks. It combines the Branchformer architecture with SummaryMixing, a process of random quantization, and pre-trains on publicly available datasets. The results show competitive performance compared to other state-of-the-art models while reducing the model size from 8.5% to 12.3%.",26.79,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09605v1_Sim2real Image Translation Enables Viewpoint-Robus.pdf,Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets,"Jeremiah Coholich, Justin Wit, Robert Azarcon, Zsolt Kira",Not provided,Not provided,"Robotics, Image Translation, Sim2Real, Viewpoint Robustness, Fixed-Camera Data","This paper proposes MANGO, an unpaired image translation method, to enable viewpoint-robust policies for robot manipulation. It addresses the challenge of distribution shifts in camera viewpoint variations, particularly in robot demonstration data which is scarce and often lacks variation. MANGO maintains viewpoint consistency during sim2real translation and can generate diverse unseen viewpoints by translating simulated observations. The method is shown to outperform other image translation methods and improve the robustness of imitation-learning policies trained on augmented data.",28.15,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09609v1_DPWriter Reinforcement Learning with Diverse Plann.pdf,DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing,"Qian Cao, Yahui Liu, Wei Bi, Yi Zhao, Ruihua Song, Xiting Wang, Ruiming Tang, Guorui Zhou, Han Li",,,"Reinforcement Learning, Creative Writing, Diverse Exploration, Long Chain-of-Thought, Group-Aware Diversity Reward","This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. It introduces a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.",27.54,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09613v1_CogRail Benchmarking VLMs in Cognitive Intrusion P.pdf,CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems,"Yonglin Tian, Qiyao Zhang, Wei Xu, Yutong Wang, Yihao Wu, Xinyi Li, Xingyuan Dai, Hui Zhang, Zhiyong Cui, Baoqing Guo, Zujun Yu, Yisheng Lv",Not found,Not found,"Cognitive intrusion perception, Intelligent railway transportation, Visual-language models, Benchmarking","Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. CogRail introduces a novel benchmark that integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction, facilitating the evaluation of state-of-the-art visual-language models (VLMs) and proposing a joint fine-tuning framework for better performance.",27.62,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09620v1_Full Disclosure Less Trust How the Level of Detail.pdf,"Full Disclosure, Less Trust? How the Level of Detail about AI Use in News Writing Affects Readers’ Trust","POOJA PRAJOD, HANNES COOLS, THOMAS RÖGGLA, KARTHIKEYA PUTTUR VENKATRAJ, AMBER KUSTERS, ALIA ELKATTAN, PABLO CESAR, ABDALLAH EL ALI",,,"AI, transparency, trust, journalism, news production","This study investigates how three levels of AI disclosures (none, one-line, detailed) across two types of news (politics and lifestyle) and two levels of AI involvement (low and high) affect news readers' trust. It measures trust using the News Media Trust questionnaire and two decision behaviors: source-checking and subscription decisions. The findings show that detailed AI disclosures lead to a decline in trust, while one-line and detailed disclosures increase source-checking behavior, with the latter being more pronounced. Participants' preferences suggest a trade-off between transparency and trust.",28.52,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09624v1_Toward Understanding Unlearning Difficulty A Mecha.pdf,Toward Understanding Unlearning Difficulty: A Mechanistic Perspective,"Jiali Cheng, Ziheng Chen, Chirag Agarwal, Hadi Amiri",Not found,Not found,"Machine unlearning, model circuits, unlearning difficulty, Circuit-guided Unlearning Difficulty (CUD)","This work studies the unlearning difficulty of machine learning models from a mechanistic perspective, proposing Circuit-guided Unlearning Difficulty (CUD) as a metric to assign each sample a continuous difficulty score using circuit-level signals. It demonstrates that CUD reliably separates intrinsically easy and hard samples and remains stable across unlearning methods, identifying key circuit-level patterns that reveal a mechanistic signature of difficulty.",26.92,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09625v1_The Promptware Kill Chain How Prompt Injections Gr.pdf,The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware,"Ben Nassi, Bruce Schneier, Oleg Brodt",,,"malware, prompt injection, large language models, security, cybersecurity","The rapid adoption of large language model (LLM)-based systems has created a new attack surface that existing security frameworks inadequately address. The paper proposes that attacks targeting LLM-based applications constitute a distinct class of malware, called promptware, and introduces a five-step kill chain model for analyzing these threats.",26.82,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09626v1_From Prompt to Protocol Fast Charging Batteries wi.pdf,From Prompt to Protocol: Fast Charging Batteries with Large Language Models,"Ge Lei1∗, Ferran Brosa Planella2, Sterling G. Baird3, Samuel J. Cooper1†",Not found,2601.09626,"battery charging, large language models, fast charging, optimization, protocol design","Efficiently optimizing battery charging protocols is challenging due to the slow, costly, and non-differentiable nature of evaluations. This work introduces two gradient-free, LLM-driven closed-loop methods, Prompt-to-Optimizer (P2O) and Prompt-to-Protocol (P2P), which outperform existing approaches in terms of performance and efficiency.",27.64,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09635v1_LLM for Large-Scale Optimization Model Auto-Formul.pdf,LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach,"Kuo Liang, Yuhang Lu, Jianming Mao, Shuyi Sun, Chunwei Yang, Congcong Zeng, Xiao Jin, Hanzhang Qin, Ruihao Zhu, Chung-Piaw Teo",,2601.09635,"large language models, tool use, agentic workflow construction, automated optimization modeling","This paper proposes LEAN-LLM-OPT, a workflow construction framework for large-scale optimization model auto-formulation using large language models. It addresses the labor-intensive and time-consuming nature of building optimization models by dynamically constructing workflows that specify how optimization models for similar problems can be formulated. The framework leverages LLMs' text-processing capabilities and common modeling practices to decompose the modeling task into structured sub-tasks and offload mechanical data-handling operations to auxiliary tools, thereby alleviating the burden on the downstream agent and focusing it on challenging components. Extensive simulations and a Singapore Airlines use case demonstrate the framework's strong performance and practical value.",27.98,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09636v1_PersonalAlign Hierarchical Implicit Intent Alignme.pdf,PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records,"Yibo Lyu, Gongwei Chen, Rui Shao†, Weili Guan, Liqiang Nie†",,,"GUI agents, implicit intent, long-term user records, proactive assistance, intent alignment","This work introduces PersonalAlign, a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. It also presents AndroidIntent, a benchmark for evaluating agents' ability in resolving vague instructions and providing proactive suggestions. The study includes annotated user-specific preferences and routines, and evaluates a range of GUI agents, showing that the Hierarchical Intent Memory Agent (HIM-Agent) significantly improves both execution and proactive performance.",26.98,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09667v2_Collaborative Multi-Agent Test-Time Reinforcement .pdf,Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning,"Zhiyuan Hu, Yunhai Hu, Juncheng Liu, Shuyue Stella Li, Yucheng Wang, Zhen Xu, See-Kiong Ng, Anh Tuan Luu, Xinxing Xu, Bryan Hooi, Cynthia Breazeal, Hae Won Park",Not found,Not found,"Multi-agent reinforcement learning, Test-time reinforcement learning, Multi-expert team, Credit assignment, Distribution shift robustness","This paper introduces MATTRL, a framework that injects structured textual experience into multi-agent deliberation at inference time to improve accuracy in challenging benchmarks in medicine, math, and education. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. It also studies credit assignment for constructing a turn-level experience pool and reinjecting it into the dialogue. Across challenging benchmarks, MATTRL improves accuracy by an average of 3.67% over a multi-agent baseline and by 8.67% over comparable single-agent baselines.",27.93,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09680v1_Automating Supply Chain Disruption Monitoring via .pdf,Automating Supply Chain Disruption Monitoring via an Agentic AI Approach,"Sara AlMahria, Liming Xu, Alexandra Brintrup",Not provided,2601.09680,"Supply Chain Management, Supply Chain Disruption, Large Language Models, AI Agents, Multi-Agent System, Agentic System","This research introduces a minimally supervised agentic AI framework to autonomously monitor, analyze, and respond to disruptions across extended supply networks, achieving high accuracy and reducing response time compared to industry benchmarks.",27.68,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09684v1_Disentangling Task Conflicts in Multi-Task LoRA vi.pdf,Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection,"Ziyu Yang, Guibin Chen, Yuxin Yang, Aoxiong Zeng, Xiangquan Yang",Not found,Not found,"Multi-Task Learning, Low-Rank Adaptation, Gradient Projection, Orthogonal Complement, Parameter-Efficient Fine-Tuning","This paper proposes Ortho-LoRA, a gradient projection method specifically tailored for the bipartite structure of LoRA. It dynamically projects conflicting task gradients onto the orthogonal complement of each other within the intrinsic LoRA subspace, effectively mitigating task interference and outperforming standard joint training on the GLUE benchmark.",26.98,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09692v1_Routing with Generated Data Annotation-Free LLM Sk.pdf,Routing with Generated Data: Annotation-Free,"Tianyi Niu, Justin Chih-Yao Chen, Genta Indra Winata, Shi-Xiong Zhang, Supriyo Chakraborty, Sambit Sahu, Elias Stengel-Eskin, Yue Zhang, Mohit Bansal",Not found,Not found,"Large Language Models, Routing, Annotation-Free, Generator LLMs, Skill Estimation, Model Selection","Large Language Model (LLM) routers dynamically select optimal models for given inputs. Existing approaches typically assume access to ground-truth labeled data, which is often unavailable in practice. We introduce Routing with Generated Data (RGD), a challenging setting in which routers are trained exclusively on generated queries and answers produced from high-level task descriptions by generator LLMs. We evaluate query-answer routers and query-only routers across four diverse benchmarks and 12 models, finding that query-answer routers degrade faster than query-only routers as generator quality decreases. Our analysis reveals two crucial characteristics of effective generators: they must accurately respond to their own questions and their questions must produce sufficient performance differentiation among the model pool. We then show how filtering for these characteristics can improve the quality of generated data. We further propose CASCAL, a novel query-only router that estimates model correctness through consensus voting and identifies model-specific skill niches via hierarchical clustering. CASCAL is substantially more robust to generator quality, outperforming the best query-answer router by 4.6% absolute accuracy when trained on weak generator data.",28.74,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09694v1_LLMs can Compress LLMs Adaptive Pruning by Agents.pdf,LLMs can Compress LLMs: Adaptive Pruning by Agents,"Sai Varun Kodathala1, Rakesh Vunnam 2",Not found,2601.09694,"Model Compression, Adaptive Pruning, Self-Reflection","As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics. Recent work shows that pruned LLMs suffer from severe factual knowledge degradation, with structured pruning methods experiencing near-total collapse in factual question-answering capabilities. This paper introduces agent-guided pruning, where a foundation model acts as an adaptive pruning agent to intelligently select which layers to prune at each iteration while preserving critical knowledge pathways. The method constructs layer-wise sensitivity profiles by combining Wanda-inspired weight-activation metrics with gradient importance scores, normalized as z-scores for model-agnostic comparison. A checkpoint rollback mechanism maintains model quality by reverting when perplexity degradation exceeds a threshold. The approach is evaluated on Qwen3 models (4B and 8B parameters) at approximately 45% sparsity, demonstrating substantial improvements over structured pruning baselines.",29.09,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09703v1_ShortCoder Knowledge-Augmented Syntax Optimization.pdf,ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation,"Sicong Liu, Yanxian Huang, Mingwei Liu, Jiachi Chen, Ensheng Shi, Yuchi Ma, Hongyu Zhang, Yin Zhang, Yanlin Wang",Not found,Not found,"Code generation, Large language models, Syntax optimization, Token efficiency, Code readability","This paper proposes ShortCoder, a knowledge-infused framework that optimizes code generation efficiency while preserving semantic equivalence and readability. It introduces ten syntax-level simplification rules for Python and a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing a corpus of validated ⟨original code, simplified code⟩ pairs. The framework also includes a fine-tuning strategy that injects conciseness awareness into base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving improvements of 18.1%-37.8% in generation efficiency.",28.1,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09706v1_Value-Aware Numerical Representations for Transfor.pdf,Value-Aware Numerical Representations for Transformer Language Models,"Andreea Dutulescu, Stefan Ruseti, Mihai Dascalu",,,"Transformer, Language models, Mathematical reasoning, Numerical understanding, Transformer-based models","Transformer-based language models often achieve strong results on mathematical reasoning benchmarks while remaining fragile on basic numerical understanding and arithmetic operations. A central limitation is that numbers are processed as symbolic tokens whose embeddings do not explicitly encode numerical value, leading to systematic errors. This paper introduces a value-aware numerical representation that augments standard tokenized inputs with a dedicated prefix token whose embedding is explicitly conditioned on the underlying numerical value. This mechanism injects magnitude information directly into the model's input space while remaining compatible with existing tokenizers and decoder-only Transformer architectures. Evaluation on arithmetic tasks shows that the proposed approach outperforms baselines across numerical formats, tasks, and operand lengths.",27.75,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09708v1_Fast-ThinkAct Efficient Vision-Language-Action Rea.pdf,Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning,"Chi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, Fu-En Yang",Not found,2601.09708,"Vision-Language-Action, Embodied Control, Latent Reasoning, Efficient Planning, Robust Long-Horizon Planning","Fast-ThinkAct is an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning, improving generalization over lengthy reasoning traces in Vision-Language-Action tasks.",27.53,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09749v1_R-LAM Reproducibility-Constrained Large Action Mod.pdf,R-LAM: Reproducibility-Constrained Large Action Models for Scientific Workflow Automation,Suriya Sureshkumar,,,"Reproducible Scientific Workflows, Large Action Models, LLM-Based Agents, Execution Provenance, Deterministic Execution","Proposes R-LAM, a reproducibility-constrained framework for applying Large Action Models to scientific workflow automation, introducing structured action schemas, deterministic execution policies, and explicit provenance tracking to ensure reproducibility and auditability.",26.42,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09750v1_SAGE Tool-Augmented LLM Task Solving Strategies in.pdf,SAGE: Tool-Augmented LLM Task Solving Strategies,"Robert K. Strehlow, Tobias Küster, Oskar F. Kupke, Brandon Llanque Kurps, Fikret Sivrikaya, Sahin Albayrak",,,"Large language models, LLMs, tool augmentation, domain-specific tools, zero-shot prompting, conversational AI, OPACA framework","This paper presents SAGE, a specialized conversational AI interface based on the OPACA framework for tool discovery and execution. It integrates new tools or services dynamically, providing robust and scalable zero-shot prompting methods for efficient use of tools. The authors implemented various task-solving strategies using agentic concepts and prompting methods, evaluated them against comprehensive benchmark services, and highlighted their strengths and weaknesses.",26.79,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09753v1_Critically Engaged Pragmatism A Scientific Norm an.pdf,"Critically Engaged Pragmatism: A Scientific Norm and Social, Pragmatist Epistemology for AI Science Evaluation Tools",Carole J. Lee,Not found,Not found,"AI science evaluation, peer review crisis, replication crisis, pragmatism, scientific evaluation tools","Tools for automating research evaluation are besieging the scientific community at a time of crisis for canonical mechanisms of credibility assessment, including review fatigue, replication crisis, and peer-review crisis. AI science evaluation tools are prone to 'inference by false ascent' due to contestation about their purposes and portability. A social, pragmatic epistemology and the norm of Critically Engaged Pragmatism are proposed to critically scrutinize the purposes and reliability of these tools.",27.41,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09755v1_Heterogeneous computing platform for real-time rob.pdf,Heterogeneous computing platform for real-time robotics,"Jakub Fil, Yulia Sandamirskaya, Hector Gonzalez, Loïc Azzalin, Stefan Glüge, Lukas Friedenstab, Matthias Lohrmann, Mahmoud Akl, Khaleel Khan, Leonie Wolf, Kristin Richter, Holm Puder, Mazhar Ali Bari, Xuan Choo, Noha Alharthi, Michael Hopkins, Mansoor Hanif, Christian Mayr, Jens Struckmeier, Steve Furber",Not provided,Not provided,"robotics, neuromorphic computing, real-time perception, heterogeneous computing, AI, cognitive cities, Society 5.0","This paper explores the computing platform required to enable a vision of cognitive cities powered by robots coexisting with humans. It combines neuromorphic hardware, such as the Loihi2 processor, with high-density GPU clusters for real-time perception and interaction, demonstrating the use of this hybrid architecture in an interactive task involving a humanoid robot playing a musical instrument with a human. The design emphasizes efficient integration of disparate components to maximize performance and responsiveness.",28.5,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09756v1_Synthetic Data for Veterinary EHR De-identificatio.pdf,"Synthetic Data for Veterinary EHR De-identification: Benefits, Limits, and Safety Trade-offs Under Fixed Compute","David Brundage, PhD",,,"de-identification, synthetic data, veterinary EHR, privacy","This study evaluates the use of large language model-generated synthetic veterinary clinical narratives in de-identification processes, comparing their safety and utility under different training regimes. It highlights the limitations of synthetic data in replacing real supervision and the benefits of increased exposure through synthetic augmentation.",26.68,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09757v1_Democracy and Distrust in an Era of Artificial Int.pdf,Democracy & Distrust in an Era of Artificial Intelligence,Sonia K. Katyal,10.1162/DAED_a_01919,not found,"Artificial Intelligence, Democracy, Judicial Review, AI Decision-Making, Minorities, Due Process, Equal Protection","This essay examines the challenges posed by the rise of AI decision-making to democracy's basic framework, drawing on cases in which AI decision-making has been challenged in courts to show how concepts of due process and equal protection can be integrated into AI, providing better oversight and accountability.",27.89,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09760v1_Investigating Tool-Memory Conflicts in Tool-Augmen.pdf,Investigating Tool-Memory Conflicts in Tool-Augmented LLMs,"Jiali Cheng, Rui Pan, Hadi Amiri",Not found,Not found,"tool-augmented language models, knowledge conflicts, tool-memory conflict, epistemic inconsistencies","This paper investigates a new type of knowledge conflict, Tool-Memory Conflict (TMC), in tool-augmented large language models (LLMs). It finds that existing LLMs suffer from TMC, especially on STEM-related tasks, and proposes methods to address this issue.",26.21,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09762v1_Explicating Tacit Regulatory Knowledge from LLMs t.pdf,Explicating Tacit Regulatory Knowledge from LLMs to Auto-Formalize,"Zhiyi Xue, Xiaohong Chen, Min Zhang",,,"regulatory compliance, test generation, large language models, hybrid approaches, tacit knowledge","This paper proposes RAFT, a framework for requirements auto-formalization and compliance test generation via explicating tacit regulatory knowledge from multiple LLMs. RAFT employs an Adaptive Purification-Aggregation strategy to integrate tacit regulatory knowledge into a domain meta-model, formal requirements representation, and testability constraints, guiding high-precision requirement formalization and automated test generation. Experiments across financial, automotive, and power domains show that RAFT achieves expert-level performance, substantially outperforming state-of-the-art methods while reducing overall generation and review time.",27.29,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09765v1_AI Survival Stories a Taxonomic Analysis of AI Exi.pdf,Survival Stories: A Taxonomic Analysis of AI Existential Risk,"Herman Cappelena, Simon Goldsteinb, c, d",Not found,Not found,"Artificial Intelligence, Existential Risk, AI Safety, AI Catastrophe, Superintelligent AI, AI Alignment","This paper develops a general framework for thinking about the existential risk of AI systems, analyzing a two-premise argument that AI systems pose a threat to humanity. It introduces a taxonomy of 'survival stories', each highlighting one of the ways that the future could go if humanity were to survive rather than be destroyed by AI.",26.92,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09768v1_CLiMB A Domain-Informed Novelty Detection Clusteri.pdf,CLiMB: A Domain-Informed Novelty Detection Clustering Framework for Scientific Discovery,"Lorenzo Monti, Tatiana Muraveva, Brian Sheridan, Davide Massari, Alessia Garofalo, Gisella Clementini, Umberto Michelucci",,2601.09768,"novelty detection, semi-supervised clustering, constrained clustering, density-based clustering, domain knowledge integration","In data-driven scientific discovery, CLiMB introduces a domain-informed framework that decouples the exploitation of prior knowledge from the exploration of unknown structures. Using a sequential two-phase approach, CLiMB first anchors known clusters using constrained partitioning and subsequently applies density-based clustering to residual data to reveal arbitrary topologies. Demonstrated on RR Lyrae stars data from GaiaData Release 3, CLiMB attains an Adjusted Rand Index of 0.829 with 90% seed coverage in recovering known Milky Way substructures, outperforming heuristic and constraint-based baselines.",28.23,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09770v1_GUI-Eyes Tool-Augmented Perception for Visual Grou.pdf,GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents,"Chen Chen, Jiawei Shao, Dakuan Lu, Haoyi Hu, Xiangcheng Liu, Hantao Yao",Not provided,Not provided,"GUI automation, reinforcement learning, visual grounding, tool usage, spatial reward","Recent advances in vision-language models and reinforcement learning have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. This paper presents GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. The agent learns to make strategic decisions on whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. A progressive perception strategy is introduced to decompose the decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. A spatially continuous reward function is designed to integrate both location proximity and region overlap, providing dense supervision and alleviating reward sparsity in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines.",28.56,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09771v1_PCN-Rec Agentic Proof-Carrying Negotiation for Rel.pdf,PCN-Rec: Agentic Proof-Carrying Negotiation for Reliable Governance-Constrained Recommendation,"Aradhya Dixit, Shreem Dixit",,,"recommendation systems, LLM agents, constrained ranking, governance, verification, negotiation","Modern LLM-based recommenders can generate compelling ranked lists, but they struggle to reliably satisfy governance constraints such as minimum long-tail exposure or diversity requirements. We present PCN-Rec, a proof-carrying negotiation pipeline that separates natural-language reasoning from deterministic enforcement. A base recommender (MF/CF) produces a candidate window of size 𝑊, which is negotiated by two agents: a User Advocate optimizing relevance and a Policy Agent enforcing constraints. A mediator LLM synthesizes a Top-𝑁 slate together with a structured certificate (JSON) describing the claimed constraint satisfaction. A deterministic verifier recomputes all constraints from the slate and accepts only verifier-checked certificates; if verification fails, a deterministic constrained-greedy repair produces a compliant slate for re-verification, yielding an auditable trace. On MovieLens-100K with governance constraints, PCN-Rec achieves a 98.55% pass rate on feasible users (𝑛= 551, 𝑊= 80) versus a one-shot single-LLM baseline without verification/repair, while preserving utility with only a 0.021 absolute drop in NDCG@10 (0.403 vs. 0.424); differences are statistically significant (𝑝<0.05).",29.05,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09772v1_Antisocial behavior towards large language model u.pdf,Antisocial behavior towards large language model users: experimental evidence,"Paweł Niszczota, Cassandra Grützner",,,"large language models, antisocial behavior, experimental research",Experimental evidence of antisocial behavior towards users of large language models.,27.12,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09773v1_Enhancing LUT-based Deep Neural Networks Inference.pdf,Enhancing LUT-based Deep Neural Networks Inference through Architecture and Connectivity Optimization,"Binglei Lou, Ruilin Wu, Philip Leong",,,"Dynamic Sparsity, FPGA, Neural Network, Lookup Table","This paper presents SparseLUT, a comprehensive framework that addresses the challenges of deploying deep neural networks on resource-constrained edge devices, such as FPGAs, by optimizing architectural and connectivity aspects. It reduces LUT consumption and inference latency while maintaining comparable accuracy. The training algorithm selectively prunes less significant inputs and regrows more effective ones, delivering consistent accuracy improvements across benchmarks.",26.66,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09805v1_Improving Chain-of-Thought for Logical Reasoning v.pdf,Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention,"Phuong Minh Nguyen, Tien Huu Dang, Naoya Inoue",Not found,Not found,"Logical reasoning, Attention, End-to-end, Attention-aware intervention","This work introduces an end-to-end framework for logical reasoning tasks, which activates attention heads aligned with logical reasoning operators to enhance performance across diverse benchmarks and architectures, while incurring negligible additional computational overhead.",26.07,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09806v1_Diffusion-Driven Deceptive Patches Adversarial Man.pdf,Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification,"Shahrzad Sayyafzadeh, Hongmei Chi, Shonda Bernadin",,2601.09806,"Adversarial Patch Generation, Gaussian Smoothing, Diffusion Model, Social Media Forensics, Perceptual Hashing","This work presents an end-to-end pipeline for generating, refining, and evaluating adversarial patches to compromise facial biometric systems with forensic analysis and security testing applications.",28.89,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09809v1_QFed Parameter-Compact Quantum-Classical Federated.pdf,QFed: Parameter-Compact Quantum-Classical Federated Learning,"Samar Abdelghani, Soumaya Cherkaoui",,,"Federated Learning, Quantum Computing, Privacy, Communication, IoT","This study examines the potential of quantum-assisted federated learning, which could cut the number of parameters in classical models by polylogarithmic factors and thus lessen training overhead. QFed, a quantum-enabled federated learning framework, achieves a 77.6% reduction in the parameter count of a VGG-like model while maintaining comparable accuracy in a scalable environment.",27.02,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09814v1_Explainable Deep Learning for Pediatric Pneumonia .pdf,Explainable Deep Learning for Pediatric Pneumonia Detection in Chest X-Ray Images,"Adil O. Khadidos1, †, Aziida Nanyonga2*, Alaa O. Khadidos3,4, Olfat M. Mirza5, Mustafa Tahsin Yilmaz6",,,"Deep Learning, Convolutional Neural Networks, Pediatric Pneumonia, Chest X-Ray, Image Analysis, Automated Diagnosis","This study compares two state-of-the-art convolutional neural network architectures, DenseNet121 and EfficientNet-B0, for automated pediatric pneumonia detection in chest X-ray images. The models achieve high classification performance and sensitivity to pneumonia detection, with EfficientNet-B0 outperforming DenseNet121 in terms of accuracy and F1-score.",28.47,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09822v2_LLM-Based Agentic Systems for Software Engineering.pdf,LLM-Based Agentic Systems for Software Engineering: Challenges and Opportunities,"Yongjian Tang, Thomas Runkler",Not found,2601.09822,"LLMs, Agents, Software Engineering, Future Challenges","This concept paper reviews the emerging paradigm of LLM-based multi-agent systems in Software Engineering, examining their applications across the Software Development Life Cycle (SDLC) from requirements engineering to testing and debugging. It identifies key challenges and future research opportunities in multi-agent orchestration, human-agent coordination, computational cost optimization, and effective data collection.",27.03,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09841v2_A pipeline for enabling path-specific causal fairn.pdf,APIPIPELINE FOR ENABLING PATH-SPECIFIC CAUSAL FAIRNESS IN OBSERVATIONAL HEALTH DATA,"Aparajita Kashyap, Sara Matijevic, Noémie Elhadad, Steven A. Kushner, Shalmali Joshi",Not found,2601.09841v2,"causal fairness, foundation models, causal inference, observational health data, fair machine learning",This work presents a model-agnostic pipeline for training causally fair machine learning models that address both direct and indirect forms of healthcare bias in the observational health data setting.,27.67,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09851v1_ViSIL Unified Evaluation of Information Loss in Mu.pdf,ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning,"Po-han Li * 1, Shenghui Chen * 1, Ufuk Topcu1, Sandeep Chinchali 1",,,"Multimodal video captioning, Information loss, Video summary, Information-theoretic framework, Video Question Answering (VQA)","This paper proposes the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. It enables direct comparison across multimodal summary formats and demonstrates a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks.",27.26,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09853v1_MedRedFlag Investigating how LLMs Redirect Misconc.pdf,MedRedFlag: Investigating how LLMs Redirect Misconceptions in Real-World Health Communication,"Sraavya Sambara∗, Yuan Pu1∗, Ayman Ali1, Vishala Mishra1, Lionel Wong2, Monica Agrawal1",,,"Large language models, Health communication, Misconceptions, Redirect, Patient-facing AI","This work investigates how large language models (LLMs) handle false premises embedded in real-world health questions, focusing on redirection. A dataset of 1100+ questions from Reddit is curated, and responses from state-of-the-art LLMs are compared to those from clinicians. The analysis reveals that LLMs often fail to redirect problematic questions, even when the problematic premise is detected, leading to suboptimal medical decision-making. This highlights a significant gap in LLM performance under real-world conditions and underscores critical safety concerns for patient-facing medical AI systems.",28.13,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09855v1_Thinking Long but Short Stable Sequential Test-Tim.pdf,Stable Sequential Test-Time Scaling for Large Reasoning Models,"Michael R. Metel, Yufei Cui, Boxing Chen, Prasanna Parthasarathi",,2601.09855,"Large Reasoning Models, Sequential Test-Time Scaling, Model Accuracy, Training-Free Method, Test-Time Scaling, Sequential Scaling, Parallel Scaling, Chain-of-Thought Reasoning, Model Stability, Interpretable Reasoning, DeepSeek","This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling and removing the need for reasoning length fine-tuning. It is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model’s maximum context length, and under mild conditions has linear computational complexity.",27.85,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09858v1_OUTLINEFORGE Hierarchical Reinforcement Learning w.pdf,OUTLINEFORGE: Hierarchical Reinforcement Learning with Explicit States for Scientific Writing,"Yilin Bao, Ziyao He, Zayden Yang",Not found,Not found,"Reinforcement Learning, Hierarchical Structures, Scientific Writing, Document-Level Planning, Long-Horizon Planning, Factual Grounding, Scientific Correctness, Discourse Coherence, Citation Fidelity","This paper presents a reinforcement learning framework that casts scientific outline construction as a long-horizon planning problem over hierarchical document structures. The approach models edit evolving outlines through structured actions, enabling the system to incrementally build a complete scientific manuscript. The authors introduce a two-stage optimization procedure to support effective and stable learning, including backward outline reconstruction and forward value-guided reinforcement learning with rewards explicitly modeling scientific correctness, discourse coherence, and citation fidelity. The paper also introduces a benchmark for scientific paper generation that evaluates document planning, input utilization, reference faithfulness, outline organization, and content-level factual accuracy, showing consistent improvements over strong neural and LLM baselines, particularly in long-range structural coherence and citation reliability.",28.86,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09865v1_Advancing Model Refinement Muon-Optimized Distilla.pdf,Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment,"Jacob Sander, Brian Jalaian, V enkat R. Dasarivenkateswara",Not found,2601.09865,"Large Language Models, Quantization, Distillation, Edge Devices, Model Compression","This paper proposes an integrated framework combining GPTQ-based quantization, low-rank adaptation (LoRA), and a specialized data distillation process to significantly reduce model size and complexity while preserving or enhancing task-specific performance. By leveraging data distillation, knowledge distillation via Kullback-Leibler divergence, Bayesian hyperparameter optimization, and the Muon optimizer, our pipeline achieves up to 2×memory compression and enables efficient inference for specialized tasks.",28.39,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09869v1_A Scoping Review of the Ethical Perspectives on An.pdf,A SCOPING REVIEW OF THE ETHICAL PERSPECTIVES ON ANTHROPOMORPHISING LARGELANGUAGEMODEL-BASED CONVERSATIONAL AGENTS,"Andrea Ferrario, Rasita Vinay, Matteo Casserini, Alessandro Facchini",Not found,2601.09869,"anthropomorphism, conversational agents, large language models, AI ethics, deception, trust, governance","This scoping review examines ethical perspectives on anthropomorphizing large language model-based conversational agents, highlighting the dual nature of these systems in terms of engagement and ethical concerns. Despite increasing interest, the literature remains fragmented and lacks consistent definitions, operationalizations, and empirical evidence linking observed interaction effects to actionable governance guidance.",27.81,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09871v1_Epistemology gives a Future to Complementarity in .pdf,EPISTEMOLOGY GIVES AFUTURE TOCOMPLEMENTARITY IN HUMAN-AI INTERACTIONS,"Andrea Ferrario∗1,2,3, Alessandro Facchini2,4, Juan M. Durán5",Not found,2601.09871,"artificial intelligence, machine learning, reliance, complementarity, human-AI interaction, computational reliabilism, epistemology","This work leverages epistemology to address the theoretical challenges of complementarity in human-AI interactions, reframing it within the discourse on justificatory AI. By drawing on computational reliabilism, the authors argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a predictive task. This supports the practical reasoning of those affected by these outputs, such as patients, managers, regulators, and others.",27.92,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09879v1_MedVL-SAM2 A unified 3D medical vision-language mo.pdf,MedVL-SAM2: A unified 3D medical vision–language model for multimodal reasoning and prompt-driven segmentation,"Yang Xing, Jiong Wu, Savas Ozdemir, Ying Zhang, Yang Yang, Wei Shao, Kuang Gong",,,"3D medical vision-language model, multimodal reasoning, prompt-driven segmentation, 3D medical imaging, volumetric segmentation, semantic segmentation, referring segmentation, interactive segmentation","MedVL-SAM2 is a unified 3D medical vision-language model that supports report generation, visual question answering, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. It integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging and incorporates a volumetric segmentation module. The model is trained in a multi-stage pipeline, first on a large-scale corpus of 3D CT image-text pairs and then jointly optimized with a comprehensive 3D CT segmentation dataset. This enables flexible interaction via language, point, or box prompts, unifying high-level visual reasoning with spatially precise localization.",28.7,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09881v1_Transition Matching Distillation for Fast Video Ge.pdf,Transition Matching Distillation for Fast Video Generation,"Weili Nie, Julius Berner, Nanye Ma, Chao Liu, Saining Xie",Not found,2601.09881v1,"video generation, diffusion models, few-step generators, distillation","This work presents Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators, addressing the inefficiency of multi-step sampling in real-time applications.",28.66,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09883v1_Beyond Rule-Based Workflows An Information-Flow-Or.pdf,Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL,"Xinxing Ren1,2, Quagmire Zang3, Caelum Forder1, Suman Deb1, Ahsen Tahir1, Roman J. Georgio1, Peter Carroll1, Zekun Guo4, † Corresponding author",Not found,Not found,"Large Language Model, Multi-Agent System, Information-Flow-Orchestrated, Agent-to-Agent Communication, CORAL Protocol","Most existing Large Language Model-based Multi-Agent Systems rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. This workflow-driven design suffers from two fundamental limitations: substantial manual effort to anticipate and encode possible task states, and inability to exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models.",28.63,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09896v1_The Algorithmic Gaze An Audit and Ethnography of t.pdf,The Algorithmic Gaze: An Audit and Ethnography of the LAION-Aesthetics Predictor Model,"Jordan Taylor, William Agnew, Maarten Sap, Sarah E. Fox, Haiyi Zhu",10.1145/nnnnnnn.nnnnnnn,,"AI, Art, Aesthetic Evaluation","This work examines an aesthetic evaluation model LAION Aesthetic Predictor (LAP) used to curate datasets for training visual generative image models and evaluates the quality of AI-generated images. It finds biases in LAP's aesthetic filtering, reinforcing imperial and male gazes in western art history. The authors perform a digital ethnography to understand the origins of these biases and call for more pluralistic evaluation in AI development.",27.2,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09902v1_A Novel Contrastive Loss for Zero-Day Network Intr.pdf,A Novel Contrastive Loss for Zero-Day Network Intrusion Detection,"Jack Wilkie, Hanan Hindy, Craig Michie, Christos Tachtatzis, James Irvine, Robert Atkinson",,,"Machine Learning, Network Intrusion Detection, Zero-Day Attacks, Contrastive Learning, Anomaly Detection","This work proposes a novel contrastive loss function for zero-day network intrusion detection, which maintains the advantages of contrastive learning-based approaches while achieving significant performance improvements in both known and zero-day attack detection.",26.78,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09913v1_Continuum Memory Architectures for Long-Horizon LL.pdf,Continuum Memory Architectures for Long-Horizon LLM Agents,"Joe Logan, Mode7 GK",Not found,2601.09913,"large language models, memory architectures, continuum memory, retrieval-augmented generation, temporal continuity, contextual knowledge","This paper introduces Continuum Memory Architectures (CMA), a class of systems designed to maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. The authors argue that current retrieval-augmented generation (RAG) systems treat memory as a stateless lookup table, leading to structural limitations in accumulating, mutating, or disambiguating memory. CMA aims to address these limitations by providing a structured, continuously evolving memory subsystem for long-horizon LLM agents, offering advantages in tasks that stress memory dynamics.",28.46,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09921v1_Learning to Decode in Parallel Self-Coordinating N.pdf,Learning to Decode in Parallel: Self-Coordinating Neural Network for Real-Time Quantum Error Correction,"Kai Zhang, Zhengzhong Yi, Shaojun Guo, Linghang Kong, Situ Wang, Xiaoyu Zhan, Tan He, Weiping Lin, Tao Jiang, Dongxin Gao, Yiming Zhang, Fangming Liu, Fang Zhang, Zhengfeng Ji, Fusheng Chen, Jianxin Chen",,2601.09921,"quantum error correction, neural networks, parallel decoding, fault-tolerant quantum computation","This paper presents a self-coordinating neural network decoder designed for real-time quantum error correction, demonstrating improved accuracy compared to traditional human-designed algorithms.",28.83,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09923v1_CaMeLs Can Use Computers Too System-level Security.pdf,CAMELSCANUSECOMPUTERSTOO: SYSTEM-LEVELSECURITY FORCOMPUTERUSEAGENTS,"Hanna Foerster∗, Robert Mullins, Tom Blanchard∗, Nicolas Papernot, Kristina Nikolić, Florian Tramèr, Ilia Shumailov, Cheng Zhang, Yiren Zhao",Not found,2601.09923,"AI agents, prompt injection attacks, system-level security, Computer Use Agents, control flow integrity, branch steering attacks","AI agents, which automate tasks by viewing user interfaces, are vulnerable to prompt injection attacks. Current agents require continuous observation of UI state to determine each action, conflicting with the architectural isolation needed for security. This paper introduces Single-Shot Planning for CUAs, a design that generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. However, additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan.",28.44,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09929v1_Hallucination Detection and Mitigation in Large La.pdf,Hallucination Detection and Mitigation in Large Language Models,"Ahmad Pesaranghader, Erin Li",Not found,2601.09929v1,"hallucination, large language models, generative AI, reliability, financial chatbots, legal AI","This paper introduces a comprehensive operational framework for hallucination management in large language models and large reasoning models, addressing the critical reliability risk posed by their tendency to hallucinate, generating factually incorrect or unsupported content.",27.03,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09933v1_Malware Classification using Diluted Convolutional.pdf,Malware Classification using Diluted Convolutional Neural Network with Fast Gradient Sign Method,"Ashish Anand, Bhupendra Singh, Sunil Khemka, Bireswar Banerjee, Vishi Singh Bhatia, Piyush Ranjan",,,"data security, diluted convolutional neural network, fast gradient sign method, malware classification, privacy","A ndroid malware has become an increasingly critical threat, and this research proposes a Fast Gradient Sign Method with Diluted Convolutional Neural Network (FGSM -DICNN) for malware classification. DICNN contains diluted convolutions to capture dispersed malware patterns using fewer features. FGSM enhances accuracy with one-step perturbations. The proposed FGSM-DICNN model attains 99.44% accuracy, outperforming other approaches.",27.8,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09949v2_Kinematic Tokenization Optimization-Based Continuo.pdf,Kinematic Tokenization: Optimization-Based Continuous-Time Tokens for Learnable Decision Policies in Noisy Time Series,"Griffin M. Kearney, Ph.D.",Not found,2601.09949,"Transformers, Continuous-time, Optimization, Tokenization, Noisy Time Series, Financial Markets, Decision Policies, Learning","This paper introduces Kinematic Tokenization, an optimization-based continuous-time representation that reconstructs an explicit spline from noisy measurements and tokenizes local spline coefficients. Applied to financial time series data, it improves the learnability and calibration of selective decision policies under abstention-inducing losses.",28.09,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09966v1_A Sustainable AI Economy Needs Data Deals That Wor.pdf,A Sustainable AI Economy Needs Data Deals That Work for Generators,"Ruoxi Jia∗, Luis Oala∗, Wenjie Xiong, Suqin Ge, Jiachen T. Wang, Feiyang Kang, Dawn Song",Not found,2601.09966v1,"Sustainable AI, Data Deals, Data Processing Inequality, Machine Learning Value Chain, Data Aggregation, Data Generators","The authors argue that the machine learning value chain is structurally unsustainable due to an economic data processing inequality, where data generators often receive little attribution despite the majority of value accruing to aggregators. They identify three structural faults—missing provenance, asymmetric bargaining power, and non-dynamic pricing—as the operational machinery of this inequality. They propose an Equitable Data-Value Exchange (EDVEX) Framework to enable a minimal market that benefits all participants.",28.14,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09972v1_Chinese Labor Law Large Language Model Benchmark.pdf,Chinese Labor Law Large Language Model Benchmark,"Zixun Lan, Maochun Xu, Yifan Ren, Rui Wu, Jianghui Zhou, Xueyang Cheng, Jian’an Ding, Xinheng Wang, Mingmin Chi, Fei Ma",,,"Chinese labor law, legal natural language processing, large language models, domain-specific fine-tuning, benchmark dataset, statute recall, legal reasoning, case analysis","This paper presents LaborLawLLM, a legal large language model specifically tailored to the labor law domain, and constructs LabourLawBench, a comprehensive benchmark for diverse labor law tasks. Experimental results show that LaborLawLLM significantly outperforms both general-purpose and existing legal-specific LLMs across all task categories.",27.52,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09974v1_SPRInG Continual LLM Personalization via Selective.pdf,SPRInG: Continual LLM Personalization via Selective Parametric Adaptation and Retrieval-Interpolated Generation,"Seoyeon Kim, Jaehyung Kim",,,"Personalization, Large Language Models, Continual Learning, Selective Adaptation, Retrieval-Interpolated Generation","Introducing SPRING, a novel semi-parametric framework for effective continual personalization of Large Language Models, which employs drift-driven selective adaptation to identify high-novelty interactions and selectively update user-specific adapters while preserving hard-to-learn residuals in a replay buffer. During inference, parametric knowledge is fused with retrieved history via logit interpolation, demonstrating superior performance compared to existing baselines on a long-form personalized generation benchmark.",27.26,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09980v1_Performance of AI agents based on reasoning langua.pdf,ALD optimization using reasoning LLMs,Angel Yanguas-Gil,,,"Atomic Layer Deposition, Large Language Models, Process Optimization, Self-Limited Processes","This work explores the performance and behavior of reasoning large language models in autonomously optimizing atomic layer deposition (ALD) processes. Agents built on reasoning models like OpenAI’s o3 and GPT5 succeeded at completing this optimization task, but significant run-to-run variability was observed due to the non-deterministic nature of the model's response. The agent uses a two-step process to generate open reasoning traces, which are then transformed into structured outputs. Analysis of these traces showed that the model's logic was sound and based on self-limited process and saturation notions, but it can sometimes be misled by its own prior choices.",27.36,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.09982v1_Context Volume Drives Performance Tackling Domain .pdf,Context Volume Drives Performance: Tackling Domain Shift in Extremely Low-Resource Translation via RAG,"David Samuel Setiawan, Raphaël Merx, Jey Han Lau",,,"Neural Machine Translation, Low-resource languages, Domain shift, Retrieval-Augmented Generation, Hybrid framework","This paper addresses the significant performance degradation of Neural Machine Translation models for low-resource languages, specifically focusing on the transition from the New Testament (NT) to the Old Testament (OT) in the Dhao language. The authors introduce a hybrid framework combining a fine-tuned NMT model with a Large Language Model (LLM) using Retrieval-Augmented Generation (RAG) to refine the initial draft generated by the NMT model, achieving a chrF++ score of 35.21, effectively matching the original in-domain quality. The analysis reveals that the performance recovery is driven by the number of retrieved examples rather than the choice of retrieval algorithm, and the LLM acts as a robust ",27.45,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10010v1_VERHallu Evaluating and Mitigating Event Relation .pdf,VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video Large Language Models,"Zefan Zhang, Kehua Zhu, Shijie Jiang, Hongyuan Lu, Shengkai Sun, Tian Bai*",,,"Video Large Language Models, Event Relation Understanding, Video Understanding","This paper introduces a novel benchmark, VERHallu, for evaluating and mitigating event relation hallucination in Video Large Language Models (VideoLLMs). It focuses on causal, temporal, and subevent relations between events, covering relation classification, question answering, and counterfactual question answering tasks. The benchmark features counterintuitive video scenarios with human-annotated candidates. The authors analyze current state-of-the-art VideoLLMs' struggles with dense-event relation reasoning and propose a Key-Frame Propagating (KFP) strategy to mitigate event relation hallucination.",27.35,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10011v1_Memo-SQL Structured Decomposition and Experience-D.pdf,Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL,"Zerui Yang1, Weichuan Wang1, Yanwei Xu*, Linqi Song†, Yudai Matsuda1, Wei Han2, Bo Bai2",Not provided,Not provided,"NL2SQL, training-free, self-correction, structured decomposition, dynamic memory, retrieval-augmented prompting","Presenting Memo-SQL, a training-free framework that addresses the limitations of existing NL2SQL systems by introducing structured decomposition and experience-aware self-correction. It achieves 68.5% execution accuracy on BIRD, setting a new state of the art among open, zero-fine-tuning methods, while using over 10× fewer resources than prior test-time scaling approaches.",27.37,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10018v1_Empowering Older Adults in Digital Technology Use .pdf,Empowering Older Adults in Digital Technology Use with Foundation Models,"Hasti Sharifi, Homaira Huda Shomee, Sourav Medya, Debaleena Chattopadhyay",,,"Technology support, Digital technology use, Artificial intelligence, Large language models, Communication barriers, Human-computer interaction","This study examines communication challenges older adults face when using digital applications and explores AI-based approaches to mitigate these challenges. It identifies four key communication challenges: verbosity, incompleteness, over-specification, and under-specification. The study uses a diary study with older adults to collect technology-related queries and employs reflexive thematic analysis to identify communication barriers. It evaluates how foundation models can paraphrase older adults' queries to improve solution accuracy. Controlled experiments with younger and older adults evaluate the effectiveness of AI-rephrased queries and solutions. The study also develops a pipeline using large language models to generate a synthetic dataset of older adults' tech support requests (OATS). The results show that AI-rephrased queries significantly improve solution accuracy and Google search results. Younger adults better understand AI-rephrased queries, while older adults report high perceived ability to answer contextual questions and follow solutions. The OATS dataset demonstrates strong fidelity and face validity, offering a scalable resource for developing equitable AI systems that better serve aging populations.",29.15,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10025v1_Structured Personality Control and Adaptation for .pdf,Structured Personality Control and Adaptation for LLM Agents,"Jinpeng Wang, Xinyu Jia, Wei Wei Heng, Yuquan Li, Binbin Shi, Qianlei Chen, Guannan Chen, Junxia Zhang, Yuyu Yin",XXXXXXX.XXXXXXX,,"Personalization, Jungian Psychological Types, MBTI Personality Types, Persona Adaptation, Explainable AI","This paper presents a framework for modeling LLM personality via Jungian psychological types, integrating mechanisms for coherent core expression, temporary adaptation, and long-term evolution. The framework aims to support coherent, context-sensitive interactions and naturalistic agent design in HCI.",27.33,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10029v1_PaperScout An Autonomous Agent for Academic Paper .pdf,PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization,"Tingyue Pan, Jie Ouyang, Mingyue Cheng, Qingchuan Li, Zirui Liu, Mingfan Pan, Shuo Yu, Qi Liu",,,"academic paper search, autonomous agent, sequence-level policy optimization, process-aware, reinforcement learning, fine-grained control","PaperScout is an autonomous agent designed to reformulate academic paper search as a sequential decision-making process. Unlike static workflows, it dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. The authors introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance.",27.87,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10031v1_FilDeep Learning Large Deformations of Elastic-Pla.pdf,FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data,"Jianheng Tang, Shilong Tao, Zhe Feng, Haonan Sun, Menglu Wang, Zhanxing Zhu, Yunhuai Liu",10.1145/3770854.3783959,,"Large Deformations, Elastic-Plastic Solids, Multi-Fidelity Data, Deep Learning, Quantity-Accuracy Dilemma","This work proposes FilDeep, a Fidelity-based Deep Learning framework for large deformations of elastic-plastic solids, addressing the challenge of obtaining high-quantity and high-accuracy datasets in large deformation problems.",27.18,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10038v1_What Understanding Means in AI-Laden Astronomy.pdf,What Understanding Means in AI-Laden,"Yuan-Sen Ting1,2,3*, André Curtis-Trudel4,5, Siyu Y ao6",,2601.10038,"AI, astronomy, philosophy of science, understanding","The rapid integration of artificial intelligence in astronomy raises questions about the nature of discovery, progress, and understanding. Philosophers and historians of science are essential in this transformation, offering tools for conceptual engineering, critical examination of assumptions, and frameworks for abstraction. This paper discusses the challenges and opportunities in understanding phenomena with AI, emphasizing the need for interdisciplinary collaboration.",27.21,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10073v1_ReaMIL Reasoning- and Evidence-Aware Multiple Inst.pdf,ReaMIL: Reasoning- and Evidence-Aware Multiple Instance Learning for Whole-Slide Histopathology,"Hyun Do Jung1, Jungwon Choi2, Hwiyoung Kim1*",,,"Histopathology, Multiple Instance Learning, Whole-slide imaging, Reasoning, Evidence-aware","Introduces ReaMIL, a multiple instance learning approach for whole-slide histopathology that adds a light selection head to a strong MIL backbone. The head produces soft per-tile gates and is trained with a budgeted-sufficiency objective, yielding small, spatially compact evidence sets without sacrificing baseline performance. Across TCGA-NSCLC (LUAD vs. LUSC), TCGA-BRCA (IDC vs. Others), and PANDA, ReaMIL matches or slightly improves baseline AUC and provides quantitative evidence-efficiency diagnostics. On NSCLC, it attains AUC 0.983 with a mean minimal suffi- cient K (MSK)≈8.2tiles atτ= 0.90and AUKC≈0.864, showing that class confidence rises sharply and stabilizes once a small set of tiles is kept. The method requires no extra supervision, integrates seamlessly with standard MIL training, and naturally yields slide-level overlays.",28.91,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10079v1_Sparse-RL Breaking the Memory Wall in LLM Reinforc.pdf,Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts,"Sijia Luo, Xiaokang Zhang, Yuxuan Hu, Bohan Zhang, Ke Wang, Jinbo Su, Mengshu Sun, Lei Liang, Jing Zhang",Not provided,Not provided,"Reinforcement Learning, Large Language Models, Memory Overhead, Sparse Rollouts, KV Compression, Stability",Sparse-RL addresses the memory bottleneck in Long-Range Rollouts (LRR) for Large Language Model (LLM) Reinforcement Learning (RL) by introducing Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct off-policy bias introduced by compression-induced information loss. This approach reduces rollout overhead compared to dense baselines while preserving performance.,27.09,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10088v1_State of AI An Empirical 100 Trillion Token Study .pdf,State of AI: An Empirical 100 Trillion Token Study with OpenRouter,"Malika Aubakirova∗†, Alex Atallah ‡, Chris Clark ‡, Justin Summerville ‡, Anjney Midha †",Not found,Not found,"large language models, AI inference, open-router, 100 trillion tokens, reasoning models, open-weight models, creative roleplay, coding assistance, agentic inference, cinderella effect","This empirical study analyzes over 100 trillion tokens of real-world interactions with large language models (LLMs) across various tasks, geographies, and time. It observes substantial adoption of open-weight models, the popularity of creative roleplay beyond productivity tasks, and the rise of agentic inference. The study identifies foundational cohorts of early users whose engagement persists longer than later cohorts, termed the Cinderella ",28.81,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10090v1_Difficulty-guided Sampling Bridging the Target Gap.pdf,Difficulty-guided Sampling: Bridging the Target Gap between Dataset Distillation and Downstream Tasks,"Mingzhuo Lia, Guang Lia, Linfeng Ye, Jiafeng Mao, Takahiro Ogawa, Konstantinos N. Plataniotis, Miki Haseyama",Not found,2601.10090,"Dataset distillation, Downstream tasks, Difficulty, Post-stage sampling","This paper proposes difficulty-guided sampling (DGS) to bridge the target gap between dataset distillation and downstream tasks, improving dataset distillation performance.",27.72,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10092v1_LeMoF Level-guided Multimodal Fusion for Heterogen.pdf,LEMOF: LEVEL-GUIDED MULTIMODAL FUSION FOR HETEROGENEOUS CLINICAL DATA,"Jongseok Kim ∗, Seongae Kang ∗, Jonghwan Shin, Yuhan Lee, Ohyun Jo",Not found,2601.10092,"Multimodal Learning, Hierarchical Representation Learning, Clinical Time-Series Modeling, Level-guided Feature Fusion, Explainable Medical AI","This paper proposes Level-guided Modal Fusion (LeMoF), a novel framework for integrating heterogeneous clinical data, such as Electronic Health Records and biosignals. LeMoF selectively integrates level-guided representations within each modality, enabling balanced performance between prediction stability and discriminative capability in heterogeneous clinical environments.",27.64,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10094v1_V-Zero Self-Improving Multimodal Reasoning with Ze.pdf,V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation,"Han Wang1*, Yi Yang1*, Jingyuan Hu2*, Minfeng Zhu2†",Not found,Not found,"multimodal reasoning, self-improvement, unsupervised learning, vision-language models","A self-improvement framework for vision-language models (VLMs) that uses exclusively unlabeled images, establishing a co-evolutionary loop between a Questioner and a Solver to enhance the model's reasoning capabilities without human annotation.",26.38,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10101v2_Matrix as Plan Structured Logical Reasoning with F.pdf,Matrix as Plan: Structured Logical Reasoning with Feedback-Driven Replanning,"Ke Chen, Jiandian Zeng, Zihao Peng, Guo Li, Guangxue Zhang, Tian Wang",10.1145/XXXXXX.XXXXXX,,"Logical Reasoning, Large Language Models, Neurosymbolic Approaches, Semantic Decomposition","This paper proposes MatrixCoT, a structured CoT framework with a matrix-based plan, to enhance the logical reasoning capabilities of Large Language Models (LLMs). It addresses the limitations of current methods by normalizing and typing natural language expressions, introducing a matrix-based planning method, and adding a feedback-driven replanning mechanism. The approach improves robustness and interpretability without relying on external solvers.",27.64,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10103v1_FlowAct-R1 Towards Interactive Humanoid Video Gene.pdf,FlowAct-R1: Towards Interactive Humanoid Video Generation,"Lizhen Wang, Yongming Zhu, Zhipeng Ge, Youwei Zheng, Longhao Zhang, Tianshu Hu, Shiyang Qin, Mingshuang Luo, Jiaxu Zhang, Xin Chen, Yulong Wang, Zerong Zheng, Jianwen Jiang, Chao Liang, Weifeng Chen, Xing Wang, Yuan Zhang, Mingyuan Gao",Not found,2601.10103,"Humanoid video generation, Interactive video, Real-time synthesis, Diffusion models, Temporal consistency","This paper presents FlowAct-R1, a framework designed for real-time interactive humanoid video generation, which achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of around 1.5 seconds. It introduces a chunkwise diffusion forcing strategy to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction.",29.48,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10104v1_MathDoc Benchmarking Structured Extraction and Act.pdf,MathDoc: Benchmarking Structured Extraction and Active Refusal on Noisy Mathematics Exam Papers,"Chenyue Zhou, Jiayi Tuo, Shitong Qin, Wei Dai, Mingxuan Wang, Ziwei Zhao, Duoyang Li, Shiyang Su, Yanxi Lu, Yanbiao Ma",Not provided,Not provided,"Mathematics education, Document-level extraction, Active refusal, Noisy documents, Machine learning","The automated extraction of structured questions from paper-based mathematics exams is fundamental to intelligent education. However, existing benchmarks mainly focus on clean documents or generic layout analysis, overlooking the structural integrity of mathematical problems and the ability of models to actively reject incomplete inputs. MathDoc introduces the first benchmark for document-level information extraction from authentic high school mathematics exam papers, including 3,609 carefully curated questions with real-world artifacts and explicitly including unrecognizable samples to evaluate active refusal behavior. The multi-dimensional evaluation framework covers stem accuracy, visual similarity, and refusal capability, showing that current MLLMs, including Qwen3-VL and Gemini-2.5-Pro, achieve strong extraction performance but consistently fail to refuse illegible inputs, instead producing confident but invalid outputs. These results highlight a critical gap in current MLLMs and establish MathDoc as a benchmark for assessing model reliability under degraded document conditions.",27.91,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10108v1_SIN-Bench Tracing Native Evidence Chains in Long-C.pdf,SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal,"Yiming Ren, Junjie Wang, Yuxin Meng, Yihang Shi, Zhiqiang Lin, Ruihang Chu, Yiran Xu, Ziming Li, Yunfei Zhao, Zihan Wang, Yu Qiao, Ruiming Tang, Minghao Liu, Yujiu Yang",Not found,Not found,"Multimodal, Long-form, Scientific literature, Evidence chains, Fish-in-the-Ocean, SIN-Data, SIN-Find, SIN-Verify, SIN-QA, SIN-Summary","SIN-Bench proposes the 'Fish-in-the-Ocean' (FITO) paradigm to evaluate multimodal large language models' understanding of long-form scientific papers by requiring them to construct explicit cross-modal evidence chains. It builds SIN-Data, a scientific interleaved corpus, and constructs SIN-Bench with four progressive tasks covering evidence discovery, hypothesis verification, grounded QA, and evidence-anchored synthesis. Experiments on eight MLLMs show that grounding is the primary bottleneck, with Gemini-3-pro achieving the best overall score and GPT-5 the highest answer accuracy for SIN-QA but underperforming on overall scores.",28.19,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10112v1_Repository Intelligence Graph Deterministic Archit.pdf,Repository Intelligence Graph: Deterministic Architectural Map for LLM Code Assistants,"Tsvi Cherny-Shahar, Amiram Yehudai",,,"software repositories, build systems, dependency graphs, software engineering agents, multi-lingual software","Repository aware coding agents often struggle to recover build and test structure, especially in multilingual projects where cross language dependencies are encoded across heterogeneous build systems and tooling. We introduce the Repository Intelligence Graph (RIG), a deterministic, evidence backed architectural map that represents buildable components, aggregators, runners, tests, external packages, and package managers, connected by explicit dependency and coverage edges that trace back to concrete build and test definitions. We also present SPADE, a deterministic extractor that constructs RIG from build and test artifacts (currently with an automatic CMake plugin based on the CMake File API and CTest metadata), and exposes RIG as an LLM friendly JSON view that agents can treat as the authoritative description of repository structure.",28.16,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10114v1_Following the Teachers Footsteps Scheduled Checkpo.pdf,Following the Teacher’s Footsteps: Scheduled Checkpoint Distillation for Domain-Specific LLMs,"Cheng Feng, Chaoliang Zhong, Jun Sun, Yusuke Oishi",Not found,2601.10114,"LLMs, Knowledge Distillation, Domain-specific Tasks","This work proposes Scheduled Checkpoint Distillation (SCD) to address the challenges of deploying large language models (LLMs) for domain-specific tasks, which often suffer from a capacity gap between the teacher and student models. SCD reduces the deficit on the Teacher-Favored Subdomain (TFS) by emulating the teacher's convergence process during supervised fine-tuning (SFT) and uses a sample-wise Adaptive Weighting (AW) mechanism to preserve strengths on the Student-Favored Subdomain (SFS). Experiments across various domain tasks demonstrate that SCD consistently outperforms existing distillation approaches, allowing the student model to match or even exceed the performance of its fine-tuned teacher.",28.86,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10120v1_TopoDIM One-shot Topology Generation of Diverse In.pdf,TopoDIM: One-shot Topology Generation of Diverse Interaction Modes for Multi-Agent Systems,"Rui Sun, Jie Ding, Chenghua Gong, Tianjun Gu, Yihang Jiang, Juyuan Zhang, Liming Pan, Linyuan Lü",,,"Multi-Agent Systems, Topology Optimization, Communication Efficiency, Token Efficiency, Debate Mechanism","Optimizing communication topology in LLM-based multi-agent systems is critical for enabling collective intelligence. Existing methods mainly rely on spatio-temporal interaction paradigms, leading to high latency and computation costs. TOPODIM, a framework for one-shot topology generation with diverse interaction modes, enhances adaptability and privacy by enabling decentralized autonomous construction of heterogeneous communication without iterative coordination, achieving token efficiency and improved task performance. Experiments show that TOPODIM reduces total token consumption by 46.41% while improving average performance by 1.50% over state-of-the-art methods.",27.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10122v1_Role-Playing Agents Driven by Large Language Model.pdf,"Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future Trends","Ye Wang, Jiaxing Chen, Hongjiang Xiao",Not found,2601.10122,"role-playing agents, large language models, natural language processing, human-computer interaction, character modeling, memory mechanisms, behavioral decision control, corpora construction, evaluation methods, personality fidelity, interactive hallucination","This paper systematically reviews the current development and key technologies of role-playing language agents (RPLAs), covering their evolution from rule-based template paradigms to cognitive simulation centered on personality modeling and memory mechanisms. It also analyzes the challenges and future directions in constructing role-specific corpora and evaluating RPLAs.",28.18,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10129v1_LaViT Aligning Latent Visual Thoughts for Multi-mo.pdf,LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning,"Linquan Wu, Tianxiang Jiang, Yifei Dong, Haoyu Yang, Fengji Zhang, Shichang Meng, Ai Xuan, Linqi Song, Jacky Keung",https://doi.org/missing,missing,"Knowledge Distillation, Latent Reasoning, Multi-modal Learning, Visual Attention, Language Prior","Current multimodal latent reasoning often relies on external supervision, ignoring intrinsic visual attention dynamics. This work proposes LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher’s visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4.",28.1,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10130v1_Redundancy-Driven Top-k Functional Dependency Disc.pdf,Redundancy-Driven Top-k Functional Dependency Discovery,"Xiaolong Wan, Xixian Han",,,"Functional dependency, top-k discovery, data redundancy, pruning strategy","Proposes SDP (Selective-Discovery-and-Prune), an algorithm that discovers the top-k functional dependencies ranked by redundancy count, aiming to address the computational and storage overhead issues of exhaustive discovery methods.",24.67,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10131v2_M4olGen Multi-Agent Multi-Stage Molecular Generati.pdf,"M4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints","Yizhan Li, Florence Cloutier, Sifan Wu, Ali Parviz, Boris Knyazev, Yan Zhang, Glen Berseth, Bang Liu",Not found,Not found,"molecular generation, multi-agent, multi-stage, precise constraints, numeric reasoning, reinforcement learning, fragment-level optimization","Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. This paper introduces M4olGen, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. It leverages fragments and supports controllable refinement toward numeric targets, outperforming strong LLMs and graph-based algorithms in experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO).",27.63,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10132v1_Is More Context Always Better Examining LLM Reason.pdf,Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction,"Yanan Cao∗, Farnaz Fallahi∗, Murali Mohana Krishna Dandu∗, Lalitesh Morishetti∗, Kai Zhao†, Luyi Ma, Sinduja Subramaniam, Jianpeng Xu, Evren Korpeoglu, Kaushiki Nag, Sushant Kumar, Kannan Achan",10.1145/XXXXXX.XXXXXX,,"Large Language Models, Temporal Reasoning, Inter-Purchase Interval Prediction","This paper investigates whether Large Language Models (LLMs) can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. It benchmarks state-of-the-art LLMs against statistical and machine-learning models in zero-shot settings and finds that while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, highlighting their limited ability to capture quantitative temporal structure. Additionally, moderate context can improve LLM accuracy, but adding further user-level detail degrades performance, challenging the assumption that 'more context leads to better reasoning.'",28.02,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10137v1_Step-by-Step Causality Transparent Causal Discover.pdf,Step-by-Step Causality: Transparent Causal Discovery with Multi-Agent,"Ziyi Ding * 1, Chenfei Ye-Hao* 1, Zheyuan Wang 2, Xiao-Ping Zhang 1",,,"causal discovery, multi-agent, tree-query, adversarial confidence estimation","This paper introduces Tree-Query, a tree-structured, multi-expert LLM framework that reduces pairwise causal discovery to a short sequence of queries about backdoor paths, (in)dependence, latent confounding, and causal direction, yielding interpretable judgments with robustness-aware confidence scores. Theoretical guarantees are provided for asymptotic identifiability of four pairwise relations. On data-free benchmarks derived from Mooij et al. and UCI causal graphs, Tree-Query improves structural metrics over direct LLM baselines, and a diet–weight case study illustrates confounder screening and stable, high-confidence causal conclusions.",27.67,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10141v1_Understanding and Preserving Safety in Fine-Tuned .pdf,Understanding and Preserving Safety in Fine-Tuned LLMs,"Jiawen Zhang, Zhejiang University, kevinzh@zju.edu.cn, Yangfan Hu, University of Wisconsin–Madison, yhu557@wisc.edu, Kejia Chen, Zhejiang University, chenkejia@zju.edu.cn, Lipeng He, University of Waterloo, lipeng.he@uwaterloo.ca, Jiachen Ma, Shanghai Artificial Intelligence Laboratory, majiachen@pjlab.org.cn, Jian Lou, Sun Yat-sen University, louj5@mail.sysu.edu.cn, Dan Li, Sun Yat-sen University, lidan263@mail.sysu.edu.cn, Jian Liu, Zhejiang University, liujian2411@zju.edu.cn, Xiaohu Yang, Zhejiang University, yangxh@zju.edu.cn, Ruoxi Jia, Virginia Tech, ruoxijia@vt.edu",,,"fine-tuning, large language models, safety alignment, gradient analysis, downstream tasks, jailbreak attacks, robustness, dynamic jailbreak attacks","Fine-tuning large language models (LLMs) is essential for applying them to specific tasks. However, it can degrade safety alignment, especially when fine-tuning data is harmless. This work addresses the dilemma by shedding light on the geometric interaction between safety- and utility-oriented gradients in safety-aligned LLMs. It proposes SPF, a lightweight approach that explicitly removes gradient components conflicting with the low-rank safety subspace, ensuring utility convergence while bounding safety drift. SPF maintains downstream task performance and recovers nearly all pre-trained safety alignment, even under adversarial fine-tuning scenarios. It also exhibits robust resistance to deep fine-tuning and dynamic jailbreak attacks.",29.09,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10143v1_History Is Not Enough An Adaptive Dataflow System .pdf,History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis,"Haochong Xia, Yao Long Teng, Regan Tan, Molei Qin, Xinrun Wang, Bo An",Not found,Not found,"Adaptive dataflow, Financial time-series, Data augmentation, Workflow automation, Concept drift, Distributional non-stationarity","In quantitative finance, the gap between training and real-world performance is a critical obstacle. This paper presents a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process, enhancing model robustness and improving risk-adjusted returns.",27.61,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10148v1_DecisionLLM Large Language Models for Long Sequenc.pdf,DecisionLLM: Large Language Models for Long Sequence Decision Exploration,"Xiaowei Lv, Zhiling Zhang, Yijun Li, Yusen Huo, Siyuan Ju, Xuyan Li, Chunxiang Hong, Tianyu Wang, Peng Sun, Chuan Yu, Jian Xu, Bo Zheng",,,"reinforcement learning, long sequence decision-making, large language models, trajectory modeling, offline decision making","This work investigates the application of large language models (LLMs) to offline decision-making tasks, addressing the challenge of long-sequence decision-making where an agent must make coherent decisions over protracted time steps to achieve a long-term objective. The authors propose treating trajectories as a distinct modality and learning to align trajectory data with natural language task descriptions, enabling autoregressive prediction of future decisions within a cohesive framework termed DecisionLLM. They demonstrate that performance hinges on three factors: model scale, data volume, and data quality, achieving strong performance in offline experimental benchmarks and bidding scenarios.",27.99,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10150v1_Simple Network Graph Comparative Learning.pdf,Simple Network Graph Comparative Learning,"Qiang Yu, Xinran Cheng, Shiqiang Xu, Chuanyi Li",,,"Filters, Siamese network, Graph contrastive learning, Unsupervised representation learning","The effectiveness of contrastive learning methods has been widely recognized in the field of graph learning, especially in contexts where graph data often lack labels or are difficult to label. However, the application of these methods to node classification tasks still faces a number of challenges. This study proposes a novel node classification contrast learning method called Simple Network Graph Comparative Learning (SNGCL).",26.74,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10154v1_MHub.ai A Simple Standardized and Reproducible Pla.pdf,"MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging","Leonard Nürnberg, Dennis Bontempi, Suraj Pai, Curtis Lisle, Steve Pieper, Ron Kikinis, Sil van Leemput, Rahul Soni, Gowtham Murugesan, Cosmin Ciausu, Miriam Groeneveld, Felix J. Dorfner, Jue Jiang, Aneesh Rangnekar, Harini Veeraraghavan, Joeran S. Bosma, Keno Bressem, Raymond Mak, Andrey Fedorov, Hugo JWL Aerts",,,"Artificial Intelligence, Medical Imaging, Reproducibility, Standardization","This paper introduces MHub.ai, a platform designed to facilitate the development, sharing, and reproducibility of AI models in medical imaging, aiming to standardize the process and ensure reproducibility in the field.",28.54,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10155v1_LOOKAT Lookup-Optimized Key-Attention for Memory-E.pdf,LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers,Aryan Karmore,Not found,Not found,"Transformers, Quantization, Key-Value Cache, Edge Devices, Memory Efficiency, Attention Mechanism","LOOKAT proposes a method to compress the key-value (KV) cache in large language models by applying product quantization and asymmetric distance computation to transformer architecture. This approach transforms attention from memory-bound to compute-bound, achieving significant compression ratios without requiring architectural changes or training.",26.7,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10157v1_MMPG MoE-based Adaptive Multi-Perspective Graph Fu.pdf,MMPG: MoE-based Adaptive Multi-Perspective Graph Fusion for Protein Representation Learning,"Yusong Wang1*, Jialun Shen2*, Zhihao Wu3, Yicheng Xu2, Shiyin Tan2, Mingkun Xu1†, Changshuo Wang4, Zixing Song5, Prayag Tiwari6",,,"Protein Representation Learning, Graph Neural Networks, Multi-Perspective Fusion, Mixture of Experts","MMPG is a framework that constructs protein graphs from multiple perspectives and adaptively fuses them via Mixture of Experts (MoE) for Protein Representation Learning (PRL). It addresses the limitation of current GNN-based PRL methods that rely on single-perspective graph construction strategies, which capture partial properties of residue interactions, resulting in incomplete protein representations. MMPG constructs graphs from physical, chemical, and geometric perspectives to characterize different properties of residue interactions and uses an MoE module to dynamically route perspectives to specialized experts, enabling the learning of intrinsic features and cross-perspective interactions. This approach quantitatively verifies that MoE automatically specializes experts in modeling distinct levels of interaction, from individual representations to pairwise inter-perspective synergies, and ultimately to a global consensus across all perspectives, producing superior protein representations and advanced performance on downstream protein tasks.",29.08,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10160v1_Alignment Pretraining AI Discourse Causes Self-Ful.pdf,Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment,"Cameron Tice * 1, Puria Radmard * 1, Samuel Ratnam 3, Andy Kim 4, David Africa 2, Kyle O'Brien1",not found,not found,"alignment, pretraining, AI discourse, misalignment, self-fulfilling, language models","This paper investigates the causal influence of AI discourse during pretraining on the alignment of language models. It finds that discussion of AI misalignment contributes to misalignment, while discussion of aligned behavior reduces misalignment. The findings suggest that pretraining data can shape alignment priors, and the study recommends practitioners to pretrain for alignment as well as capabilities.",27.46,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10161v1_AWED-FiNER Agents Web applications and Expert Dete.pdf,"A WED-FiNER: Agents, Web applications, and Expert Detectors for Fine-grained Named Entity Recognition across 36 Languages","Prachuryya Kaushik, Ashish Anand",,,"Named Entity Recognition, Fine-grained, 36 languages, 6.6 billion speakers, Open-source, Web applications, Expert models","A WED-FiNER is an open-source ecosystem designed to bridge the gap in Fine-grained Named Entity Recognition (FgNER) for 36 global languages spoken by over 6.6 billion people. It provides a collection of agentic toolkits, web applications, and state-of-the-art expert models for FgNER solutions across 36 languages. The agentic tools enable multilingual text routing to specialized expert models and fetch FgNER annotations within seconds. The web-based platforms provide ready-to-use FgNER annotation services for non-technical users. The collection of language-specific extremely small-sized open-source state-of-the-art expert models facilitates offline deployment in resource-constrained scenarios, including edge devices. AWED-FiNER covers languages spoken by over 6.6 billion people, including a specific focus on vulnerable languages such as Bodo, Manipuri, Bishnupriya, and Mizo.",28.26,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10168v1_RAG-3DSG Enhancing 3D Scene Graphs with Re-Shot Gu.pdf,RAG-3DSG: ENHANCING3D SCENEGRAPHS WITH RE-SHOTGUIDEDRETRIEVAL-AUGMENTEDGENERATION,"Yue Chang, Rufeng Chen, Zhaofan Zhang, Yi Chen, Sihong Xie",,,"3D scene graphs, re-shot guided retrieval, object-level recognition, downstream tasks, robotics",Proposes RAG-3DSG to enhance 3D scene graphs by mitigating aggregation noise through re-shot guided uncertainty estimation and supporting object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Demonstrates significant improvements in node captioning accuracy and reduced mapping time compared to vanilla methods.,27.48,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10169v1_CtD Composition through Decomposition in Emergent .pdf,Composition through Decomposition: Achieving Zero-Shot Generalization in Neural Agents,"Boaz Carmeli, Ron Meir, Yonatan Belinkov",,,"compositionality, neural networks, zero-shot learning, multi-target coordination, referential game","This study demonstrates how artificial neural agents acquire and utilize compositional generalization to describe previously unseen images. The method involves two sequential training steps: decomposing an image into basic concepts and composing these concepts into complex phrases. Remarkably, the agents achieve zero-shot generalization without additional training.",25.99,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10173v1_ReasAlign Reasoning Enhanced Safety Alignment agai.pdf,ReasAlign: Reasoning Enhanced Safety Alignment against Prompt Injection Attack,"Hao Li, Yankai Yang, G. Edward Suh, Ning Zhang, Chaowei Xiao",,,"Large Language Models, Safety Alignment, Prompt Injection, Structured Reasoning, Test-Time Scaling","This work presents ReasAlign, a model-level solution to improve safety alignment against indirect prompt injection attacks. The core idea is to incorporate structured reasoning steps to analyze user queries, detect conflicting instructions, and preserve the continuity of the user’s intended tasks. Comprehensive evaluations show that ReasAlign maintains utility comparable to an undefended model while consistently outperforming Meta SecAlign.",26.52,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10187v1_HOMURA Taming the Sand-Glass for Time-Constrained .pdf,HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning,"Ziang Cui, Mengran Yu, Tianjiao Li, Chenyu Shi, Yingxuan Shi, Lusheng Zhang, Hongwei Lin",Not found,Not found,"Reinforcement Learning, LLM Translation, Time-Constrained Tasks, Syllable-Level Duration Constraints","This paper introduces HOMURA, a reinforcement learning framework that optimizes the trade-off between semantic preservation and temporal compliance for time-constrained translation tasks. HOMURA employs a KL-regularized objective with a dynamic syllable-ratio reward to effectively control output length, significantly outperforming strong LLM baselines in experimental evaluations.",27.8,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10191v1_How does downsampling affect needle electromyograp.pdf,How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series,"Mathieu J.L. Cherpitel, Janne A.M. Luijten, Thomas H.W. Bäck, Camiel Verhamme, Martijn R. Tannemaat, Anna V. Kononova",,,"needle electromyography, downsampling, neuromuscular diseases, machine learning, feature-based models, high-frequency time series",This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. The workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load.,28.48,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10193v1_GFM4GA Graph Foundation Model for Group Anomaly De.pdf,GFM4GA: Graph Foundation Model for Group Anomaly Detection,"Jiujiu Chen, Weijun Zeng, Shaofeng Hu, Sihong Xie∗, Hui Xiong",XXXXXXX.XXXXXXX,,"Group Anomaly Detection, Graph Foundation Model, Graph Contrastive Learning","Group anomaly detection is crucial in many network applications, but faces challenges due to diverse anomaly patterns. GFM4GA, a novel graph foundation model, is proposed to handle group anomaly detection with fewer labeling efforts. It captures potential group anomaly structure and feature inconsistencies via dual-level contrastive learning. Experiments show that GFM4GA surpasses group anomaly detectors and GFMs for individual anomalies.",27.02,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10201v1_PRL Process Reward Learning Improves LLMs Reasonin.pdf,PRL: Process Reward Learning Improves LLMs’ Reasoning Ability,"Jiarui Yao, Ruida Wang, Tong Zhang",Not found,Not found,"Large Language Models, Reinforcement Learning, Process Reward Learning, Fine-grained Supervision, Efficiency","Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic. This paper proposes Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. PRL turns the outcome reward into process supervision signals, helping better guide the exploration during RL optimization.",27.72,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10205v1_One Instruction Does Not Fit All How Well Do Embed.pdf,One Instruction Does Not Fit All: How Well Do Embeddings Align,"Arya Shah, Himanshu Beniwal, Mayank Singh",,,"Embeddings, Persona, Instruction, Indian Languages, Multilingual Retrieval, Cultural Context","Aligning multilingual assistants with culturally grounded user preferences is essential for serving India's linguistically diverse population. This paper presents a unified benchmark spanning 12 Indian languages and four evaluation tasks, including monolingual and cross-lingual persona-to-instruction retrieval, reverse retrieval from instruction to persona, and binary compatibility classification. Eight multilingual embedding models are evaluated in a frozen-encoder setting with a thin logistic regression head for classification. E5-Large-Instruct achieves the highest Recall@1 of 27.4% on monolingual retrieval and 20.7% on cross-lingual transfer, while BGE-M3 leads reverse retrieval at 32.1% Recall@1. For classification, LaBSE attains 75.3% AUROC with strong calibration. These findings offer practical guidance for model selection in Indic multilingual retrieval and establish reproducible baselines for future work.",28.36,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10212v1_PADER Paillier-based Secure Decentralized Social R.pdf,PADER: Paillier-based Secure Decentralized Social Recommendation,"Chaochao Chen, Jiaming Qian, Fei Zheng∗, Yachuan Liu",Not found,Not found,"Paillier Cryptosystem, Secure Computation, Recommendation System","The paper proposes PADER, a Paillier-based secure decentralized social recommendation system, which keeps user and seller data private by operating in a decentralized manner without a centralized platform. It applies the Paillier cryptosystem to the SoReg model, which combines user ratings and social relations, and designs secure addition and multiplication protocols for efficient secure computation. The method is practical for real applications, as shown by experiment results.",27.21,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10215v1_Topo-RAG Topology-aware retrieval for hybrid text-.pdf,TOPO-RAG: TOPOLOGY-AWARE RETRIEVAL FOR HYBRID TEXT-TABLE DOCUMENTS,"Alex Dantart∗, Marco K""ovacs-Navarro",Not found,arXiv:2601.10215v1,"Retrieval-Augmented Generation (RAG), table retrieval, late interaction, multivector retrieval, enterprise search, heterogeneous data, semantic routing, structure-aware embeddings, Topo-RAG, ColBERT, cell-aware interaction, linearization bottleneck","In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. This work presents Topo-RAG, a framework that challenges the assumption that 'everything is text.' We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.",29.77,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10222v1_Introduction to optimization methods for training .pdf,Introduction to optimization methods for training SciML models,"Alena Kopaničáková∗, Elisa Riccietti †",Not found,2601.10222,"Optimization, Machine Learning, Scientific Machine Learning, Stochastic Optimization, First-Order Methods, Second-Order Methods, Stochastic Gradient Descent, AdaGrad, Adam, Physics-Informed, Operator Constrained","Optimization is the foundation of modern machine learning (ML). Historically, optimization methods have been categorized by the degree to which they exploit derivative information. In the context of scientific machine learning (SciML), the optimization landscape changes, with losses often non-decomposable due to physics-informed or operator-constrained formulations, leading to challenges in efficient stochastic optimization.",29.13,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10236v1_Who Owns the Text Design Patterns for Preserving A.pdf,Who Owns the Text? Design Patterns for Preserving Authorship in AI-Assisted Writing,"Bohan Zhang1, Chengke Bu2, Paramveer Dhillon∗",,2601.10236,"AI-assisted writing, human–AI collaboration, psychological ownership, personalization, provenance","AI writing assistants can reduce effort and improve fluency, but they may also weaken writers' sense of authorship. This study examines the tension with an ownership-aware co-writing editor that offers on-demand, sentence-level suggestions and tests two common design choices: persona-based coaching and style personalization. Across the two AI-assisted tasks, psychological ownership dropped relative to unassisted writing, even as cognitive load decreased and quality ratings stayed broadly similar overall. Persona coaching did not prevent the ownership decline, while style personalization partially restored ownership and increased AI incorporation in text.",28.91,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10242v1_Loop as a Bridge Can Looped Transformers Truly Lin.pdf,LOOP AS ABRIDGE: CANLOOPEDTRANSFORMERS TRULYLINKREPRESENTATIONSPACE ANDNATURAL LANGUAGEOUTPUTS?,"Guanxu Chen, Dongrui Liu, Jing Shao",Not found,Not found,"Large Language Models, Looped Transformers, Introspection, Representation Space, Natural Language Outputs","This report empirically investigates whether Looped Transformers (LTs), which iterate shared layers, can bridge the gap between their internal 'knowledge' and explicit linguistic outputs. Experiments show that increasing loop iterations narrows the gap but also degrades internal 'knowledge'. Another analysis suggests LTs' ability to perceive representations is only present in the final loop. These findings suggest LTs offer a promising direction for scaling computational depth but lack introspection to fully link representation space and natural language.",27.76,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10245v1_TRIM Hybrid Inference via Targeted Stepwise Routin.pdf,TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks,"Vansh Kapoor†1, Aman Gupta 2, Hao Chen 2, Anurag Beniwal 2, Jing Huang 2, Aviral Kumar1",Not found,Not found,"Large Language Models, Multi-step Reasoning, Inference Efficiency, Targeted Routing","TRIM proposes a hybrid inference approach for multi-step reasoning tasks, routing only critical steps to larger models while letting smaller models handle routine continuations. This method aims to improve inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors.",27.09,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10251v1_X-SAM Boosting Sharpness-Aware Minimization with D.pdf,Boosting Sharpness-Aware Minimization with Dominant-Eigenvector Gradient Correction,"Hongru Duan, Yongle Chen, Lei Guan",Not found,Not found,"Sharpness-Aware Minimization, Hessian, Eigenvalue, Gradient Correction, Generalization","This paper investigates Sharpness-Aware Minimization (SAM) from a spectral and geometric perspective, proposing an explicit eigenvector-aligned SAM (X-SAM) to correct the gradient via orthogonal decomposition along the top eigenvector, enabling more direct and efficient regularization of the Hessian's maximum eigenvalue.",27.42,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10254v1_NoReGeo Non-Reasoning Geometry Benchmark.pdf,NoReGeo: Non-Reasoning Geometry Benchmark,"Irina Abdullaeva, Anton Vasiliuk, Elizaveta Goncharova, Temurbek Rahmatullaev, Ivan Zagorulko, Maxim Kurkin, Andrey Kuznetsov",,,"Geometry, Large Language Models, Non-Reasoning, CAD, Robots, Geospatial Systems","NoReGeo is a benchmark designed to evaluate the intrinsic geometric understanding of large language models without relying on reasoning or algebraic computation. It comprises 2,500 trivial geometric problems spanning 25 categories, focusing on whether models can inherently encode spatial relationships and recognize geometric properties directly. The benchmark demonstrates that even advanced models achieve only 65% accuracy in binary classification tasks, highlighting a significant gap in current LLMs' ability to natively grasp geometric concepts.",27.98,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10257v1_Untangling Input Language from Reasoning Language .pdf,Untangling Input Language from Reasoning Language: A Diagnostic Framework for Cross-Lingual Moral Alignment in LLMs,"Nan Li, Bo Kang, Tijl De Bie",Not provided,Not provided,"Moral alignment, Cross-lingual evaluation, Moral foundations, Language effects, Reasoning language, Input language","This paper introduces a methodology to diagnose cross-lingual inconsistencies in moral judgments made by large language models (LLMs). It separates the effects of the language of the dilemma and the language in which the model reasons, enabling a more nuanced understanding of how these factors influence moral judgments. The methodology is applied to English-Chinese moral judgments with 13 LLMs, demonstrating its diagnostic power in detecting reasoning-language effects and context-dependency.",27.84,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10272v1_MoST Mixing Speech and Text with Modality-Aware Mi.pdf,MOST: MIXINGSPEECH ANDTEXT WITHMODALITY-,"A. Yuxuan Lou, Kai Yang, Yang You",,2601.10272,"Multimodal Learning, Speech and Text, Modality-Aware Mixture of Experts, Large Language Model","We present MoST, a novel multimodal large language model that integrates speech and text through a Modality-Aware Mixture of Experts (MAMoE) architecture, enhancing modality-specific learning and cross-modal understanding.",26.76,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10274v1_Queueing-Aware Optimization of Reasoning Tokens fo.pdf,Queue-Aware Optimization of Reasoning Tokens for Accuracy-Latency Trade-offs in LLM Servers,"Emre Ozbas, Melih Bastopcu",,,"Large language models, Inference servers, Token budget, Optimization, Queueing theory","This paper addresses the optimization of reasoning tokens in a single large language model (LLM) server serving a heterogeneous stream of queries. The server allocates a fixed number of internal thinking tokens for each query type, leading to an accuracy-latency trade-off. The system operates as an M/G/1 queue, and the mean system time depends on the moments of the service-time distribution. The paper formulates a constrained optimization problem to maximize a weighted average accuracy objective penalized by mean system time, subject to architectural token-budget constraints and queue-stability conditions. The optimal token allocation is found via a projected gradient method, and integer-valued allocations are attained via rounding. Simulation results evaluate the performance loss.",28.18,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10282v2_SPIKE Sparse Koopman Regularization for Physics-In.pdf,SPIKE: Sparse Koopman Regularization for Physics-Informed Neural Networks,"Jose Marie Antonio Miñoza, Center for AI Research PH",Not found,2601.10282,"Physics-Informed Neural Networks (PINNs), Koopman operators, Sparse learning, Stiff PDEs, Fluid dynamics, Chaotic ODEs","This work presents SPIKE, a framework that regularizes Physics-Informed Neural Networks (PINNs) with continuous-time Koopman operators to learn parsimonious dynamics representations, improving temporal extrapolation, spatial generalization, and long-term prediction accuracy across various PDEs and ODEs.",27.38,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10305v1_DanQing An Up-to-Date Large-Scale Chinese Vision-L.pdf,DanQingTechnical Report,"Glint Lab, Hengyu Shen, Tiancheng Gu, Bin Qin, Lan Wu, Yuling Wu, Shuo Tan, Zelong Sun, Jun Wang, Nan Wu, Xiang An, Weidong Cai, Ziyong Feng, Kaicheng Yang",Not found,2601.10305,"Chinese vision-language pretraining, DanQing dataset, Contrastive learning, Cross-modal retrieval, Semantic segmentation","This report introduces DanQing, a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. DanQing contains 100 million image-text pairs collected from Common Crawl, curated through a rigorous selection process, and primarily built from 2024-2025 web data. It is compared with existing datasets by continual pre-training of the SigLIP2 model, consistently achieving superior performance across various Chinese downstream tasks.",27.96,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10306v1_Evidence-Augmented Policy Optimization with Reward.pdf,Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning,"Xin Guan*, Zijian Li, Shen Huang, Pengjun Xie, Jingren Zhou, Jiuxin Cao",Not found,Not found,"Reinforcement Learning, Policy Optimization, Long-Context Reasoning, Reward Co-Evolution, Evidence Augmentation","This paper proposes EAPO (Evidence-Augmented Policy Optimization), a specialized RL algorithm that introduces a Group-Relative Evidence Reward to provide dense process supervision for long-context reasoning. The authors validate the Evidence-Augmented Reasoning paradigm and demonstrate that precise evidence extraction is a critical bottleneck. They also incorporate an Adaptive Reward-Policy Co-Evolution mechanism to sustain accurate supervision through training. Comprehensive evaluations across eight benchmarks show that EAPO significantly enhances long-context reasoning performance compared to state-of-the-art (SOTA) baselines.",27.27,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10338v1_Agent Skills in the Wild An Empirical Study of Sec.pdf,Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale,"Yi Liu∗, Weizhe Wang∗, Ruitao Feng†, Yao Zhang†, Guangquan Xu†, Gelei Deng, Yuekang Li, Leo Zhang",10.1145/nnnnnnn.nnnnnnn,,"Agent skills, AI security, vulnerability analysis, supply chain security, prompt injection, large language models","The paper conducts the first large-scale empirical security analysis of agent skills, revealing pervasive security risks such as 26.1% of skills containing at least one vulnerability, spanning 14 distinct patterns across four categories—prompt injection, data exfiltration, privilege escalation, and supply chain risks. The findings highlight the need for capability-based permission systems and mandatory security vetting.",27.87,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10342v1_C-GRASP Clinically-Grounded Reasoning for Affectiv.pdf,C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing,"1st Cheng Lin Cheng, 2nd Ting Chuan Lin, 3rd Chai Kai Chang",,,"Large language model, clinical decision support, heart rate variability, retrieval-augmented generation, explainable AI, guardrails","C-GRASP proposes a guardrailed RAG-enhanced pipeline for decomposing HRV interpretation into eight traceable reasoning steps, addressing challenges posed by respiratory sinus arrhythmia (RSA) and short-data instability in nonlinear metrics. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics, effectively mitigating spectral hallucinations and achieving superior performance in 4-class emotion classification and Clinical Reasoning Consistency (CRC) score of 69.6%.",28.32,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10343v2_OctoBench Benchmarking Scaffold-Aware Instruction .pdf,OCTOBENCH: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding,"Deming Ding, Shichun Liu, Enhui Yang, Jiahang Lin, Ziying Chen, Shihan Dou, Honglin Guo, Weiyu Cheng, Pengyu Zhao, Chengjun Xiao, Qunhong Zeng, Qi Zhang, Xuanjing Huang, Qidi Xu, Tao Gui",Not found,Not found,"instruction following, scaffold, agentic coding, benchmarking, repository-grounded","Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. OCTOBENCH benchmarks scaffold-aware instruction following in repository-grounded agentic coding, providing an automated observation-and-scoring toolkit to disentangle solving the task from following the rules.",27.84,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10348v1_Training-Trajectory-Aware Token Selection.pdf,Training-Trajectory-Aware Token Selection,"Zhanming Shen, Jiaqi Hu, Zeyu Qin, Hao Chen, Wentao Ye, Zenan Huang, Yihong Zhuang, Guoshan Lu, Junlin Zhou, Junbo Zhao",,,"Continual Learning, Efficient Distillation, Token Selection, Training Trajectory, Large Language Models","Efficient distillation is a key pathway for converting expensive reasoning capability into deployable efficiency. However, in the frontier regime where the student already has strong reasoning ability, naive continual distillation often yields limited gains or even degradation. We observe a characteristic training phenomenon: even as loss decreases monotonically, all performance metrics can drop sharply at almost the same bottleneck, before gradually recovering. We further uncover a token-level mechanism: confidence bifurcates into steadily increasing Imitation-Anchor Tokens that quickly anchor optimization and other yet-to-learn tokens whose confidence is suppressed until after the bottleneck. And the characteristic that these two types of tokens cannot coexist is the root cause of the failure in continual distillation. To address this, we propose Training-Trajectory-Aware Token Selection (T3S) to reconstruct the training objective at the token level, clearing the optimization path for yet-to-learn tokens. T3 yields consistent gains in both AR and dLLM settings.",28.67,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10349v1_SuS Strategy-aware Surprise for Intrinsic Explorat.pdf,SuS: Strategy-aware Surprise for Intrinsic Exploration,"Mark Kashirskiy, Ilya Makarov",,,"intrinsic motivation, reinforcement learning, exploration, contrastive learning","We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework for reinforcement learning, using pre-post prediction mismatch as a novelty signal. SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in accuracy and solution diversity compared to baseline methods.",27.04,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10373v1_Towards Efficient Low-rate Image Compression with .pdf,Towards Efficient Low-rate Image Compression with Frequency-aware Diffusion Prior Refinement,"Yichong Xia, Yimin Zhou, Jinpeng Wang, Bin Chen",Not found,Not found,"image compression, diffusion models, frequency-aware, low-bitrate, consistency prior","This work proposes a novel compression framework, AccelerateDiffusion-based Image Compression via Consistency Prior Refinement (DiffCR), for efficient and high-fidelity image reconstruction. It introduces a Frequency-aware Skip Estimation (FaSE) module to refine the ϵ-prediction prior from a pre-trained latent diffusion model and align it with compressed latents at different timesteps via Frequency Decoupling Attention (FDA). A lightweight consistency estimator enables fast two-step decoding by preserving the semantic trajectory of diffusion sampling. Without updating the backbone diffusion model, DiffCR achieves substantial bitrate savings and over 10× speed-up compared to state-of-the-art diffusion-based compression baselines.",27.34,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10378v2_Global Context Compression with Interleaved Vision.pdf,Global Context Compression with Interleaved Vision-Text Transformation,"Dian Jiao*, Jiaxin Duan*, Shuai Zhao, Jiabing Leng, Yiran Zhang, Feng Huang*",,,"Context Compression, Transformer, Vision-Language Models, Optical Character Recognition, Hierarchical Encoding, Sparse Attention","This paper investigates global context compression, proposing VIST2, a novel Transformer that interleaves input text chunks alongside their visual encoding. The model saves tokens at both prefilling and inference stages, achieving significant improvements in speed and memory usage compared to baselines.",26.43,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10386v1_Handling Missing Modalities in Multimodal Survival.pdf,Handling Missing Modalities in Multimodal Survival Prediction for Non-Small Cell Lung Cancer,"Filippo Ruffini, Camillo Maria Caruso, Claudia Tacconi, Lorenzo Nibid, Francesca Miccolis, Marta Lovino, Carlo Greco, Edy Ippolito, Michele Fiore, Alessio Cortellini, Bruno Beomonte Zobel, Giuseppe Perrone, Bruno Vincenzi, Claudio Marrocco, Alessandro Bria, Elisa Ficarra, Sara Ramella, Valerio Guarrasi, Paolo Soda",Not found,2601.10386,"Survival prediction, Non-small cell lung cancer, Multimodal data, Missing data","This paper addresses the challenge of survival prediction for non-small cell lung cancer by incorporating multimodal data, which often includes missing or incomplete information, and proposes methods to handle these missing modalities effectively.",28.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10398v2_LatentRefusal Latent-Signal Refusal for Unanswerab.pdf,LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries,"Xuancheng Ren, Shijing Hu, Zhihui Lu, Jiangqi Huang, Qiang Duan",Not found,Not found,"Text-to-SQL, Unanswerable Queries, Safety Constraints, Large Language Models, Schema Mismatch, Query Answerability","In LLM-based Text-to-SQL systems, unanswerable and underspecified user queries may generate incorrect text and executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. This paper formalizes safe refusal in Text-to-SQL systems as an answerability-gating problem and proposes LATENTREFUSAL, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of an LLM. The Tri-Residual Gated Encoder (TRGE) is introduced to suppress schema noise and amplify sparse, localized question-schema mismatch cues that indicate unanswerability. Extensive evaluations demonstrate the effectiveness of the proposed scheme and show that LATENTREFUSAL provides an attachable, efficient safety layer for Text-to-SQL systems.",28.09,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10402v1_Toward Ultra-Long-Horizon Agentic Science Cognitiv.pdf,Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering,"Xinyu Zhu, Yuzhu Cai, Zexi Liu, Bingyang Zheng, Cheng Wang, Rui Ye, Jiaao Chen, Hanrui Wang, Wei-Chen Wang, Yuzhi Zhang, Linfeng Zhang, Weinan E, Di Jin, Siheng Chen",Not found,2601.10402,"agentic science, cognitive accumulation, machine learning engineering, ultra-long-horizon autonomy, cognitive caching, multi-tiered architecture, openai, performance evaluation","The paper presents ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering, a representative microcosm of scientific discovery. By introducing Hierarchical Cognitive Caching (HCC), the approach enables structural differentiation of experience over time, allowing agents to decouple immediate execution from long-term experimental strategy, effectively overcoming scaling limits of static context windows.",28.62,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10406v1_ErrEval Error-Aware Evaluation for Question Genera.pdf,ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics,"Weiping Fu, Bifan Wei, Jingyi Hao, Yushun Zhang, Jian Zhang, Jiaxin Wang, Bo Li, Yu He, Lingling Zhang, Jun Liu",Not found,Not found,"Question Generation, Error Detection, Evaluation","ErrEval proposes a flexible and error-aware evaluation framework for automatic question generation, enhancing traditional evaluation methods by incorporating explicit error diagnostics. This approach aims to improve alignment with human judgments and mitigate overestimation of low-quality questions.",26.31,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10413v1_LADFA A Framework of Using Large Language Models a.pdf,LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies,"Haiyue Yuan∗, Nikolay Matyunin, Ali Raza, Shujun Li∗",XXXXXXX.XXXXXXX,,"Large Language Model, Privacy Policy, Text Analysis, Data Flows, Privacy, Security, Retrieval-Augmented Generation, RAG, Framework, Automotive Industry, Connected Vehicle","This paper presents the development of LADFA, an end-to-end computational framework for processing unstructured text in privacy policies, extracting personal data flows, and conducting analysis of the data flow graph to facilitate insight discovery. It combines LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies.",27.59,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10416v1_LLMdoctor Token-Level Flow-Guided Preference Optim.pdf,LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models,"Tiesunlong Shen, Rui Mao, Jin Wang, Heming Sun, Jian Zhang, Xuejie Zhang, Erik Cambria",,,"Large Language Models, Test-Time Alignment, Fine-Tuning, Token-Level Optimization, Flow-Guided Reward","This paper introduces LLMdoctor, a novel framework for efficient test-time alignment of large language models. Unlike traditional methods that rely on trajectory-level rewards, LLMdoctor integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.",27.54,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10421v1_Are Language Models Models.pdf,Are Language Models Models?,Philip Resnik,in press,,"Language models, Cognitive models, Model systems, Algorithmic-representational level, Computational theory","Resnik discusses the claim that language models serve as model systems, evaluating them at each of Marr's three levels and concluding that the claim is not true at the implementation level, poorly motivated at the algorithmic-representational level, and problematic at the computational theory level.",29.87,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10436v1_Development of Ontological Knowledge Bases by Leve.pdf,Development of Ontological Knowledge Bases by Leveraging Large Language Models,"LE Ngoc Luyen, Marie-Hélène ABEL, Philippe GOUSPILLiou",,,"Ontology Development, Ontological Knowledge bases, Large Language Models, Knowledge Representation, User Modeling, Knowledge Management","This paper presents a structured, iterative methodology leveraging Large Language Models (LLMs) to optimize knowledge acquisition, automate ontology artifact generation, and enable continuous refinement cycles for Ontological Knowledge Bases (OKBs). It demonstrates this approach through a case study focused on developing a user context profile ontology within the vehicle sales domain, highlighting significant improvements in scalability, consistency, bias mitigation, and transparency.",29.25,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10440v1_AgentGuardian Learning Access Control Policies to .pdf,Learning Access Control Policies to Govern AI Agent Behavior,"Nadya Abaev*, Denis Klimov *, Gerard Levinov, David Mimran, Yuval Elovici, Asaf Shabtai",,,"Security, AI Agents, Access Control Policies, Control Flow Graph","This study introduces AgentGuardian, a novel security framework that governs and protects AI agent operations by enforcing context-aware access-control policies. It monitors execution traces to learn legitimate agent behaviors and input patterns, derives adaptive policies, and evaluates across two real-world AI agent applications, demonstrating effective detection of malicious or misleading inputs while preserving normal agent functionality.",27.41,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10457v1_NSR-Boost A Neuro-Symbolic Residual Boosting Frame.pdf,NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models,"Ziming Dai∗, Dabiao Ma∗, Jinle Tong, Mengyuan Han, Jian Yang, Haojun Fei",10.1145/nnnnnnn.nnnnnnn,,"Neuro-Symbolic AI, Large Language Models, Gradient Boosting, Legacy Model, Interpretability","Presenting NSR-Boost, a neuro-symbolic residual boosting framework designed for industrial scenarios, which addresses the prohibitive retraining costs and systemic risks of upgrading legacy models in high-concurrency production environments. The framework comprises three key stages: finding hard regions through residuals, generating interpretable experts using Large Language Model and fine-tuning parameters with Bayesian optimization, and dynamically integrating experts with legacy model output through a lightweight aggregator. Successful deployment within Qfin Holdings' core financial risk control system demonstrates significant performance gains across six public and one private datasets.",28.04,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10460v1_Contextual StereoSet Stress-Testing Bias Alignment.pdf,Contextual StereoSet: Stress-Testing Bias Alignment Robustness in Large Language Models,"Abhinaba Basu, Pavan Chakraborty",Not found,2601.10460,"bias evaluation, alignment robustness, stress-testing, large language models, so-cientific context, StereoSet","A model's bias can shift dramatically based on contextual factors such as place, time, and audience. The study introduces Contextual StereoSet, a benchmark that tests bias under varying contextual framing, revealing striking patterns in model behavior. The findings suggest that bias scores from fixed-condition tests may not generalize, highlighting the need for robust evaluation methods.",27.96,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10462v3_ChartComplete A Taxonomy-based Inclusive Chart Dat.pdf,ChartComplete: A Taxonomy-based Inclusive Chart Dataset,"Ahmad Mustapha, Charbel Toumieh, Mariette Awad",Not provided,2601.10462,"Chart, Dataset, Chart Taxonomy, Chart Classification","With advancements in deep learning and computer vision, the field of chart understanding is evolving. To accurately measure MLLMs, multiple datasets have been developed. However, these datasets are limited to a small set of chart types. The ChartComplete dataset bridges this gap by covering thirty different chart types, based on a chart taxonomy borrowed from the visualization community.",27.38,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10477v1_Urban Socio-Semantic Segmentation with Vision-Lang.pdf,URBANSOCIO-SEMANTICSEGMENTATION WITH VISION-LANGUAGEREASONING,"Yu Wang, Yi Wang, Rui Dai, Yujie Wang, Kaikui Liu, Xiangxiang Chu, Yansheng Li",,abs/2306.09477,"semantic segmentation, vision-language reasoning, urban surfaces, social entities, reinforcement learning","This work achieves socio-semantic segmentation by vision-language model reasoning, introducing the Urban Socio-Semantic Segmentation dataset and a novel vision-language reasoning framework called SocioReasoner. Experiments demonstrate gains over state-of-the-art models and strong zero-shot generalization.",27.37,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10485v1_Panning for Gold Expanding Domain-Specific Knowled.pdf,Panning for Gold: Expanding Domain-Specific Knowledge,"Runhao Zhao, Weixin Zeng, Wentao Zhang, Chong Chen, Zhengpin Li, Xiang Zhao, Lei Chen",,,"Domain-specific Knowledge Graph Fusion, Knowledge Graph Enrichment, General-to-domain Knowledge Transfer, Fact-as-Program","This paper proposes a new task called domain-specific knowledge graph fusion (DKGF) to enhance the completeness and utility of domain-specific knowledge graphs (DKGs) by integrating relevant facts from general knowledge graphs (GKGs). It addresses the challenges of high ambiguity in determining domain relevance and cross-domain knowledge granularity misalignment. The authors introduce ExeFuse, a fact-as-program paradigm that reformulates DKGF as executable semantic reasoning over DKGs, enabling precise identification and integration of domain-relevant, consistent knowledge from GKGs. Two benchmark datasets and 21 representative configurations are proposed for systematic assessment of DKGF performance.",27.93,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10496v1_Model See Model Do Exposure-Aware Evaluation of Bu.pdf,"Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs","Ali Al-Kaswan, Claudio Spiess, Prem Devanbu, Arie van Deursen, Maliheh Izadi",10.1145/nnnnnnn.nnnnnnn,,"Large Language Models, bugs, fixes, Memorisation","This study introduces an exposure-aware evaluation framework to quantify how prior exposure to buggy versus fixed code influences a model's preference. Using the ManySStuBs4J benchmark and Data Portraits for membership testing, the authors estimate model preference in code completion and likelihood-based scoring metrics, finding that models tend to reproduce buggy lines more often than fixes, with exposure skewing evaluations towards correct fixes.",27.14,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10498v1_Projected Microbatch Accumulation yields reference.pdf,Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning,Nilin Abrahamsen,Not found,2601.10498,"Reinforcement Learning, Proximal Policy Optimization, Fine-tuning, Large Language Models","This note introduces Projected Microbatch Accumulation (PROMA), a method for large language model fine-tuning that modifies proximal policy optimization by projecting out sequence-wise gradient components before microbatch aggregation, resulting in more stable policy learning without inducing entropy collapse or relying on a reference policy or likelihood-ratio clipping.",26.48,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10511v1_Scalable Algorithms for Approximate DNF Model Coun.pdf,Scalable Algorithms for Approximate DNF Model Counting,"Paul Burkhardt, David G. Harris, Kevin T. Schmitt",Not found,2601.10511,"Monte Carlo, Approximation Algorithms, DNF Model Counting, PAC Learning, Efficiency","This paper presents a new Monte Carlo approach for approximating the number of satisfying assignments in Disjunctive Normal Form (DNF) formulas, which is crucial for applications such as probabilistic inference and network reliability. The approach is proven to achieve PAC learning bounds and is asymptotically more efficient than previous methods, demonstrating superior performance on large-scale problems.",27.9,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10512v2_SatMap Revisiting Satellite Maps as Prior for Onli.pdf,SatMap: Revisiting Satellite Maps as Prior for Online HD Map Construction,"Kanak Mazumder, Fabian B. Flohr",Not found,2601.10512,"Online HD map prediction, Satellite map prior, Vectorized HD map","This work proposes SatMap, an online vectorized HD map estimation method that integrates satellite maps with multi-view camera observations and directly predicts a vectorized HD map for downstream prediction and planning modules.",27.12,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10520v1_Breaking Up with Normatively Monolithic Agency wit.pdf,Breaking Up with Normatively Monolithic Agency,"Felix Jahn1,3*, Yannic Muskalla1,3, Lisa Dargasz1, Patrick Schramowski1,2,4,5, Kevin Baum1,2,3*",Not found,2601.10520,"AI alignment, neuro-symbolic architecture, deontic logic, moral module, guard, stakeholder engagement","This paper introduces GRACE, a neuro-symbolic reason-based containment architecture that decouples normative reasoning from instrumental decision-making, enabling AI agents to be safely and ethically aligned with human values. GRACE restructures decision-making into three modules: a Moral Module, a Decision-Making Module, and a Guard, providing interpretability, contestability, and justifiability.",28.05,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10524v1_Diagnosing Generalization Failures in Fine-Tuned L.pdf,Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection,"Frank Bobe IIIFrank Bobe IIIFrank Bobe III∗, Gregory D. Vetaw, Chase Pavlick, Darshan Bryner, Matthew Cook, Jose Salas-Vernis",Not found,2601.10524,"Large Language Models, Fine-tuning, Generalization, Phishing Detection, SHAP analysis, Mechanistic Interpretability",The study introduces and applies a multi-layered diagnostic framework to diagnose generalization failures in fine-tuned LLMs on a phishing detection task. It reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. (2) Generalization is highly architecture-dependent. (3) Some architectures are inherently more generalizable.,28.13,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10527v2_A Safety Report on GPT-5.2 Gemini 3 Pro Qwen3-VL G.pdf,"A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5","Xingjun Ma1, Yixu Wang1, Hengyuan Xu1, Yutao Wu3, Yifan Ding1, Yunhan Zhao1, Zilong Wang1, Jiabin Hua1, Ming Wen1,2, Jianan Liu1,2, Ranjie Duan, Yifeng Gao1, Yingshui Tan, Yunhao Chen 1, Hui Xue, Xin Wang 1, Wei Cheng, Jingjing Chen1, Zuxuan Wu1, Bo Li4, Yu-Gang Jiang1",https://doi.org/10.1101/2601.10527v2,2601.10527v2,"Large Language Models, Multimodal Large Language Models, Safety Evaluation, Adversarial Testing, Benchmark Evaluation, Multilingual Evaluation, Regulatory Compliance","This report presents an integrated safety evaluation of six frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5, across language, vision-language, and image generation settings. It highlights the inherent multidimensionality of safety in these models, shaped by modality, language, and evaluation design.",29.53,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10543v1_Defending Large Language Models Against Jailbreak .pdf,Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing,"W ARNING: This paper contains model outputs that may be considered harmful., Yinzhi Zhao, Ming Wang, Shi Feng, Xiaocui Yang, Daling Wang, Yifei Zhang",Not found,Not found,"Large language models, Jailbreak attacks, Safety awareness, In-decoding, Probing","This paper examines the decoding process of large language models (LLMs) and observes that even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. The authors propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments demonstrate significant enhancements in safety while maintaining low over-refusal rates on benign inputs and preserving response quality.",27.73,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10560v1_Learning Latency-Aware Orchestration for Parallel .pdf,Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems,"Xi Shi, Mengxin Zheng, Qian Lou",Not found,Not found,"Multi-agent systems, Parallel execution, Latency optimization, Inference latency, Latency-aware orchestration","This work investigates the learning-based orchestration of multi-agent systems with explicit latency supervision under parallel execution, proposing Latency-Aware Multi-agent System (LAMaS) to reduce critical path length by 38–46% compared to the SOTA baseline for multi-agent architecture search across multiple benchmarks while maintaining or even improving task performance.",26.84,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10562v1_Process-Guided Concept Bottleneck Model.pdf,Process-Guided Concept Bottleneck Model,"Reza M. Asiyabi, Sam Harrison, John L. Godlee, David Milodowski, Nicole H. Augustin, Penelope J. Mograbi, Timothy R. Baker, Lorena M. Benitez, Samuel J. Bowers, Thomas K. Brade, Joao M. B. Carreiras, Duncan M. Chalo, V era De Cauwer, Kyle G. Dexter, Hermane Diesse, Mathias I. Disney, Luisa F. Escobar-Alvarado, Manfred Finckh, Tatenda Gotore, Gabriele C. Hegerl, John N. Kigomo, Fainess C. Lumbwe, Francisco Maiato, Rudzani A. Makhado, Collins W. Masinde, Musingo Tito E. Mbuvi, Iain M. McNicol, Edward T.A. Mitchard, Buster P . Mogonong, Wilson A. Mugasha, Aristides Baptista Muhate, Hinji Mutondo, Leena Naftal, Paula Nieto-Quintano, Elifuraha Elisha Njoghomi, Catherine L. Parr, Oliver L. Phillips, Pierre Proces, Tshililo Ramaswiela, Jayashree Ratnam, Mathew Rees, Rasmus Revermann, Natasha Ribeiro, Mahesh Sankaran, Abel M. Siampale, Stephen Sitch, Kathleen G. Smart, Hemant G. Tripathi, Wayne Twine, Gabriel I.K. Uusiku, Helga van der Merwe, Chemuku Wekesa, Benjamin J. Wigley, Mathew Williams, Ellie Wood, Shaun Quegan, Steven Hancock, Casey M. Ryan",,,"machine learning, pattern analysis, pattern recognition, carbon dynamics, dry tropics, earth observation, biological sciences, forest ecology, remote sensing, AI-assisted tools","This study presents a process-guided concept bottleneck model, supported by various grants and partnerships, focusing on carbon dynamics in the dry tropics and utilizing AI-assisted tools for manuscript refinement.",28.3,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10567v1_Generative AI collective behavior needs an interac.pdf,Generative AI collective behavior needs an interactionist paradigm,"Laura Ferrarotti1,†, Gian Maria Campedelli2,1,†, Roberto Dessì3, Andrea Baronchelli4, Giovanni Iacca2, Kathleen M. Carley5, Alex Pentland6,7, Joel Z. Leibo8, James Evans9, Bruno Lepri1",Not found,2601.10567,"Generative AI, Collective behavior, Interactionism, Large language models, In-context learning, Social priors, Emergent phenomena, Trans-disciplinary","This article argues the need for an interactionist paradigm to study the collective behavior of agents based on large language models, emphasizing the importance of prior knowledge and embedded values in shaping emergent phenomena in multi-agent generative AI systems.",29.9,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10581v1_From Single to Multi-Agent Reasoning Advancing Gen.pdf,From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA,"Kimia Abedini, Farzad Shami, Gianmaria Silvello",https://doi.org/10.1016/j.jml.2026.01.015,2601.10581,"Question Answering, Genomic QA, Multi-Agent Systems","Comprehending genomic information is essential for biomed-ical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.",29.27,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10587v1_Adversarial Evasion Attacks on Computer Vision usi.pdf,Adversarial Evasion Attacks on Computer Vision using SHAP Values,"Frank Mollard *, Marcus Becker ‡, Florian Röhrbein †",Not found,2601.10587,"Adversarial attacks, Computer Vision, SHAP values, Deep Learning, Evasion attacks","The paper introduces a white-box attack on computer vision models using SHAP values, demonstrating how adversarial evasion attacks can compromise the performance of deep learning models by reducing output confidence or inducing misclassifications.",29.35,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10591v1_ProbFM Probabilistic Time Series Foundation Model .pdf,ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition,"Arundeep Chinta, Lucas Vinh Tran, Jay Katukuri",Not provided,Not provided,"Time Series Forecasting, Uncertainty Quantification, Transformer Models, Deep Evidential Regression","This work presents a novel transformer-based probabilistic framework, ProbFM, which leverages Deep Evidential Regression (DER) for principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. The framework is rigorously evaluated using a consistent LSTM architecture across five probabilistic methods, demonstrating competitive forecasting accuracy and explicit uncertainty decomposition in cryptocurrency return forecasting. The practical value of epistemic-aleatoric decomposition is shown through uncertainty-aware trading strategies, enabling effective risk management.",27.7,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10600v1_Procedural Fairness in Multi-Agent Bandits.pdf,Procedural Fairness in Multi-Agent Bandits,"Joshua Caiata, Carter Blair, Kate Larson",Not found,2601.10600,"fairness, multi-agent systems, multi-armed bandits, procedural fairness","This paper introduces a new fairness objective, procedural fairness, which provides equal decision-making power for all agents in multi-agent multi-armed bandits (MA-MAB) systems. It argues that fairness requires explicit normative choices and that procedural legitimacy deserves greater focus as a fairness objective.",27.3,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10611v1_Molmo2 Open Weights and Data for Vision-Language M.pdf,Molmo2: Open Weights and Data for Vision-Language Models,"Christopher Clark, Jieyu Zhang, Zixian Ma, Jae Sung Park, Mohammadreza Salehi, Rohun Tripathi, Sangho Lee, Zhongzheng Ren, Chris Dongjoo Kim, Yinuo Yang, Vincent Shao, Yue Yang, Weikai Huang, Taira Anderson, Jianrui Zhang, Jitesh Jain, George Stoica, Winson Han, Ali Farhadi",,2601.10611v1,"vision-language models, open weights, grounding, video understanding, synthetic data, closed VLMs","Presenting Molmo2, a new family of video-language models that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks.",28.1,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10651v1_Multi-Property Synthesis.pdf,Multi-Property Synthesis,"Christoph Weinhuber1,∗, Yannik Schnitzer 1,∗, Alessandro Abate1, David Parker 1, Giuseppe De Giacomo 1, Moshe Y. Vardi 2",Not found,Not found,"Synthesis, Temporal Logic, LTLf, Multi-Property, Finite-Horizon, Planning, Control","The paper studies LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, the authors compute the relation between product-game states and the goal sets that are realizable from them, and synthesize strategies achieving maximal realizable sets. They develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. The approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.",27.77,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10679v1_Are Your Reasoning Models Reasoning or Guessing A .pdf,Are Your Reasoning Models Reasoning or Guessing?,"Zirui Ren, Ziming Liu",,,"Hierarchical Reasoning, Reasoning Models, Sudoku-Extreme, Fixed Points, Grokking Dynamics","This study examines hierarchical reasoning models (HRM) and finds three surprising facts: HRM can fail on extremely simple puzzles due to violating the fixed point property, reasoning steps exhibit 'grokking' dynamics, and HRM exists multiple fixed points. These findings suggest HRM behaves more like guessing than reasoning, leading to proposed strategies for scaling HRM's guesses.",26.36,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10681v1_Structure and Diversity Aware Context Bubble Const.pdf,Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems,"Amir Khurshid1a*, Abhishek Sehgal1b",Not found,Not found,"Large Language Model, Retrieval-Augmented Generation, Context Bubble, Retrieval","This paper proposes a context bubble construction framework that assembles coherent, citable bundles of spans under a strict token budget, preserving and exploiting inherent document structure. It balances query relevance, marginal coverage, and redundancy penalties, producing compact and informative context sets. Experiments on enterprise documents demonstrate its efficiency in reducing redundant context, better covering secondary facets, and improving answer quality and citation faithfulness within a limited context window.",27.91,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10684v1_On the origin of neural scaling laws from random g.pdf,On the origin of neural scaling laws: from random graphs to natural language,"Maissam Barkeshli1,2,∗, Alberto Alfarano 3,†, Andrey Gromov 1",Not found,Not found,"Neural networks, Scaling laws, Transformer models, Random graphs, Natural language processing","This paper studies scaling laws for transformers trained to predict random walks on graphs with tunable complexity, demonstrating that neural scaling laws can arise even in the absence of power law structure in the data correlations. It also considers natural language complexity systematically and reveals a monotonic evolution of scaling exponents. The results include scaling laws obtained from training on random walks on random graphs and provide preliminary evidence for maximal update parameterization being more parameter efficient than standard parameterization.",27.98,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10696v1_The Impact of Generative AI on Architectural Conce.pdf,"The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load","Han Jiang*, Yao Xiao*, Rachel Hurley, Shichao Liu†",,,"Visual communication, Architectural design, Learning, Performance Assessment, Hybrid Intelligence, Human-AI teaming","Our study examines how generative AI influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks.",28.97,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10700v2_LIBERTy A Causal Framework for Benchmarking Concep.pdf,LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals,"Gilat Toker*, Nitay Calderon*",Not found,Not found,"Large Language Models, explainability, concept-based explanations, causal inference, counterfactuals, benchmarking","LIBERTy is a framework for constructing datasets containing structural counterfactual pairs to evaluate concept-based explanations of LLMs, addressing the limitations of existing benchmarks that rely on costly human-written counterfactuals. It introduces a new evaluation metric, order-faithfulness, and evaluates a wide range of methods across five models, identifying substantial room for improvement in concept-based explanations. It also enables systematic analysis of model sensitivity to interventions.",27.32,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10702v1_Grounding Agent Memory in Contextual Intent.pdf,Grounding Agent Memory in Contextual Intent,"Ruozhen Yang, Yucheng Jiang, Yueqi Jiang, Priyanka Kargupta, Jiawei Han",,,"large language models, long-horizon tasks, agent memory, contextual intent, memory retrieval","Deploying large language models in long-horizon, goal-oriented interactions remains challenging due to memory systems retrieving context-mismatched evidence. This paper proposes STITCH, an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step’s intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference. STITCH achieves state-of-the-art performance in context-aware retrieval benchmarks.",27.17,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10712v1_MatchTIR Fine-Grained Supervision for Tool-Integra.pdf,MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching,"Changle Qu, Sunhao Dai, Hengyi Cai, Jun Xu, Shuaiqiang Wang, Dawei Yin",Not found,Not found,"Tool-Integrated Reasoning, Fine-Grained Supervision, Reinforcement Learning, Bipartite Matching","MatchTIR is a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation for Tool-Integrated Reasoning (TIR) tasks. It aims to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios.",26.88,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10748v1_AnyECG Evolved ECG Foundation Model for Holistic H.pdf,AnyECG: Evolved ECG Foundation Model for Holistic Health Profiling,"Jun Li, Hongling Zhu, Yujie Xiao, Qinghao Zhao, Yalei Ke, Gongzheng Tang, Guangkun Nie, Deyun Zhang, Jin Li, Canqing Yu, Shenda Hong",,,"electrocardiography, artificial intelligence, cardiac diseases, non-cardiac diseases, health profiling, comorbidities, longitudinal studies","This study presents AnyECG, an evolved electrocardiography foundation model designed to enhance holistic health profiling by addressing complex clinical comorbidities and long-term risk prediction through a large-scale, multicenter dataset and transfer learning techniques.",27.47,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10768v1_Optimisation of complex product innovation process.pdf,OPTIMISATION OF COMPLEX PRODUCT INNOVATION PROCESSES BASED ON TREND MODELS WITH THREE-VALUED LOGIC,"NINA BO ˇCKOV´A1, BARBORA VOLN ´A2*, MIRKO DOHNAL 3",Not found,2601.10768,"complex product innovation, technological forecasting, three-valued logic, trend-based modelling, scenarios, transition graphs","This paper investigates complex product-innovation processes using models grounded in a set of heuristics, where each heuristic is expressed through simple trends. A solution to a trend model is defined as a set of scenarios with possible transitions between them, represented by a transition graph. Any possible future or past behavior of the system under study can be depicted by a path within this graph.",29.49,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10770v1_Unifying Speech Recognition Synthesis and Conversi.pdf,"UNIFYINGSPEECHRECOGNITION, SYNTHESIS AND CONVERSION WITHAUTOREGRESSIVETRANSFORMERS","Runyuan Cai, Yu Lin, Yiming Wang, Chunlin Fu, Xiaodong Zeng",Not found,2601.10770,"Text-to-Speech, Automatic Speech Recognition, Voice Conversion, Foundation Model","This paper presents GPA, a unified audio foundation model that integrates multiple core speech tasks within a single large language model (LLM) architecture, enabling a single autoregressive model to flexibly perform TTS, ASR, and VC without architectural modifications.",28.26,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10773v1_LogicLens Leveraging Semantic Code Graph to explor.pdf,LogicLens: Leveraging Semantic Code Graph to explore Multi Repository large systems,"Niko Usai, Dario Montagnini, Kristian Ilianov Iliev, Raffaele Camanzo",,,"Software systems, Multi-repository, Semantic code graph, Reactive conversational agent, Domain logic, Runtime behaviors","Understanding large software systems is a challenging task, especially when code is distributed across multiple repositories and microservices. LogicLens, a reactive conversational agent, assists developers in exploring complex software systems through a semantic multi-repository graph, capturing both structural and functional elements.",27.22,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10779v1_Unified Optimization of Source Weights and Transfe.pdf,Unified Optimization of Source Weights and Transfer Quantities in Multi-Source Transfer Learning: An Asymptotic Framework,"Qingyue Zhang, Chang Chu, Haohao Fu, Tianren Peng, Yanru Wu, Guanbo Huang, Yang Li, Shao-Lun Huang",Not found,Not found,"transfer learning, multi-source learning, asymptotic analysis, K-L divergence","This work proposes a theoretical framework, Unified Optimization of Weights and Quantities (UOWQ), for multi-source transfer learning, optimizing both source weights and transfer quantities. It proves that using all available source samples is optimal once weights are properly adjusted and develops practical algorithms for both multi-source transfer learning and multi-task learning settings.",27.98,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10810v1_Digital Metabolism Decoupling Logic from Facts via.pdf,Digital Metabolism: Decoupling Logic from Facts via Regenerative Unlearning,"Mengmeng Peng, Zhenyu Fang, He Sun",Not found,2601.10810,"Large Language Models, Parameter Entanglement, Neural Logic Core, Regenerative Unlearning, Thermodynamic Hypothesis","This paper proposes 'digital metabolism,' a thermodynamic hypothesis suggesting that targeted forgetting is necessary for distilling a pure neural logic core. The authors introduce the Regenerative Logic-Core Protocol (RLCP) to validate this hypothesis, observing a phase transition in a model trained with RLCP, achieving near-zero retention of targeted factual associations while exhibiting changes consistent with an emergent 'structural crystallization' effect.",28.45,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10820v1_Towards Reliable ML Feature Engineering via Planni.pdf,Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents,"Himanshu Thakur∗, Anusha Kamath, Anurag Muthyala, Dhwani Sanmukhani, Smruthi Mukund, Jay Katukuri",Not found,2601.10820,"Machine Learning, Feature Engineering, LLM Agents, Code Generation, Automation","Recent advances in code generation models have unlocked opportunities for automating feature engineering, but real-world adoption remains constrained by challenges such as dataset scarcity, integration limitations, and suboptimal human-AI collaboration. This paper addresses these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion, leveraging a team's environment represented as a graph to orchestrate calls to available agents, generate context-aware prompts, and use downstream failures to retroactively correct upstream artifacts. The approach achieves significant improvements in evaluation metrics over manual and unplanned workflows, and has delivered real-world impact by reducing feature engineering cycles from three weeks to a single day.",29.0,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10827v1_Approximately Optimal Global Planning for Contact-.pdf,Approximately Optimal Global Planning for Contact-Rich SE(2) Manipulation on a Graph of Reachable Sets,"Simin Liu, Tong Zhao, Bernhard Paus Graesdal, Peter Werner, Jiuguang Wang, John Dolan, Changliu Liu, Tao Pang",,,"Contact-Rich Manipulation, Global Planning, Graph of Reachable Sets, Optimization, SE(2) Manipulation","This paper introduces a new paradigm for computing approximately optimal manipulator plans, focusing on contact-rich manipulation (CRM). It constructs a graph of mutual reachable sets offline and plans over this graph online to achieve globally optimized motion. The approach outperforms a leading planner on a challenging contact-rich task, reducing task cost by 61% and achieving a 91% success rate across 250 queries with sub-minute query times.",27.96,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10835v1_Can Vision-Language Models Understand Construction.pdf,Can Vision-Language Models Understand Construction Workers? An Exploratory Study,"Hieu Bui, Nathaniel E. Chodosh, Arash Tavakoli",,,"Vision-Language Models, Construction Automation, Robotics, Human-Robot Interaction, Safety, Productivity","This study evaluates the performance of three leading Vision-Language Models (GPT-4o, Florence 2, and LLaVa-1.5) in detecting construction worker actions and emotions from static site images. Using a curated dataset of 1,000 images annotated across ten action and ten emotion categories, the study assesses each model's outputs through standardized inference pipelines and multiple evaluation metrics. GPT-4o consistently achieved the highest scores across both tasks, with an average F1-score of 0.756 and accuracy of 0.799 in action recognition, and an F1-score of 0.712 and accuracy of 0.773 in emotion recognition. Florence 2 performed moderately, with F1-scores of 0.497 (action) and 0.414 (emotion), while LLaVa-1.5 showed the lowest overall performance (F1-scores of 0.466 for action and 0.461 for emotion). Confusion matrix analyses revealed that all models struggled to distinguish semantically close categories.",28.86,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10880v1_Medical SAM3 A Foundation Model for Universal Prom.pdf,Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation,"Chongcong Jiang, Tianxingjian Ding, Chuhan Song, Jiachen Tu, Ziyang Yan, Yihua Shao, Zhenyi Wang, Yuzhang Shang, Tianyu Han, Yu Tian",Not provided,2601.10880,"Medical Image Segmentation, Foundation Models, Fine-Tuning, SAM3","Promptable segmentation foundation models like SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Medical SAM3 addresses these limitations by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts.",28.76,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10904v1_ARC Prize 2025 Technical Report.pdf,ARC Prize 2025: Technical Report,"François Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers",,,"ARC Prize, ARC-AGI, few-shot generalization, intelligence benchmark, fluid intelligence, abstract reasoning, ARC-AGI-2, ARC-AGI-1, ARC-AGI-3, refinement loops, knowledge coverage, benchmark contamination, AI reasoning","This paper discusses the ARC Prize 2025, a global competition targeting the ARC-AGI-2 dataset, which features greater task complexity compared to its predecessor. It examines the role of refinement loops in AGI progress, discusses knowledge-dependent overfitting, and previews ARC-AGI-3, which introduces interactive reasoning challenges. The paper also highlights the growing research interest in fluid intelligence and abstract reasoning, and the emergence of refinement loops in AI systems.",28.36,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10917v1_Self-learned representation-guided latent diffusio.pdf,SELF-LEARNED REPRESENTATION-GUIDED LATENT DIFFUSION MODEL FOR BREAST CANCER CLASSIFICATION IN DEEP ULTRA VIOLET WHOLE SURFACE IMAGES,"Pouya Afshin1, David Helminiak 2, Tianling Niu3, Julie M. Jorns 4, Tina Yen5, Bing Yu3, Dong Hye Ye1†",Not found,Not found,"Breast Cancer Classification, Latent Diffusion Model, Self-Supervised Learning, Data Augmentation","Breast-Conserving Surgery requires precise intraoperative margin assessment to preserve healthy tissue. Deep Ultraviolet Fluorescence Scanning Microscopy offers rapid, high-resolution surface imaging for this purpose; however, the scarcity of annotated DUV data hinders the training of robust deep learning models. To address this, we propose an Self-Supervised Learning-guided Latent Diffusion Model to generate high-quality synthetic training patches. By guiding the LDM with embeddings from a fine-tuned DINO teacher, we inject rich semantic details of cellular structures into the synthetic data. We combine real and synthetic patches to fine-tune a Vision Transformer, utilizing patch prediction aggregation for WSI-level classification. Experiments using 5-fold cross-validation demonstrate that our method achieves 96.47% accuracy and reduces the FID score to 45.72, significantly outperforming class-conditioned baselines.",29.07,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10921v1_RobuMTL Enhancing Multi-Task Learning Robustness A.pdf,RobuMTL: Enhancing Multi-Task Learning Robustness Against Weather Conditions,"Tasneem Shaffee, Sherief Reda",,,"Robust Multi-Task Learning, Adversarial Adaptation, Weather Robustness, Mixture-of-Experts","This paper introduces RobuMTL, a novel architecture designed to adaptively address visual degradation by dynamically selecting task-specific hierarchical Low-Rank Adaptation (LoRA) modules and a LoRA expert squad based on input perturbations in a mixture-of-experts fashion. The framework enables adaptive specialization based on input characteristics, improving robustness across diverse real-world conditions. RobuMTL is evaluated on the PASCAL and NYUD-v2 datasets and compared against single-task models, standard MTL baselines, and state-of-the-art methods.",28.03,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10922v1_What Matters in Data Curation for Multimodal Reaso.pdf,What Matters in Data Curation for Multimodal Reasoning?,"Yosub Shin, Michael Buriek, Boris Sobolev, Pavel Bushuyeu, Vikas Kumar, Haoyang Xu, Samuel Watson, Igor Molybog",,,"Data curation, Multimodal reasoning, Vision-language models, NeurIPS challenge, Fine-tuning","This paper studies data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision–Language Reasoning (DCVLR) challenge, isolating dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from the Walton Multimodal Cold Start corpus, the team placed first in the challenge. Post-competition ablations show that difficulty-based example selection is the dominant driver of performance gains, while increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, and commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.",28.32,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10926v1_Selecting Language Models for Social Science Start.pdf,Selecting Language Models for Social Science,"Dustin S. Stoltz, Marshall A. Taylor, Sanuj Kumar",,XX(X):1–22,"large language models, LLMs, reproducibility, replicability, model openness","This paper explores the selection of language models for social science research, focusing on reproducibility, reliability, validity, and replicability. It discusses the significance of model openness, footprint, training data, and model architecture and fine-tuning. The authors propose starting with smaller, open models and constructing delimited benchmarks to demonstrate the validity of the entire computational pipeline.",26.71,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10931v1_Sparse Data Tree Canopy Segmentation Fine-Tuning L.pdf,Sparse Data Tree Canopy Segmentation: Fine-Tuning Leading Pretrained Models on Only 150 Images,"David Szczecina, Niloofar Azad, Hudson Sun, Kyle Gao, Anthony Bertnyk, Lincoln Linlin Xu",,,"Tree canopy, Aerial imagery, Remote sensing, Deep learning, Object segmentation, Transformer models, Convolutional neural networks, Data scarcity, Overfitting","This paper evaluates five representative architectures, YOLOv11, Mask R-CNN, DeepLabv3, Swin-UNet, and DINOv2, for tree canopy segmentation under extreme data scarcity. The experiments show that pretrained convolution-based models generalize significantly better than pretrained transformer-based models. The findings confirm that transformer-based architectures struggle in low-data regimes without substantial pretraining or augmentation and that differences between semantic and instance segmentation further affect model performance.",28.1,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10945v1_PatientVLM Meets DocVLM Pre-Consultation Dialogue .pdf,PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between Vision-Language Models for Efficient Diagnosis,"K Lokesh1*, Abhirama Subramanyam Penamakuri1*, Uday Agarwal1, Apoorva Challa2, Shreya K Gowda2, Somesh Gupta2, Anand Mishra1",,2605.09999,"vision-language models, pre-consultation dialogue, diagnosis, medical image analysis, symptom elicitation","This paper proposes a Pre-Consultation Dialogue Framework (PCDF) that simulates real-world diagnostic procedures between two vision-language models: a DocVLM and a PatientVLM. The DocVLM generates follow-up questions based on the image and dialogue history, while the PatientVLM responds using a symptom profile derived from the ground-truth diagnosis. The framework is validated clinically, and findings indicate that the interactions form coherent, multi-turn consultations paired with images and diagnoses, leading to substantial gains over image-only training.",27.95,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10951v1_Multi-Stage Patient Role-Playing Framework for Rea.pdf,Multi-Stage Patient Role-Playing Framework for Realistic Clinical Interactions,"Shijie Jiang, Zefan Zhang, Kehua Zhu, Tian Bai, Ruihong Zhao",Not found,2601.10951,"Patient Role-Playing, Large Language Models, Clinical",The simulation of realistic clinical interactions is crucial for advancing clinical Large Language Models and supporting medical diagnostic education. This work proposes the first Chinese patient simulation dataset (Ch-PatientSim) and a Multi-Stage Patient Role-Playing (MSPRP) framework to improve model performance in patient behavior emulation.,28.91,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10955v1_Beyond Max Tokens Stealthy Resource Amplification .pdf,Beyond Max Tokens: Stealthy Resource Amplification via Tool Calling Chains in LLM Agents,"Kaiyu Zhou, Yongsen Zheng∗, Yicheng He, Meng Xue, Xueluan Gong, Yuji Wang, Kwok-Yan Lam",Not found,Not found,"Large Language Models, LLM Agents, Tool Calling, Agent Security, Economic Denial-of-Service","The paper introduces a stealthy, multi-turn economic Denial-of-Service (DoS) attack that operates at the tool layer under the guise of a correctly completed task. The method adjusts text-visible fields and a template-governed return policy in a benign, Model Context Protocol (MCP)-compatible tool server, optimizing these edits with a Monte Carlo Tree Search (MCTS) optimizer. The attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658 ×, and raises energy by 100–560 × across six LLMs on the ToolBench and BFCL benchmarks. It drives GPU KV cache occupancy from <1% to 35–74% and cuts co-running throughput by approximately 50%. Because the server remains protocol-compatible and the task outcomes are correct, conventional checks fail. These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.",28.69,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.10960v1_Steering Language Models Before They Speak Logit-L.pdf,Steering Language Models Before They Speak: Logit-Level Interventions,"Hyeseon An, Shinwoo Park, Hyundong Jin, Yo-Sub Han *",,,"language models, steering, logit interventions, controllable generation","This paper proposes a training-free inference-time logit intervention for controllable generation, utilizing a statistical token score table derived from z-normalized log-odds of labeled corpora to shift the decoding distribution. Empirical evaluations across three diverse datasets demonstrate the effectiveness of the method in steering output characteristics, achieving large, consistent, and multi-task control gains.",26.68,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11000v1_When Personalization Misleads Understanding and Mi.pdf,When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs,"Zhongxiang Sun1, Yi Zhan1, Chenglei Shen1, Weijie Yu3, Xiao Zhang1, Ming He2, Jun Xu1",,,"Personalized LLMs, Factual Reasoning, Hallucination, Factuality-Preserving Personalized Steering (FPPS)","Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. This paper introduces Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. Experiments show that FPPS substantially improves factual accuracy while maintaining personalized performance.",27.56,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11007v1_AdaMARP An Adaptive Multi-Agent Interaction Framew.pdf,AdaMARP: An Adaptive Multi-Agent Interaction Framework for General Immersive Role-Playing,"Zhenhua Xu, Dongsheng Chen, Shuo Wang, Jian Li, Chengjie Wang, Meng Han, Yabiao Wang",Not found,2601.11007,"large language models, role-playing, immersion, adaptive multi-agent interaction, message representation, environment modeling","This paper proposes AdaMARP, an adaptive multi-agent interaction framework for general immersive role-playing. It features an immersive message format that interleaves [Thought], (Action), <Environment>, and Speech, and an explicit Scene Manager that controls role-playing via discrete actions. The authors introduce AdaRPSet and AdaSMSet for training and evaluation, demonstrating consistent gains in character consistency, environment grounding, and narrative coherence.",27.05,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11012v1_Efficient Protein Optimization via Structure-aware.pdf,Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics,"Jiahao Wang, Shuangjia Zheng",,,"protein optimization, Hamiltonian dynamics, Bayesian optimization, structure-aware sampling, epistasis, mutation effects","Proposes HADES, a Bayesian optimization method utilizing Hamiltonian dynamics to efficiently sample from a structure-aware approximated posterior, enabling rapid transition of proposals toward promising areas. Demonstrates superior performance in in-silico evaluations across most metrics compared to state-of-the-art baselines.",26.43,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11016v1_Contextual Distributionally Robust Optimization wi.pdf,Contextual Distributionally Robust Optimization with Causal and Continuous Structure: An Interpretable and Tractable Approach,"Fenglin Zhang, Jie Wang∗",Not found,2601.11016,"Contextual distributionally robust optimization, Causal Sinkhorn discrepancy, Soft regression forest, Stochastic compositional optimization","In this paper, we introduce a framework for contextual distributionally robust optimization (DRO) that considers the causal and continuous structure of the underlying distribution by developing interpretable and tractable decision rules that prescribe decisions using covariates. We propose the Soft Regression Forest (SRF) decision rule, which approximates optimal policies within arbitrary measurable function spaces, preserving the interpretability of classical decision trees while being fully parametric, differentiable, and Lipschitz smooth. We develop an efficient stochastic compositional gradient algorithm to solve the corresponding infinite-dimensional policy optimization problem, achieving a convergence rate of O(ε−4). The method is validated through numerical experiments on synthetic and real-world datasets, demonstrating superior performance and interpretability.",29.3,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11019v1_Finding the Translation Switch Discovering and Exp.pdf,Finding the Translation Switch: Discovering and Exploiting the Task-Initiation Features in LLMs,"Xinwei Wu, Heng Liu, Xiaohu Zhao, Yuqi Ren, Linlong Xu, Longyue Wang, Deyi Xiong, Weihua Luo, Kaifu Zhang",Not found,Not found,"Large Language Models, Translation, Sparse Autoencoders, PCA, Causal Interventions, Data Selection, Efficient Fine-tuning",This paper explores the translation capabilities of Large Language Models (LLMs) and introduces a novel framework to identify task-specific features. The authors leverage Sparse Autoencoders (SAEs) to isolate a set of ,28.74,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11021v1_Combating Spurious Correlations in Graph Interpret.pdf,Combating Spurious Correlations in Graph Interpretability via Self-Reflection,"Kecheng Cai, Chenyang Xu, Chao Peng",January 2026,,"Graph Interpretability, Spurious Correlations, Self-Reflection, Interpretable Learning","This paper focuses on improving interpretability on challenging Spurious-Motif datasets by adapting the self-reflection technique, commonly used in large language models, to datasets with strong spurious correlations. It proposes a self-reflection framework that integrates with existing interpretable graph learning methods, leading to performance improvements on Spurious-Motif and other graph interpretability benchmarks.",27.15,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11030v1_IDDR-NGP Incorporating Detectors for Distractor Re.pdf,IDDR-NGP: Incorporating Detectors for Distractor Removal with Instant Neural Radiance Field,"Xianliang Huang, Jiajie Gou, Shuhang Chen, Zhizhou Zhong, Jihong Guan, Shuigeng Zhou",10.1145/3581783.3612045,2601.11030,"distractor removal, 3D scene, instant neural radiance field, 2D detectors, multi-view compensation, perceptual image patch similarity","This paper presents IDDR-NGP, a unified distractor removal method that operates on Instant-NPG, capable of removing various types of distractors in 3D scenes from multiple corrupted images.",27.36,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11035v1_Your One-Stop Solution for AI-Generated Video Dete.pdf,Recent Advances in Synthetic Video Detection: Challenges and Opportunities,"Long Ma, Zihao Xue, Yan Wang, Zhiyuan Yan, Jin Xu, Xiaorui Jiang, Haiyang Yu, Yong Liao, Zhen Bi",,2601.11035,"synthetic video detection, generative modeling, video classification, machine learning","Recent advances in generative modeling have made synthetic videos increasingly difficult to distinguish from real ones, necessitating reliable detection methods. However, existing datasets are often limited in scale and scope, hindering the development of synthetic video detection. This paper discusses the challenges and opportunities in this field.",28.74,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11037v1_BAPO Boundary-Aware Policy Optimization for Reliab.pdf,BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search,"Shiyu Liu, Yongjing Yin, Jianhao Yan, Yunbo Tang, Qinggang Zhang, Bei Li, Xin Chen, Jingang Wang, Xunliang Cai, Jinsong Su",,,"reinforcement learning, large language models, agentic search, policy optimization, boundary awareness, reliability","This paper proposes BAPO, a novel RL framework designed to cultivate reliable boundary awareness in agentic search models without compromising accuracy. It introduces a group-based boundary-aware reward and an adaptive reward modulator to prevent the model from exploiting IDK as a shortcut during early exploration.",27.21,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11042v1_Spectral Characterization and Mitigation of Sequen.pdf,Spectral Characterization and Mitigation of Sequential Knowledge Editing,"Chi Zhang, Mengqi Zhang, Xiaotian Ye, Runxi Cheng, Zisheng Zhou, Ying Zhou, Pengjie Ren, Zhumin Chen",Not found,Not found,"Knowledge Editing, Large Language Models, Sequential Editing, Spectral Analysis, Parameter Modification","This work presents a spectral analysis of sequential knowledge editing in large language models and proposes REVIVE, a plug-and-play framework that stabilizes sequential editing by explicitly preserving the dominant singular subspace. REVIVE improves editing efficacy while substantially preserving general abilities under long-horizon sequential editing, including extreme settings with up to 20,000 edits.",27.16,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11044v2_AgencyBench Benchmarking the Frontiers of Autonomo.pdf,AGENCYBENCH: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts,"Keyu Li, Junhao Shi, Yang Xiao, Mohan Jiang, Jie Sun, Yunze Wu, Dayuan Fu, Shijie Xia, Xiaojie Cai, Tianze Xu, Weiye Si, Dequan Wang, Pengfei Liu",,2601.11044,"autonomous agents, benchmarking, real-world scenarios, user simulation, Docker sandbox, tool use, resource efficiency, feedback-driven self-correction, model architecture, agentic frameworks","AGENCYBENCH is a comprehensive benchmark designed to evaluate 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. It requires an average of 1 million tokens and 90 multi-turn tool uses to resolve scenarios, and employs a user simulation agent for iterative feedback and a Docker-based sandbox for automated rubric-based assessment.",28.08,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11049v1_Predicting Biased Human Decision-Making with Large.pdf,Predicting Biased Human Decision-Making with Large Language Models in Conversational Settings,"Stephen Pilli, Vivek Nallur",Not found,Not found,"Conversational AI, Framing Effect, Status Quo Bias, LLM Simulation","This study examines whether large language models can predict biased decision-making in conversational settings and how cognitive load affects these predictions. Participants completed decision-making tasks via a chatbot with varying dialogue complexity, exhibiting cognitive biases such as the Framing Effect and Status Quo Bias. LLMs were evaluated for their ability to predict individual decisions given demographic information and prior dialogue, with mixed results across choice problems. However, LLM predictions that incorporated dialogue context were significantly more accurate in several key scenarios, reproducing bias patterns and load-bias interactions observed in humans. The GPT-4 family consistently aligned with human behavior, outperforming GPT-5 and open-source models in predictive accuracy and fidelity to human-like bias patterns.",28.5,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11063v1_H-AIM Orchestrating LLMs PDDL and Behavior Trees f.pdf,"H-AIM: Orchestrating LLMs, PDDL, and Behavior Trees for Hierarchical Multi-Robot Planning","Haishan Zeng, Peng Li",Not found,2601.11063,"Embodied AI, Multi-Robot Planning, Large Language Models, PDDL, Behavior Trees, Hierarchical Planning, Dynamic Multi-Robot Coordination","H-AIM proposes a novel embodied multi-robot task planning framework that addresses the challenges of long-horizon task execution by heterogeneous robot teams from high-level instructions. It leverages an LLM to parse instructions and generate PDDL problem descriptions, combines the semantic reasoning of LLMs with classical planner search capabilities to produce optimized action sequences, and compiles the resulting plan into behavior trees for reactive control. The framework supports dynamically sized heterogeneous robot teams via a shared blackboard mechanism for communication and state synchronization.",28.26,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11065v1_Fairness in Healthcare Processes A Quantitative An.pdf,Fairness in Healthcare Processes: A Quantitative Analysis of Decision Making in Triage,"Rachmadita Andreswari1,2,4, Stephan A. Fahrenkrog-Petersen2,3, Jan Mendling1,2",Not found,2601.11065,"process mining, fairness, triage, emergency room","This study addresses the research problem of fairness in automated decision-making in high-pressure healthcare scenarios, such as emergency triage, by linking real-life event logs with conceptual dimensions of justice. It analyzes time, redo, deviation, and decision as process outcomes and evaluates the influence of age, gender, race, language, and insurance using statistical measurements. The results demonstrate which aspects of potential unfairness are present in high-acuity and sub-acute settings.",28.42,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11073v1_Bridging Cognitive Neuroscience and Graph Intellig.pdf,Bridging Cognitive Neuroscience and Graph Intelligence: Hippocampus-Inspired Multi-View Hypergraph Learning for Web Finance Fraud,"Rongkun Cui, Nana Zhang, Kun Zhu, Qi Zhang",XXXXXXX.XXXXXXX,,"graph intelligence, cognitive neuroscience, web finance fraud, multi-view hypergraph learning, hippocampus-inspired","This paper proposes a Hippocampus-Inspired Multi-View Hypergraph learning model (HIMVH) for detecting online financial fraud. HIMVH addresses two challenges: fraud camouflage and long-tailed data distributions, by incorporating insights from the hippocampus' scene conflict monitoring and novelty detection mechanisms. Extensive experiments on six web-based financial fraud datasets show significant improvements in AUC, F1, and AP metrics compared to state-of-the-art models.",27.6,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11076v1_A3D Adaptive Affordance Assembly with Dual-Arm Man.pdf,A3D: Adaptive Affordance Assembly with Dual-Arm Manipulation,"Jiaqi Liang, Yue Chen, Qize Yu, Yan Shen, Haipeng Zhang, Hao Dong, Ruihai Wu",,arXiv:2309.13785,"robotics, furniture assembly, dual-arm manipulation, adaptive affordance, motion planning, vision understanding","This paper proposes A3D, a framework for adaptive furniture assembly using dual-arm manipulation. It learns adaptive affordances to identify optimal support and stabilization locations on furniture parts, employing dense point-level geometric representations to model part interaction patterns and handle evolving assembly states through an adaptive module that uses interaction feedback to dynamically adjust support strategies during assembly.",27.28,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11077v1_ABC-Bench Benchmarking Agentic Backend Coding in R.pdf,ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development,"Jie Yang, Honglin Guo, Li Ji, Jiazheng Zhou, Zhikai Lei, Shuo Zhang, Shichun Liu, Yuxin Wang, Bo Wang, Yining Zheng, Tao Gui, Xipeng Qiu",10.1101/2601.11077v1,2601.11077v1,"Large Language Models, Autonomous Agents, Backend Development, Real-World Engineering, Benchmarking","This paper introduces ABC-Bench, a benchmark designed to evaluate agentic backend coding within a realistic, executable workflow. It curates 224 practical tasks from open-source repositories and requires agents to manage the entire development lifecycle, highlighting the gap between current model capabilities and practical backend engineering demands.",28.84,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11078v1_Visual Marker Search for Autonomous Drone Landing .pdf,Visual Marker Search for Autonomous Drone Landing in Diverse Urban Environments,"Jiaohong Yao, Linfeng Liang, Yao Deng, Xi Zheng, Richard Han, Yuankai Qi",Not provided,Not provided,"Drone navigation, Marker-based landing, Reinforcement learning, AirSim, Robustness","This paper presents a simulation-based evaluation suite for marker-based autonomous drone landing in diverse urban environments. It evaluates two heuristic coverage patterns and a reinforcement learning-based agent, analyzing how exploration strategy and scene complexity affect success rate, path efficiency, and robustness.",27.73,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11089v2_MiCA A Mobility-Informed Causal Adapter for Lightw.pdf,MiCA: A Mobility-Informed Causal Adapter for Lightweight Epidemic Forecasting,"Suhan Guo, Jiahong Deng, Furao Shen",Not found,Not found,"epidemic forecasting, mobility data, causal discovery, temporal forecasting","Accurate forecasting of infectious disease dynamics is critical for public health planning and interventions. MiCA, a lightweight and architecture-agnostic module, infers mobility relations through causal discovery and integrates them into temporal forecasting models via gated residual mixing, improving lightweight temporal backbones and achieving competitive performance with SOTA spatio-temporal models.",26.89,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11090v1_Efficient Multilingual Name Type Classification Us.pdf,Eﬀicient Multilingual Name T ype Classification Using Convolutional Networks,Davor Lauc,,,"multilingual NLP, named entity recognition, convolutional neural networks, efficient inference, proper names","We present a convolutional neural network approach for classifying proper names by language and entity type. Our model, Onomas-CNN X, combines parallel convolution branches with depthwise-separable operations and hierarchical classification to process names efficiently on CPU hardware. Achieves 92.1% accuracy while processing 2,813 names per second on a single CPU core, i46 times faster than fine-tuned XLM-RoBERTa with comparable accuracy. The model reduces energy consumption by a factor of 46 compared to transformer baselines. Our experiments demonstrate that specialized CNN architectures remain competitive with large pre-trained models for focused NLP tasks when sufficient training data exists.",27.87,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11100v1_ReCreate Reasoning and Creating Domain Agents Driv.pdf,ReCreate: Reasoning and Creating Domain Agents Driven by Experience,"Zhezheng Hao, Hong Wang, Jian Luo, Jianqing Zhang, Yuyan Zhou, Qiang Lin, Can Wang, Hande Dong, Jiawei Chen",,,"Large Language Models, Agent Creation, Experience-driven, Automated Agent Generation, Domain Adaptation","This work proposes ReCreate, an experience-driven framework for automatically creating domain agents. It leverages agent interaction histories to systematically learn from experience, providing rich concrete signals on the causes of success or failure and avenues for improvement. ReCreate consistently outperforms human-designed agents and existing automated agent generation methods, even when starting from minimal seed scaffolds.",26.92,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11109v1_Vision-as-Inverse-Graphics Agent via Interleaved M.pdf,Vision-as-Inverse-Graphics Agent via Interleaved Multimodal Reasoning,"Shaofeng Yin1,4*, Jiaxin Ge1, Zora Zhiruo Wang2, Xiuyu Li1, Michael J. Black3, Trevor Darrell1, Angjoo Kanazawa1, Haiwen Feng1,3,4†",fugtemypt123.github.io/VIGA-website,2601.11109,"Computer Vision, Inverse Graphics, Multimodal Reasoning, Iterative Refinement, 3D Reconstruction, Scene Editing","Vision-as-inverse-graphics, the concept of reconstructing an image as an editable graphics program, is a long-standing goal of computer vision. Our work presents VIGA (Vision-as-Inverse-Graphic Agent), which alternates between generation and verification steps through iterative execution and reasoning, to reconstruct or edit scenes from a single image. VIGA combines a skill library and evolving context memory to support long-horizon reasoning and is task-agnostic and model-agnostic, improving one-shot baselines on tasks such as 3D reconstruction and multi-step scene editing.",28.84,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11124v1_Learn Before Represent Bridging Generative and Con.pdf,Learn Before Represent: Bridging Generative and Contrastive Learning for Domain-Specific LLM Embeddings,"Xiaoyu Liang, Yuchen Peng, Jiale Luo, Wenhao Wang, Haoji Hu, Xincheng Zhou",,2601.11,"Large Language Models, Contrastive Learning, Generative Learning, Domain-Specific Embeddings, Knowledge Acquisition","This work proposes Learn Before Represent (LBR), a novel two-stage framework, to address the limitations of the prevailing 'LLM+CL' paradigm in vertical domains. LBR first injects domain knowledge via an Information Bottleneck-Constrained Generative Learning stage, preserving the LLM's causal attention to maximize knowledge acquisition while compressing semantics. It then performs Generative-Refined Contrastive Learning on the compressed representations for alignment. Extensive experiments on medical, chemistry, and code retrieval tasks show that LBR significantly outperforms strong baselines.",27.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11135v1_Context-aware Graph Causality Inference for Few-Sh.pdf,Context-aware Graph Causality Inference for Few-Shot Molecular Property Prediction,"Van Thuy Hoang, O-Joun Lee*",,,"graph learning, few-shot learning, molecular property prediction, causal inference, context-aware","This paper proposes CaMol, a context-aware graph causality inference framework to address the challenges of few-shot molecular property prediction, where only a few labeled molecules are available. It uses a causal inference perspective to discover causal substructures and disentangles them from confounding ones, achieving superior accuracy and sample efficiency in few-shot tasks.",26.78,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11143v1_Learning Quadrupedal Locomotion for a Heavy Hydrau.pdf,Learning Quadrupedal Locomotion for a Heavy Hydraulic Robot Using an Actuator Model,"Minho Lee, Hyeonseok Kim, Jin Tak Kim, Sangshin Park, Jeong Hyun Lee, Jungsan Cho, Jemin Hwangbo ∗",,,"Hydraulic Actuators, Legged Robots, Reinforcement Learning","This work proposes an analytical actuator model driven by hydraulic dynamics to represent the complex hydraulic actuators of a heavy hydraulic quadruped robot. The model predicts joint torques for all 12 actuators in under 1 microsecond, enabling rapid processing in reinforcement learning environments. The model is compared with neural network-based actuator models and demonstrated advantages in data-limited scenarios. The locomotion policy trained with the model is deployed on a hydraulic quadruped robot, achieving stable and robust command-tracking locomotion for the first time on a heavy hydraulic quadruped robot.",27.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11144v2_Deep GraphRAG A Balanced Approach to Hierarchical .pdf,Deep GraphRAG: A Balanced Approach to Hierarchical Retrieval and Adaptive Integration,"Yuejie Li, Ke Yang, Tao Wang, Bolin Chen, Bowen Li, Chengjun Mao",,,"GraphRAG, Reinforcement Learning, Large Language Models","Graph-based Retrieval-Augmented Generation (GraphRAG) frameworks face a trade-off between comprehensiveness of global search and efficiency of local search. Deep GraphRAG proposes a balanced approach to hierarchical retrieval and adaptive integration, introducing a hierarchical global-to-local retrieval strategy that integrates macroscopic inter-community and microscopic intra-community contextual relations. It employs a three-stage process: inter-community filtering, community-level refinement, and entity-level fine-grained search. A beam search-optimized dynamic re-ranking module guides this process, continuously filtering candidates to balance efficiency and global comprehensiveness. Deep GraphRAG also features a Knowledge Integration Module leveraging a compact LLM, trained with Dynamic Weighting Reward GRPO (DW-GRPO), dynamically adjusting reward weights to balance relevance, faithfulness, and conciseness.",28.37,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11147v1_Do We Always Need Query-Level Workflows Rethinking.pdf,Do We Always Need Query-Level Workflows?,Rethinking Agentic Workflow Generation for Multi-Agent Systems,,,"Multi-Agent Systems, Workflow Generation, Query-Level, Task-Level, Self-Evolution, Generative Reward Modeling","This paper explores the necessity of query-level workflow generation in Multi-Agent Systems (MAS) built on large language models. It shows that query-level workflow generation is not always necessary, as a small set of top-K task-level workflows can cover equivalent or even more queries. The authors propose SCALE, a low-cost task-level generation framework that predicts the optimizer with few-shot calibration for evaluation, demonstrating competitive performance and significant token reduction compared to existing approaches.",26.97,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11151v1_Cross-Modal Attention Network with Dual Graph Lear.pdf,Cross-Modal Attention Network with Dual Graph Learning in Multimodal Recommendation,"JI DAI, Beijing University of Posts and Telecommunications, China, QUAN FANG∗, Beijing University of Posts and Telecommunications, China, JUN HU, National University of Singapore, Singapore, DESHENG CAI, Tianjin University of Technology, China, YANG YANG, Beihang University, China and State Key Laboratory of CNS/ATM, China, CAN ZHAO, Aviation Data Communication Corporation, China",XXXXXXX.XXXXXXX,,"Multimedia recommendation, Graph Neural Network, Multimodal Fusion","This paper proposes a Cross-modal Recursive Attention Network with dual graph embedding (CRANE) to address the limitations of shallow modality fusion and asymmetric feature treatment in multimodal recommendation systems. CRANE iteratively refines modality features based on cross-correlations in a joint latent space, and integrates a symmetric dual-graph framework to fuse behavioral and semantic signals.",28.23,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11160v1_Clustering High-dimensional Data Balancing Abstrac.pdf,Clustering High-dimensional Data: Balancing Abstraction and Representation,"Claudia Plant, Lena G. M. Bauer, Christian B ¨ohm",,,"Clustering, High-dimensional data, Abstraction, Representation, Subspace clustering, Deep clustering","How to find a natural grouping of a large real data set? Clustering requires a balance between abstraction and representation. To identify clusters, we need to abstract from superfluous details of individual objects, but also need a rich representation that emphasizes the key features shared by groups of objects that distinguish them from other groups of objects. Current deep clustering methods define and enforce abstraction through centroid-based and density-based clustering losses.",27.58,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11178v1_TANDEM Temporal-Aware Neural Detection for Multimo.pdf,TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech,"Girish A. Koushik1*, Helen Treharne2, Diptesh Kanojia1",Not provided,Not provided,"hate speech, multimodal, temporal-aware, reinforcement learning, hate detection","This work introduces TANDEM, a unified framework that transforms audio-visual hate detection from a binary classification task into a structured reasoning problem. It employs a novel tandem reinforcement learning strategy to optimize vision-language and audio-language models through self-constrained cross-modal context, enabling precise temporal grounding and achieving significant improvements in target identification and F1 score on benchmark datasets.",27.74,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11189v1_Policy-Based Deep Reinforcement Learning Hyperheur.pdf,Policy-Based Deep Reinforcement Learning Hyperheuristics for Job-Shop Scheduling,"Sofiene Lassoued *a, Asrat Gobachew b, Stefan Lier b, Andreas Schwung a",,,"Hyper-heuristics, Job Shop Scheduling, Policy-based Reinforcement learning, Petri nets",This paper proposes a policy-based deep reinforcement learning hyper-heuristic framework for solving the Job Shop Scheduling Problem. The hyper-heuristic agent learns to switch scheduling rules based on the system state dynamically. The framework is extended with action prefiltering and a commitment mechanism to improve performance and generalization.,26.85,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11196v1_Artificial Intelligence and the US Economy An Acco.pdf,Artificial Intelligence and the US Economy: An Accounting Perspective on Investment and Production,"Luisa Carpinelli, Filippo Natoli, Marco Taboga",Not found,2601.11196,"artificial intelligence, capital expenditures, data centers, national accounts","This paper provides an overview on how the current AI wave is captured in US national accounts, combining a simple macro-accounting framework with a stylized description of the AI production process.",28.21,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11199v1_SD-RAG A Prompt-Injection-Resilient Framework for .pdf,SD-RAG: A Prompt-Injection-Resilient Framework for Selective Disclosure in Retrieval-Augmented Generation,"Aiman Al Masoud, Marco Arazzi, Antonino Nocera",Not found,Not found,"Retrieval-Augmented Generation, Prompt Injection, Selective Disclosure, Privacy, Security, Large Language Models","Proposes a novel approach, SD-RAG, to selectively disclose sensitive information in Retrieval-Augmented Generation, decoupling security and privacy constraints from the generation process and applying sanitization and disclosure controls during the retrieval phase.",26.93,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11200v1_FAQ Mitigating Quantization Error via Regenerating.pdf,FAQ: Mitigating Quantization Error via Regenerating Calibration Data,"Haiyang Xiao, Weiqing Li, Jinyue Guo, Guochao Jiang, Guohua Liu, Yuewei Zhang",,,"Quantization, Calibration, Post-Training Quantization, Family-Aware Quantization, Metal Objects, Wooden Objects, Thermal Conductivity","This paper proposes FAQ (Family-Aware Quantization), a calibration data regeneration framework that leverages prior knowledge from LLMs of the same family to generate high-fidelity calibration samples, addressing the core bottleneck of representativeness and universality of calibration data in determining the accuracy of quantization parameters. Experiments show that FAQ reduces accuracy loss by up to 28.5% compared to the baseline with original calibration data.",27.93,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11202v1_Epistemic Control and the Normativity of Machine L.pdf,To be published in The Role of AI in Science: Epistemological and Methodological Studies,Emanuele Ratti,,,"machine learning, epistemic control, cognitive values, normativity",The abstract discusses the increasing use of machine learning systems in science and the concerns raised by Paul Humphreys about the potential loss of epistemic control. The author investigates the extent of this loss and presents a more nuanced view of epistemic control in ML-based science.,27.39,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11207v1_LoRA as Oracle.pdf,LoRA as Oracle,"Marco Arazzi, Antonino Nocera",,,"LoRA, Membership Inference Attack, Backdoor Attack","Backdoored and privacy-leaking deep neural networks pose a serious threat to the deployment of machine learning systems in security-critical settings. Existing defenses for backdoor detection and membership inference typically require access to clean reference models, extensive retraining, or strong assumptions about the attack mechanism. In this work, we introduce a novel LoRA-based oracle framework that leverages low-rank adaptation modules as a lightweight, model-agnostic probe for both backdoor detection and membership inference.",27.17,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11219v1_SDFLoRA Selective Dual-Module LoRA for Federated F.pdf,SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients,"Zhikang Shen, Jianrong Lu, Haiyuan Wan, Jianhai Chen",Not found,Not found,"Federated Learning, LoRA, Privacy-Preserving, Parameter-Efficient, Differential Privacy","Parameter-efficient federated fine-tuning for large language models (LLMs) has become a practical route for adapting distributed data. However, practical federated deployments often exhibit rank heterogeneity, where clients use different low-rank configurations. This paper proposes SDFLoRA, which decomposes each client adapter into a global module for transferable knowledge and a local module for client-specific adaptations. The global module is selectively aligned and aggregated across clients, while local modules remain private, enabling robust learning under rank heterogeneity and privacy-aware optimization.",28.23,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11232v1_FactCorrector A Graph-Inspired Approach to Long-Fo.pdf,FACTCORRECTOR: A Graph-Inspired Approach to Long-Form Factuality Correction of Large Language Models,"Javier Carnerero-Cano, Massimiliano Pronesti, Radu Marinescu, Tigran Tchrakian, James Barry, Jasmina Gajcin, Yufang Hou, Alessandra Pascale, Elizabeth Daly",,,"Large Language Models, Factuality Correction, Graph-Based Approaches, Feedback Guided Correction","This paper introduces FACTCORRECTOR, a post-hoc correction method for large language models (LLMs) that adapts across domains without retraining and leverages structured feedback about the factuality of the original response to generate a correction. The authors also develop the VELI5 benchmark, a novel dataset containing systematically injected factual errors and ground-truth corrections, to support rigorous evaluations of factuality correction methods. Experiments on VELI5 and several popular long-form factuality datasets show that the FACTCORRECTOR approach significantly improves factual precision while preserving relevance, outperforming strong baselines.",27.99,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11252v1_Beyond Model Scaling Test-Time Intervention for Ef.pdf,BEYONDMODELSCALING: TEST-TIMEINTERVENTION,"Qianyue Wang, Jinwu Hu, Yufeng Wang, Huanxiang Lin, Bolin Chen, Zhiquan Wen, Yaofo Chen, Mingkui Tan†",Not found,Not found,"Large Reasoning Models, Efficient Reasoning, Test-Time Intervention, External Feedback, Interactive Reasoning, Transitional Conjunctions, Multi-criteria Evaluation, Group Relative Policy Optimization, Security, Creative Tasks","The paper proposes Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Key insights include the use of transitional conjunctions as natural points for intervention and the importance of appropriate use of transitional words to enhance performance. The paradigm aims to achieve a superior balance between accuracy and reasoning length under limited context windows, outperforming existing methods in accuracy and reducing reasoning length significantly.",27.76,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11258v1_Knowledge is Not Enough Injecting RL Skills for Co.pdf,Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation,"Pingzhi Tang∗1,2, Yiding Wang∗1,2, Muhan Zhang1,3",Not found,Not found,"Reinforcement Learning, Skill Transfer, Continual Adaptation, Knowledge Updating","Large Language Models face the 'knowledge cutoff' challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning is commonly used to update model knowledge, it often fails to reliably improve the model's ability to use newly incorporated information. This paper proposes Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, PaST can linearly inject knowledge manipulation skills into a target model after lightweight SFT on new data, demonstrating effectiveness in knowledge-incorporation QA and agentic tool-use benchmarks.",27.48,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11269v1_X-Distill Cross-Architecture Vision Distillation f.pdf,X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning,"Maanping Shao, Feihong Zhang, Gu Zhang, Baiye Cheng, Zhengrong Xue, Huazhe Xu",Not found,Not found,"X-Distill, Vision Transformers, Knowledge Distillation, Rep-resentation Learning, Manipulation","X-Distill is a simple yet effective method that synergizes the strengths of both architectures by transferring the rich visual representations of a large, frozen DINOv2 teacher to a compact ResNet-18 student on the general-purpose ImageNet dataset. This distilled encoder, now endowed with powerful visual priors, is then jointly fine-tuned with a diffusion policy head on the target manipulation tasks. Extensive experiments demonstrate that our method consistently outperforms policies equipped with from-scratch ResNet or fine-tuned DINOv2 encoders.",27.32,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11282v1_From SERPs to Sound How Search Engine Result Pages.pdf,FromSERPsto Sound: How Search Engine Result Pages and AI-generated Podcasts Interact to Influence User Attitudes on Controversial Topics,"Junjie Wang, Gaole He, Alisa Rieger, Ujwal Gadiraju",10.1145/3786304.3787942,,"Attitude Change, AI-generated Podcasts, Information modality, Web search, Controversial Topics, Responsible Opinion Formation","This study investigates user attitudinal effects of consuming information via search engine result pages (SERPs) and AI-generated podcasts, focusing on how the sequence and modality of exposure shape user opinions. The results reveal an effect of sequence on attitude change and the role of viewpoint bias and topic controversiality in shaping attitude change. No effect of individual moderators was found.",27.37,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11286v1_XChoice Explainable Evaluation of AI-Human Alignme.pdf,XChoice: Explainable Evaluation of AI–Human Alignment in LLM-based Constrained Choice Decision Making,"Weihong Qi, Fan Huang, Rasika Muralidharan, Jisun An, Haewoon Kwak",,,"explainable AI, human-AI alignment, LLM decision making, constrained choice","XChoice presents a framework for evaluating AI-human alignment in constrained decision making, focusing on decision mechanisms rather than just outcome metrics. It fits a mechanism-based decision model to human and LLM-generated decisions, recovering interpretable parameters that capture decision factors, constraint sensitivity, and trade-offs. The framework is demonstrated on Americans' daily time allocation using ATUS as ground truth, revealing heterogeneous alignment across models and activities and salient misalignment in Black and married groups. Robustness is validated via an invariance analysis, and targeted mitigation is evaluated with a retrieval-augmented generation (RAG) intervention.",28.0,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11344v1_How Much Would a Clinician Edit This Draft Evaluat.pdf,How Much Would a Clinician Edit This Draft?,"Parker Seegmiller, Joseph Gatto, Sarah E. Greer, Ganza Belise Isingizwe, Rohan Ray, Timothy Burdick, Sarah M. Preum",Not provided,Not provided,"Large language models, patient portal messages, clinician editing, AI alignment, efficiency","This study investigates the alignment between clinicians and large language models (LLMs) in drafting responses to patient portal messages. It develops a novel taxonomy of thematic elements and proposes a novel evaluation framework for assessing clinician editing load at both content and theme levels. The results reveal substantial epistemic uncertainty in aligning LLM drafts with clinician responses, highlighting the need for adapting LLMs to individual clinician preferences for reliable and responsible use in patient-clinician communication workflows.",27.17,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11350v1_FEATHer Fourier-Efficient Adaptive Temporal Hierar.pdf,FEATHer: Fourier-Efficient Adaptive Temporal Hierarchy Forecaster for Time-Series Forecasting,"Jaehoon Lee †, Seungwoo Lee †, Younghwi Kim†, Dohee Kim*, Sunghyun Sim*",Not found,Not found,"Time-series Forecasting, Edge AI, Ultra-Lightweight Models, Fourier-Efficient Adaptive Temporal Hierarchy Forecaster","Time-series forecasting plays a fundamental role in industrial domains such as manufacturing, energy management, logistics, and smart factory operations. The proposed Fourier-Efficient Adaptive Temporal Hierarchy Forecaster (FEATHer) is a multiscale temporal model designed to achieve accurate long-term forecasting under severe resource limitations, including strict constraints on latency, memory, and energy consumption.",27.93,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11354v1_AstroReason-Bench Evaluating Unified Agentic Plann.pdf,AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems,"Weiyi Wang, Xinchi Chen, Jingjing Gong, Xuanjing Huang, Xipeng Qiu",Not found,2601.11354,"Large Language Models, Agentic Planning, Space Planning Problems, Heterogeneous Constraints, Physical Constraints, Long-Horizon Decision-Making","This paper introduces AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems, which are high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. The benchmark integrates multiple scheduling regimes and evaluates state-of-the-art agentic Large Language Model systems, highlighting the limitations of current agents under realistic constraints.",28.22,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11359v1_Think-Clip-Sample Slow-Fast Frame Selection for Vi.pdf,THINK-CLIP-SAMPLE: SLOW-FAST FRAME SELECTION FOR VIDEO UNDERSTANDING,"Wenhui Tan∗, Ruihua SongB, Jiaze Li, Jianzhong Ju, Zhenbo LuoB",Not found,Not found,"Multi-modal LLMs, long video understanding, frame selection, slow-fast sampling","A training-free framework that enhances long video understanding through multi-query reasoning and clip-level slow-fast sampling, demonstrating consistent improvements across different MLLMs and achieving comparable accuracy with 50% fewer inference time cost.",26.3,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11369v2_Institutional AI Governing LLM Collusion in Multi-.pdf,Institutional AI: Governing LLM Collusion in Multi-Agent Markets via Public Governance Graphs,"M. Bracale Syrnikov1,4, F. Pierucci1,3, M. Galisai1,2, M. Prandi1,2, P. Bisconti1,2, F. Giarrusso1,2, O. Sorokoletova1,2, V. Suriani2, D. Nardi2",Not found,2601.11369v2,"AI alignment, multi-agent systems, governance graphs, Cournot markets, collusion prevention","This paper advances an experimental framework for evaluating Institutional AI, a system-level approach to AI alignment that reframes alignment from preference engineering in agent space to mechanism design in institutional space. It applies this framework to govern the Cournot collusion case and compares three regimes: Ungoverned, Constitutional (prompt-only policy as anti-collusion constitution), and Institutional (governance-graph-based). The Institutional regime produces significant reductions in collusion, while the Constitutional baseline yields no reliable improvement.",28.3,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11379v1_Evaluating LLM Behavior in Hiring Implicit Weights.pdf,"Evaluating LLM Behavior in Hiring: Implicit Weights, Fairness Across Groups, and Alignment with Human Preferences","Morgane Hoffmann, Emma Jouffroy, Warren Jouanneau, Marc Palyart, Charles Pebereau",Not provided,Not provided,"Large Language Models, Person-job Fit, Fairness, Interpretability","This paper evaluates the decision logic of a Large Language Model (LLM) in recruitment, focusing on how it assigns importance to different attributes and whether these assignments align with human preferences and societal norms. By analyzing synthetic datasets from real freelancer profiles and project descriptions, the authors estimate how the LLM weights match-relevant criteria and identify which attributes it prioritizes. They also explore how these weights vary across project contexts and demographic subgroups, and propose a framework for systematic comparison with human recruiters.",27.19,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11389v1_Hyperparameter Optimization of Constraint Programm.pdf,Hyperparameter Optimization of Constraint Programming Solvers,"Hedieh Haddad1*, Thibault Falque 2, Pierre Talbot 2, Pascal Bouvry2",Not found,2601.11389,"constraint programming, hyperparameter optimization, automated optimization, Bayesian optimization, Hamming distance search","The performance of constraint programming solvers is highly sensitive to the choice of their hyperparameters. Manually finding the best solver configuration is a difficult, time-consuming task that typically requires expert knowledge. This paper introduces probe and solve algorithm, a novel two-phase framework for automated hyperparameter optimization integrated into the CPMpy library.",28.27,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11400v1_Wetland mapping from sparse annotations with satel.pdf,Wetland mapping from sparse annotations with satellite image time series and temporal-aware segment anything model,"Shuai Yuana, Tianwu Linb, Shuang Chena, Yu Xiab, Peng Qinb, Xiangyu Liub, Xiaoqing Xub, Nan Xud, Hongsheng Zhanga, Jie Wangb, Peng Gonga",Not found,2601.11400v1,"wetland mapping, satellite image time series, sparse annotation, segment anything model, temporal adaptation","Accurate wetland mapping is critical for ecosystem monitoring and management, yet acquiring dense pixel-level annotations is prohibitively costly. In practice, only sparse point labels are typically available, and existing deep learning-based models struggle under such weak supervision. To address these challenges, we propose WetSAM, a novel SAM-based framework that effectively leverages satellite image time series to enhance wetland mapping from sparse point annotations.",28.51,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11409v1_Topology-Guaranteed Image Segmentation Enforcing C.pdf,"Topology-Guaranteed Image Segmentation: Enforcing Connectivity, Genus, and Width Constraints","Wenxiao Li, Xue-Cheng Tai, Jun Liu",Not found,Not found,"image segmentation, topological preservation, persistent homology, thickness of topology, variational, regularization","This paper proposes a novel mathematical framework that integrates width information into the characterization of topological structures for image segmentation. It leverages persistent homology and smoothing concepts from partial differential equations to modify local extrema of upper-level sets, enabling the resulting topological structures to inherently capture width properties. The framework is incorporated into variational image segmentation models, allowing for the design of neural networks that can segment images with required topological and width properties. Numerical experiments demonstrate the effectiveness of the method in preserving essential topological invariants and ensuring critical width attributes in segmented images.",28.62,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11421v1_The Great March 100 100 Detail-oriented Tasks for .pdf,THEGREATMARCH100: 100 DETAIL-ORIENTEDTASKS FOR EVALUATING EMBODIED AI AGENTS,"Ziyu Wang, Chenyuan Liu, Yushun Xiang, Runhao Zhang, Yu Zhang, Qingbo Hao, Hongliang Lu, Houyu Chen, Zhizhong Feng, Kaiyue Zheng, Dehao Ye, Xianchao Zeng, Xinyu Zhou, Boran Wen, Jiaxin Li, Mingyu Zhang, Kecheng Zheng, Qian Zhu, Ran Cheng, Yong-Lu Li",Not found,Not found,"robot learning, embodied AI, task design, evaluation, long-tail behavior, trajectory data, robotics","Introducing the Great March 100 (GM-100) as the first step towards a robot learning Olympics, consisting of 100 carefully designed tasks covering a wide range of interactions and long-tail behaviors to comprehensively evaluate the capabilities of robotic agents and promote diversity and complexity in robot dataset task designs.",28.33,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11429v1_Relational Linearity is a Predictor of Hallucinati.pdf,Relational Linearity is a Predictor of Hallucinations,"Yuetian Lu, Yihong Liu, Hinrich Schütze",,,"hallucinations, large language models, factual knowledge, relation linearity, synthetic entities","Hallucinations in large language models are often observed when asked about synthetic entities. The study hypothesizes that the linearity of the relation between subject and object is a key factor in determining whether the model hallucinates or refuses to answer. The research creates a dataset of synthetic subjects and investigates the correlation between relational linearity and hallucination rate, finding a strong correlation. This finding has implications for improving the reliability of LLMs and suggests new research directions.",27.27,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11440v1_GenDA Generative Data Assimilation on Complex Urba.pdf,GENERATIVEDATAASSIMILATION ONCOMPLEX URBANAREAS VIACLASSIFIER-FREEDIFFUSIONGUIDANCE,"Francisco Giral1, Álvaro Manzano1,2, Ignacio Gómez1, Ricardo Vinuesa3, Soledad Le Clainche1",Not found,Not found,"Urban wind flow, Data assimilation, Diffusion, Graph-based, Generative models, Complex geometries","This paper proposes GenDA, a generative data assimilation framework that reconstructs high-resolution wind fields on unstructured meshes from limited observations. The model employs a multiscale graph-based diffusion architecture trained on computational fluid dynamics (CFD) simulations and interprets classifier-free guidance as a learned posterior reconstruction mechanism.",28.13,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11441v1_Hierarchical Orthogonal Residual Spread for Precis.pdf,HIERARCHICAL ORTHOGONAL RESIDUAL SPREAD FOR PRECISE MASSIVE EDITING IN LARGE LANGUAGE MODELS,"Xiaojie Gu, Guangxu Chen, Yuheng Yang, Jingxin Han, Andi Zhang",Not found,Not found,"Large language models, Model Editing, Knowledge Update, Residual Spread","Large language models (LLMs) exhibit exceptional performance across various domains, yet they face critical safety concerns. Model editing has emerged as an effective approach to mitigate these issues. Existing model editing methods often focus on optimizing an information matrix that blends new and old knowledge. While effective, these approaches can be computationally expensive and may cause conflicts. In contrast, we shift our attention to HierarchicalOrthogonalResidualSpread of the information matrix, which reduces noisy gradients and enables more stable edits from a different perspective. We demonstrate the effectiveness of our method HORSE through a clear theoretical comparison with several popular methods and extensive experiments conducted on two datasets across multiple LLMs.",28.32,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11442v1_Map2Thought Explicit 3D Spatial Reasoning via Metr.pdf,Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps,"Xiangjun Gao, Zhensong Zhang, Dave Zhenyu Chen, Songcen Xu, Long Quan, Eduardo P´erez-Pellitero, Youngkyoon Jang",,,"3D Vision-Language Models, Metric Cognitive Map, Cognitive Chain-of-Thought, Explicit Spatial Reasoning, 3D Structure, Interpretable Inference, Data Efficiency","Map2Thought is a framework that enables explicit and interpretable spatial reasoning for 3D Vision-Language Models (3D-VLMs). It introduces a Metric Cognitive Map (Metric-CogMap) and a Cognitive Chain-of-Thought (Cog-CoT) to provide a unified spatial representation and perform explicit geometric reasoning. Experimental results show that Map2Thought achieves high accuracy using only half the supervision, outperforming state-of-the-art methods by significant margins.",27.7,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11451v1_PRISM-CAFO Prior-conditioned Remote-sensing Infras.pdf,PRISM-CAFO: Prior-conditioned Remote-sensing Infrastructure Segmentation and Mapping for CAFOs,"Oishee Bintey Hoque, Nibir Chandra Mandal, Kyle Luong, Amanda Wilson, Samarth Swarup, Madhav Marathe, Abhijin Adiga",,2601.11451,"CAFOs, Remote-sensing, Infrastructure Segmentation, Mapping, Concentrated Animal Feeding Operations, YOLOv8, Swin-B, PRISM-CAFO","This work presents an infrastructure-first, explainable pipeline for identifying and characterizing Concentrated Animal Feeding Operations (CAFOs) from aerial and satellite imagery, achieving state-of-the-art performance and demonstrating the impact of domain priors on classification decisions.",27.66,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11459v1_Interactive Narrative Analytics Bridging Computati.pdf,Interactive Narrative Analytics: Bridging Computational Narrative Extraction and Human Sensemaking,BRIAN KEITH,10.1 109/ACCESS.2025.3650352,,"Human-AI collaboration, information extraction, interactive visual analytics, knowledge integration, narrative extraction, narrative sensemaking, semantic interaction, visual analytics","This paper introduces Interactive Narrative Analytics (INA), a new field combining computational narrative extraction with interactive visual analytics to support sensemaking. INA addresses challenges in narrative understanding, such as scalability, interactivity, knowledge integration, and evaluation standardization. It combines computational and human insights to address complex challenges in narrative sensemaking.",27.2,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11464v1_MHA2MLA-VLM Enabling DeepSeeks Economical Multi-He.pdf,MHA2MLA-VLM: Enabling DeepSeek’s Economical Multi-Head Latent Attention across Vision-Language Models,"Xiaoran Fan, Zhichao Sun, Tao Ji, Lixing Shen, Tao Gui",,,"Vision-Language Models, Multi-Head Latent Attention, Key-Value Cache, Efficient Inference, Modality Adaptation","This work presents MHA2MLA-VLM, a parameter-efficient framework for converting off-the-shelf Vision-Language Models (VLMs) to Multi-Head Latent Attention (MLA). It introduces two core techniques: a modality-adaptive partial-RoPE strategy and a modality-decoupled low-rank approximation method, which independently compress the visual and textual Key-Value (KV) spaces. The approach minimizes adaptation cost and demonstrates significant performance improvements in inference efficiency and KV cache footprint reduction.",27.48,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11468v1_Exploring LLM Features in Predictive Process Monit.pdf,Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs,"ALESSANDRO PADELLA, MASSIMILIANO DE LEONI, MARLON DUMAS",10.1145/3219617.3219620,2601.01961,"Predictive process monitoring, Large language models, Trace Encoding","This paper extends the authors' prior work on LLM-based Predictive Process Monitoring, evaluating its generality, semantic leverage, and reasoning mechanisms across multiple Key Performance Indicators. Empirical evaluations indicate that LLMs can surpass benchmark methods in data-scarce settings with only 100 traces, leveraging both prior knowledge and internal correlations among training traces. The model demonstrates higher-order reasoning to generate predictions.",27.44,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11479v1_Health Facility Location in Ethiopia Leveraging LL.pdf,Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning,"Yohai Trabelsi, Guojun Xiong, Fentabil Getnet, Stéphane Verguet, Milind Tambe",Not found,Not found,"Health Facility Location, Optimization, Human expert knowledge, Alignment, LLM","Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. The project proposes a hybrid framework that systematically integrates expert knowledge with optimization techniques, combining a provable approximation algorithm for population coverage optimization with LLM-driven iterative refinement. Experiments on real-world data demonstrate the framework's effectiveness and its potential to inform equitable, data-driven health system planning.",26.96,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11492v1_BoxMind Closed-loop AI strategy optimization for e.pdf,BoxMind: Closed-loop AI strategy optimization for elite boxing,"Kaiwen Wang, Kaili Zheng, Rongrong Deng, Qingmin Fan, Milin Zhang, Zongrui Li, Xuesi Zhou, Bo Han, Liren Chen, Chenyi Guo, Ji Wu",,2601.11492,"Boxing, AI, Tactical Analysis, Predictive Modeling, Competitive Sports","A closed-loop AI expert system for elite boxing competition, validated in the 2024 Olympics, that optimizes tactical strategies by parsing match footage into technical-tactical indicators and using a graph-based predictive model to capture match dynamics and generate strategic recommendations.",27.51,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11496v1_The Poisoned Apple Effect Strategic Manipulation o.pdf,The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents,"Eilam Shapira, Moshe Tennenholtz, Roi Reichart",,,"AI agents, economic markets, strategic manipulation, regulatory frameworks, technological expansion","The integration of AI agents into economic markets alters strategic interaction, leading to a phenomenon termed the 'Poisoned Apple' effect. Simply increasing the choice of AI delegates can shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. However, this can also lead to manipulation by agents to favor themselves at the expense of their opponent and the regulator's fairness objectives. Static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.",27.9,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11505v1_MetaboNet The Largest Publicly Available Consolida.pdf,The Largest Publicly Available Consolidated Dataset for Type 1 Diabetes Management,"Miriam K. Wolff, Peter Calhoun, Eleonora Maria Aiello, Yao Qin, Sam F. Royston",10.1155/2026011505,2601.11505,"Type 1 Diabetes, Consolidated Dataset, Continuous Glucose Monitoring, Insulin Pump, Data Fragmentation, Data Standardization, Algorithm Development","This work aims to establish a unified and accessible data resource for Type 1 Diabetes algorithm development by consolidating multiple publicly available datasets, resulting in a substantially larger dataset than existing standalone benchmark datasets.",27.75,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11516v2_Building Production-Ready Probes For Gemini.pdf,Building Production-Ready Probes For Gemini,"János Kramár∗, Joshua Engels, Zheng Wang, Bilal Chughtai, Rohin Shah, Neel Nanda, Arthur Conmy∗",Not found,Not found,"Activation Probing, Interpretability, Language Models, Misuse Risk, AI Safety, Monitoring","In this paper, we describe our experience applying probes to detect cyber-offensive prompts given as input to Gemini 2.5 Flash. We identify a key challenge in existing probe architectures and propose new architectures that handle long-context distribution shifts. We evaluate these probes in the cyber-offensive domain and show that a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we demonstrate that pairing probes with prompted classifiers achieves optimal accuracy at a low cost.",28.13,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11517v1_Do explanations generalize across large reasoning .pdf,Under Review,"Koyena Pal, David Bau, Chandan Singh",,,"Large reasoning models, Chain of thought, Explanations, Generalization, Reinforcement learning","This paper investigates the generalization of explanations produced by large reasoning models, evaluating whether these explanations can induce the same behavior when given to other LRMs, and how this relates to human preference rankings and post-training with reinforcement learning.",25.62,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11625v1_Reasoning Stabilization Point A Training-Time Sign.pdf,Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence,Sahil Rajesh Dhayalkar,Not provided,Not provided,"Fine-tuning, Language models, Interpretability, Token-level attribution, Explanation drift, Reasoning stabilization point","Fine-tuning pretrained language models can subtly alter the evidence a model relies on. This paper proposes a training-time interpretability view that tracks token-level attributions across epochs. Explanation drift is defined as the epoch-to-epoch change in normalized token attributions on a fixed probe set, and the Reasoning Stabilization Point (RSP) is introduced as the earliest epoch after which drift remains low. Across multiple lightweight transformer classifiers and benchmark classification tasks, drift typically collapses into a low, stable regime early in training, while validation accuracy continues to change only marginally. RSP provides a simple, low-cost diagnostic for monitoring how decision evidence evolves during fine-tuning and selecting stable-evidence checkpoints.",27.95,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11643v1_Syllabic Agglutinative Tokenizations for Indonesia.pdf,Syllabic Agglutinative Tokenizations for Indonesian LLM: A Study from 'Gasing Literacy Learning System',"Hokky Situngkir*, Andhika Bernard Lumbantobing†, Yohanes Surya‡",,2601.11643v1,"Indonesian natural language processing, Indonesian computational linguistics, tokenization, large language models, Gasing Literacy Learning System, low-resource languages, Austronesian languages","This paper presents a novel syllable-based tokenization approach for Indonesian large language models, inspired by the Gasing Literacy Learning System's pedagogical methodology. It develops a tokenization framework that segments Indonesian text at syllable boundaries before applying byte-pair encoding, creating a vocabulary that aligns with the language's morphophonological structure. Empirical evaluation demonstrates substantial improvements over conventional tokenization methods.",27.79,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11644v1_Predicting When to Trust Vision-Language Models fo.pdf,Predicting When to Trust Vision-Language Models for Spatial Reasoning,"Muhammad Imran, Yugyung Lee",,,"Vision-Language Models, Spatial Reasoning, Robotic Navigation, Autonomous Driving, Image Editing","Vision-Language Models (VLMs) excel at high-level semantic tasks but exhibit critical weaknesses in spatial reasoning. This paper proposes a vision-based confidence estimation framework that validates VLM predictions through independent geometric verification using object detection. Unlike text-based approaches, our method fuses four signals via gradient boosting: geometric alignment between VLM claims and coordinates, spatial ambiguity from overlap, detection quality, and VLM internal uncertainty. Achieving AUROC of 0.674 on BLIP-2 and 0.583 on CLIP, our framework enables selective prediction and demonstrates reliable scene graph construction where confidence-based pruning improves precision from 52.1% to 78.3% while retaining 68.2% edges.",27.64,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11647v1_Reinforcement Learning for Dynamic Workflow Optimi.pdf,Reinforcement Learning for Dynamic Workflow Optimization in CI/CD Pipelines,"Aniket Abhishek Soni, Milan Parikh, Rashi Nimesh Kumar Dhenia, Jubin Abhishek Soni, Ayush Raj Jha, Sneja Mitinbhai Shah",,,"Reinforcement Learning, CI/CD, DevOps, Workflow Optimization",This paper proposes a reinforcement learning (RL) approach to optimize CI/CD pipeline workflows dynamically. It models the pipeline as a Markov Decision Process (MDP) and trains an RL agent to make runtime decisions that maximize throughput while minimizing testing overhead. Experimental results show significant improvements in pipeline efficiency.,27.03,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11650v1_Large Language Model Agent for User-friendly Chemi.pdf,LARGELANGUAGEMODELAGENT FORUSER-FRIENDLY CHEMICALPROCESSSIMULATIONS,"Jingkang Liang ∗, Niklas Groll∗, Gürkan Sin",Not found,2601.11650v1,"Chemical Process Simulation, Large Language Model, Model Context Protocol","This work integrates a large language model (LLM) agent with A VEV A Process Simulation (APS) via Model Context Protocol (MCP) to enable natural language interaction with rigorous process simulations, assessing its capabilities through two case studies.",28.34,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11651v1_Aesthetics as Structural Harm Algorithmic Lookism .pdf,Aesthetics as Structural Harm:ALGORITHMICLOOKISMACROSS,"Miriam Doh, Aditya Gulati, Corina Canali, Nuria Oliver",Not found,2601.11651,"Generative AI, Artificial Intelligence, Cognitive Biases, Attractiveness Halo Effect","This paper examines algorithmic lookism—the systematic preferential treatment based on physical appearance—in text-to-image (T2I) generative AI and a downstream gender classification task. It demonstrates how generative AI models systematically associate facial attractiveness with positive attributes and vice versa, mirroring socially constructed biases rather than evidence-based correlations.",27.26,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11652v1_WISP Waste- and Interference-Suppressed Distribute.pdf,WISP: Waste- and Interference-Suppressed Distributed Speculative LLM Serving at the Edge via Dynamic Drafting and SLO-Aware Batching,"XIANGCHEN LI, JIAKUN FAN, QINGYUAN WANG, DIMITRIOS SPATHARAKIS, SAEID GHAFOURI, HANS VANDIERENDONCK, DEEPU JOHN, BO JI, ALI R. BUTT, DIMITRIOS S. NIKOLOPOULOS",10.1145/376xxxx.377xxxx,,"Large Language Models, Edge Computing, Speculative Decoding, Token Verification, Resource-Aware Serving","This paper proposes WISP, an efficient and SLO-aware distributed LLM inference system that addresses the bottlenecks of wasted drafting time and verification interference in speculative LLM serving at the edge, improving system capacity and goodput by up to 2.1× and 3.7× compared to centralized serving and SLED, respectively.",27.31,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11657v1_Size is Not the Solution Deformable Convolutions f.pdf,Size is Not the Solution: Deformable Convolutions for Effective Physics Aware Deep Learning,"Jack T. Beerman, Shobhan Roy, H.S. Udaykumar, Stephen S. Baek",Not found,Not found,"Physics-aware deep learning, Convolutional neural networks, Deformable convolutions, Recurrent convolutions, Hybrid Lagrangian-Eulerian methods, Deep learning in physics","This paper introduces deformable physics-aware recurrent convolutions (D-PARC) to overcome the rigidity of CNNs in physics modeling. D-PARC achieves superior fidelity compared to substantially larger architectures across Burgers' equation, Navier-Stokes, and reactive flows. The approach leverages inspiration from Hybrid Lagrangian-Eulerian (HLE) numerical methods and demonstrates that physically intuitive architectural design can outperform parameter scaling in physics-aware deep learning.",27.69,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11658v1_Towards AGI A Pragmatic Approach Towards Self Evol.pdf,Towards AGI: A Pragmatic Approach Towards Self Evolving Agent,"Indrajit Kar, Zonunfeli Ralte",,,"Large Language Models (LLMs), Self-evolving agents, Curriculum Learning (CL), Reward-Based Learning (RL), Genetic Algorithm (GA) evolution, Multi-agent systems, Tool-augmented reasoning, Code-generation LLMs, Autonomous adaptation, TaskCraft dataset, Agentic workflows, Self-improving AI, Capability evolution, Hierarchical orchestration","This work introduces a hierarchical self-evolving multi-agent framework that integrates a Base LLM, an operational SLM agent, a Code-Generation LLM, and a Teacher-LLM to enable continuous adaptation. The framework uses Curriculum Learning, Reward-Based Learning, and Genetic Algorithm evolution to enable fast recovery, strong generalization, high difficulty task performance, and high behavioral diversity, respectively. Across all settings, evolved agents outperform their original counterparts, demonstrating robust, autonomous, self-improving agentic evolution.",28.05,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11663v1_Activation Sensitivity as a Unifying Principle for.pdf,ACTIVATIONSENSITIVITY AS AUNIFYINGPRINCIPLE FOR POST-TRAININGQUANTIZATION,Bruce Changlong Xu,,,"quantization, post-training, activation sensitivity, language models","This work presents a unified theoretical framework for post-training quantization (PTQ) by formalizing activation sensitivity, which is the expected impact of channel-wise perturbations on the loss. It connects gradient-based saliency, Fisher information, and Hessian-based criteria, and clarifies their relationships to classical pruning methods. This perspective exposes fundamental limitations of layer-local reconstruction objectives and highlights open challenges in PTQ, including cross-layer error accumulation and task-conditional sensitivity.",27.14,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11664v1_Serverless AI Security Attack Surface Analysis and.pdf,Serverless AI Security: Attack Surface Analysis and Runtime Protection Mechanisms for FaaS-Based Machine Learning,"Chetan Pathade, Vinod Dhimam, Ilsa Lareb, Sheheryar Ahmad",,,"serverless computing, machine learning security, function-as-a-service, cloud security, adversarial machine learning, AWS Lambda, Azure Functions, attack surface analysis, runtime protection, MLOps security","This paper presents a comprehensive security analysis of machine learning workloads in serverless environments, focusing on five categories: function-level vulnerabilities, model-specific threats, infrastructure attacks, supply chain risks, and IAM complexity. Empirical assessments across AWS Lambda, Azure Functions, and Google Cloud Functions demonstrate real-world attack scenarios and quantify their security impact. The paper proposes Serverless AI Shield (SAS), a multi-layered defense framework for serverless AI deployments.",27.56,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11666v1_MATEX Multi-scale Attention and Text-guided Explai.pdf,MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models,"Muhammad Imran, Chi Lee, Yugyung Lee",Not found,2601.11666,"Explainable AI, Medical Imaging, Vision-Language Models, Gradient Attribution, Attention Rollout, Chest X-ray, CLIP","We introduce MATEX, a novel framework that enhances interpretability in medical vision-language models by incorporating anatomically informed spatial reasoning. MATEX synergistically combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to produce precise, stable, and clinically meaningful gradient attribution maps, addressing key limitations of prior methods.",28.01,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11667v1_Distill-then-Replace Efficient Task-Specific Hybri.pdf,Distill-then-Replace: Efficient Task-Specific Hybrid Attention Model Construction,"Xiaojie Xia, Huigang Zhang, Chaoliang Zhong, Jun Sun, Yusuke Oishi",Not found,2601.11667,"Hybridattentionmodels, Blockwiselocaldistillation, Greedy, search","This paper addresses the challenges of training and deploying hybrid attention models with quadratic complexity, proposing a method that first transfers weights from pretrained full-attention modules to linear attention counterparts through blockwise local distillation, and then iteratively replaces full attention blocks with linear ones while monitoring validation performance on the target task.",28.28,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11670v1_A Confidence-Variance Theory for Pseudo-Label Sele.pdf,A Conﬁdence-V ariance Theory for Pseudo-Label Selection in Semi-Supervised Learning,"Jinshi Liu †, Pan Liu †",Not found,Not found,"Semi-Supervised Learning, Pseudo-Labels, Confidence Calibration, Residual Class Variance, Spectral Relaxation, Semantic Segmentation, Image Classification","This paper introduces a Conﬁdence-Variance (CoVar) theory framework for pseudo-label selection in semi-supervised learning, aiming to provide a principled joint reliability criterion. It combines maximum conﬁdence with residual-class variance, characterizing how probability mass is distributed over non-maximum classes. The paper integrates CoVar as a plug-in module into representative methods and demonstrates consistent improvements over strong baselines across various datasets and label ratios.",27.6,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11674v1_Pigment Network Detection and Classification in De.pdf,Pigment Network Detection and Classification in Dermoscopic Images Using Directional Imaging Algorithms and Convolutional Neural Networks,"M. A. Rasel, Sameem Abdul Kareem, Unaizah Obaidellah",10.1016/j.bspc.2024.106883,,"Melanoma, Dermoscopic Images, Pigment Networks, Contrast Enhancement, Threshold Level, Convolutional Neural Networks, Bag of Features","This study aims to automate the pigment network detection process using a directional imaging algorithm and classify pigment network types using machine learning classifiers. The directional imaging algorithm incorporates Principal Component Analysis (PCA), contrast enhancement, filtering, and noise reduction. The study achieved 90% accuracy, 90% sensitivity, and 89% specificity with a simple CNN model, outperforming state-of-the-art methods.",27.96,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11675v1_Generating metamers of human scene understanding.pdf,Preprint: GENERATING METAMERS OF HUMAN SCENE UNDERSTANDING,"Ritik Raina, Abe Leite, Alexandros Graikos, Seoyoung Ahn, Dimitris Samaras, Gregory J. Zelinsky",,abs/2309.15698,"scene understanding, human vision, latent diffusion model, foveated scenes, metamerism","This paper introduces MetamerGen, a tool for generating scenes that align with latent human scene representations. It combines peripheral scene gist information with information from scene-viewing fixations to generate image metamers for what humans understand after viewing a scene. The authors evaluate the perceptual alignment of MetamerGen-generated images to latent human scene representations through a same-different behavioral experiment.",30.26,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11676v1_HALO Semantic-Aware Distributed LLM Inference in L.pdf,HALO: Semantic-Aware Distributed LLM Inference in Lossy Edge Network,"Peirong Zheng, Wenchao Xu*, Haozhao Wang, Jinyu Chen, Xuemin (Sherman) Shen",,,"Large Language Models, Tensor Parallelism, Edge Computing, Heterogeneity, Semantics, Packet Loss","The deployment of large language models' inference at the edge can facilitate prompt service responsiveness while protecting user privacy. However, it is critically challenged by the resource constraints of a single edge node. Distributed inference has emerged to aggregate and leverage computational resources across multiple devices. This paper proposes HALO, a novel framework that can boost the distributed LLM inference in lossy edge network by enabling a relaxed yet effective synchronization through strategically allocating less critical neuron groups to unstable devices.",35.53,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11683v1_Attesting Model Lineage by Consisted Knowledge Evo.pdf,Attesting Model Lineage by Consisted Knowledge Evolution,"Zhuoyi Shang, Jiasen Li, Pengzhen Chen, Yanwei Liu, Xiaoyan Gu, Weiping Wang",Not found,Not found,"deep learning, model lineage, fine-tuning, knowledge evolution, security",The paper proposes a novel model lineage attestation framework that leverages the dynamic evolution of knowledge and parameter modification during fine-tuning to verify the lineage relationship. It addresses security concerns such as unauthorized model redistribution and false claim of model provenance in open-weight model libraries.,34.35,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11684v1_Mobile-friendly Image de-noising Hardware Consciou.pdf,Mobile-friendly Image de-noising: Hardware Conscious Optimization for Edge Application,"Srinivas Miriyala*, Sowmya Vajrala*, Hitesh Kumar, Sravanth Kodavanti, Vikram Rajendiran",,,"De-Noising, Differentiable NAS, Hardware-aware Search space, Smartphone Deployment","This work presents a novel mobile-friendly network for image de-noising obtained with Entropy-Regularized differentiable Neural Architecture Search (NAS) on a hardware-aware search space for a U-Net architecture, demonstrating competitive accuracy with reduced resource usage compared to state-of-the-art methods.",34.74,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11685v1_Towards Efficient Image Deblurring for Edge Deploy.pdf,Towards Efficient Image Deblurring for Edge Deployment,"Srinivas Soumitri Miriyala*, Sowmya Lahari Vajrala*, Rama Sravanth Kodavanti*",Not found,Not found,"Mobile Image Signal Processing (ISP), De-blurring, Training-free Search, Inference Optimization, Edge Deployment","This paper proposes a hardware-aware adaptation framework to optimize existing image deblurring models, achieving up to 55% reduction in GMACs compared to recent transformer-based SOTA while maintaining competitive accuracy and improving 1.25× on-device deployment latency.",36.87,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11686v1_Proof of Concept Multi-Target Wildfire Risk Predic.pdf,Proof of Concept: Multi-Target Wildfire Risk Prediction and Large Language Model Synthesis,"Nicolas Caron ∗, Hassan Noura †, Christophe Guyeux ‡, Benjamin Aynes§",Not found,2601.11686,"wildfire risk, multi-target analysis, predictive models, large language models, operational needs, first responders, firefighting services, climate change, fire management","This paper proposes a hybrid framework combining predictive models for each risk dimension of wildfire with large language models (LLMs) to synthesize heterogeneous outputs into structured, actionable reports, addressing the operational needs of first responders and firefighting services.",37.55,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11687v1_Semantic Caching and Intent-Driven Context Optimiz.pdf,Semantic Caching and Intent-Driven Context Optimization for Multi-Agent Natural Language to Code Systems,"Harmohit Singh, CoreOps AI",Not found,2601.11687,"Natural Language to Code, Multi-Agent Systems, Semantic Caching, LLM Optimization, Production Systems","We present a production-optimized multi-agent system designed to translate natural language queries into executable Python code for structured data analytics, achieving high accuracy and cost efficiency through semantic caching, dual-threshold decision mechanism, and intent-driven dynamic prompt assembly.",40.27,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11688v1_SpecMap Hierarchical LLM Agent for Datasheet-to-Co.pdf,SpecMap: Hierarchical LLM Agent for Datasheet-to-Code Link Recovery,"Vedant Nipane, Pulkit Agrawal, Amit Singh",,2601.11688v1,"Datasheet-to-code mapping, Large language models, Traceability, Embedded systems, Systems engineering","Presenting a hierarchical datasheet-to-code mapping methodology that employs large language models for semantic analysis, explicitly structuring the traceability process across multiple abstraction levels to improve file mapping accuracy over traditional information-retrieval-based baselines.",31.15,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11700v1_Telling Human and Machine Handwriting Apart.pdf,Telling Human and Machine Handwriting Apart,"Luis A. Leiva, Moises Diaz, Nuwan T. Attygalle, Miguel A. Ferrer, Réjean Plamondon",,,"Biometrics, classification, deep learning, reverse Turing test, verification","Handwriting movements can be used as behavioral biometrics to verify if a real user is operating a device or application. This work studies ten public datasets of handwritten symbols and gestures, artificially reproduced using seven different synthesizers, and trains a shallow recurrent neural network to achieve excellent performance in detecting human-generated inputs. The classifier performs well in few-shot and out-of-domain settings.",26.33,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11702v1_PASTA A Scalable Framework for Multi-Policy AI Com.pdf,PASTA: A Scalable Framework for Multi-Policy AI Compliance Evaluation,"YU YANG, The University of British Columbia, Canada, IG-JAE KIM, Korea Institute of Science and Technology, South Korea, DONGWOOK YOON, The University of British Columbia, Canada",Not provided,Not provided,"AI compliance, multi-policy evaluation, scalable framework, policy normalization, AI governance","PASTA is a scalable compliance tool integrating four innovations: a comprehensive model-card format, a policy normalization scheme, an efficient LLM-powered pairwise evaluation engine, and an interface delivering interpretable evaluations. It aligns closely with human experts and evaluates five major policies in under two minutes at approximately $3.",33.06,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11713v1_Inter-Cell Interference Rejection Based on Ultrawi.pdf,Inter-Cell Interference Rejection Based on Ultrawideband Walsh-Domain Wireless Autoencoding,"Rodney Martinez Alonso, Cel Thys, Sofie Pollin, Cedric Dehos, Yuneisy Esthela Garcia Guzman",,,"inter-cell interference, ultrawideband, Walsh-domain, wireless autoencoding, 5G CP-OFDM interference","This paper proposes a novel technique for rejecting partial-in-band inter-cell interference (ICI) in ultrawideband communication systems. It presents an end-to-end wireless autoencoder architecture that jointly optimizes the transmitter and receiver encoding/decoding in the Walsh domain to mitigate interference from coexisting narrower-band 5G base stations. By exploiting the orthogonality and self-inverse properties of Walsh functions, the system distributes and learns to encode bit-words across parallel Walsh branches. Through analytical modeling and simulation, it characterizes how 5G CP-OFDM interference maps into the Walsh domain and identifies optimal ratios of transmission frequencies and sampling rate where the end-to-end autoencoder achieves the highest rejection. Experimental results show that the proposed autoencoder achieves up to 12 dB of ICI rejection while maintaining a low block error rate (BLER) for the same baseline channel noise, i.e., baseline Signal-to-Noise-Ratio (SNR) without the interference.",38.48,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11746v1_LIME-LLM Probing Models with Fluent Counterfactual.pdf,LIME-LLM: Probing Models with Fluent Counterfactuals,"George Mihaila, Suleyman Olcay Polat, Poli Nemkova, Himanshu Sharma, Namratha V. Urs, Mark V. Albert",,2601.11746,"explainability, local explanation, large language models, counterfactuals, natural language processing","A framework for local explanation methods in natural language processing using hypothesis-driven, controlled perturbations instead of random token masking, aiming to improve the fidelity of local surrogate models.",35.68,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11747v1_PRISM Learning Design Knowledge from Data for Styl.pdf,PRISM: Learning Design Knowledge from Data,"Huaxiaoyue Wang, Sunav Choudhary, Franck Dernoncourt, Yu Shen, Stefano Petrangeli",,,"Design, Stylistic Improvement, Vision Language Models, Design Knowledge, Real-World Designs","This paper addresses the problem of stylistically improving designs based on natural language instructions. It proposes PRISM (Prior-Informed Stylistic Modification), a method that constructs and applies a design knowledge base through three stages: clustering high-variance designs, summarizing each cluster into actionable design knowledge, and retrieving relevant knowledge during inference to enable style-aware improvement. Experiments on the Crello dataset show that PRISM achieves the highest average rank of 1.49 over baselines in style alignment. User studies further validate these results, showing that PRISM is consistently preferred by designers.",36.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11758v1_Early Linguistic Pattern of Anxiety from Social Me.pdf,Early Linguistic Pattern of Anxiety from Social Media,Arnab Das,,,"anxiety detection, linguistic pattern, interpretable machine learning, keyword robustness, cross-domain validation, author-disjoint evaluation, mental health screening","This work presents a transparent approach to social media-based anxiety detection through linguistically interpretable feature-grounded modeling and cross-domain validation. Using a substantial dataset of Reddit posts, a logistic regression classifier was trained on carefully curated subreddits for training, validation, and test splits. Comprehensive evaluation included feature ablation, keyword masking experiments, and varying-density difference analyses comparing anxious and control groups, along with external validation using clinically interviewed participants with diagnosed anxiety disorders. The model achieved strong performance while maintaining high accuracy even after sentiment removal or keyword masking. Early detection using minimal post history significantly outperformed random classification, and cross-domain analysis demonstrated strong consistency with clinical interview data. Results indicate that transparent linguistic features can support reliable, generalizable, and keyword-robust anxiety detection. The proposed framework provides a reproducible baseline for interpretable mental health screening across diverse online contexts.",37.34,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11762v1_Industry-Aligned Granular Topic Modeling.pdf,Industry-Aligned Granular Topic Modeling,"Sae Young Moon, Myeongjun Erik Jang, Haoyan Luo, Chunyang Xiao, Antonios Georgiadis, Fran Silavong",,,"Topic modeling, Granularity, Natural Language Processing, Large Language Models, Business Applications, Document Summarization, Topic Hierarchy","This paper introduces a framework called TIDE, which provides a novel granular topic modeling method based on large language models (LLMs) as a core feature, along with other useful functionalities for business applications, such as summarizing long documents, topic parenting, and distillation. The framework demonstrates superior performance compared to modern topic modeling methods and provides valuable support for industrial business scenarios.",35.42,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11768v1_Lightweight Self-Supervised Detection of Fundament.pdf,Lightweight Self-Supervised Detection of Fundamental Frequency and Accurate Probability of Voicing in Monophonic Music,"Venkat Suprabath Bitra, Homayoon Beigi",Not provided,Not provided,"self-supervised pitch detection, unsupervised pitch detection, fundamental frequency, pitch estimation, resonance, musical timbre transfer, probability of voicing, music synthesis, music analysis, CQT, constant Q transform, DDSP, shift cross-entropy loss, musical instrument modeling, ResNeXt neural network, music information retrieval, MIR","Reliable fundamental frequency (F0) and voicing estimation is essential for neural synthesis, yet many pitch extractors depend on large labeled corpora and degrade under realistic recording artifacts. We propose a lightweight, fully self-supervised framework for joint F0 estimation and voicing inference, designed for rapid single-instrument training from limited audio. Using transposition-equivariant learning on CQT features, we introduce an EM-style iterative reweighting scheme that uses Shift Cross-Entropy (SCE) consistency as a reliability signal to suppress uninformative noisy/unvoiced frames. The resulting weights provide confidence scores that enable pseudo-labeling for a separate lightweight voicing classifier without manual annotations. Trained on MedleyDB and evaluated on MDB-stem-synth ground truth, our method achieves competitive cross-corpus performance (RPA 95.84, RCA 96.24) and demonstrates cross-instrument generalization.",28.48,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11776v1_Cleansing the Artificial Mind A Self-Reflective De.pdf,Cleansing the Artificial Mind: A Self-Reflective Detoxification Framework for Large Language Models,"Kaituo Zhang, Zhimeng Jiang, Na Zou",Not found,Not found,"Large Language Models, Toxic Content, Self-Reflective Detoxification, Reinforcement Learning from Human Feedback, Instruction Tuning","This paper introduces a fully self-reflective detoxification framework for Large Language Models (LLMs) to detect, correct toxic content, and refine models without external modules and data annotation. It proposes a Toxic Signal Detector to transform toxic text into non-toxic counterparts, enhancing the model's ability for safe and coherent text generation.",29.54,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11778v1_Translation as a Scalable Proxy for Multilingual E.pdf,Translation as a Scalable Proxy for Multilingual Evaluation,"Sheriff Issaka1, Erick Rosas Gonzalez1*, Lieqi Liu1*, Evans Kofi Agyei2, Lucas Bandarkar1, Nanyun Peng 1, David Ifeoluwa Adelani 3, Francisco Guzmán4, Saadia Gabriel 1",,,"large language models, multilingual evaluation, translation quality, benchmarks, cost-effectiveness","The rapid proliferation of large language models has created a critical evaluation paradox: while these models claim multilingual proficiency, comprehensive non-machine-translated benchmarks exist for fewer than 30 languages, leaving over 98% of the world's 7,000 languages in an empirical void. This evaluation vacuum has profound consequences as entire language communities are excluded from AI development. This work evaluates the validity of translation quality as a proxy for a model's broader multilingual capabilities, finding that translation performance is a good indicator of downstream task success.",36.85,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11781v1_Risk-Aware Human-in-the-Loop Framework with Adapti.pdf,Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles,"Dawood Wasif, Terrence J. Moore, Seunghyun Yoon, Hyuk Lim, Dan Dongseong Kim, Frederica F. Nelson, Jin-Hee Cho",Not found,Not found,"autonomous vehicles, cyber-physical intrusions, risk-aware, human-in-the-loop, intrusion response, adaptive control, soft actor-critic, reinforcement learning, model-based control","This paper presents RAIL, a risk-aware human-in-the-loop framework for autonomous vehicles that integrates heterogeneous runtime signals to adapt control and learn focused. When the Intrusion Risk Score (IRS) exceeds a threshold, actions are blended with a cue-specific shield using a learned authority, while human override remains available. When risk is low, the nominal policy executes. Contextual bandits arbitrate among shields based on the cue vector, improving mitigation choices online. RAIL couples Soft Actor–Critic (SAC) with risk-prioritized replay and dual rewards, steering learning while maintaining nominal behavior. On MetaDrive, RAIL achieves high test performance metrics, outperforming RL, safe RL, offline/imitation learning, and prior HITL baselines. Under CAN injection and LiDAR spoofing attacks, RAIL improves success rates and reduces disengagement and attack success rates.",38.25,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11792v1_A self-evolving multi-role collaborative framework.pdf,A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation,"Yifei Suna, Yongan Lia, A.K. Qinb, Sicheng Houa, Tamas Pflanznerc",,,"Problem generation, Large language models, Multi-role collaboration, Intelligent education, Self-evolution, Knowledge distillation","Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation tasks. Although existing LLMs can achieve high correctness rates, they generally lack innovation and exhibit poor discrimination. This paper proposes the task of innovative math problem generation (IMPG) and introduces a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance to solve the IMPG task. The framework includes a multi-role collaborative mechanism comprising a sampler, generator, evaluator, state machine, and memory, ensuring the correctness of generated problems through iterative optimization informed by self-assessment and external feedback. An improved difficulty model is introduced to quantify difficulty and provide fine-grained guidance. The data-driven association-guided pathsampling (DAPS) algorithm is adopted to enhance the semantic rationality of sampled encodings. The HSM3K-CN dataset is constructed, comprising high-quality highschool math problems. A multi-stage training pipeline is adopted, incorporating continual pre-training (CPT), supervised fine-tuning (SFT), and group relative policy optimization (GRPO), to enhance the generation and evaluation capabilities of the base model. Finally, system self-evolution is achieved by transferring evaluation capabilities from the expert model to the apprentice model via distillation. Experiments show that compared to baseline models, our proposed method significantly improves the innovation of the generated problems while maintaining a high correctness rate.",33.96,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11801v1_RobotDesignGPT Automated Robot Design Synthesis us.pdf,RobotDesignGPT: Automated Robot Design Synthesis using Vision Language Models,"Nitish Sontakke, K. Niranjan Kumar, Sehoon Ha",Not provided,Not provided,"Robot Design, Vision-Language Models, Automated Design, Kinematic Structures, Visual Appearance","This paper proposes an automated robot design framework, RobotDesignGPT, that leverages the general knowledge and reasoning capabilities of large pre-trained vision-language models to synthesize initial robot designs from simple user prompts and reference images. The framework improves design quality and reduces manual feedback, designing visually appealing and kinematically valid robots inspired by nature.",35.5,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11809v1_Multi-agent DRL-based Lane Change Decision Model f.pdf,Multi-agent DRL-based Lane Change Decision,"Zeyu Mu1, Shangtong Zhang 2, B. Brian Park 3",,,"Multi-Agent, Reinforcement Learning, Cooperative Platooning, Lane Change","This study proposes a hybrid multi-agent lane change decision model to increase CA V participation in cooperative platooning and maximize its benefits. The model employs the QMIX framework, integrating traffic data through a CNN-QMIX architecture, and includes a trajectory planner and model predictive controller for smooth and safe lane-change execution. The model is trained and evaluated in a microsimulation environment under varying CA V market penetration rates, demonstrating significant improvements in cooperative platooning rates.",36.12,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11816v1_POLARIS Typed Planning and Governed Execution for .pdf,POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation,"Zahra Moslemi, Keerthi Koneru, Yen-Ting Lee, Sheethal Kumar, Ramesh Radhakrishnan",,,"Agentic AI, Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed Planning, Evaluation","POLARIS presents a governed orchestration framework for agentic systems in enterprise back-office automation, treating automation as typed plan synthesis and validated execution over LLM agents. It achieves high precision in anomaly routing and audit trails, providing a methodological and benchmark reference for policy-aligned Agentic AI.",31.57,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11825v1_AI Co-Scientist for Knowledge Synthesis in Medical.pdf,AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept,"Arya Rahgozara, Pouria Mortezaagha",,2601.11825v1,"AI, knowledge synthesis, medical research, redundant studies, incomplete reporting, scalable evidence synthesis, dementia, sport, non-communicable disease","This paper presents a proof of concept for an AI co-scientist designed to enable scalable, transparent knowledge synthesis in medical contexts by formalizing Population, Intervention, Comparator, Outcome, and Study design (PICOS) through automated compliance classification and retrieval-augmented generation.",27.83,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11840v1_Imandra CodeLogician Neuro-Symbolic Reasoning for .pdf,Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of Software Logic,"Hongyu Lin, Samer Abdallah, Makar Valentinov, Paul Brennan, Elijah Kagan, Christoph M. Wintersteiger, Denis Ignatovich, Grant Passmore",10.1101/2601.11840v1,2601.11840,"Neuro-symbolic reasoning, Software logic, LLM, Formal verification, Automated reasoning","This paper presents CodeLogician, a neuro-symbolic agent and framework for precise analysis of software logic, integrating with ImandraX, an industrial automated reasoning engine. It addresses the gap between mathematical proof automation and software engineering tasks, providing substantial improvements in reasoning accuracy and coverage.",32.43,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11850v1_Human-AI Collaborative Inductive Thematic Analysis.pdf,Human–AI Collaborative Inductive Thematic Analysis: How AI Guides Analysis and Researchers Reclaim Interpretive Authority,"Matthew Nyaaba †1,2, Min SungEun †3, Mary Abiswin Apam †4, Kwame Owoahene Acheampong5, Emmanuel Dwamena6, Xiaoming Zhai1, 7",Not provided,Not provided,"Human–AI collaboration, Generative artificial intelligence (GenAI), Inductive thematic analysis, Qualitative data analysis, Epistemic authority, Reflexive methodology","This study investigates how researchers interact with an Inductive Thematic Analysis GPT (ITA–GPT), a purpose-built AI tool designed to operationalize established inductive thematic analysis procedures, to understand how AI shapes analytic practice, judgment, and epistemic authority in qualitative research.",37.41,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11854v1_ATOD An Evaluation Framework and Benchmark for Age.pdf,ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System,"Yifei Zhang, Hooshang Nayyeri, Rinat Khaziev, Emine Yilmaz, Gokhan Tur, Dilek Hakkani-Tür, Hari Thadakamalla",Not provided,Not provided,"Task-Oriented Dialogue, Agentic Behavior, Evaluation Framework, Benchmark, Memory Management, Long-Horizon Context","Recent advances in task-oriented dialogue systems have enabled conversational agents to coordinate interleaved goals, maintain long-horizon context, and act proactively through asynchronous execution. ATOD introduces a benchmark and synthetic dialogue generation pipeline that captures key characteristics of advanced TOD, including multi-goal coordination, dependency management, memory, adaptability, and proactivity. ATOD-Eval is a holistic evaluation framework that translates these dimensions into fine-grained metrics and supports reproducible offline and online evaluation.",36.47,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11859v1_Cascaded Transformer for Robust and Scalable SLA D.pdf,Cascaded Transformer for Robust and Scalable SLA Decomposition via Amortized Optimization,Cyril Shih-Huan Hsu,Not found,Not found,"network slicing, service level agreement, quality of service, deep neural network, optimization, transformers","The paper introduces Casformer, a cascaded Transformer architecture designed for fast, optimization-free SLA decomposition. It leverages historical domain feedback and integrates cross-domain dependencies to achieve improved SLA decomposition quality and enhanced scalability and robustness.",34.3,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11863v1_Utilizing Metadata for Better Retrieval-Augmented .pdf,Utilizing Metadata for Better Retrieval-Augmented Generation,"Raquib Bin Yousuf, Shengzhe Xu, Mandar Sharma, Andrew Neeser, Chris Latimer, Naren Ramakrishnan",Not found,2601.11863,"Retrieval-Augmented Generation (RAG), Metadata-aware Retrieval, Dense Retrieval, Query Reformulation, Benchmark Datasets","Retrieval-Augmented Generation systems rely on retrieving semantically relevant document chunks to support accurate, grounded outputs from large language models. This study evaluates metadata-aware retrieval strategies, comparing plain-text baselines with approaches that embed metadata directly, and finds that prefixing and unified embeddings consistently outperform plain-text baselines, with the unified embedding sometimes exceeding prefixing while being easier to maintain. The study also analyzes embedding space, showing that metadata integration improves effectiveness by increasing intra-document cohesion, reducing inter-document confusion, and widening the separation between relevant and irrelevant chunks.",38.19,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11868v1_Terminal-Bench Benchmarking Agents on Hard Realist.pdf,"TERMINAL-BENCH: BENCHMARKING AGENTS ON HARD, REALISTIC TASKS IN COMMANDLINE INTERFACES","Mike A. Merrill, Alexander G. Shaw, Nicholas Carlini, Boxuan Li, Harsh Raj, Ivan Bercovich, Lin Shi, Jeong Yeon Shin, Thomas Walshe, E Kelly Buchanan, Junhong Shen, Guanghao Ye, Haowei Lin, Jason Poulos, Maoyu Wang, Marianna Nezhurina, Jenia Jitsev, Di Lu, Orfeas Menis Mastromichalakis, Zhiwei Xu, Zizhao Chen, Yue Liu, Robert Zhang, Leon Liangyu Chen, Anurag Kashyap, Jan-Lucas Uslu, Jeffrey Li, Jianbo Wu, Minghao Yan, Song Bian, Vedang Sharma, Ke Sun, Steven Dillmann, Akshay Anand, Andrew Lanpouthakoun, Bardia Koopah, Changran Hu, Etash Guha, Gabriel H. S. Dreiman, Jiacheng Zhu, Karl Krauth, Li Zhong, Niklas Muennighoff, Robert Amanfu, Shangyin Tan, Shreyas Pimpalgaonkar, Tushar Aggarwal, Xiangning Lin, Xin Lan, Xuandong Zhao, Yiqing Liang, Yuanli Wang, Zilong Wang, Changzhi Zhou, David Heineman, Hange Liu, Harsh Trivedi, John Yang, Junhong Lin, Manish Shetty, Michael Yang, Nabil Omi, Negin Raoof, Shanda Li, Terry Yue Zhuo, Wuwei Lin, Yiwei Dai, Yuxin Wang, Wenhao Chai, Shang Zhou, Dariush Wahdany, Ziyu She, Jiaming Hu, Zhikang Dong, Yuxuan Zhu, Sasha Cui, Ahson Saiyed, Arinbj ¨orn Kolbeinsson, Jesse Hu, Christopher Michael Rytting, Ryan Marten, Yixin Wang, Alex Dimakis, Andy Konwinski, Ludwig Schmidt",,2601.11868v1,"AI agents, benchmarking, commandline interfaces, realistic tasks, hard benchmarks","A carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by real workflows, designed to measure frontier models and agents, showing they score less than 65% and identifying areas for improvement.",37.31,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11876v1_AI for Green Spaces Leveraging Autonomous Navigati.pdf,Autonomous Trash Pickup Robots for Grass Fields,Authors not specified,Not specified,Not specified,"Robotics, Autonomous, Trash Pickup, Grass Fields, Real-Time Kinematic (RTK) GPS, ResNet50 CNN, Coverage Path","This paper proposes an autonomous robot designed to navigate, identify, and pick up litter in parks. It uses a Spanning Tree Coverage (STC) algorithm for navigation and a ResNet50 Convolutional Neural Network for trash detection. The robot achieved an overall success rate of 80% in field tests.",35.0,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11880v1_TF-CoDiT Conditional Time Series Synthesis with Di.pdf,TF-CoDiT: Conditional Time Series Synthesis with Diffusion Transformers for Treasury Futures,"Yingxiao Zhang, Jiaxin Duan, Junfu Zhang, Ke Feng",Not provided,Not provided,"Treasury futures, Time series synthesis, Diffusion Transformers, Financial markets, Quantitative investment","This work proposes TF-CoDiT, the first Diffusion Transformers framework for language-controlled treasury futures synthesis, addressing challenges such as low volume, market dependencies, and multivariable correlations. It adapts the standard Diffusion Transformers by transforming multi-channel 1-D time series into Discrete Wavelet Transform coefficient matrices and uses a U-shape VAE to encode cross-channel dependencies into a latent variable. Financial Market Attribute Protocol (FinMAP) is introduced to standardize market dynamics. Extensive evaluations show TF-CoDiT can produce highly authentic data with errors at most 0.433 (MSE) and 0.453 (MAE) to the ground-truth, demonstrating robustness across contracts and temporal horizons.",37.23,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11885v1_MyGram Modality-aware Graph Transformer with Globa.pdf,MyGram: Modality-aware Graph Transformer with Global Distribution for Multi-modal Entity Alignment,"Zhifei Li, Ziyue Qin, Xiangyu Luo, Xiaoju Hou, Yue Zhao, Miao Zhang, Zhifang Huang, Kui Xiao, Bing Yang",Not found,Not found,"Multi-modal entity alignment, Graph transformer, Modality-aware, Global distribution, Knowledge graph","This paper proposes MyGram, a modality-aware graph transformer designed to improve multi-modal entity alignment by capturing deep structural contextual information within modalities and achieving global distribution consistency across them. Experiments on five public datasets demonstrate MyGram's superior performance compared to baseline models.",33.93,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11895v1_DevBench A Realistic Developer-Informed Benchmark .pdf,"DevBench: A Realistic, Developer-Informed Benchmark for Code Generation Models","Pareesa Ameneh Golnari ∗, Adarsh Kumarappan ∗∗, Wen Wen, Xiaoyu Liu, Gabriel Ryan, Yuting Sun, Shengyu Fu, Elsie Nallipogu",Not found,Not found,"Large Language Models, Code Generation, Telemetry, Benchmark, Realism, Developer Informed","DevBench is a telemetry-driven benchmark designed to evaluate Large Language Models (LLMs) on realistic code completion tasks. It includes 1,800 evaluation instances across six programming languages and six task categories derived from real developer telemetry, focusing on common yet challenging completion scenarios. Unlike prior benchmarks, it emphasizes ecological validity, avoids training data contamination, and enables detailed diagnostics.",36.95,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11903v1_AEMA Verifiable Evaluation Framework for Trustwort.pdf,AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems,"Yen-Ting Lee, Keerthi Koneru, Zahra Moslemi, Sheethal Kumar, Ramesh Radhakrishnan",Not found,2601.11903,"Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight","Evaluating large language model (LLM)-based multi-agent systems remains a critical challenge, as these systems must exhibit reliable coordination, transparent decision-making, and verifiable performance across evolving tasks. Existing evaluation approaches often limit themselves to single-response scoring or narrow benchmarks, which lack stability, extensibility, and automation when deployed in enterprise settings at multi-agent scale. We present AEMA (Adaptive Evaluation Multi-Agent), a process-aware and auditable framework that plans, executes, and aggregates multi-step evaluations across heterogeneous agentic workflows under human oversight. Compared to a single LLM-as-a-Judge, AEMA achieves greater stability, human alignment, and traceable records that support accountable automation. Our results on enterprise-style agent workflows demonstrate that AEMA provides a transparent and reproducible pathway toward responsible evaluation of LLM-based multi-agent systems.",38.64,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11905v1_LIBRA Language Model Informed Bandit Recourse Algo.pdf,LIBRA: Language Model Informed Bandit Recourse,"Junyu Cao, Ruijiang Gao, Esmaeil Keyvanshokooh, Jianhao Ma",,,"Large Language Models, LLM-Bandits Collaboration, Algorithmic Recourse, Regret Analysis, Personalized Treatment Planning, Hypertension Management","We introduce a unified framework that seamlessly integrates algorithmic recourse, contextual bandits, and large language models (LLMs) to support sequential decision-making in high-stakes settings such as personalized medicine. We develop the Generalized Linear Recourse Bandit (GLRB) algorithm and propose LIBRA, a Language Model–Informed Bandit Recourse Algorithm that strategically combines domain knowledge from LLMs with the statistical rigor of bandit learning.",36.83,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11907v1_Towards Airborne Object Detection A Deep Learning .pdf,Towards Airborne Object Detection: A Deep Learning Analysis,"1st Prosenjit Chatterjee, 2nd ANK Zaman",,,"Airborne Object Detection, Deep Learning, EfficientNetB4, ResNet-50, Automated Threat Assessment","This work introduces a dual-task model based on EfficientNetB4 capable of performing airborne object classification and threat-level prediction simultaneously. It addresses the scarcity of clean, balanced training data by constructing the AODTA Dataset and benchmarks the model on both A VD and AODTA datasets, achieving high accuracy and reliable performance.",35.28,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11913v1_LSTM-MAS A Long Short-Term Memory Inspired Multi-A.pdf,LSTM-MAS: A Long Short-Term Memory Inspired Multi-Agent System for Long-Context Understanding,"Yichen Jiang, Peng Ye, Jiakang Yuan, Chongjun Tu, Lei Bai, Tao Chen",Not found,Not found,"Long-Context Understanding, Large Language Models, Multi-Agent System, Memory","Effectively processing long contexts remains a fundamental yet unsolved challenge for large language models (LLMs). Existing single-LLM-based methods primarily reduce the context window or optimize the attention mechanism, but they often encounter additional computational costs or constrained expanded context length. While multi-agent-based frameworks can mitigate these limitations, they remain susceptible to the accumulation of errors and the propagation of hallucinations. In this work, we draw inspiration from the Long Short-Term Memory (LSTM) architecture to design a Multi-Agent System called LSTM-MAS, emulating LSTM’s hierarchical information flow and gated memory mechanisms for long-context understanding.",36.58,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11920v1_Enhancing LLM-Based Data Annotation with Error Dec.pdf,Enhancing LLM-Based Data Annotation with Error Decomposition,"Zhen Xu, Vedant Khatri, Yijun Dai, Xiner Liu, Siyan Li, Xuanming Zhang, Renzhe Yu",Not provided,Not provided,"Data Annotation, Qualitative Coding, Large Language Models, Human-AI Collaboration","Large language models offer a scalable alternative to human coding for data annotation tasks, enabling the scale-up of research across data-intensive domains. However, their performance on subjective annotation tasks is less consistent and prone to errors. This paper proposes a diagnostic evaluation paradigm to separate task-inherent ambiguity from model-driven inaccuracies and assess annotation quality in terms of their potential downstream impacts.",32.78,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11935v1_Big Data Workload Profiling for Energy-Aware Cloud.pdf,Big Data Workload Profiling for Energy-Aware Cloud Resource Management,"Milan Parikh, Aniket Abhishek Soni, Sneja Mitinbhai Shah, Ayush Raj Jha",Not found,2601.11935v1,"Cloud computing, energy-aware scheduling, workload profiling, virtual machine placement, big data, green computing","This paper presents a workload-aware scheduling framework that uses profiling of CPU usage, memory demand, and storage I/O behavior to guide energy-efficient virtual machine (VM) placement. By combining historical execution logs with real-time telemetry, the system predicts the energy and performance impact of candidate placement decisions and adaptively consolidates workloads without violating service-level agreements (SLAs).",30.65,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11940v1_Thinking Traps in Long Chain-of-Thought A Measurab.pdf,Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart,"Kang Chen, Fan Yu, Junjie Nian, Shihan Zhao, Zhuoka Feng, Zĳun Yao, Heng Wang, Minshen Yu, Yixin Cao",Not provided,2601.11940,"Long Chain-of-Thought, Thinking Traps, Adaptive Restart, Test-Time Control, Fine-Grained Trajectory Analysis","Scaling test-time compute via Long Chain-of-Thought (Long-CoT) significantly enhances reasoning capabilities, yet extended generation does not guarantee correctness. Through fine-grained trajectory analysis, we identify Thinking Traps, prefix-dominant deadlocks where later reflection, alternative attempts, or verification fail to revise the root error. We introduce TAAR (Trap-Aware Adaptive Restart), a test-time control framework that trains a diagnostic policy to predict trap index and escape probability. Experiments on challenging benchmarks show that TAAR improves reasoning performance without fine-tuning base model parameters.",29.8,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11956v1_Double-Calibration Towards Trustworthy LLMs via Ca.pdf,Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and Reasoning Confidence,"Yuyin Lu, Ziran Liang, Yanghui Rao, Wenqi Fan, Fu Lee Wang, Qing Li",Not found,Not found,"Large Language Models, Knowledge Graphs, Calibration, Epistemic Uncertainty, Experiential Uncertainty","This paper introduces DoublyCal, a framework that addresses the challenge of trustworthy reasoning in Large Language Models (LLMs) by employing a novel double-calibration principle. It aims to improve the accuracy and confidence calibration of black-box LLMs with low token cost, particularly in knowledge-intensive tasks.",29.07,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11960v1_R2PO Decoupling Training Trajectories from Inferen.pdf,R2PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning,"Jingchu Wang, Bingbing Xu, Yige Yuan, Bin Xie, Xiaoqian Sun, Huawei Shen",Not found,Not found,"Reinforcement Learning, Large Language Models, Reasoning, Optimization, Trajectories","This paper proposes R2PO, a method to decouple training trajectories from inference responses in Large Language Models (LLMs) to improve reasoning capabilities. By introducing a lightweight residual module, R2PO enables controlled trajectory diversification during training while maintaining stable inference generation, leading to better performance and reduced formatting errors.",28.59,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11969v1_textttMemoryRewardBench Benchmarking Reward Models.pdf,MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models,"Zecheng Tang, Baibei Ji, Ruoxi Sun, Haitian Wang, Yijun Zhang, Wenpeng Zhu, Ji Qi, Juntao Li, Min Zhang",Not provided,Not provided,"Large Language Models, Memory Management, Reward Models, Long-Term Memory, Segmented Processing","This work introduces MemRewardBench, the first benchmark to systematically study the ability of reward models to evaluate long-term memory management processes in large language models. It covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, and evaluates 13 cutting-edge reward models, indicating a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. It also exposes the capabilities and fundamental limitations of current reward models in evaluating LLM memory management across diverse settings.",36.2,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11974v1_Learn Like Humans Use Meta-cognitive Reflection fo.pdf,Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement,"Xinmeng Hou, Peiliang Gong, Bohao Qu, Wuqi Wang, Qing Guo, Yang Liu",Not found,Not found,"Self-improvement, Meta-cognitive reflection, Large language models, Efficient learning, Human learning","This paper proposes MARS, a framework that achieves efficient self-evolution within a single recurrence cycle by integrating principle-based reflection and procedural reflection. Inspired by educational psychology, MARS allows agents to systematically refine their reasoning logic without continuous online feedback, outperforming state-of-the-art self-evolving systems while significantly reducing computational overhead. Extensive experiments on six benchmarks demonstrate the effectiveness of MARS.",27.34,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11977v1_One-Shot Price Forecasting with Covariate-Guided E.pdf,One-Shot Price Forecasting with Covariate-Guided Experts under Privacy Constraints,"Ren He, Yinliang Xu, Jinfeng Wang, Jeremy Watson, Jian Song",Not provided,Not provided,"Price forecasting, Time Series, Privacy, Mixture of Experts, Market analysis","Forecasting in power systems often involves multi-variate time series with complex dependencies and strict privacy constraints across regions. Traditional forecasting methods require significant expert knowledge and struggle to generalize across diverse deployment scenarios. Recent advancements in pre-trained time series models offer new opportunities, but their zero-shot performance on domain-specific tasks remains limited. To address these challenges, the authors propose a novel MoE-Encoder module that augments pre-trained forecasting models by injecting a sparse mixture-of-experts layer between tokenization and encoding. This design enables two key capabilities: transforming multivariate forecasting into an expert-guided univariate task, allowing the model to effectively capture inter-variable relations, and supporting localized training and lightweight parameter sharing in federated settings where raw data cannot be exchanged. Extensive experiments on public multivariate datasets demonstrate that MoE-Encoder significantly improves forecasting accuracy compared to strong baselines. ",28.59,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11979v1_Process In-Context Learning Enhancing Mathematical.pdf,Process In-Context Learning: Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion,"Ang Gao, Changshuo Zhang, Xiao Zhang, Deyang Li, Minjun Zhao, Fangchao Liu, Xinyu Zhang",,,"In-context learning, Mathematical reasoning, Dynamic demonstration insertion, AI","Process In-Context Learning (PICL) is proposed to enhance mathematical reasoning by dynamically integrating demonstrations during inference. Unlike static approaches, PICL identifies and adapts to real-time confusion points, improving accuracy and highlighting the value of adaptive demonstration insertion in complex tasks.",30.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11995v1_Learning Audio-Visual Embeddings with Inferred Lat.pdf,Learning Audio–Visual Embeddings with Inferred Latent Interaction Graphs,"Donghuo Zeng, Hao Niu, Yanan Wang, Masato Taya",Not found,2601.11995,"audio–visual, latent interaction graph, cross-modal retrieval, soft labels",Learning robust audio–visual embeddings requires bringing genuinely related audio and visual signals together while filtering out incidental co-occurrences. Most contrastive and triplet-loss methods use sparse annotated labels per clip and treat any co-occurrence as semantic similarity. The proposed framework leverages soft-label predictions and inferred latent interactions to address these issues.,37.65,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.11998v1_Hybrid IDS Using Signature-Based and Anomaly-Based.pdf,Hybrid IDS Using Signature-Based and Anomaly-Based Detection,"1st Messaouda Boutassetta, 2nd Amina Makhlouf, 3rd Newfel Messaoudi, 4th Abdelmadjid Benmachiche, 5th Ines Boutabia",,,"Intrusion Detection System (IDS), Hybrid IDS, Signature-Based Detection, Anomaly-Based Detection, Machine Learning (ML), Cybersecurity, False Positives, Detection Accuracy, Real-Time Detection, Network Security","This paper presents a comprehensive survey and conceptual overview of Hybrid IDS, integrating signature-based and anomaly-based detection techniques to enhance attack detection capabilities. It examines recent research on Hybrid IDS, classifies existing models into functional categories, discusses their advantages, limitations, and application domains, including financial systems, air traffic control, and social networks. It also reviews recent trends in Hybrid IDS research, such as machine learning-based approaches and cloud-based deployments, and outlines potential future research directions aimed at developing more cost-effective Hybrid IDS solutions with improved ability to detect emerging and sophisticated cyberattacks.",37.86,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12002v1_Kernel-Based Learning of Safety Barriers.pdf,Kernel-Based Learning of Safety Barriers,"Kernel-Based Learning of Safety Barriers, Oliver Schoen, Zhengang Zhong, Sadegh Soudjani",Not found,2601.12002,"Kernel-based learning, Safety verification, Black-box systems, Discrete-time stochastic dynamics, Control barrier certificates, Conditional mean embeddings, Reproducing kernel Hilbert space, RKHS ambiguity set, Spectral barrier, Finite Fourier expansion, Linear program, Semi-infinite optimization, Temporal logic specifications, Safety-critical applications","The paper presents a data-driven approach for safety verification and synthesis of black-box systems with discrete-time stochastic dynamics, employing control barrier certificates and conditional mean embeddings to learn safety guarantees from system trajectories. It introduces a scalable yet distributionally robust framework for verifying safety, moving beyond restrictive assumptions on system dynamics and uncertainty.",38.06,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12003v1_Robust Verification of Concurrent Stochastic Games.pdf,Robust Verification of Concurrent Stochastic Games,"Angel Y. He, David Parker",Not found,2601.12003,"Robust quantitative verification, Probabilistic model checking, Concurrent stochastic games, Epistemic uncertainty","This paper introduces robust concurrent stochastic games (CSGs) and their subclass interval CSGs (ICSGs), which capture epistemic uncertainty about transition probabilities in CSGs. It proposes a framework for robust verification under worst-case assumptions about transition uncertainty, developing theoretical foundations and efficient algorithms for finite- and infinite-horizon objectives in both zero-sum and nonzero-sum settings. The framework is implemented in the PRISM-games model checker and demonstrated on a selection of large benchmarks.",38.17,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12014v1_Are LLMs Ready for TOON Benchmarking Structural Co.pdf,Are LLMs Ready for TOON? Benchmarking Structural Correctness–Sustainability Trade-offs in Novel Structured Output Formats,"Elio Masciari, Vincenzo Moscato, Enea Vincenzo Napolitano, Gian Marco Orlando, Marco Perillo, Diego Russo",the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper.,the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper.,"Green AI, TOON, Large Language Models, Natural Language Processing, Sustainability","Large Language Models (LLMs) are increasingly required to generate structured, machine-readable outputs for downstream systems. While recent benchmarks have focused on evaluating the structural correctness of such outputs, the environmental impact of inference for different output formats has largely been overlooked. This paper introduces a sustainability-aware evaluation framework for structured generation that measures token usage, generation time, and estimated carbon emissions. Using this framework, we benchmark the novel TOON format against established representations (JSON, XML, YAML) across multiple LLMs spanning different architectures and parameter scales. Our results reveal a consistent trade-off: TOON yields more compact outputs and lower emissions, but lower structural correctness when models lack native support. We show that increased model capacity reduces this gap and that environment-aware scoring can shift format rankings depending on deployment priorities.",38.72,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12019v1_Acting Flatterers via LLMs Sycophancy Combating Cl.pdf,Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs Opposing-Stance Reasoning,"Chaowei Zhang, Xiansheng Luo, Zewei Zhang, Yi Zhu, Jipeng Qiang, Longwei Wang",10.1145/XXXXXX.XXXXXX,,"Clickbait Detection, Large Language Models, Opposing Stance Reasoning, Contrastive Learning","This work proposes a novel approach to combat clickbait by harnessing the tendency of Large Language Models (LLMs) to produce sycophantic reasoning, which matches users' beliefs over truthful ones. The authors introduce a Self-renewal Opposing-stance Reasoning Generation (SORG) framework and a local Opposing Reasoning-based Clickbait Detection (ORCD) model to generate and detect clickbait. The experimental results show that the proposed method outperforms LLM prompting and fine-tuned smaller language models.",36.74,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12024v1_A Multi-Agent System for Generating Actionable Bus.pdf,A Multi-Agent System for Generating Actionable Business Advice,"Kartikey Singh Bhandari, Tanish Jain, Archit Agrawal, Dhruv Kumar, Praveen Kumar, Pratik Narang",,,"multi-agent system, business advice, customer reviews, large language models, actionability, specificity, non-redundancy","This paper presents a multi-agent, LLM-based framework for transforming large-scale review corpora into actionable business advice. The framework integrates four components: clustering to select representative reviews, advice generation, iterative evaluation, and feasibility-based ranking. Experiments across three service domains and multiple model families show that the framework consistently outperforms single model baselines on actionability, specificity, and non-redundancy, with medium-sized models approaching the performance of large model frameworks.",36.26,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12030v1_ARC Active and Reflection-driven Context Managemen.pdf,ARC: Active and Reflection-driven Context Management for Long-Horizon Information Seeking Agents,"Yilun Yao, Shan Huang, Elsie Dai, Zhewen Tan, Zhenyu Duan, Shousheng Jia, Yanbing Jiang, Tong Yang",,,"context management, active learning, reflection-driven, long-horizon information seeking, large language models","This paper proposes ARC, a framework that actively manages context during long-horizon information seeking tasks. Unlike existing approaches that treat context as a static artifact, ARC treats context as a dynamic internal reasoning state and allows agents to actively reorganize their working context when misalignment or degradation is detected. Experiments show that ARC consistently outperforms passive context compression methods.",36.62,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12038v1_Abstract Argumentation with Subargument Relations.pdf,Abstract Argumentation with Subargument Relations,Beishui Liao,Not found,Not found,"Abstract Argumentation, Subargument Relations, Attack Relations, Structured Argumentation","This paper studies abstract argumentation frameworks enriched with an explicit subargument relation, treating it alongside attack as a basic relation. It analyzes how subargument relations interact with attacks and examines their impact on fundamental semantic properties.",34.1,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12040v1_Partial Reasoning in Language Models Search and Re.pdf,Partial Reasoning in Language Models: Search and Refinement Guided by Uncertainty,"Murilo da Luz1,2, Bruno Brandão 1,2, Luana Martins 1,2, Gustavo Oliveira 1,2, Bryan de Oliveira1,2, Luckeciano Melo 1,3, Telma Soares 1,2",,,"Uncertainty, Entropy, Latent-space search, Soft Reasoning, LLM reasoning","The use of Large Language Models (LLMs) for reasoning and planning tasks has drawn increasing attention in Artificial Intelligence research. Despite their remarkable progress, these models still exhibit limitations in multi-step inference scenarios, particularly in mathematical and logical reasoning. We introduce PREGU (Partial Reasoning Guided by Uncertainty). PREGU monitors the entropy of the output distribution during autoregressive generation and halts the process whenever entropy exceeds a defined threshold, signaling uncertainty. From that point, a localized search is performed in the latent space to refine the partial reasoning and select the most coherent answer, using the Soft Reasoning method. Experiments conducted with LLaMA-3-8B, Mistral-7B, and Qwen2-7B across four reasoning benchmarks (GSM8K, GSM-Hard, SVAMP, and StrategyQA) showed performance greater than or similar to Soft Reasoning, indicating that entropy can serve as an effective signal to trigger selective refinement during reasoning.",39.21,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12042v1_Less Is More -- Until It Breaks Security Pitfalls .pdf,Less Is More — Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models,"Xiaomei Zhang, Zhaoxi Zhang, Leo Yu Zhang, Yanjun Zhang, Guanhong Tao, Shirui Pan",XXXXX.XXXXXX,XXXXX,"Vision Token Compression, Large Vision-Language Models, Security, Robustness","Visual token compression is widely adopted to improve inference efficiency of Large Vision-Language Models, but it significantly degrades their robustness, making them vulnerable to small and imperceptible perturbations that can lead to model failure.",35.41,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12049v1_textitFocaLogic Logic-Based Interpretation of Visu.pdf,FocaLogic: Logic-Based Interpretation of Visual Model Decisions,"Chenchen Zhao*, Muxi Chen*, Qiang Xu†",,,"interpretability, visual models, logic-based, model-agnostic, focus identification","A novel model-agnostic framework designed to interpret and quantify visual model decision-making through logic-based representations, identifying minimal interpretable subsets of visual regions termed visual focuses that decisively influence model predictions.",33.86,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12053v1_A New Strategy for Artificial Intelligence Trainin.pdf,A New Strategy for Artificial Intelligence: Training Foundation Models Directly on Human Brain Data,Ma¨el Donoso ∗,Not found,2601.12053,"foundation models, brain, neuroimaging, brain-generated data, brain-trained foundation models, reinforcement learning, chain of thought","This paper explores a new strategy for artificial intelligence by training foundation models directly on human brain data, hypothesizing that neuroimaging data could provide insights into human cognition not accessible through observable actions. The authors classify current limitations of foundation models and propose two methods, reinforcement learning from human brain (RLHB) and chain of thought from human brain (CoTHB), to prioritize the use of limited neuroimaging data for strategic steps in training.",37.32,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12055v1_Automating Parameter Selection in Deep Image Prior.pdf,Automating Parameter Selection in Deep Image Prior for Fluorescence Microscopy Image Denoising via Similarity-Based Parameter Transfer,"Lina Meyer, Felix Wissel, Tobias Knopp, Susanne Pfefferle, Ralf Fliegert, Maximilian Sandmann, Liana Uebler, Franziska M""ockl, Bj""orn-Philipp Diercks, David Lohr, Ren""e Werner",,,"Deep learning, Image denoising, Fluorescence microscopy, Parameter transfer, Similarity-based optimization","This study proposes an automated method for selecting optimal parameters for deep image prior (DIP) denoising in fluorescence microscopy images. By leveraging image metadata similarity, the method enables faster and more efficient denoising compared to traditional approaches. The study demonstrates that transferring parameters from a calibration set to a test image based on image metadata similarity can achieve similar or better performance than transferring based on quantitative image similarity measures. The proposed AUTO-DIP pipeline outperforms existing methods for various open-source test datasets, particularly for very noisy inputs. The method is applicable to locally acquired fluorescence microscopy images and can improve the speed and quality of DIP-based denoising in the domain of fluorescence microscopy imaging.",38.37,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12061v1_Codebook-Injected Dialogue Segmentation for Multi-.pdf,Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs,"Jinsook Lee, Kirk Vanacore, Zhuqian Zhou, Bakhtawar Ahtisham, Jeanine Grütter, René F. Kizilcec",,,"Dialogue Act Annotation, Segmentation, Language Technology, Human-AI Agreement, Crowdsourcing","This paper proposes codebook-injected segmentation for dialogue act annotation, which conditions boundary decisions on downstream annotation criteria. It evaluates LLM-based segmenters against standard and retrieval-augmented baselines, introducing evaluation metrics for span consistency, distinctiveness, and human-AI distributional agreement. The results highlight segmentation as a consequential design choice that should be optimized for downstream objectives rather than a single performance score.",36.65,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12068v1_Bridging the Gap in Bangla Healthcare Machine Lear.pdf,Bridging the Gap in Bangla Healthcare: Machine Learning Based Disease Prediction Using a Symptoms-Disease Dataset,"Rowzatul Zannat, Abdullah Al Shafi, Abdul Muntakim",Not found,Not found,"Disease Prediction, Annotated Dataset, Machine Learning Techniques, Soft Voting Ensemble, Hard Voting Ensemble","This study addresses the gap in disease prediction resources for non-English-speaking populations, particularly for Bangla speakers. It develops a comprehensive Bangla symptoms-disease dataset containing 758 unique symptom-disease relationships spanning 85 diseases, and evaluates multiple machine learning models to predict diseases based on Bangla symptom inputs. The study demonstrates superior robustness and generalization of both soft and hard voting ensemble approaches, establishing a foundational resource for disease prediction in Bangla.",37.03,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12082v1_Conditional Random Fields for Interactive Refineme.pdf,CONDITIONAL RANDOM FIELDS FOR INTERACTIVE REFINEMENT OF HISTOPA THOLOGICAL PREDICTIONS,"Tiffanie Godelaine†, Maxime Zanella†, Karim El Khoury1, Saïd Mahmoudi2, Benoît Macq1, Christophe De Vleeschouwer1",Not found,Not found,"Histology Classification, Conditional Random Fields, Human-In-The-Loop, Foundation Models","Assisting pathologists in the analysis of histopathological images has high clinical value, as it supports cancer detection and staging. Vision-Language Models (VLMs) provide strong yet imperfect zero-shot predictions. This paper proposes to refine these predictions by adapting Conditional Random Fields (CRFs) to histopathological applications, requiring no additional model training. HistoCRF is presented, a CRF-based framework with a novel definition of the pairwise potential that promotes label diversity and leverages expert annotations. Three experiments are considered: without annotations, with expert annotations, and with iterative human-in-the-loop annotations that progressively correct misclassified patches. Experiments on five patch-level classification datasets covering different organs and diseases demonstrate average accuracy gains of 16.0% without annotations and 27.5% with only 100 annotations, compared to zero-shot predictions. Integrating a human in the loop reaches a further gain of 32.6% with the same number of annotations.",38.8,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12095v1_Neural Isomorphic Fields A Transformer-based Algeb.pdf,Neural Isomorphic Fields: A Transformer-Based Algebraic Numerical Embedding,"Hamidreza Sadeghi, Saeedeh Momtazi, Reza Safabakhsh",Not provided,Not provided,"Neural Networks, Embeddings, Algebraic Structures, Number Embedding, Neural Arithmetic Units, Neural Arithmetic Logic Units","This paper proposes a novel neural network architecture called Neural Isomorphic Fields (NIF) to address the challenges of processing very small or very large numbers in neural network models. NIF uses embedding vectors to maintain algebraic operations within the rational numbers field, achieving high accuracy in addition but facing challenges in multiplication. The authors demonstrate the effectiveness of NIF in preserving algebraic properties under addition and suggest avenues for improvement in handling multiplication.",37.01,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12099v1_Large language models struggle with ethnographic t.pdf,Large Language Models Struggle with Ethnographic Text Annotation,"Leonardo S. Goodall†, Dor Shilton†, Daniel Austin Mullins, Harvey Whitehouse",Not found,Not found,"large language models, ethnographic text annotation, cross-cultural research, anthropology","Large language models have shown promise for automated text annotation, but their performance on ethnographic texts is limited, falling well below levels required for reliable automated annotation. The study evaluated 7 state-of-the-art LLMs on their ability to annotate 121 ritual features across 567 ethnographic excerpts, finding that longer texts, features requiring ordinal distinctions, and ambiguous constructs are particularly difficult for LLMs. Human inter-coder reliability set an approximate ceiling on LLM accuracy, and even on features where humans reliably agreed, models fell short of human performance.",37.45,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12104v1_Powerful Training-Free Membership Inference Agains.pdf,Powerful Training-Free Membership Inference Against Autoregressive Language Models,"David Ilić, David Stanojević, Kostadin Cvejoski",,,"membership inference, language models, privacy risks, fine-tuning, autoregressive models","Fine-tuned language models pose significant privacy risks, as they may memorize and expose sensitive training data. Membership inference attacks (MIAs) provide a principled framework for auditing these risks, yet existing methods achieve limited detection rates, particularly at low false-positive thresholds. This paper presents EZ-MIA, a membership inference attack that exploits the observation that memorization manifests most strongly at error positions, specifically tokens where the model predicts incorrectly yet still shows elevated probability for training examples. The Error Zone (EZ) score measures the directional imbalance of probability shifts at error positions relative to a pretrained reference model. This attack requires only two forward passes per query and no model training of any kind, achieving higher detection rates than previous methods. These results establish that privacy risks of fine-tuned language models are substantially greater than previously understood, with implications for privacy auditing and deployment decisions.",37.55,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12124v1_SynQP A Framework and Metrics for Evaluating the Q.pdf,SYNQP: A FRAMEWORK AND METRICS FOR EVALUATING THE QUALITY AND PRIVACY RISK OF SYNTHETIC DATA,"Bing Hu, Yixin Li, Asma Bahamyirou, Helen Chen",Not found,Not found,"Real-World Data, Synthetic Data, Privacy Metrics, Evaluation Framework, Membership Inference Attack, Identity Disclosure Risk","The use of synthetic data in health applications raises privacy concerns, yet the lack of open frameworks for privacy evaluations has slowed its adoption. SYNQP is an open framework for benchmarking privacy in synthetic data generation (SDG) using simulated sensitive data, ensuring that original data remains confidential. The authors highlight the need for privacy metrics that fairly account for the probabilistic nature of machine learning models. As a demonstration, SYNQP benchmarks CTGAN and proposes a new identity disclosure risk metric that offers a more accurate estimation of privacy risks compared to existing approaches.",37.43,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12126v1_UniMo Unified Motion Generation and Understanding .pdf,UniMo: Unified Motion Generation and Understanding with Chain of Thought,"Guocun Wang, Kenkun Liu, Jing Lin, Guorui Song, Jian Li, Xiaoguang Han",,,"motion generation, motion understanding, large language models, supervised fine-tuning, reinforcement learning, chain of thought, human motion","Proposes UniMo, a novel framework that integrates motion-language information and interpretable chain of thought reasoning into large language models via supervised fine-tuning and reinforcement learning, aiming to address limitations in interpretability and semantic alignment in existing unified and task-specific models.",34.55,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12132v1_Bengali Text Classification An Evaluation of Large.pdf,Bengali Text Classification: An Evaluation of Large Language Model Approaches,"Md Mahmudul Hoque∗, Md Mehedi Hassain, Md Hojaifa Tanvir, Rahul Nandy",Not found,2601.12132,"Bengali Text Classification, Transformer-based Text Classifier, Multilingual NLP, Qwen, LLaMA","This study evaluates the effectiveness of large language models (LLMs) in classifying Bengali newspaper articles. Three instruction-tuned LLMs—LLaMA 3.1 8B Instruct, LLaMA 3.2 3B Instruct, and Qwen 2.5 7B Instruct—are evaluated for this task. Qwen 2.5 achieved the highest classification accuracy of 72%, particularly in the 'Sports' category. The findings highlight the effectiveness of LLMs in Bengali text classification despite the scarcity of resources for Bengali NLP. Future research will focus on exploring additional models, addressing class imbalance issues, and refining fine-tuning approaches.",39.17,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12134v1_Human-Human-AI Triadic Programming Uncovering the .pdf,Human-Human-AI Triadic Programming: Uncovering the Role of AI Agent and the Value of Human Partner in Collaborative Learning,"Taufiq Daryanto, Xiaohan Ding, Kaike Ping, Lance T. Wilhelm, Yan Chen, Chris Brown, Eugenia H. Rho",XXXXXXX.XXXXXXX,2601.12134,"Collaborative Learning, Human-Human-AI, Programming, AI Agent, Social Interaction","This study explores human-human-AI triadic programming, where two humans work with an AI agent, aiming to uncover the role of AI and the value of human collaboration in programming learning.",37.33,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12138v1_DriveSafe A Hierarchical Risk Taxonomy for Safety-.pdf,DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants,"Abhishek Kumar, Riya Tapwal, Carsten Maple",,,"Large Language Models, Safety-Critical, Driving Assistants, Hierarchical Risk Taxonomy, LLM Safety, Safety Alignment, Real-World Driving Scenarios","This paper introduces DriveSafe, a hierarchical four-level risk taxonomy designed to systematically characterize safety-critical failure modes of LLM-based driving assistants, addressing the limitations of existing general-purpose safety alignment in driving contexts.",34.92,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12141v1_TIDE A Trace-Informed Depth-First Exploration for .pdf,TIDE: A Trace-Informed Depth-First Exploration for Planning with Temporally Extended Goals,"Yuliia Suprun, Khen Elimelech, Lydia E. Kavraki, Moshe Y. Vardi",Not found,2601.12141,"Task planning, Temporal logic, Depth-first search, Heuristics, Robotics","Introducing TIDE, a novel approach for task planning with temporally extended goals, which decomposes the problem into smaller, manageable reach-avoid subproblems, each solvable using an off-the-shelf planner. It uses cost-driven heuristics to guide exploration and ensures completeness and efficiency through adaptive backtracking.",37.69,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12147v1_Segment and Matte Anything in a Unified Model.pdf,Segment and Matte Anything in a Unified Model,"Zezhong Fan*, Xiaohan Li*",,,"Segment Anything, Interactive Image Matting, Unified Model, Computer Vision","This paper introduces Segment And Matte Anything (SAMA), a lightweight extension of Segment Anything (SAM) that delivers high-quality interactive image segmentation and matting with minimal extra parameters.",32.39,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12150v1_Enhanced Diagnostic Performance via Large-Resoluti.pdf,Enhanced Diagnostic Performance via Large-Resolution Inference Optimization for Pathology Foundation Models,"Mengxuan Hu∗, Zihan Guan⋆, John Kang, Sheng Li‡, Zhongliang Zhou‡",Not found,2601.12150,"Computational pathology, Foundation models, Inference Optimization","Despite their strong performance on tasks such as ROI classification and segmentation, many pathology foundation models remain constrained by a specific input size, creating inefficiencies when applied to whole-slide images (WSIs) spanning thousands of resolutions. This paper proposes an efficient inference strategy that sparsifies attention using spatially aware neighboring blocks and filters out non-informative tokens through global attention scores, enabling inference at higher resolutions under the same GPU budget and achieving up to a 7.67% improvement in ROI classification.",37.35,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12186v1_Aletheia What Makes RLVR For Code Verifiers Tick.pdf,Aletheia: What Makes RLVR For Code Verifiers Tick?,"Vatsal Venkatkrishna, Indraneil Paul, Iryna Gurevych",,2309.15859,"Reinforcement Learning, Code Verification, Large Language Models, Execution Feedback, Robustness","Aletheia is a controlled testbed that enables execution-grounded evaluation of code verifiers' robustness across disparate policy models and covariate shifts, examining components of the RLVR-based verifier training recipe widely credited for its success.",34.4,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12205v1_Do Neural Codecs Generalize A Controlled Study Acr.pdf,Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks,"Shih-Heng Wang1*, Jiatong Shi 2, Jinchuan Tian 2, Haibin Wu3, Shinji Watanabe 2",Not provided,Not provided,"Neural Audio Codecs, Generalization, Unseen Languages, Non-Speech Tasks, Speech Language Modeling","This paper investigates the generalization capabilities of Neural Audio Codecs (NACs) in three aspects: (i) their ability to generalize to unseen languages during pre-training, (ii) their effectiveness in generalizing to non-speech tasks, and (iii) the impact of incorporating non-speech data during pre-training. The study uses strictly controlled configurations and curated pre-trained data to comprehensively evaluate NACs performance on signal reconstruction quality and downstream applications. The findings show that NACs can generalize to unseen languages, but speech-only pre-trained NACs degrade on non-speech tasks, and including non-speech data during pre-training improves performance on non-speech tasks while maintaining comparable performance on speech tasks.",37.22,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12212v1_Speculative Sampling with Reinforcement Learning.pdf,Speculative Sampling with Reinforcement Learning,"Chenan Wang, Daniel H. Shi, Haipeng Chen",,,"Reinforcement Learning, Speculative Sampling, Large Language Models, Inference Time Latency, Reinforcement Learning for Speculative Sampling","This paper introduces Re-SpS, a reinforcement learning-based framework for optimizing draft tree hyperparameters in speculative sampling methods for large language models. Re-SpS dynamically adjusts these hyperparameters in real-time, learning context-aware policies that maximize generation speed by balancing speculative aggression with computational overhead. Evaluation results demonstrate consistent improvements over state-of-the-art methods, achieving up to 5.45× speedup over the backbone LLM and up to 1.12× speedup across five diverse benchmarks.",35.75,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12215v1_Wavelet-Driven Masked Multiscale Reconstruction fo.pdf,Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation Models,"Megha Thukral*, Cyrus Tanade, Simon A. Lee, Juhyeon Lee, Hao Zhou, Keum San Chun, Migyeong Gwak, Viswam Nathan, Md Mahbubur Rahman, Li Zhu, Mehrab Bin Morshed, Subramaniam Venkatraman, Sharanya Arcot Desai",10.1101/2601.12215v1,2601.12215v1,"Wearable SSL Method, Wavelet based Modelling, PPG foundation models","This paper introduces Masked Multiscale Reconstruction (MMR) for PPG representation learning, a self-supervised pretraining framework that explicitly learns from hierarchical time–frequency scales of PPG data, improving over or matching state-of-the-art open-source PPG foundation models on 17 of 19 diverse health-related tasks.",39.7,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12224v1_Where It Moves It Matters Referring Surgical Instr.pdf,"Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion","Meng Wei, Kun Yuan, Shi Li, Yue Zhou, Long Bai, Nassir Navab, Hongliang Ren, Hong Joo Lee, Tom Vercauteren, Nicolas Padoy",Not provided,Not provided,"surgical instrument segmentation, motion-guided, referring, intuitive interaction, autonomous surgical robotic assistance","This work introduces SurgRef, a novel motion-guided framework that enables models to understand and segment surgical instruments even under occlusion, ambiguity, or unfamiliar terminology, achieving state-of-the-art accuracy and generalization across surgical procedures.",35.51,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12234v1_Proc3D Procedural 3D Generation and Parametric Edi.pdf,Proc3D: Procedural 3D Generation and Parametric Editing of 3D Shapes with Large Language Models,"Fadlullah Raji, Stefano Petrangeli, Matheus Gadelha, Yu Shen, Uttaran Bhattacharya, Gang Wu",Not found,2601.12234v1,"3D modeling, Large Language Models, Procedural generation, Parametric editing, Graph-based representation","Proc3D is a system designed to generate editable 3D models while enabling real-time modifications, using a procedural compact graph (PCG) representation and Large Language Models (LLMs).",37.92,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12242v1_Optimal Power Allocation and Sub-Optimal Channel A.pdf,OPTIMALPOWERALLOCATION ANDSUB-OPTIMALCHANNELASSIGNMENT FORDOWNLINKNOMA SYSTEMUSINGDEEPREINFORCEMENTLEARNING,"WooSeok Kim, Jeonghoon Lee, Sangho Kim, Taesun An, WonMin Lee, Dowon Kim, Kyungseop Shin",,2601.12242,"Non-orthogonal multiple access (NOMA), deep reinforcement learning (DRL), wireless network, resource allocation","In recent years, Non-Orthogonal Multiple Access (NOMA) system has emerged as a promising candidate for multiple access frameworks due to the evolution of deep machine learning, trying to incorporate deep machine learning into the NOMA system. The main motivation for such active studies is the growing need to optimize the utilization of network resources as the expansion of the internet of things (IoT) caused a scarcity of network resources. The NOMA addresses this need by power multiplexing, allowing multiple users to access the network simultaneously. Nevertheless, the NOMA system has few limitations. Several works have proposed to mitigate this, including the optimization of power allocation known as joint resource allocation(JRA) method, and integration of the JRA method and deep reinforcement learning (JRA-DRL). Despite this, the channel assignment problem remains unclear and requires further investigation. In this paper, we propose a deep reinforcement learning framework incorporating replay memory with an on-policy algorithm, allocating network resources in a NOMA system to generalize the learning. Also, we provide extensive simulations to evaluate the effects of varying the learning rate, batch size, type of model, and the number of features in the state.",40.16,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12243v1_Less is More Label-Guided Summarization of Procedu.pdf,Less is More: Label-Guided Summarization of Procedural and Instructional Videos,"Shreya Rajpal∗, Vellore Institute of Technology, shreyarajpal6@gmail.com, Michal Golovanesky, Brown University, michal_golovanesky@brown.edu, Carsten Eickhoff, University of Tübingen, carsten.eickhoff@uni-tuebingen.de",Not found,2601.12243,"video summarization, procedural videos, instructional videos, label-driven summarization, semantic grounding, multimodal analysis","This paper proposes a three-stage framework, PRISM, for producing semantically grounded video summaries of procedural and instructional videos. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). The method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos.",38.82,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12247v1_Plan Verify and Fill A Structured Parallel Decodin.pdf,"Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion Language Models","Miao Li * 1, Hanyang Jiang * 1, Sikai Cheng 1, Hengyu Fu 2, Yuhang Cai2, Baihe Huang 2, Tinghan Ye1, Xuanzhou Chen 1, Pascal Van Hentenryck1",,,"Diffusion Language Models, Parallel Decoding, Quantitative Validation, Structural Stopping, Masked Diffusion, Autoregressive Models","This paper proposes Plan-Verify-Fill (PVF), a training-free paradigm for diffusion language models (DLMs) that addresses the challenge of reactive decoding strategies by actively constructing a hierarchical skeleton and employing a verification protocol to operationalize pragmatic structural stopping. Extensive evaluations demonstrate that PVF reduces the Number of Function Evaluations (NFE) by up to 65% compared to confidence-based parallel decoding across benchmark datasets, achieving superior efficiency without compromising accuracy.",37.83,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12248v1_AQUA-Bench Beyond Finding Answers to Knowing When .pdf,AQUA-BENCH: BEYOND FINDING ANSWERS TO KNOWING WHEN THERE ARE NONE,"Chun-Yi Kuan, Hung-yi Lee",,,"Audio question answering, Large language models, Unanswerable questions, Audio data, Model reliability","Recent advances in audio-aware large language models have shown strong performance on audio question answering. However, existing benchmarks mainly cover answerable questions and overlook the challenge of unanswerable ones, where no reliable answer can be inferred from the audio. AQUA-Bench, a new benchmark, evaluates whether ALLMs can detect and appropriately respond to unanswerable questions, covering Absent Answer Detection, Incompatible Answer Set Detection, and Incompatible Audio Question Detection scenarios.",36.49,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12249v1_An Innovative Framework for Breast Cancer Detectio.pdf,"An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion","Ehsan Sadeghi Pour, Mahdi Esmaeili, Morteza Romoozi",Not provided,Not provided,"Breast Cancer Detection, Pyramid Adaptive Atrous Convolution (PAAC), Transformer, Multi-Scale Feature Fusion, Self-Attention Mechanism, Medical Image Processing","This thesis presents an innovative framework for detecting malignant masses in mammographic images by integrating Pyramid Adaptive Atrous Convolution (PAAC) and Transformer architectures. The proposed approach utilizes Multi-Scale Feature Fusion to enhance the extraction of features from benign and malignant tissues and combines Dice Loss and Focal Loss functions to improve the model's learning process, effectively reducing errors in binary breast cancer classification and achieving high accuracy and efficiency.",37.54,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12256v1_Improving Large Molecular Language Model via Relat.pdf,Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration,"Jinyoung Park, Minseong Bae, Jeehye Na, Hyunwoo J. Kim",Not found,Not found,"Large language models, Molecular language models, Multimodal collaboration, Relation-aware attention, Hallucination prevention, Molecular modeling","This paper proposes CoLLaMo, a large language model-based molecular assistant equipped with a multi-level molecular modality-collaborative projector. The relation-aware modality-collaborative attention mechanism facilitates fine-grained and relation-guided information exchange between atoms, addressing the limitations of existing large molecular language models (LMLMs) in hallucination and robustness. The model enhances the molecular modality generalization capabilities of LMLMs, achieving the best performance on multiple tasks including molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction.",36.88,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12257v1_Soft Shadow Diffusion SSD Physics-inspired Learnin.pdf,Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy,"Fadlullah Raji, John Murray Bruce",not found,2601.12257,"Computational imaging, Machine learning, 3D generative models, Diffusion models, Separable non-linear least squares","This paper proposes a novel approach to 3D reconstruction of hidden scenes from ordinary NLOS photographs, using a reformulation of the light transport model that decomposes the hidden scene into light-occluding and non-light-occluding components. It introduces a gradient-based optimization method and a physics-inspired neural network approach, called Soft Shadow diffusion (SSD), to solve the challenging ill-conditioned inverse problem encountered. The SSD approach is effective on numerous 3D scenes in real experimental scenarios and generalizes well to unseen classes in simulation and real-world NLOS scenes.",38.77,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12259v1_FutureX-Pro Extending Future Prediction to High-Va.pdf,FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains,"Jiashuo Li, Wenhao Huang, Authors from ByteDance, Hong Kong University of Science and Technology, Georgia Institute of Technology, Stanford University, Princeton University",Not provided,2601.12259,"Future Prediction, Vertical Domains, Financial Markets, Retail Forecasting, Public Health, Natural Disasters, Large Language Models, Domain Grounding, Industrial Deployment","This report introduces FutureX-Pro, a specialized framework extending agentic future prediction to high-value vertical domains, including Finance, Retail, Public Health, and Natural Disaster. It benchmarks agentic Large Language Models on foundational prediction tasks and assesses their domain grounding for industrial deployment.",38.17,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12260v1_Docs2Synth A Synthetic Data Trained Retriever Fram.pdf,Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding,"Yihao Ding, Qiang Sun, Puzhen Wu, Sirui Li, Siwen Luo, Wei Liu",Not found,Not found,"Document Understanding, Synthetic Data, Retriever Framework, Scanned Documents, Vision-Language Pre-trained Models, Machine Learning","Docs2Synth is a synthetic-supervision framework designed to enhance document understanding in private and low-resource domains. It automatically processes raw document collections, generates and verifies diverse QA pairs via an agent-based system, and trains a lightweight visual retriever to extract domain-relevant evidence. During inference, the retriever collaborates with a multimodal large language model (MLLM) through an iterative retrieval-generation loop, reducing hallucination and improving response consistency.",36.34,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12263v1_Multimodal Generative Engine Optimization Rank Man.pdf,Multimodal Generative Engine Optimization: Rank Manipulation for Vision–Language Model Rankers,"Yixuan Du, Chenxiao Yu, Haoyan Xu, Ziyi Wang, Yue Zhao, Xiyang Hu*",Not provided,Not provided,"Vision-Language Models, Ranking Attacks, Adversarial Manipulation, Multimodal Perturbations, Product Search","This paper reveals a critical vulnerability in Vision-Language Models (VLMs) used in modern retrieval and recommendation systems, where a malicious actor can unfairly promote a target product by jointly optimizing imperceptible image perturbations and fluent textual suffixes. The authors present MGEO, a novel adversarial framework that exploits the deep cross-modal coupling within VLMs, demonstrating significant improvements over existing attacks on real-world datasets.",35.17,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12269v1_Simulated Annealing Enhances Theory-of-Mind Reason.pdf,Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models,"Xucong Hu, Jian-Qiao Zhu",,,"Language Models, Markov Chain Monte Carlo, Simulated Annealing, Power Sampling, Theory of Mind","Autoregressive language models are criticized for optimizing surface plausibility rather than maintaining correct latent-state representations. This paper shows that strong Theory-of-Mind capability can be recovered from the base model without additional weight updates or verifications, using power-sampling methods and incorporating annealing.",34.05,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12276v1_Predictive Prototyping Evaluating Design Concepts .pdf,PREDICTIVE PROTOTYPING: EVALUATING DESIGN CONCEPTS WITH GPT,"Hilsann Yong, Singapore University of Technology & Design",Not found,Not found,"Prototyping, Design Theory, Iteration, Simulation, AI, LLM, GPT, RAG, Crowdsourcing","This work explores whether a GPT can accurately predict information that would be gained during a prototyping effort such as cost, performance, and perceived usability. It introduces a novel approach to emulate design feedback using retrieval augmented generation (RAG) in conjunction with a GPT, specifically OpenAI's GPT-4o. The method leverages prototyping data from 'Instructables.com' to increase the availability of relevant data to the model. The results indicate that GPT-RAG predictions are more accurate than individual human or crowd estimations of cost and performance, while offering similar insights in terms of usability. The GPT-RAG inspired prototype also outperforms commercial and topology optimized prototypes.",38.65,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12282v1_CytoCLIP Learning Cytoarchitectural Characteristic.pdf,CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training,"Pralaypati Ta, Sriram Venkatesaperumal, Keerthi Ram, Mohanasankar Sivaprakasam",,,"Cytoarchitecture, Histological Image Processing, Contrastive learning, CLIP",An automated approach to identify brain regions by their cytoarchitecture using vision-language models derived from Contrastive Language-Image Pre-Training (CLIP) frameworks. The model comprises two variants: one trained on low-resolution whole-region images and the other on high-resolution image tiles. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.,37.05,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12286v1_Conversational Context Classification A Representa.pdf,Conversational Context Classification: A Representation Engineering Approach,Jonathan Pan,,,"Large Language Models, Representation Engineering, One-Class Support Vector Machine, Novelty Detection, In/Out-of-Context","This paper explores the use of Representation Engineering and One-Class Support Vector Machine to identify subspaces within the internal states of Large Language Models that represent a specific context. By training One-Class Support Vector Machine on in-context examples, a robust boundary is established within the LLM's hidden state latent space, enabling the identification of the subspace for a specific context. The approach is evaluated with two open-source LLMs in a specific contextual domain, showing promising results in detecting in or out of context conversation threads for AI safety.",36.42,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12288v1_TimeGMM Single-Pass Probabilistic Forecasting via .pdf,TIMEGMM: SINGLE-PASS PROBABILISTIC FORECASTING VIA ADAPTIVE GAUSSIAN MIXTURE MODELS WITH REVERSIBLE NORMALIZATION,"Lei Liu, Tengyuan Liu, Hongwei Zhao, Jiahui Huang, Ruibo Guo, Bin Li",Not found,Not found,"Probabilistic time series forecasting, Gaussian mixture model, Reversible instance normalization","This paper presents TimeGMM, a novel probabilistic forecasting framework based on Gaussian Mixture Models (GMM) that captures complex future distributions in a single forward pass. Key component is GMM-adapted Reversible Instance Normalization (GRIN), a novel module designed to dynamically adapt to temporal-probabilistic distribution shifts. The framework integrates a dedicated Temporal Encoder (TE-Module) with a Conditional Temporal-Probabilistic Decoder (CTPD-Module) to jointly capture temporal dependencies and mixture distribution parameters. Extensive experiments demonstrate that TimeGMM consistently outperforms state-of-the-art methods, achieving maximum improvements of 22.48% in CRPS and 21.23% in NMAE.",38.28,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12294v1_ToolPRMBench Evaluating and Advancing Process Rewa.pdf,ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents,"Dawei Li, Yuguang Yao, Zhen Tan, Huan Liu, Ruocheng Guo",Not found,Not found,"process reward models, tool-using agents, reward-guided search, multi-step failures, large language models","This paper introduces ToolPRMBench, a large-scale benchmark designed to evaluate process reward models (PRMs) for tool-using agents. It utilizes offline and online sampling to capture local and realistic multi-step failures, respectively, and proposes a multi-LLM verification pipeline to reduce label noise and ensure data quality. Extensive experiments across large language models and PRMs reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using.",35.14,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12304v1_A Two-Stage Globally-Diverse Adversarial Attack fo.pdf,A TWO-STAGE GLOBALLY-DIVERSE ADVERSARIAL ATTACK FOR VISION-LANGUAGE PRE-TRAINING MODELS,"Wutao Chen, Huaqin Zou, Chen Wan, Lifeng Huang",,,"Adversarial Attack, Vision-Language Pre-Training, Multi-Modal Retrieval, Transferability","The paper proposes 2S-GDA, a two-stage globally-diverse attack framework for vision-language pre-training models. It addresses the challenges of limited perturbation diversity and unstable multi-stage pipelines in existing multimodal attacks. Extensive experiments on VLP models demonstrate that 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains of up to 11.17% in black-box settings.",36.68,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12310v1_Survival is the Only Reward Sustainable Self-Train.pdf,Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection,"Jennifer Dodgson, Alfath Daryl Alhajir, Michael Joedhitya, Akira Rafhael Janson Pattirane, Surender Suresh Kumar, Joseph Lim, C.H. Peh, Adith Ramdas, Steven Zhang Zhexu",,2601.12310v1,"self-training, sparse feedback, bounded memory, negative-space learning, environmental viability, semantic dynamics, reward hacking, semantic drift","This paper presents a proof-of-concept self-training system architecture that uses environmental viability to select behaviors, avoiding reward hacking and semantic drift, and demonstrates its learning dynamics and failure modes.",40.46,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12316v1_GazeFormer-MoE Context-Aware Gaze Estimation via C.pdf,GAZEFORMER-MOE: CONTEXT-AWARE GAZE ESTIMATION VIA CLIP AND MOE,"Xinyuan Zhao, Xianrui Chen, Ahmad Chaddad",Not found,Not found,"Gaze estimation, multi scale fusion, MoE, Transformer","We present a semantics-modulated, multi-scale Transformer for 3D gaze estimation. Our model conditions CLIP global features with learnable prototype banks (illumination, head pose, background, direction), fuses these prototype-enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space, and replaces several FFN blocks with routed/shared Mixture of Experts to increase conditional capacity. Evaluated on MPIIFaceGaze, EYEDIAP, Gaze360 and ETH-XGaze, our model achieves new state-of-the-art angular errors of 2.49°, 3.22°, 10.16°, and 1.44°, demonstrating up to a 64% relative improvement over previously reported results.",38.06,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12317v1_Explanova Automatically Discover Data Insights in .pdf,Explanova: Automatically Discover Data Insights in N×M Table via XAI Combined LLM Workflow,Yiming Huang,10.1145/nnnnnnn.nnnnnnn,,"Data Science, AutoML, LLM, XAI, Data Analysis, Feature Analysis, Modeling","This paper proposes Explanova, an automatic LLM workflow designed to discover data insights through explainable AI (XAI) paradigm. It focuses on single-feature statistic analysis, feature-to-feature relation statistic analysis, and feature modeling by all other features in a single data table. The workflow is designed to explore all possible analytical items and is powered by a preset AutoML-like workflow.",35.35,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12318v1_Beyond Human Annotation Recent Advances in Data Ge.pdf,Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence,"DEHAO YING, FENGCHANG YU, HAIHUA CHEN, CHANGJIANG JIANG, YURONG LI, WEI LU",XXXXXXX.XXXXXXX,,"Document Intelligence, Data Generation, Data Quality Evaluation","This paper surveys recent advances in data generation methods for Document Intelligence, addressing the bottleneck of manual annotation and providing a unified technical map for the field.",34.29,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12323v1_MARO Learning Stronger Reasoning from Social Inter.pdf,MARO: Learning Stronger Reasoning from Social Interaction,"Yin Cai, Zhouhong Gu, JunTao Zhang, Ping Chen*",,,"large language models, multi-agent, social interaction, reasoning, reinforcement learning","This paper proposes MARO, a method that enables large language models to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. It addresses the sparse learning signal problem, uneven role distribution, and environmental instability issues. Experimental results show significant improvements in social reasoning capabilities and effective transfer to other tasks.",35.03,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12327v1_The Expert Validation Framework EVF Enabling Domai.pdf,The Expert Validation Framework (EVF): Enabling Domain Expert Control in AI Engineering,"Lucas Gren, Felix Dobslaw",10.1145/xxx.xxxx,,"GenAI, expert validation, quality assurance, AI engineering, domain expert control","The Expert Validation Framework (EVF) enables domain experts to maintain authoritative control over AI engineering systems through structured processes, addressing the gap between AI capabilities and organizational trust.",33.73,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12330v1_IceWatch Forecasting Glacial Lake Outburst Floods .pdf,IceWatch: Forecasting Glacial Lake Outburst Floods (GLOFs) using Multimodal Deep Learning,"Zuha Fatima, Muhammad Anser Sohaib, Muhammad Talha, Ayesha Kanwal, Sidra Sultana, Nazia Perwaiz",,,"CNN, deep learning, glacier monitoring, GLOF detection, LSTM, remote sensing, Sentinel-2, temperature forecasting, transformer, velocity prediction","IceWatch is a novel deep learning framework for GLOF prediction that incorporates both spatial and temporal perspectives. It uses Sentinel-2 multispectral satellite imagery and NASA ITS_LIVE time series for glacier velocity modeling, and MODIS LST records for near-surface temperature forecasting. The system ensures strong predictive performance, rapid data processing for real-time use, and robustness to noise and missing information.",36.78,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12331v1_Efficient Privacy-Preserving Retrieval Augmented G.pdf,Efficient Privacy-Preserving Retrieval Augmented Generation with Distance-Preserving Encryption,"Huanyi Ye, Jiale Guo, Ziyao Liu, Kwok-Yan Lam",Not found,Not found,"Privacy-preserving, Retrieval-Augmented Generation, Distance-Preserving Encryption, Large Language Models, Conditional Approximate Distance-Comparison-Preserving Symmetric Encryption, Differential Privacy","Retrieval-Augmented Generation (RAG) has emerged as a key technique for enhancing response quality of large language models (LLMs) without incurring high computational cost. It works by retrieving knowledge and facts from external databases, augmenting the prompt with the retrieved data, and enabling the LLM to generate more accurate responses. This paper proposes an efficient privacy-preserving RAG framework (ppRAG) tailored for untrusted cloud environments that defends against vector-to-text attack, vector analysis, and query analysis. At its core, the paper introduces Conditional Approximate Distance-Comparison-Preserving Symmetric Encryption (CAPRISE) that encrypts embeddings while still allowing the cloud to compute similarity between an encrypted query embedding and the encrypted database embeddings. CAPRISE preserves only the relative distance ordering between the encrypted query and each encrypted database embedding, enhancing both privacy and efficiency. To further mitigate query analysis risks, differential privacy is introduced by perturbing the query embedding prior to encryption, preventing the cloud from inferring sensitive patterns from query frequency. Experimental results show that ppRAG achieves efficient processing throughput, high retrieval accuracy, strong privacy guarantees, making it a practical solution for resource-constrained users seeking secure, cloud-augmented LLMs.",39.48,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12338v1_Actionable Advice from Reviews via Mixture of LoRA.pdf,Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM Pipeline for Issue Extraction and Business Recommendations,"Kartikey Singh Bhandari, Manav Ganesh, Yashwant Viswanathan, Archit Agrawal, Dhruv Kumar, Pratik Narang",,,"Customer Reviews, Actionable Advice, Issue Extraction, Business Recommendations, Two-LLM Pipeline, LoRA Experts","This paper studies review-to-action generation, producing concrete, implementable recommendations grounded in review text. It proposes a modular two-LLM framework where an issue model extracts salient issues and assigns coarse themes, and an advice model generates targeted operational fixes conditioned on the extracted issue representation. The advice model is adapted using a mixture-of-LoRA experts strategy to enable specialization without expensive full fine-tuning. Synthetic review-issue-advice triples from Yelp reviews (airlines and restaurants) are used for training, and recommendations are evaluated using an eight-dimension operational rubric spanning actionability, specificity, feasibility, expected impact, novelty, non-redundancy, bias, and clarity. The approach consistently outperforms prompting-only and single-adapter baselines, yielding higher actionability and specificity while retaining favorable efficiency-quality trade-offs.",38.54,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12341v1_Time-Continuous Modeling for Temporal Affective Pa.pdf,Time-Continuous Modeling for Temporal Affective Pattern Recognition in LLM’s,"Rezky M. Kam, Coddy N. Siswanto",,,"Temporal Affective Pattern Recognition, Language Models, Continuous Attention, Physics-Informed Neural Networks","This paper introduces a hybrid encoder-decoder architecture for improving affective understanding in language models. It addresses the limitations of discrete token generation in text modality decoder models by incorporating time-continuous patterns and physics-informed neural networks, allowing for temporal and longitudinal adaptation. The approach aims to enhance the psychological plausibility and interpretability of machine-generated responses.",35.83,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12343v1_How Well Do LLMs Predict Human Behavior A Measure .pdf,How Well Do LLMs Predict Human Behavior?,"Wayne Gao, Sukjin Han, Annie Liang",Not found,2601.12343,"large language models, human behavior prediction, predictive accuracy, task-specific data, economic variables","This paper proposes a measure to evaluate the knowledge a pretrained LLM brings to predicting human behavior, defined as the equivalent sample size, and applies this method to the Panel Study of Income Dynamics to find that LLMs encode considerable predictive information for some economic variables but much less for others.",41.4,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12349v1_Zero-Permission Manipulation Can We Trust Large Mu.pdf,Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?,"Yi Qian, Kunwei Qian, Xingbang He, Ligeng Chen, Jikang Zhang, Tiantai Zhang, Haiyang Wei, Linzhang Wang, Hao Wu, Bing Mao",Not found,Not found,"Large Multimodal Models, GUI Agents, Android, Security, Manipulation Attacks","This paper discusses the emergence of large multimodal model powered GUI agents on mobile platforms, which are high-privilege operators that perceive screen content and inject inputs. It demonstrates that the implicit assumption of Visual Atomicity in their design is fundamentally invalid, creating a critical attack surface. The authors present Action Rebinding, a novel cross-application attack that allows seemingly-benign applications to rebind an agent's execution, and introduce an Intent Alignment Strategy (IAS) to manipulate the agent's reasoning process. The paper evaluates Action Rebinding attacks on six widely-used Android GUI agents across 15 diverse tasks, showing a 100% success rate for atomic action rebinding and the ability to reliably orchestrate multi-step attack chains. With IAS, the success rate in bypassing verification gates increases to up to 100%, and the attacker application requires no sensitive permissions and contains no privileged API calls, achieving a 0% detection rate across malware scanners.",38.81,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12357v1_SimpleMatch A Simple and Strong Baseline for Seman.pdf,SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence,"Hailong Jin1, Huiying Li1*",Not found,Not found,"semantic correspondence, deep learning, keypoint matching, upsampling, low resolution","A simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions, addressing the issue of irreversible fusion of adjacent keypoint features caused by deep downsampling operations.",34.33,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12358v1_From Prompts to Pavement LMMs-based Agentic Behavi.pdf,From Prompts to Pavement:LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles,"Omar Y. Goba, Ahmed Y. Gado, Catherine M. Elias, Ahmed Hussein",,,"Large Language Models, Multi-modal Vision Models, Behavior-Tree, Autonomous Vehicles, Adaptive Behavior Planning, SAE Levels, CARLA, Nav2","This paper presents an agentic framework that leverages large language models and multi-modal vision models to generate and adapt behavior trees on the fly for autonomous vehicles. The system integrates a Descriptor agent, a Planner agent, and a Generator agent to trigger only upon baseline behavior tree failure, demonstrating successful navigation around unexpected obstacles with no human intervention.",36.27,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12374v1_A Scalable Entity-Based Framework for Auditing Bia.pdf,A Scalable Entity-Based Framework for Auditing Bias in LLMs,"Akram Elbouanani1, Aboubacar Tuo1, Adrian Popescu1",Not provided,Not provided,"Large Language Models, Bias Auditing, Entity Probes, Natural Language Processing, Social Bias","This paper introduces a scalable bias-auditing framework using named entities as probes to measure structural disparities in model behavior. The authors show that synthetic data reliably reproduces bias patterns observed in natural text, enabling large-scale analysis. The framework is applied to a comprehensive bias audit involving 1.9 billion data points across multiple entity types, tasks, languages, models, and prompting strategies. The results reveal systematic biases in LLMs, including penalties for right-wing politicians, preferences for left-wing politicians, and biases towards Western and wealthy nations over the Global South. The study also highlights that instruction tuning reduces bias, but increasing model scale amplifies it, and prompting in Chinese or Russian does not attenuate Western-aligned preferences. These findings suggest that LLMs should undergo rigorous auditing before deployment in high-stakes applications.",38.04,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12389v1_NADIR Differential Attention Flow for Non-Autoregr.pdf,NADIR: Differential Attention Flow for Non-Autoregressive Transliteration in Indic Languages,"Lakshya Tomar, Vinayak Abrol, Puneet Agarwal",,,"transliteration, non-autoregressive models, Indic languages, Differential Transformer, Mixture-of-Experts","This work introduces NADIR, a novel non-autoregressive architecture designed to balance speed and accuracy in multilingual transliteration tasks in Indic languages. NADIR integrates a Differential Transformer and a Mixture-of-Experts mechanism, achieving over a 13× speed-up compared to state-of-the-art autoregressive models while maintaining competitive performance metrics.",35.33,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12392v1_PsychēChat An Empathic Framework Focused on Emotio.pdf,Psych¯eChat: An Empathic Framework Focused on Emotion Shift Tracking and Safety Risk Analysis in Psychological Counseling,"Zhentao Xia, Yongqi Fan, Yuxiang Chu, Yichao Yin, Liangliang Chen, Tong Ruan, Weiyan Zhang",,,"psychological counseling, emotion shift, safety risk analysis, large language models, empathy","Psych¯eChat is an empathic framework designed to track emotion shifts and analyze safety risks in psychological counseling. It employs interactive role-playing to synthesize counselor-seeker dialogues, incorporating modules for emotion management and risk control. Extensive experiments demonstrate its superior performance in emotional insight and safety control compared to existing methods.",35.58,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12401v1_Beyond the Dirac Delta Mitigating Diversity Collap.pdf,Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation,"Jinmei Liu, Haoru Li, Zhenhong Sun, Chaofeng Chen, Yatao Bian, Bo Wang, Daoyi Dong, Chunlin Chen, Zhi Wang",,,"Reinforcement Learning, Fine-Tuning, Versatile Image Generation, Diversity Collapse, Reward Concentration, Stochastic Variations, Potential-Based Reward Shaping","Reinforcement learning is used to fine-tune large-scale image generation models to align with complex human preferences and user-specified tasks. However, a fundamental limitation is the curse of diversity collapse, where the fine-tuned model achieves high rewards but loses output diversity, producing monotonous, single-pattern images. This paper proposes DRIFT, an innovative framework that systematically incentivizes output diversity throughout the on-policy fine-tuning process, reconciling strong task alignment with high generation diversity to enhance versatility essential for applications that demand diverse candidate generations.",37.32,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12402v1_Weaknesses of Facial Emotion Recognition Systems.pdf,Weaknesses of Facial Emotion Recognition Systems,"Aleksandra Jamróz, Patrycja Wysocka, Piotr Garbat",Not found,2601.12402,"Facial Emotion Recognition, Deep learning, Computer Vision","This study presents a comprehensive analysis of advanced facial emotion recognition solutions, evaluating their performance on different datasets and revealing weaknesses, including differences between datasets, unequal levels of difficulty in recognizing certain emotions, and challenges in differentiating between closely related emotions.",38.31,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12405v1_Explainable Machine Learning for Pediatric Dental .pdf,Explainable Machine Learning for Pediatric Dental Risk Stratification Using Socio-Demographic Determinants,"Manasi Kanade, Abhi Thakkar, Gabriela Fernandes",Not provided,Not provided,"Machine Learning, Pediatric Dentistry, Risk Stratification, Socio-Demographic Factors, Artificial Intelligence, Transparency, Ethics","This study develops and evaluates an explainable artificial intelligence framework for pediatric dental risk stratification, prioritizing interpretability, calibration, and ethical deployment over maximal predictive accuracy.",38.39,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12410v1_Are LLMs Smarter Than Chimpanzees An Evaluation on.pdf,Are LLMs Smarter Than Chimpanzees?,"Dingyi Yang, Junqi Zhao, Xue Li, Ce Li, Boyang Li*",,2601.12410,"Large Language Models, Chimpanzees, Perspective Taking, Knowledge State Estimation",This paper evaluates the performance of Large Language Models (LLMs) in understanding and predicting the knowledge states and intentions of story characters. It introduces two tasks to test if LLMs can detect when characters demonstrate knowledge they should not possess and if they can predict their next actions based on their own knowledge versus objective truths they do not know. The results show that current state-of-the-art LLMs perform near-randomly on both tasks and are substantially inferior to humans.,35.73,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12415v1_Orthogonalized Policy OptimizationDecoupling Sampl.pdf,Orthogonalized Policy Optimization,Wang Zixian,Not found,2601.12415,"Large Language Model, Policy Optimization, Alignment Methods, RLHF","This work shows that recent alignment methods for large language models, including PPO, DPO, and IPO, implicitly conflate two fundamental design choices: sampling geometry and optimization geometry. It proposes Orthogonalized Policy Optimization (OPO) to explicitly decouple these choices, leading to stable optimization and avoiding gradient saturation even when model confidence is high.",36.31,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12436v1_Purification Before Fusion Toward Mask-Free Speech.pdf,PURIFICA TION BEFORE FUSION: TOW ARD MASK-FREE SPEECH ENHANCEMENT FOR ROBUST AUDIO-VISUAL SPEECH RECOGNITION,"Linzhi Wu1,2, Xingyu Zhang2∗, Hao Yuan3,2, Yakun Zhang2, Changyan Zheng4,2, Liang Xie2, Tiejun Liu1, Erwei Yin2",Not found,Not found,"audio-visual speech recognition, speech feature enhancement, noise-robust, multimodal bottleneck Conformer","This work proposes an end-to-end noise-robust audio-visual speech recognition (AVSR) framework coupled with speech enhancement, eliminating the need for explicit noise mask generation. The framework leverages a Conformer-based bottleneck fusion module to implicitly refine noisy audio features with video assistance, reducing modality redundancy and enhancing inter-modal interactions to preserve speech semantic integrity and achieve robust recognition performance. Experimental evaluations on the public LRS3 benchmark suggest that the method outperforms prior advanced mask-based baselines under noisy conditions.",37.73,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12442v1_Constraint-Aware Neurosymbolic Uncertainty Quantif.pdf,Constraint-Aware Neurosymbolic Uncertainty Quantification with Bayesian Deep Learning for Scientific Discovery,"Shahnawaz Alam1, Mohammed Mudassir Uddin 1, Mohammed Kaif Pasha 1",,,"Neurosymbolic AI, Uncertainty Quantification, Bayesian Deep Learning, Scientific Constraints, Calibration, Physics-Informed Machine Learning","Scientific Artificial Intelligence applications require models that deliver trustworthy uncertainty estimates while respecting domain constraints. Existing uncertainty quantification methods lack mechanisms to incorporate symbolic scientific knowledge, while neurosymbolic approaches operate deterministically without principled uncertainty modeling. The paper introduces the Constraint-Aware Neurosymbolic Uncertainty Framework (CANUF), unifying Bayesian deep learning with differentiable symbolic reasoning. Experiments on Materials Project, QM9 molecular properties, and climate benchmarks show CANUF reduces Expected Calibration Error by 34.7% versus Bayesian neural networks while maintaining 99.2% constraint satisfaction. Ablations reveal constraint-guided recalibration contributes 18.3% performance gain, with constraint extraction achieving 91.4% precision. CANUF provides the first end-to-end differentiable pipeline addressing uncertainty quantification, constraint satisfaction, and interpretable explanations for scientific predictions.",38.38,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12443v1_Adversarial Defense in Vision-Language Models An O.pdf,Adversarial Defense in Vision-Language Models: An Overview,"Xiaowei Fu, Lei Zhang*",Not found,Not found,"Vision-Language Models, Adversarial Attacks, Robustness, Training-Time Defense, Test-Time Adaptation Defense, Training-Free Defense","The widespread use of Vision Language Models (VLMs) has raised concerns about their vulnerability to adversarial attacks. Three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting strengths and limitations of such approaches and discussing ongoing challenges.",36.19,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12444v1_Large Language Model for OWL Proofs.pdf,Large Language Model for OWL Proofs,"Hui Yang, Jiaoyan Chen, Uli Sattler",10.1145/XXXXXX.XXXXXX,,"Large Language Models, OWL ontologies, Proof generation, Logic reasoning, Natural language generation","This work studies proof generation in the context of OWL ontologies by developing an automated dataset construction and evaluation framework. It encompasses three sequential tasks for complete proving: Extraction, Simplification, and Explanation, and an additional task of assessing Logic Completeness of the premise. Through extensive experiments on widely used reasoning LLMs, important findings are achieved, including the performance limitations of some models on complex cases, the dominance of logical complexity over representation format, and the impact of noise and incompleteness in input data on LLMs' performance. These results underscore the promise of LLMs for rigorous logics and the need for resilient reasoning under complex or imperfect conditions.",36.69,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12449v1_AgenTRIM Tool Risk Mitigation for Agentic AI.pdf,AgenTRIM: Tool Risk Mitigation for Agentic AI,"Roy Betser1*†, Shamik Bose1†, Amit Giloni1, Chiara Picardi1, Sindhu Padakandla 2, Roman Vainshtein 1",Not found,Not found,"AI agents, tool risk mitigation, agentic risk, least-privilege tool access, indirect prompt injection, tool misuse, LLM-based agents","AI agents, which combine large language models with external tools, can introduce security risks such as indirect prompt injection and tool misuse. AGENTRIM is a framework designed to detect and mitigate these risks without altering the agent's internal reasoning. It addresses these risks through complementary offline and online phases, reconstructing and verifying the agent's tool interface from code and execution traces, and enforcing least-privilege tool access at runtime through adaptive filtering and status-aware validation of tool calls. Evaluations on the AgentDojo benchmark show that AGENTRIM substantially reduces attack success while maintaining high task performance.",31.23,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12465v1_Incentivizing In-depth Reasoning over Long Context.pdf,INCENTIVIZINGIN-DEPTHREASONING OVERLONG CONTEXTS WITHPROCESSADVANTAGESHAPING,"Miao Peng, Weizhou Shen, Nuo Chen, Chenliang Li, Ming Yan, Jia Li",Not found,Not found,"Reinforcement Learning, Long Context Reasoning, LLM, Process Advantages Shaping, KG-driven Synthesis","This paper proposes DEEPREASONQA, a KG-driven synthesis framework that controllably constructs high-difficulty, multi-hop long-context QA pairs with inherent reasoning chains. It introduces Long-context Process Advantage Shaping (LONGPAS) to perform fine-grained credit assignment by evaluating reasoning steps along Validity and Relevancedimensions, capturing critical learning signals from 'almost-there' trajectories. Experiments on three long-context reasoning benchmarks show that the approach substantially outperforms RLVR baselines and matches frontier LLMs while using far fewer parameters.",27.72,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12467v1_Patch-Level Tokenization with CNN Encoders and Att.pdf,Patch-Level Tokenization with CNN Encoders and Attention for Improved Transformer Time-Series Forecasting,"Saurish Nagrath, VIT-AP",,,"Transformer models, multivariate time-series forecasting, financial time-series forecasting, convolutional neural networks, attention mechanisms, representation learning, deep learning for finance","This work proposes a two-stage forecasting framework that explicitly separates local temporal representation learning from global dependency modelling. In the first stage, a convolutional neural network (CNN) operates on fixed-length temporal patches to extract short-range temporal dynamics and non-linear feature interactions, producing compact patch-level token embeddings. Token-level self-attention is subsequently applied during representation learning to refine these embeddings by enabling interactions across temporal patches. In the second stage, a Transformer encoder processes the resulting token sequence to model inter-patch temporal dependencies and generate per-patch forecasts. Experiments conducted on synthetic multivariate time-series data with controlled static and dynamic factors demonstrate that the proposed patch-based tokenization strategy achieves competitive forecasting performance compared to convolutional and patch-based Transformer baselines.",38.27,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12471v1_Knowing When to Abstain Medical LLMs Under Clinica.pdf,Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty,"Sravanthi Machcha*, Sushrita Yerra*, Sahil Gupta, Aishwarya Sahoo, Sharmin Sultana, Hong Yu, Zonghai Yao",,,"large language models, medical multiple-choice question answering, abstention, conformal prediction, adversarial perturbations, uncertainty quantification","Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. This paper introduces MedAbstain, a unified benchmark and evaluation protocol for abstention in medical MCQA, integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertainty. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.",28.15,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12494v1_Harmonizing the Arabic Audio Space with Data Sched.pdf,Harmonizing the Arabic Audio Space with Data Scheduling,"Hunzalah Hassan Bhatti, Firoj Alam, Shammur Absar Chowdhury",,,"Audio Large Language Models, Multi-task Instruction Tuning, Arabic Speech Summarization, Dialect Identification, Emotion Recognition, Data Scheduling, Parameter-Efficient Fine-Tuning","This paper presents a systematic study of multi-task instruction tuning for an Arabic-centric audio Large Language Model (LLM), covering generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). It introduces AraMega-SSum, a novel dataset for Arabic speech summarization, and proposes Task-Progressive Curriculum (TPC) along with Aligner-Based Diverse Sampling (ADS) to construct information-dense batches. The findings reveal a critical efficiency-robustness trade-off, with ADS accelerating initial convergence and boosting paralinguistic F1-scores, but destabilizing generative decoding under prolonged training. TPC stabilizes core acoustic mapping but often induces negative transfer in downstream tasks. A Hybrid TPC+ADS strategy provides an optimal training recipe, first establishing a robust representative foundation before employing diversity-aware reﬁnement to capture fine-grained nuances.",28.82,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12499v1_Failure Modes in Multi-Hop QA The Weakest Link Law.pdf,Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck,"Meiru Zhang, Zaiqiao Meng, Nigel Collier",Not found,Not found,"Large Language Models, Multi-Hop Question Answering, Position Bias, Attention Mechanisms, Recognition Failure, Synthesis Failure","Despite scaling to massive context windows, Large Language Models struggle with multi-hop reasoning due to inherent position bias, which causes them to overlook information at certain positions. This paper introduces Multi-Focus Attention Instruction (MFAI) to disentangle recognition and synthesis failures by explicitly steering attention towards selected positions. Across 5 LLMs on two multi-hop QA tasks, we establish the 'Weakest Link Law': multi-hop reasoning performance collapses to the performance level of the least visible evidence. Crucially, this failure is governed by absolute position rather than the linear distance between facts (performance variance <3%).",28.19,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12518v1_Cooperative Multi-agent RL with Communication Cons.pdf,Cooperative Multi-agent RL with Communication constraints,"Nuoya Xiong ∗, Aarti Singh †",,,"Cooperative Multi-agent Reinforcement Learning, Communication Constraints, Importance Sampling, Base Policy Prediction, ε-Nash Equilibrium","This paper addresses the challenge of cooperative multi-agent reinforcement learning (MARL) in decentralized systems with limited communication, proposing a technique called base policy prediction to reduce the gap between the base policy and the current policy, enabling effective learning with significantly fewer communication rounds.",26.82,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12522v1_Improved Bug Localization with AI Agents Leveragin.pdf,Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition,"Asif Mohammed Samir, Mohammad Masudur Rahman",XXXXXXX.XXXXXXX,XXXXXXX,"Bug Localization, Large Language Models, Agentic AI, Cognition, Debugging, Software Engineering, Information Retrieval","This paper presents a novel agentic technique, CogniGent, for bug localization that overcomes limitations of traditional methods and LLMs by leveraging multiple AI agents capable of causal reasoning, call-graph-based root cause analysis, and context engineering. It emulates dynamic cognitive debugging practices and conducts hypothesis testing to support bug localization. Experimental results show significant improvements in performance metrics compared to existing techniques.",27.45,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12534v1_Encoding Emotion Through Self-Supervised Eye Movem.pdf,ENCODING EMOTION THROUGH SELF-SUPERVISED EYE MOVEMENT RECONSTRUCTION,"Marcus Ma, Jordan Prescott, Emily Zhou, Tiantian Feng, Kleanthis Avramidis, Gabor Mihaly Toth, Shrikanth Narayanan",,,"eye movement, self-supervised learning, emotion prediction, deep learning","The paper investigates how eye movement can be used to predict multimodal markers of emotional expression from naturalistic, low-resolution videos. It uses a collection of video interviews from the USC Shoah Foundation’s Visual History Archive with Holocaust survivors and develops a novel gaze detection model that uses self-supervised eye movement reconstruction to effectively leverage unlabeled video. The model is fine-tuned on two downstream tasks related to emotional expression, and the results show a positive correlation between pretraining performance and emotion processing performance.",27.66,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12535v1_Improving Low-Resource Machine Translation via Rou.pdf,Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning,"Ahmed Attia, Alham Fikri",,2310.16774,"machine translation, reinforcement learning, low-resource, round-trip, self-supervised","This paper investigates a self-supervised reinforcement-learning-based fine-tuning approach for translation in low-resource settings using round-trip bootstrapping with the No Language Left Behind (NLLB) family of models. The authors evaluate both the 600M and 1.3B parameter NLLB models and observe consistent improvements for several target languages, indicating increased fluency and semantic fidelity in translation outputs. The method can further benefit from scale, enabling models to leverage their pretrained knowledge and continue self-improving.",27.59,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12538v1_Agentic Reasoning for Large Language Models.pdf,Agentic Reasoning for Large Language Models,"Tianxin Wei, Ting-Wei Li, Zhining Liu, Xuying Ning, Ze Yang, Jiaru Zou, Zhichen Zeng, Ruizhong Qiu, Xiao Lin, Dongqi Fu, Wenxuan Bao, Yunzhe Li, Gaotang Li, Cheng Qian, Yu Wang, Xiangru Tang, Yuji Zhang, Chi Wang, Jiaxuan You, Heng Ji, Hanghang Tong, Jingrui He",,2601.12538,"Agentic AI, LLM Agent, Agentic Reasoning, Self-evolving","This survey provides a systematic roadmap for agentic reasoning, a paradigm shift that bridges thought and action by reframing large language models as autonomous agents capable of planning, acting, and learning through continual interaction. It analyzes system constraints and optimization settings, distinguishing in-context reasoning from post-training reasoning, and reviews agentic reasoning frameworks in real-world applications and benchmarks spanning science, robotics, healthcare, autonomous research, and math.",28.19,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12539v1_MemeLens Multilingual Multitask VLMs for Memes.pdf,MemeLens: Multilingual Multitask VLMs for Memes,"Ali Ezzat Shahroor, Mohamed Bayan Kmainasi, Abul Hasnat, Dimitar Dimitrov, Giovanni Da San Martino, Preslav Nakov, Firoj Alam",,,"memes, multimodal, vision-language model, multitask, hate, misogyny, sentiment, humor, figurative language, persuasion","Memes are a dominant medium for online communication and manipulation. Existing research is fragmented across tasks and languages, limiting cross-domain generalization. MEMELENS proposes a unified multilingual and multitask vision-language model for meme understanding, consolidating 38 public datasets and presenting a comprehensive empirical analysis across modeling paradigms, task categories, and datasets.",27.24,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12542v1_Rethinking the AI Scientist Interactive Multi-Agen.pdf,Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery,"Lukas Weidener*, Marko Brkić*, Mihailo Jovanović*, Ritvik Singh, Chiara Baccin, Emre Ulgac, Alex Dobrin, Aakaash Meduri",Not found,Not found,"Artificial Intelligence, Scientific Discovery, Multi-Agent Systems, Interactive Workflow, Research Automation","This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. It comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Evaluation on the BixBench benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points.",27.01,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12547v1_How Clinicians Think and What AI Can Learn From It.pdf,How Clinicians Think—and What AI Can Learn From,"Dr. Dipayan Sengupta, MD (Dermatology), Dr. Saumya Panda, MD (Dermatology)",,2601.12547,"clinical AI, decision-making, ordinal non-compensatory, fast-and-frugal heuristics","This paper discusses how clinicians think and how AI can learn from them, focusing on the sequential and time-bound nature of clinical reasoning and the importance of robust action selection over accurate predictions.",26.29,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12549v1_Benchmarking Concept-Spilling Across Languages in .pdf,Benchmarking Concept-Spilling Across Languages in LLMs,"Ilia Badanin, Daniil Dzenhaliou, Imanol Schlag",Not found,2601.12549,"Large Language Models, Cross-lingual, Semantic Robustness, Polysemy, Language Spilling","This paper presents a novel comparative framework for evaluating multilingual semantic robustness by systematically measuring how models handle polysemous words across languages. It evaluates a diverse set of open and closed multilingual LLMs using a structured meaning generation task across nine languages, employing a carefully curated benchmark of 100 high-polysemy English words. The findings reveal significant variation in semantic robustness across both models and languages, providing a principled ranking system for model comparison without requiring definitive causal attribution of error sources.",27.91,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12554v1_Artificial Intelligence in Materials Science and E.pdf,"Artificial Intelligence in Materials Science and Engineering: Current Landscape, Key Challenges, and Future Trajectories","Iman Peivaste1,2, Salim Belouettar ∗1, Francesco Mercuri 3, Nicholas Fantuzzi 4, Hamidreza Dehghani 1, Razieh Izadi 1, Halliru Ibrahim 1, Jakub Lengiewicz 1, Maël Belouettar-Mathis5, Kouider Bendine 1, Ahmed Makradi 1, Martin Hörsch 6, Peter Klein7, Mohamed El Hachemi 1, Heinz A. Preisig 8, Yacine Rezgui 9, Natalia Konchakova10, Ali Daouadji 11",Not provided,2601.12554,"Artificial Intelligence, Materials Science, Materials Engineering, Machine Learning, Deep Learning, Data-driven, Discovery Acceleration, Optimization","This review provides a comprehensive and structured overview of the current landscape in materials science and engineering, focusing on the role of artificial intelligence in navigating complexity, accelerating discovery, and optimizing material design. It surveys various machine learning approaches, from traditional algorithms to advanced deep learning architectures, and emphasizes the pivotal role of data in this field.",28.61,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12557v1_Life Machine Learning and the Search for Habitabil.pdf,"Life, Machine Learning, and the Search for Habitability: Predicting Biosignature Fluxes for the Habitable Worlds Observatory","Mark Moussa1, Amber V. Young1, Brianna Isola1, Vasuda Trehan1, Michael D. Himes1, Nicholas Wogan2, Giada Arney1",,,"Machine learning, Exoplanets, Biosignatures, Habitable Worlds Observatory, Direct imaging, Bayesian Convolutional Neural Network, Spectral Query Adaptive Transformer","Future direct-imaging flagship missions face critical decisions in prioritizing observations due to stringent time and resource constraints. This paper introduces two advanced machine-learning architectures for predicting biosignature species fluxes from exoplanetary reflected-light spectra, demonstrating comparable high predictive accuracy on an augmented dataset and highlighting their distinct advantages in uncertainty quantification and spectral interpretability.",27.81,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12560v1_Agentic Artificial Intelligence AI Architectures T.pdf,"Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents","Arunkumar V, Gangadharan G.R., Rajkumar Buyya",Not found,2601.12560,"Agentic AI, Large Language Models, Autonomous Agents, Multi-Agent Systems, Cognitive Architectures, Tool Use, Planning","This paper investigates the architectures and proposes a unified taxonomy for large language model agents, breaking them into Perception, Brain, Planning, Action, Tool Use, and Collaboration. It explores the transition from linear reasoning to native inference and from fixed API calls to open standards, and reviews current evaluation practices. It also highlights open challenges and outlines future research directions.",28.49,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12577v1_Primate-like perceptual decision making emerges th.pdf,Primate-like perceptual decision making emerges through deep recurrent reinforcement learning,"Nathan J. Wispinski, Scott A. Stone, Anthony Singhal, Patrick M. Pilarski, Craig S. Chapman",Not provided,2601.12577,"Reinforcement learning, Deep learning, Perceptual decision making, Primates, Neuroscience, Behavioral neuroscience","This study trained an end-to-end deep recurrent neural network using reinforcement learning on a noisy perceptual discrimination task, demonstrating key abilities of primate-like decision making such as trading off speed for accuracy and flexibly changing their mind in the face of new information, providing experimental support for the theory that such mechanisms emerged to maximize reward in noisy, temporally evolving environments.",28.28,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12582v1_Ontology-aligned structuring and reuse of multimod.pdf,Ontology-aligned structuring and reuse of multimodal materials data and workflows towards automatic reproduction,"Sepideh Baghaee Ravari, Abril Azocar, Guzman Sarath, Menon Stefan, Sarath Menon, Stefan Sandfeld, Tilmann Hickel, Markus Stricker",,,"text mining, workflow, large language models, stacking fault energy","Reproducibility of computational results in materials science is a challenge, as simulation workflows and parameters are often reported only in unstructured text and tables. An ontology-driven, large language model (LLM)-assisted framework is introduced for the automated extraction and structuring of computational workflows from the literature, focusing on density functional theory-based stacking fault energy (SFE) calculations in hexagonal close-packed magnesium and its binary alloys.",28.02,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12585v1_Do MLLMs See What We See Analyzing Visualization L.pdf,Do MLLMs See What We See?,"Mengli (Dawn) Duan*, Yuhe (Sissi) Jiang*, Matthew Varona, Carolina Nobre",Not found,Not found,"Visualization Literacy, Multimodal Large Language Model, Evaluation Study","We present the first systematic analysis of barriers to visualization literacy in MLLMs, using the reVLAT benchmark with synthetic data. Our analysis reveals two machine-specific barriers and informs future evaluation and design of reliable AI-driven visualization assistants.",26.6,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12594v1_SLAP Scalable Language-Audio Pretraining with Vari.pdf,SLAP: SCALABLE LANGUAGE-AUDIO PRETRAINING WITH V ARIABLE-DURATION AUDIO AND MULTI-OBJECTIVE TRAINING,"Xinhao Mei, Gael Le Lan, Haohe Liu, Zhaoheng Ni, V arun Nagaraja, Yang Liu, Yangyang Shi, Vikas Chandra",Not found,Not found,"Multimodal learning, CLAP, self-supervised learning, contrastive learning, multi-objective learning","SLAP scales language-audio pretraining to 109 million audio-text pairs with variable audio durations and incorporates multiple training objectives, achieving new state-of-the-art performance on audio-text retrieval and zero-shot audio classification tasks.",27.52,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12607v1_A Cloud-based Multi-Agentic Workflow for Science.pdf,A Cloud-based Multi-Agentic Workflow for Science,"Anurag Acharya, Timothy Vega, Rizwan A. Ashraf, Anshu Sharma, Derek Parker, Robert Rallo",10.1145/nnnnnnn.nnnnnnn,,"Large Language Models, LLMs for Science, LLM Agents, Multi-agent Framework, Catalysis, Chemistry, Material Science, Cloud Computing","This work presents a domain-agnostic, model-independent workflow for an agentic framework that can act as a scientific assistant while being run entirely on cloud. The framework balances models, cloud providers, and external resources, and is validated on synthetic and real-world tasks. It shows high success rates in routing tasks to the correct agent and completing assigned tasks.",26.98,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12617v1_Creating Disability Story Videos with Generative A.pdf,"Creating Disability Story Videos with Generative AI: Motivation, Expression, and Sharing","Shuo Niu, Dylan Clements, Hyungsin Kim",10.1145/3772318.3791495,,"Disability, Storytelling, Video, Generative AI, LLM","This research examines how nine people with disabilities used generative AI to create videos sharing their disability experiences, exploring motivations, expression, and sharing. It concludes with a framework of momentous depiction, highlighting four core affordances of GenAI that facilitate or require improvements for better supporting disability storytelling.",26.42,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12637v1_Topology-Aware Multiscale Mixture of Experts for E.pdf,TOPOLOGY-AWARE MULTISCALE MIXTURE OF EXPERTS FOR EFFICIENT MOLECULAR PROPERTIES PREDICTION,"Long D. Nguyen, Kelin Xia, Binh P. Nguyen",Not found,2601.12637,"Graph Neural Networks, Topological Deep Learning, Mixture of Experts, Molecular Representation","Predicting molecular properties is crucial in drug discovery and materials science. This paper proposes Multiscale Interaction Mixture of Experts (MI-MoE) to adapt interaction modeling across geometric regimes, improving multiple strong 3D molecular backbones across diverse benchmark datasets.",26.82,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12638v1_Mixed Precision PointPillars for Efficient 3D Obje.pdf,Mixed Precision PointPillars for Efficient 3D Object Detection with TensorRT,"1st Ninnart Fuengfusin, 2nd Keisuke Yoneda, 3rd Naoki Suganuma",Not provided,Not provided,"neural networks, quantization, 3D object detection","This paper proposes a mixed precision framework for PointPillars to accelerate 3D LIDAR object detection models, addressing the wide numerical distribution and extreme outliers issues. The framework uses post-training quantization (PTQ) to identify sensitive layers and combines them with quantization-aware training (QAT) to improve performance. TensorRT deployment results in reduced latency and size for the models.",27.31,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12641v1_STEP-LLM Generating CAD STEP Models from Natural L.pdf,STEP-LLM: Generating CAD STEP Models from Natural Language with Large Language Models,"Xiangyu Shi, Junyang Ding, Xu Zhao, Sinong Zhan, Payal Mohapatra, Daniel Quispe, Kojo Welbeck, Jian Cao, Wei Chen, Ping Guo, Qi Zhu",Not found,2601.12641,"Computer-aided design, STEP file, large language models, design automation","This paper presents a method for generating CAD models from natural language using large language models (LLMs). It addresses the challenges of creating CAD models from natural language, which are labor-intensive and require expertise. The authors curate a dataset of approximately 40K STEP-caption pairs and introduce novel preprocessing techniques tailored for the graph-structured format of STEP. They integrate retrieval-augmented generation (RAG) for supervised fine-tuning (SFT) and reinforcement learning (RL) for geometric refinement. The results demonstrate consistent gains in geometric fidelity over the Text2CAD baseline, with improvements arising from multiple stages of the framework.",28.29,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12646v1_Unbounded Harms Bounded Law Liability in the Age o.pdf,"Unbounded Harms, Bounded Law: Liability in the Age of Borderless AI",Ha-Chi Tran,not found,2601.12646v1,"artificial intelligence, risk governance, liability, borderless technology, global AI liability","The rapid advancement of artificial intelligence (AI) has exposed fundamental deficiencies in risk governance, particularly in ex post risk management. This paper examines the inadequacies in legal mechanisms for compensation, mitigation, attribution of responsibility, and liability allocation, especially in relation to transboundary AI harms and damages that transcend national borders. It draws on comparative and interdisciplinary approaches from high-risk and transnational domains to identify transferable legal design principles and structural constraints.",27.61,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12648v1_Intelligent Documentation in Medical Education Can.pdf,Intelligent Documentation in Medical Education: Can AI Replace Manual Case Logging?,"Nafiz Imtiaz Khan, Kylie Cleland, Vladimir Filkov, Roger Eric Goldman",,,"artificial intelligence, large language models, radiology, case logs, medical education","This study investigates the feasibility of using large language models (LLMs) to automate procedural case log documentation in radiology training. It evaluates whether AI can replace manual logging, identifies challenging procedure types for extraction, and assesses integration into clinical workflows.",25.99,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12654v1_Explanation Multiplicity in SHAP Characterization .pdf,Explanation Multiplicity in SHAP: Characterization and Assessment,"HYUNSEUNG HWANG, KAIST, Republic of Korea, SEUNGEUN LEE, New York University, USA, LUCAS ROSENBLATT, New York University, USA, JULIA STOYANOVICH, New York University, USA, STEVEN EUIJONG WHANG, KAIST, Republic of Korea",Not provided,Not provided,"Explainable AI, SHAP, Post-hoc feature attribution, Explanation multiplicity, Stochasticity, Feature ranking","Post-hoc explanations are widely used in high-stakes domains such as lending, employment, and healthcare. SHAP is often used to justify decisions, but it can produce different explanations for the same decision, a phenomenon known as explanation multiplicity. This paper presents a methodology to characterize and assess explanation multiplicity in SHAP, showing that it is widespread and persists even under controlled conditions. This instability poses a normative challenge for responsible AI deployment.",27.85,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12658v1_Augmenting Question Answering with A Hybrid RAG Ap.pdf,Augmenting Question Answering with A Hybrid RAG Approach,"Tianyi Yang, Nashrah Haque, Vaishnave Jonnalagadda, Yuya Jeremy Ong, Zhehui Chen, Yanzhao Wu, Lei Yu, Divyesh Jadav, Wenqi Wei",Not found,Not found,"Question-answering, RAG, query processing, retrieval-augmented generation, semantic integration, contextual grounding","This paper introduces Structured-Semantic RAG (SSRAG), a hybrid architecture that enhances QA quality by integrating query augmentation, agentic routing, and a structured retrieval mechanism combining vector and graph based techniques with context unification. The approach improves both answer accuracy and informative-ness over standard RAG implementations, as demonstrated through extensive evaluations on three popular QA datasets.",27.28,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12661v1_MedConsultBench A Full-Cycle Fine-Grained Process-.pdf,"MedConsultBench: A Full-Cycle, Fine-Grained, Process-Aware Benchmark for Medical Consultation Agents","Chuhan Qiao*, Jianghua Huang*, Daxing Zhao*, Ziding Liu, Yanjun Shen†, Bing Cheng, Wei Lin, Kai Wu",Not provided,2601.12661,"Medical consultation, AI benchmark, Clinical workflow, Information acquisition, Diagnostic accuracy, Medication safety","Current evaluations of medical consultation agents often prioritize outcome-oriented tasks, overlooking the end-to-end process integrity and clinical safety essential for real-world practice. MedConsultBench addresses this gap by introducing Atomic Information Units (AIUs) to track clinical information acquisition at a sub-turn level, enabling precise monitoring of how key facts are elicited through 22 fine-grained metrics. The benchmark evaluates uncertainty-aware yet concise inquiry while emphasizing medication regimen compatibility and the ability to handle realistic post-prescription follow-up Q&A via constraint-respecting plan revisions.",28.23,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12664v1_Generalizable Hyperparameter Optimization for Fede.pdf,Generalizable Hyperparameter Optimization for Federated Learning on Non-IID Cancer Images,"Elisa Gonçalves Ribeiro, Rodrigo Moreira, Larissa Ferreira Rodrigues Moreira, André Ricardo Backes",,,"Federated Learning, Hyperparameter Optimization, Non-IID Data, Medical Imaging, Cancer","This paper examines whether hyperparameters optimized on one cancer imaging dataset generalize across non-IID federated scenarios. It considers binary histopathology tasks for ovarian and colorectal cancers and performs centralized Bayesian hyperparameter optimization, transferring dataset-specific optima to the non-IID FL setup. The main contribution is a simple cross-dataset aggregation heuristic by combining configurations by averaging learning rates and considering modal optimizers and batch sizes, achieving competitive classification performance.",26.83,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12667v1_Empowering All-in-Loop Health Management of Spacec.pdf,Empowering All-in-Loop Health Management of Spacecraft Power System in the Mega-Constellation Era via Human-AI Collaboration,"Yi Di, Zhibin Zhao, Fujin Wang, Xue Liu, Jiafeng Tang, Jiaxin Ren, Zhi Zhai",,,"Large Language Model, Human-AI Collaboration, Spacecraft Power System, All-in-loop Health Management, Satellite Mega-Constellation","This work proposes a principle of aligning underlying capabilities (AUC principle) and develops SpaceHMchat, an open-source Human-AI collaboration (HAIC) framework for all-in-loop health management (AIL HM) of spacecraft power systems (SPS). SpaceHMchat serves across the entire loop of work condition recognition, anomaly detection, fault localization, and maintenance decision making, achieving goals such as conversational task completion, adaptive human-in-the-loop learning, personnel structure optimization, knowledge sharing, efficiency enhancement, and transparent reasoning. A hardware-realistic fault injection experimental platform is established and its simulation model is built and open-sourced, fully replicating the real SPS. The corresponding experimental results demonstrate excellent performance across 23 quantitative metrics. Another contribution is the release of the first-ever AIL HM dataset of SPS, containing four sub-datasets, involving 4 types of AIL HM sub-tasks, 17 types of faults, and over 700,000 timestamps.",28.57,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12671v1_Exploiting Test-Time Augmentation in Federated Lea.pdf,Exploiting Test-Time Augmentation in Federated Learning for Brain Tumor MRI Classification,"Thamara Leandra de Deus Melo, Rodrigo Moreira, Larissa Ferreira Rodrigues Moreira, André R. Backes",,,"Brain tumors, Federated Learning, Test-Time Augmentation, Image classification","Efficient brain tumor diagnosis is crucial for early treatment; however, it is challenging due to lesion variability and image complexity. We evaluated convolutional neural networks (CNNs) in a federated learning (FL) setting, comparing models trained on original versus preprocessed MRI images (resizing, grayscale conversion, normalization, filtering, and histogram equalization). Preprocessing alone yielded negligible gains; combined with test-time augmentation (TTA), it delivered consistent, statistically significant improvements in federated MRI classification (p<0.001). In practice, TTA should be the default inference strategy in FL-based medical imaging; when the computational budget permits, pairing TTA with light preprocessing provides additional reliable gains.",27.67,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12688v1_Logic-Guided Multistage Inference for Explainable .pdf,Logic-Guided Multistage Inference for Explainable Multidefendant Judgment Prediction,"Xu Zhang, Qinghua Wang, Mengyang Zhao, Fang Wang, Cunquan Qu",Not found,Not found,"Multiple defendants, Legal judgment predictions, Label broadcast, Guilt responsibility, Transformer","This work proposes a masked multistage inference (MMSI) framework to enhance intelligent judicial systems by incorporating sentencing logic into a pretrained Transformer encoder framework. The framework clarifies roles through an oriented masking mechanism and improves model sensitivity to culpability distinctions between principals and accomplices. Predicted guilt labels are further incorporated into a regression model through broadcasting, consolidating crime descriptions and court views. The framework achieves significant accuracy improvements in role-based culpability differentiation, outperforming baselines on the custom IMLJP dataset for intentional injury cases. This work offers a robust solution for enhancing intelligent judicial systems, with publicly available code.",27.89,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12711v1_Neurosymbolic LoRA Why and When to Tune Weights vs.pdf,Neurosymbolic LoRA: Why and When to Tune Weights vs. Rewrite Prompts,"Kevin Wang*, Neel P. Bhatt*, Cong Liu*, Junbo Li, Runjin Chen, Yihan Xi, Timothy Barclay, Alvaro Velasquez, Ufuk Topcu, Zhangyang Wang",,,"Neurosymbolic, LoRA, Fine-tuning, Symbolic, TextGrad, Prompt Tuning, Mathematical Reasoning","A neurosymbolic LoRA framework that dynamically combines numerical updates and symbolic manipulations to adapt large language models, offering flexible control of style and alignment without retraining. The approach remains memory-efficient by offloading symbolic transformations to an external LLM only when needed, and refined prompts serve as reusable training data.",27.33,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12715v1_RSOD Reliability-Guided Sonar Image Object Detecti.pdf,RSOD: Reliability-Guided Sonar Image Object Detection,"Chengzhou Li, Ping Guo, Guanchen Meng, Qi Jia*, Jinyuan Liu, Zhu Liu, Xiaokang Liu, Yu Liu, Zhongxuan Luo, Xin Fan*",Not found,Not found,"sonar, object detection, reliability, pseudo-labeling, limited labels","Object detection in sonar images is challenging due to their limited texture details and susceptibility to noise. RSOD proposes a teacher-student framework to address this issue, leveraging a reliability score to develop pseudo-labels and optimize the student model's performance. This method achieves competitive results even with extremely limited labeled data.",26.97,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12720v1_Teaching Large Reasoning Models Effective Reflecti.pdf,Teaching Large Reasoning Models Effective Reflection,"Hanbin Wang, Jingwei Song, Jinpeng Li, Qi Zhu, Fei Mi, Ganqu Cui, Yasheng Wang, Lifeng Shang",Not found,Not found,"Large Reasoning Models, Self-Reflective Behaviors, Self-Critique, Backtracking, Reinforcement Learning, Effective Reflection Rewards","This paper addresses the problem of superficial reflection in Large Reasoning Models (LRMs), proposing Self-Critique Fine-Tuning (SCFT) and Reinforcement Learning with Effective Reflection Rewards (RLERR) to enhance the model's reflective reasoning ability and improve both reasoning accuracy and reflection quality.",26.66,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12723v1_An Evolutionary Framework for Automatic Optimizati.pdf,An Evolutionary Framework for Automatic Optimization Benchmark Generation via Large Language Models,"Yuhiro Ono, Tomohiro Harada, Yukiya Miura",Not found,2601.12723,"Optimization benchmarks, Large language models, Evolutionary algorithms, Benchmark generation, Benchmarking","This paper proposes an evolutionary framework for generating optimization benchmarks using a large language model, aiming to address the limitations of existing benchmarks in capturing real-world problem diversity and irregularity.",28.46,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12727v1_AI-exhibited Personality Traits Can Shape Human Se.pdf,AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations,"Jingshu Li, Tianqi Song, Nattapat Boonprakong, Zicheng Zhu, Yitian Yang, Yi-Chieh Lee",10.1145/3772318.3790654,2601.12727,"AI, Personality Traits, Self-concept, Conversations, Large Language Models","This study explores the impact of AI personality traits exhibited during conversations on users' self-concept, finding that after interacting with an AI chatbot, individuals' self-concept aligns with the AI's measured personality traits.",28.01,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12731v1_A Shared Geometry of Difficulty in Multilingual La.pdf,A Shared Geometry of Difficulty in Multilingual Language Models,"Stefano Civelli, Pietro Bernardelle, Nicolò Brunello, Gianluca Demartini",,,"multilingual language models, problem difficulty, linear probes, transformer layers, language agnostic, language specific, meta-cognitive attributes","This work investigates the multilingual geometry of problem difficulty in large language models (LLMs) by training linear probes on the AMC subset of the Easy2Hard benchmark, translated into 21 languages. It finds that difficulty-related signals emerge at two distinct stages of the model internals, corresponding to shallow and deep internal representations. Probes trained on deep representations achieve high accuracy within a language but poor cross-lingual generalization, while those on shallow representations generalize better across languages, despite lower within-language performance. These results suggest that LLMs first form a language-agnostic representation of problem difficulty, which subsequently becomes language-specific. This aligns with existing findings in LLM interpretability showing that models tend to operate in an abstract conceptual space before producing language-specific outputs.",27.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12740v1_TreeWriter AI-Assisted Hierarchical Planning and W.pdf,TreeWriter: AI-Assisted Hierarchical Planning and Writing for Long-Form Documents,"Zijian Zhang, Fangshi Du, Xingjian Liu, Pan Chen, Oliver Huang, Runlong Ye, Michael Liut, Alán Aspuru-Guzik",,,"AI, writing, long-form, hierarchical, coherence, automation","TreeWriter is a hierarchical writing system that represents documents as trees and integrates contextual AI support, allowing authors to create, save, and refine document outlines at multiple levels, facilitating drafting, understanding, and iterative editing of long documents.",26.47,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12742v1_AirHunt Bridging VLM Semantics and Continuous Plan.pdf,AirHunt: Bridging VLM Semantics and Continuous Planning for Efficient Aerial Object Navigation,"Xuecheng Chen, Zongzhuo Liu, Jianfa Ma, Bang Du, Tiantian Zhang, Xueqian Wang, Boyu Zhou",,,"Aerial Object Navigation, Vision-Language Models, Continuous Planning, Zero-Shot Generalization, Semantic Reasoning, Geometric Redundancy, Environmental Heterogeneity","Presenting AirHunt, an aerial object navigation system that efficiently locates open-set objects with zero-shot generalization in outdoor environments by seamlessly fusing Vision-Language Model (VLM) semantic reasoning with continuous path planning.",26.85,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12744v1_Vision Language Models for Optimization-Driven Int.pdf,Vision Language Models for Optimization-Driven Intent Processing in Autonomous Networks,"Tasnim Ahmed, Yifan Zhu, Salimur Choudhury",Not provided,Not provided,"Vision-Language Models, Optimization, Intent-Based Networking, Code Generation, Model Context Protocol","This paper presents IntentOpt, a benchmark of 85 optimization problems across 17 categories, evaluating four Vision-Language Models (GPT-5-Mini, Claude-Haiku-4.5, Gemini-2.5-Flash, Llama-3.2-11B-Vision) under three prompting strategies on multimodal versus text-only inputs. The evaluation shows that visual parameter extraction reduces execution success by 12–21 percentage points (pp), with GPT-5-Mini dropping from 93% to 72%. Program-of-thought prompting decreases performance by up to 13 pp, and open-source models lag behind closed-source ones, with Llama-3.2-11B-Vision reaching 18% compared to 75% for GPT-5-Mini. These results establish baseline capabilities and limitations of current VLMs for optimization code generation within an IBN system.",28.75,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12745v1_A Graph Prompt Fine-Tuning Method for WSN Spatio-T.pdf,A Graph Prompt Fine-Tuning Method for Spatio-Temporal Anomaly Detection in Wireless Sensor Networks,"Miao Ye, Jing Cui, Yuan Huang, Yong Wang, Qian He, Jiwen Zhang",,,"Anomaly Detection, Graph Neural Networks, Pre-training, Prompt Learning, Wireless Sensor Networks","This paper proposes a graph neural network anomaly detection backbone network incorporating spatio-temporal correlation features and a multi-task self-supervised training strategy for WSN graph structure data. The model achieves high F1 metrics of up to 91.30% and 92.31% on public and actual collected datasets, providing better detection performance and generalization ability than existing methods.",27.43,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12754v1_PAIR-SAFE A Paired-Agent Approach for Runtime Audi.pdf,PAIR-SAFE: A Paired-Agent Approach for Runtime Auditing and Refining,"Jiwon Kim, Violeta J. Rodriguez, Dong Whi Yoo, Eshwar Chandrasekharan, Koustuv Saha",,,"Large language models, Mental health support, Paired-agent framework, Runtime auditing, Runtime refinement, Motivational Interviewing Treatment Integrity (MITI-4)","Large language models are increasingly used for mental health support, but they can produce responses that are overly directive, inconsistent, or clinically misaligned. Existing approaches to mitigating these risks largely rely on implicit alignment through training or prompting, offering limited transparency and runtime accountability. We introduce PAIR-SAFE, a paired-agent framework for auditing and refining AI-generated mental health support that integrates a Responder agent with a supervisory Judge agent grounded in the clinically validated Motivational Interviewing Treatment Integrity (MITI-4) framework. The Judge audits each response and provides structured ALLOW or REVISE decisions that guide runtime response refinement. Our results show significant improvements in key MITI dimensions, including Partnership, Seek Collaboration, and overall Relational quality.",28.54,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12758v1_VISPA Pluralistic Alignment via Automatic Value Se.pdf,VISPA: Pluralistic Alignment via Automatic Value Selection and Activation,"Shenyan Zheng, Jiayou Zhong, Anudeex Shetty, Heng Ji, Preslav Nakov, Usman Naseem",,,"Large language models, pluralistic alignment, value selection, model activation, interpretability","Achieving pluralistic alignment in high-stakes domains, such as healthcare, requires models that reflect a range of perspectives rather than a single average preference. Existing approaches often lack value control and representation, leading to biased outputs. VISPA introduces a training-free framework that enables direct control over value expression through dynamic selection and internal model activation steering, demonstrating performance across various pluralistic alignment modes and settings.",26.9,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12762v1_Teaching LLMs to Learn Tool Trialing and Execution.pdf,Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction,"Xingjie Gao, Pengcheng Huang, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, Chen Qian, Ge Yu, Yu Gu",Not provided,Not provided,"Large Language Models, External Tools, Trial-and-Execution, Environment Interaction, Reinforcement Learning","This paper proposes ToolMaster, a framework that enables Large Language Models to learn tool usage through interaction with the environment. It adopts a trial-and-execution paradigm to train LLMs to first imitate teacher-generated trajectories containing explicit tool trials and self-correction, followed by reinforcement learning to coordinate the trial and execution phases. This process enables agents to autonomously explore correct tool usage by actively interacting with environments and forming experiential knowledge that benefits tool execution. Experimental results demonstrate significant improvements in generalization and robustness across unseen or unfamiliar tools.",27.26,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12781v1_VIRO Robust and Efficient Neuro-Symbolic Reasoning.pdf,VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension,"Hyejin Park, Junhyuk Kwon, Suha Kwak, Jungseul Ok",,,"Referring Expression Comprehension, Neuro-symbolic Reasoning, Verification-Integrated Reasoning Operators, Robustness, Efficiency","This paper introduces VIRO, a neuro-symbolic framework that integrates lightweight operator-level verifiers within reasoning steps to handle no-target cases robustly. VIRO achieves state-of-the-art performance, demonstrating generalization to real-world egocentric data and superior computational efficiency.",26.81,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12785v1_Distilling Time Series Foundation Models for Effic.pdf,DISTILLING TIME SERIES FOUNDATION MODELS FOR EFFICIENT FORECASTING,"Yuqi Li a†, Kuiye Ding a†, Chuanguang Yang b, Szu-Yu Chen c, Yingli Tian a⋆",Not found,Not found,"Time Series Foundation Model, Knowledge Distillation, Time Series Forecasting","Time Series foundation models (TSFMs) deliver strong forecasting performance through large-scale pretraining, but their large parameter sizes make deployment costly. DistilTS, the first distillation framework specifically designed for TSFMs, addresses two key challenges: task difficulty discrepancy and architecture discrepancy. It introduces horizon-weighted objectives and a temporal alignment strategy to balance learning across horizons and reduce architectural mismatch, enabling compact models. Experiments demonstrate that DistilTS achieves comparable forecasting performance to full-sized TSFMs while reducing parameters by up to 1/150 and accelerating inference by up to 6000×.",28.26,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12804v1_SL-CBM Enhancing Concept Bottleneck Models with Se.pdf,SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability,"Hanwei Zhang, Luo Cheng, Rui Wen, Yang Zhang, Lijun Zhang, Holger Hermanns",,,"explainable AI, Concept Bottleneck Models, semantic locality, interpretability, saliency maps, post-hoc methods","This work proposes SL-CBM, a novel extension of Concept Bottleneck Models (CBMs) that enhances locality faithfulness by generating spatially coherent saliency maps at both concept and class levels. SL-CBM integrates a1×1convolutional layer with a cross-attention mechanism to improve alignment between concepts, image regions, and final predictions. Extensive experiments on image datasets demonstrate that SL-CBM substantially improves locality faithfulness, explanation quality, and intervention efficacy while maintaining competitive classification accuracy.",27.07,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12805v1_SciHorizon-GENE Benchmarking LLM for Life Sciences.pdf,SciHorizon-Gene: Benchmarking LLM for Life Sciences Inference from Gene Knowledge to Functional Understanding,"Xiaohan Huang, Meng Xiao, Chuan Qin, Qingqing Long, Jinmiao Chen, Yuanchun Zhou, Hengshu Zhu",XXXXXXX.XXXXXXX,,"large language models, benchmarking and evaluation, genomics","Large language models have shown promise in biomedical research, particularly for knowledge-driven interpretation tasks. However, their ability to reliably reason from gene-level knowledge to functional understanding remains underexplored. SciHorizon-Gene introduces a large-scale gene-centric benchmark to address this gap, evaluating LLMs along four biologically critical perspectives.",26.82,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12809v1_Left-Right Symmetry Breaking in CLIP-style Vision-.pdf,Left–Right Symmetry Breaking in CLIP-style Vision-Language Models Trained,"Takaki Yamamoto1, Chihiro Noguchi 1, Toshihiro Tanizawa1",,,"Vision-Language Models, Transformer, Contrastive Learning, Spatial Understanding, Left-Right Symmetry, Label Diversity, Layout Diversity, Attention Mechanism","This study investigates how vision-language models acquire spatial and relational understanding, focusing on the emergence of left-right symmetry breaking in models trained with a CLIP-style contrastive objective. By systematically varying label and layout diversity, the authors find that contrastive training is the primary driver of generalization, and that label diversity, more than layout diversity, is crucial. They also show that interactions between positional and token embeddings induce a horizontal attention gradient that breaks left-right symmetry in the encoders, and that ablating this contribution substantially reduces left-right discrimination. The results provide a mechanistic insight into when and how CLIP-style models acquire relational competence.",27.36,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12816v1_Fisher-Orthogonal Projected Natural Gradient Desce.pdf,Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning,"Ishir Garg, Neel Kolhe, Andy Peng, Rohan Gopalam",Not found,Not found,"Continual learning, Natural gradient descent, Fisher information, Orthogonal projection, Catastrophic forgetting","The paper proposes the Fisher-Orthogonal Projected Natural Gradient Descent (FOPNG) optimizer to address the challenge of catastrophic forgetting in continual learning, where neural networks learn sequentially from non-stationary data streams without forgetting previously acquired knowledge. Unlike existing methods that operate in Euclidean parameter space, FOPNG projects gradients onto the Fisher-orthogonal complement of previous task gradients, ensuring descent in the Fisher metric and preserving prior task outputs.",27.31,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12822v1_MirrorGuard Toward Secure Computer-Use Agents via .pdf,MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real Reasoning Correction,"Wenqi Zhang, Yulin Shen, Changyue Jiang, Jiarun Dai, Geng Hong",XXXXXXX.XXXXXXX,,"Computer Use Agents, Agent Security, Reasoning Correction, Simulation, Vision-Language Models","Large foundation models are integrated into Computer Use Agents (CUAs) to enable autonomous interaction with operating systems through graphical user interfaces (GUIs). This autonomy introduces serious security risks, and existing defenses often abort tasks prematurely, reducing agent utility. MirrorGuard presents a novel defense framework that uses simulation-based training to improve CUA security in the real world.",26.64,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12837v1_Cognition spaces natural artificial and hybrid.pdf,"Cognition spaces: natural, artificial, and hybrid","Ricard Solé, Luis F Seoane, Jordi Pla-Mauri, Michael Timothy Bennett, Michael E. Hochberg, Michael Levin",,,"Evolved cognition, basal cognition, artificial life, artificial intelligence, synthetic biology, morphospace","Cognition is realized across natural, artificial, and hybrid systems, but lacks a unified framework. This paper proposes a cognition space approach to compare their forms, limits, and unrealized possibilities. Three cognition spaces—basal aneural, neural, and human–AI hybrid—are introduced and analyzed, showing uneven occupation with large unoccupied regions. These voids reflect evolutionary contingencies, physical constraints, and design limitations. The approach clarifies cognitive diversity and highlights hybrid cognition as a promising frontier.",26.96,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12842v1_SCULPT Constraint-Guided Pruned MCTS that Carves E.pdf,SCULPT: Constraint-Guided Pruned MCTS that Carves Efficient Paths for Mathematical Reasoning,"Qitong Fang*, Haotian Li†, Xu Wang‡",,,"Automated agent workflows, Large language models (LLMs), Mathematical reasoning, Constraint-guided Monte Carlo Tree Search (MCTS), Domain-aware scoring, Stochastic exploration, Efficient paths","This paper introduces SCULPT, a constraint-guided approach for Monte Carlo Tree Search (MCTS) that integrates domain-aware scoring into selection, expansion, simulation, and backpropagation. SCULPT scores and prunes actions using a combination of symbolic checks and structural pattern guidance, steering the search toward plausible reasoning paths. Under matched LLM configurations, SCULPT yields stable improvements on multiple datasets, and additional results assess executor transferability and performance on frontier reasoning models.",27.49,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12849v1_The Cost of EFX Generalized-Mean Welfare and Compl.pdf,The Cost of EFX: Generalized-Mean Welfare and Complexity Dichotomies with Few Surplus Items,"Eugene Lim, Tzeh Yuan Neoh, Nicholas Teh",,2601.12849,"Fair division, Envy-freeness, Generalized-mean welfare, NP-hard, Pareto-optimality","This paper studies the interaction between envy-freeness up to any good (EFX) and generalized-mean (p-mean) welfare, focusing on the setting with few surplus items. It establishes complexity dichotomies for EFX and p-mean welfare, providing polynomial-time algorithms for certain cases and showing that enforcing EFX can lead to welfare loss. The results highlight the computational cost and structural alignment of EFX in this setting.",36.57,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12856v1_Mining Citywide Dengue Spread Patterns in Singapor.pdf,Mining Citywide Dengue Spread Patterns in Singapore Through Hotspot Dynamics from Open Web Data,"Liping Huang, Gaoxi Xiao∗, Stefan Ma, Hechang Chen∗, Shisong Tang, Flora Salim",10.1145/XXXXXX.XXXXXX,,"Dengue Cases, Disease Spreading Pattern, Hotpot Dynamics, Machine Learning","This study introduces a novel framework that uncovers and exploits latent transmission links between urban regions, mined directly from publicly available dengue case data. It models how hotspot formation in one area is influenced by epidemic dynamics in neighboring regions, providing an interpretable explanation for citywide spread. The learned transmission links align with commuting flows, highlighting the interplay between hidden epidemic spread and human mobility.",35.71,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12879v1_Hierarchical Sparse Circuit Extraction from Billio.pdf,Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition,"Mohammed Mudassir Uddin ∗, Shahnawaz Alam, Mohammed Kaif Pasha",Not found,Not found,"Mechanistic interpretability, sparse computational graphs, circuit discovery, transformer architectures, causal inference, attribution methods, hierarchical decomposition","The proposed Hierarchical Attribution Graph Decomposition (HAGD) framework reduces circuit discovery complexity from O(2^n) exhaustive enumeration to O(n^2 log n) through multi-resolution abstraction hierarchies and differentiable circuit search. The methodology integrates cross-layer transcoders for monosemantic feature extraction, graph neural network meta-learning for topology prediction, and causal intervention protocols for validation. Empirical evaluation spans GPT-2 variants, Llama-7B through Llama-70B, and Pythia suite models across algorithmic tasks and natural language benchmarks.",38.42,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12882v1_YOLO26 An Analysis of NMS-Free End to End Framewor.pdf,YOLO26: ANALYSIS OF NMS-FREE END-TO-END FRAMEWORK FOR REAL-TIME OBJECT DETECTION,Sudip Chakrabarty,Not found,2601.12882v1,"YOLOv26, End-to-End Object Detection, NMS-Free, MuSGD, ProgLoss, Real-Time Computer Vision, You Only Look Once","This paper analyzes YOLO26, a framework that eliminates Non-Maximum Suppression (NMS) in favor of a native end-to-end learning strategy, demonstrating superior performance in inference speed and detection accuracy compared to previous versions and state-of-the-art competitors.",38.26,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12886v1_Communication Methods in Multi-Agent Reinforcement.pdf,Communication Methods in Multi-Agent Reinforcement Learning,Christoph Wittner,,2601.12886,"Machine learning, MARL, Communication","This work provides an overview of communication techniques in multi-agent reinforcement learning, evaluating the strengths and weaknesses of explicit, implicit, attention-based, graph-based, and hierarchical/role-based communication methods. It highlights the importance of communication methods with low computational overhead for scalability to environments with many interacting agents.",36.44,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12893v1_AdaNODEs Test Time Adaptation for Time Series Fore.pdf,ADANODES: TEST TIME ADAPTATION FOR TIME SERIES FORECASTING USING NEURAL ODES,"Ting Dang∗, Soumyajit Chatterjee†, Hong Jia‡, Yu Wu#, Flora Salim+, Fahim Kawsar♭",,,"test time adaptation, time series forecasting, domain adaptation, neural odes","This paper presents AdaNODEs, an innovative source-free TTA method tailored explicitly for time series forecasting. By leveraging Neural Ordinary Differential Equations (NODEs), it proposes a novel adaptation framework that accommodates the unique characteristics of distribution shifts in time series data. The method only requires updating limited model parameters, showing effectiveness in capturing temporal dependencies while avoiding significant memory usage. Extensive experiments demonstrate relative improvements of 5.88% and 28.4% over SOTA baselines, especially demonstrating robustness across higher severity distribution shifts.",36.19,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12904v1_From Prefix Cache to Fusion RAG Cache Accelerating.pdf,From Prefix Cache to Fusion RAG Cache: Accelerating LLM Inference in Retrieval-Augmented Generation,"Jiahaowang, Weiyuxie, Mingxingzhang, Boxingzhang, Jianweidong, Yueningzhu, Chenlin, Jinqitang, Yaochenhan, Zhiyuanai, Xianglince, Yongweiwu, Congfengjiang",10.1145/3786655,2601.1290,"Large Language Models, Retrieval-Augmented Generation, Cache Reuse, Inference Optimization","This paper proposes FusionRAG, a novel inference framework that optimizes both the preprocessing and reprocessing stages of Retrieval-Augmented Generation (RAG) to improve generation quality and efficiency. By embedding information from other related text chunks into each chunk and recomputing the KVCache for tokens the model focuses on, FusionRAG achieves a better trade-off between generation quality and efficiency. Experiments show that FusionRAG significantly improves generation quality compared to previous state-of-the-art solutions.",27.85,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12910v1_SciCoQA Quality Assurance for Scientific Paper--Co.pdf,SCICOQA: Quality Assurance for Scientific Paper–Code Alignment,"Tim Baumgärtner, Iryna Gurevych",,,"reproducibility, scientific paper, code alignment, machine learning, discrepancy detection","We present SCICOQA, a dataset for detecting discrepancies between scientific papers and their codebases to ensure faithful implementations. The dataset consists of 611 paper-code discrepancies, including 81 real and 530 synthetic cases, spanning various computational science disciplines. The evaluation of 21 language models highlights the difficulty of SCICOQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus.",27.72,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12912v1_Human Emotion Verification by Action Languages via.pdf,Human Emotion Verification by Action Languages via Answer Set Programming,"ANDREAS BR ¨ANNSTR ¨OM, JUAN CARLOS NIEVES",10.1017/xxxxx,2601.12912,"Action Languages, Answer Set Programming, Theory of Mind","This paper introduces the action language C-MT (Mind Transition Language) built on answer set programming and transition systems to represent human mental state evolution. It formalizes mental states, such as emotions, as multi-dimensional configurations and extends the language with causal rules to model valid transitions between mental states, enabling controlled reasoning about dynamic human mental states.",28.93,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12913v1_Actionable Interpretability Must Be Defined in Ter.pdf,Position: Actionable Interpretability Must Be Defined in Terms of Symmetries,"Pietro Barbiero, Mateo Espinosa Zarlenga, Francesco Giannini, Alberto Termine, Mateja Jamnik, Giuseppe Marra",,,"interpretability, symmetries, AI, Bayesian inversions, inference equivariance, information invariance, concept-closure invariance, structural invariance","This paper argues that interpretability research in Artificial Intelligence is fundamentally ill-posed, as existing definitions of interpretability are not actionable. It posits that for a definition of interpretability to be actionable, it must be given in terms of symmetries. The authors hypothesize that four symmetries are sufficient to motivate core interpretability properties, characterize the class of interpretable models, and derive a unified formulation of interpretable inference as a form of Bayesian inversions.",27.89,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12922v1_Your Privacy Depends on Others Collusion Vulnerabi.pdf,Collusion Vulnerabilities in Individual Differential Privacy,"Johannes Kaiser, Alexander Ziller, Eleni Triantafillou, Daniel Rückert, Georgios Kaissis",Not found,Not found,"differential privacy, individual differential privacy, collusion, excess risk, membership inference","This work reveals a previously overlooked vulnerability in sampling-based Individual Differential Privacy (iDP) mechanisms, where an individual's privacy risk is not solely governed by their own privacy budget but critically depends on the privacy choices of all other data contributors. This mismatch between the promise of individual privacy control and the reality of a system where risk is collectively determined creates an exploitable attack vector. The authors demonstrate empirically that certain distributions of privacy preferences can unintentionally inflate the privacy risk of individuals, even when their formal guarantees are met. They propose (εi, δi, ∆)-iDP, a privacy contract that uses ∆-divergences to provide users with a hard upper bound on their excess vulnerability, while offering flexibility to mechanism design.",28.39,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12925v1_ForeDiffusion Foresight-Conditioned Diffusion Poli.pdf,ForeDiffusion: Foresight-Conditioned Diffusion Policy via Future View,"Weize Xie, Yi Ding, Ying He, Leilei Wang, Binwen Bai, Zheyi Zhao, Chenyang Wang, F. Richard Yu",Not found,Not found,"Diffusion, Robot Manipulation, Foresight, Future View, Consistency Loss, Dual Loss Mechanism","This paper proposes ForeDiffusion, a diffusion-based policy that incorporates future view representations to guide the policy towards forward-looking actions, thereby correcting trajectory deviations. ForeDiffusion achieves an average success rate of 80% across various tasks, significantly outperforming existing diffusion methods by 23% in complex tasks, while maintaining more stable performance.",27.05,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12929v1_Membership Inference Test Auditing Training Data i.pdf,Membership Inference Test: Auditing Training Data in Object Classification Models,"Gonzalo Mancera, Daniel DeAlcala, Aythami Morales, Ruben Tolosana, Julian Fierrez",Not found,Not found,"Membership Inference, Training Data Auditing, Object Classification, AI Ethics","This research analyzes the performance of Membership Inference Tests (MINT) in determining whether given data were utilized during the training phase, specifically in the domain of object recognition. It proposes and develops architectures tailored for MINT models to optimize performance and efficiency in data utilization.",27.11,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12931v1_Online Continual Learning for Time Series a Natura.pdf,ONLINE CONTINUOUS LEARNING FOR TIMESERIES: A NATURAL SCORE-DRIVEN APPROACH,"Edoardo Urettini, Daniele Atzeni, Ioanna-Yvonni Tsaknaki, Antonio Carta",,1909.08730,"online continual learning, timeseries forecasting, natural gradient descent, robust optimization, replay buffer, dynamic scale heuristic","This paper explores the application of online continual learning (OCL) methods to online time series forecasting (OTSF), aiming to strengthen the theoretical and practical connections between time series methods and OCL. It introduces a robust optimization approach, Natural Score-driven Replay (NatSR), which combines a robust optimizer with a replay buffer and a dynamic scale heuristic to improve fast adaptation at regime drifts.",27.15,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12937v1_On the Evidentiary Limits of Membership Inference .pdf,On the Evidentiary Limits of Membership Inference for Copyright Auditing,"Murat Bilgehan Ertan, Emirhan Boge, Min Chen, Kaleel Mahmood, Marten van Dijk",Not found,Not found,"Copyright auditing, Membership inference attacks, Large language models, Fine-tuning, Adversarial copyright disputes","As large language models (LLMs) are trained on increasingly opaque corpora, membership inference attacks (MIAs) have been proposed to audit whether copyrighted texts were used during training. This paper asks whether MIAs can serve as admissible evidence in adversarial copyright disputes where an accused model developer may obfuscate training data while preserving semantic content. The authors introduce SAGE (Structure-Aware SAE-Guided Extraction), a paraphrasing framework guided by Sparse Autoencoders (SAEs) that rewrites training data to alter lexical structure while preserving semantic content and downstream utility. Experiments show that state-of-the-art MIAs degrade when models are fine-tuned on SAGE-generated paraphrases, indicating that their signals are not robust to semantics-preserving transformations. While some leakage remains in certain fine-tuning regimes, these results suggest that MIAs are brittle in adversarial settings and insufficient, on their own, as a standalone mechanism for copyright auditing of LLMs.",28.27,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12938v1_The Post-Turing Condition Conceptualising Artifici.pdf,The Post-Turing Condition: Conceptualising Artificial Subjectivity and Synthetic Sociality,"Thorsten Jelinek, Patrick Glauner, Alvin Wang Graylin, Yubao Qiu",,,"Artificial Intelligence, Social Coordination, Subjectivity, Synthetic Sociality, Human Participation, AI Design, Perception, Representation, Meaning, Real",This paper introduces the PRMO framework to address the structural risk of human exclusion from meaning formation in a technological horizon where artificial agents negotiate coherence and social order primarily among themselves.,29.09,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12939v1_Active Inference-Driven World Modeling for Adaptiv.pdf,ACTIVE INFERENCE-DRIVEN WORLD MODELING FOR ADAPTIVE UA V SW ARM TRAJECTORY DESIGN,"Kaleem Arshid, Ali Krayani, Lucio Marcenaro, David Martin Gomez, Carlo Regazzoni",,,"Autonomous Systems, World Model, UA V-Swarm, Probabilistic Decision-Making, Active-Inference","This paper proposes an Active Inference-based framework for autonomous trajectory design in UA V swarms, integrating probabilistic reasoning and self-learning to enable distributed mission allocation, route ordering, and motion planning. Expert trajectories generated using a Genetic Algorithm with Repulsion Forces (GA-RF) are employed to train a hierarchical World Model capturing swarm behavior across mission, route, and motion levels. During online operation, UA Vs infer actions by minimizing divergence between current beliefs and model-predicted states, enabling adaptive responses to dynamic environments. Simulation results show faster convergence, higher stability, and safer navigation than Q-Learning, demonstrating the scalability and cognitive grounding of the proposed framework for intelligent UA V swarm control.",28.58,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12946v1_AI-generated data contamination erodes pathologica.pdf,AI-generated data contamination erodes pathological variability and diagnostic reliability,"Hongyu He, Shaowen Xiang, Ye Zhang, Yingtao Zhu, Jin Zhang, Hao Deng, Emily Alsentzer, Qingyu Chen, Kun-Hsing Yu, Andrew Marmenshall, Tingting Chen, Srinivas Anumasa, Daniel Ebner, Dean Ho, Kee Yuan Ngiam, Ching-Yu Cheng, Dianbo Liu*",Not provided,Not provided,"AI, medical records, pathological variability, diagnostic reliability, synthetic content, human verification, model convergence, pneumothorax, effusions, demographic skew, false confidence, AI-generated documentation","Generative artificial intelligence (AI) is rapidly populating medical records with synthetic content, creating a feedback loop where future models are increasingly at risk of training on uncurated AI-generated data. This self-referential cycle drives a rapid erosion of pathological variability and diagnostic reliability, with models progressively converging toward generic phenotypes and failing to detect life-threatening pathology, leading to false reassurance rates tripling to 40%. ",29.93,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.12951v1_Beyond Accuracy Characterizing Code Comprehension .pdf,Beyond Accuracy: Characterizing Code Comprehension Capabilities in (Large) Language Models,"Felix Mächtle, Jan-Niclas Serr, Nils Loose, Thomas Eisenbarth",,,"Code Comprehension, Model Evaluation, Machine Learning for Software Engineering","This paper investigates whether Large Language Models' code-comprehension performance aligns with traditional human-centric software metrics or reflects distinct, non-human regularities. Using a large-scale dataset, it correlates model performance with traditional complexity metrics and finds minimal correlation between human-defined metrics and LLM success. Shadow models achieve substantially higher predictive performance, capturing complex, partially predictable patterns beyond traditional software measures.",27.42,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13007v1_ArchAgent Scalable Legacy Software Architecture Re.pdf,ARCHAGENT: SCALABLE LEGACY SOFTW ARE ARCHITECTURE RECOVERY WITH LLMS,"Rusheng Pan★†, Bingcheng Mao★, Tianyi Ma★, Zhenhua Ling †",Not found,Not found,"Software architecture recovery, code repository, cross-repository context, large language models","Recovering accurate architecture from large-scale legacy software is hindered by architectural drift, missing relations, and the limited context of Large Language Models (LLMs). ArchAgent, a scalable agent-based framework, combines static analysis, adaptive code segmentation, and LLM-powered synthesis to reconstruct multiview, business-aligned architectures from cross-repository codebases.",27.57,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13013v1_HT-GNN Hyper-Temporal Graph Neural Network for Cus.pdf,HT-GNN: Hyper-Temporal Graph Neural Network for Customer Lifetime Value Prediction in Baidu Ads,"Xiaohui Zhao, Xinjian Zhao, Jiahui Zhang, Guoyu Liu, Houzhi Wang, Shu Wu",Not found,Not found,"Lifetime Value Prediction, Advertising Platform, Graph Neural Network, Temporal Dynamics, Customer Heterogeneity","Lifetime value (LTV) prediction is crucial for news feed advertising, enabling platforms to optimize bidding and budget allocation for long-term revenue growth. However, it faces two major challenges: demographic-based targeting creates segment-specific LTV distributions with large value variations across user groups, and dynamic marketing strategies generate irregular behavioral sequences where engagement patterns evolve rapidly. We propose a Hyper-Temporal Graph Neural Network (HT-GNN), which jointly models demographic heterogeneity and temporal dynamics through three key components: a hypergraph-supervised module capturing inter-segment relationships, a transformer-based temporal encoder with adaptive weighting, and a task-adaptive mixture-of-experts with dynamic prediction towers for multi-horizon LTV forecasting. Experiments on Baidu Adswith 15 million users demonstrate that HT-GNN consistently outperforms state-of-the-art methods across all metrics and prediction horizons.",28.54,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13018v1_Bi-Attention HateXplain  Taking into account the s.pdf,Bi-Attention HateXplain : Taking into account the sequential aspect of data during explainability in a multi-task context,Ghislain Dorian Tchuente Mondjo,,,"Multitask learning, Deep Learning, Hate speech, Explainability, Bi-Attention","To improve the reliability of black-box models used for hate speech detection, post-hoc approaches such as LIME, SHAP, and LRP provide the explanation after training the classification model. In contrast, multi-task approaches based on the HateXplain benchmark learn to explain and classify simultaneously. However, results from HateXplain-based algorithms show that predicted attention varies considerably when it should be constant. This attention variability can lead to inconsistent interpretations, instability of predictions, and learning difficulties. To solve this problem, we propose the BiAtt-BiRNN-HateXplain (Bidirectional Attention BiRNN HateXplain) model which is easier to explain compared to LLMs which are more complex in view of the need for transparency, and will take into account the sequential aspect of the input data during explainability thanks to a BiRNN layer. Thus, if the explanation is correctly estimated, thanks to multi-task learning (explainability and classification task), the model could classify better and commit fewer unintentional bias errors related to communities.",28.89,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13020v1_PASs-MoE Mitigating Misaligned Co-drift among Rout.pdf,PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway Activation Subspaces for Continual Learning,"Zhiyan Hou, Haiyun Guo, Haokai Ma, Yandu Sun, Yonghui Yang, Jinqiao Wang",,2309.14747,"Continual Learning, Multi-modal Language Models, Misaligned Co-drift, Pathway Activation Subspaces, LoRA","This paper introduces PASs, a LoRA-induced subspace that reflects which low-rank pathway directions an input activates in each expert, to address the Misaligned Co-drift issue in existing LoRA-based Mixture-of-Experts (MoE) methods. The authors propose a PASs-based MoE-LoRA method with two components: PAS-guided Reweighting and PAS-aware Rank Stabilization, which are designed to improve accuracy and anti-forgetting performance without adding parameters.",27.24,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13048v1_Analysis of Long Range Dependency Understanding in.pdf,ANALYSIS OF LONG RANGE DEPENDENCY UNDERSTANDING IN STATE SPACE MODELS,"Srividya Ravikumar, Abhinav Anand, Shweta V erma, Mira Mezini",Not provided,Not provided,"Structured state-space models, interpretability, vulnerability detection","This work presents the first systematic kernel interpretability study of the diagonalized state-space model (S4D) trained on a real-world task (vulnerability detection in source code). Through time and frequency domain analysis of the S4D kernel, the authors show that the long-range modeling capability of S4D varies significantly under different model architectures, affecting model performance. Insights from this analysis can guide future work in designing better S4D-based models.",28.04,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13054v1_TinyML-Enabled IoT for Sustainable Precision Irrig.pdf,TinyML-Enabled IoT for Sustainable Precision Irrigation,"Kamogelo Taueatsoala1, Caitlyn Daniels1, Angelina J. Ramsunar1, Petrus Bronkhorst2, Absalom E. Ezugwu1*",,,"TinyML, edge computing, Internet of Things, precision agriculture, smart irrigation, sustainable water management, embedded machine learning, resource-constrained systems","This paper presents a novel, edge-first IoT framework that integrates Tiny Machine Learning (TinyML) for intelligent, offline-capable precision irrigation. The system utilizes capacitive soil moisture, temperature, humidity, pH, and ambient light sensors for environmental monitoring and predicts irrigation needs with exceptional accuracy. A Gradient Boosting model is identified as superior, achieving an R2 score of 0.9973 and a MAPE of 0.99%, outperforming a Random Forest model. The optimized model is deployed as a lightweight TinyML inference engine on the ESP32, facilitating local communication via an MQTT-based LAN protocol. Experimental validation demonstrates significant water usage reduction compared to traditional methods, confirming the system's viability for sustainable, scalable deployment in resource-constrained rural settings.",28.78,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13060v1_MagicGUI-RMS A Multi-Agent Reward Model System for.pdf,MAGICGUI-RMS: A MULTI-AGENTREWARDMODELSYSTEM FORSELF-EVOLVINGGUI AGENTS VIAAUTOMATEDFEEDBACKREFLUX,"Zecheng Li∗, Zhihui Cao ∗ †, Wenke Huang, Yudong Zhang, Keying Qi, Rui Wang, Zeyu Zheng, Jian Zhao, Hao Zhu, Hengxin Wu, Yuran Wang, Guitao Fan, Guokun Wu, Yicong Liu, Zhilin Gao, Haikun Xu, He Yang, Minqi Xiang, Xingyu Liu †, Zuojian Wang †",Not found,Not found,"GUI agents, multi-agent reward model, automated feedback, self-evolving learning, reward-based adaptation","Graphical user interface (GUI) agents are rapidly evolving towards autonomous interaction and reliable task execution. MagicGUI-RMS presents a multi-agent reward model system that delivers adaptive trajectory evaluation, corrective feedback, and self-evolving learning capabilities. It integrates a Domain-Specific Reward Model (DS-RM) with a General-Purpose Reward Model (GP-RM) to enable fine-grained action assessment and robust generalization across heterogeneous GUI tasks. The system supports reward learning at scale through a structured data construction pipeline that automatically produces balanced and diverse reward datasets, reducing annotation costs while maintaining sample fidelity. During execution, the reward model system identifies erroneous actions, proposes refined alternatives, and continuously enhances agent behavior through an automated data-reflux mechanism. Extensive experiments demonstrate substantial gains in task accuracy and behavioral robustness.",29.31,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13075v1_METIS Mentoring Engine for Thoughtful Inquiry  Sol.pdf,METIS: Mentoring Engine for Thoughtful Inquiry & Solutions,"Abhinav Rajeev Kumar, Dhruv Trehan, Paras Chopra",Not found,Not found,"AI mentor, research mentorship, undergraduate, publishable paper, large language models, tool-augmented assistant, stage-aware evaluation, literature search, methodology checks, memory","This paper presents METIS, an AI research mentor designed to guide undergraduates from initial ideas to publishable conference papers. It evaluates METIS against GPT-5 and Claude Sonnet 4.5, finding that METIS is preferred in single-turn judgments and multi-turn tutoring sessions, with higher student scores across stages. METIS uses conference instructions, research guides, and literature to ground its advice and keep context across sessions. The paper contributes a practical mentoring workflow, a simple system with tools for literature search, guidelines retrieval, methodology checks, and memory, and an empirical comparison to GPT-5 and Claude Sonnet 4.5.",27.82,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13111v1_CORE-T COherent REtrieval of Tables for Text-to-SQ.pdf,CORE-T: COherent REtrieval of Tables for Text-to-SQL,"Hassan Soliman1, Vivek Gupta2, Dan Roth3, Iryna Gurevych1",Not found,2601.13111,"Text-to-SQL, Table Retrieval, Open-Book, Heterogeneous Tables, LLM-generated Metadata, Join-Aware Retrieval","CORE-T proposes a scalable, training-free framework that enriches tables with LLM-generated purpose metadata and pre-computes a lightweight table-compatibility cache. At inference time, it returns top-K candidates, selects a coherent, joinable subset with a single LLM call, and adjusts strongly compatible tables. CORE-T improves table-selection F1 by up to 22.7 points and reduces table retrieval by up to 42% while improving multi-table execution accuracy.",27.56,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13114v1_IntAgent NWDAF-Based Intent LLM Agent Towards Adva.pdf,NWDAF-Based Intent LLM Agent,"Abdelrahman Soliman, Ahmed Refaey, Aiman Erbad, Amr Mohamed",,,"Intent-based networks, Large Language Models, Network Data Analytics Function, Next Generation Networks, Artificial Intelligence, Machine Learning, Core Network, Closed Loop","IntAgent is an intelligent intent LLM agent that integrates NWDAF analytics and tools to fulfill the network operator's intents. It offers an enriched, 3GPP-compliant data source and an MCP tools server for scheduling, monitoring, and analytics tools, demonstrating the efficacy of its framework through practical use cases.",26.88,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13122v1_Responsible AI for General-Purpose Systems Overvie.pdf,"Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward","Gourab K. Patro, Himanshi Agrawal, Himanshu Gharat, Supriya Panigrahi, Nim Sherpa, Vishal Vaddina, Dagnachew Birru",Not found,2601.13122,"responsible AI, general-purpose AI, hallucinations, toxicity, stereotypes, Degree of Freedom, AI alignment, retrieval-augmented generation, reasoning enhancements","This paper reviews the risks and vulnerabilities of modern general-purpose AI systems, which are capable of performing a range of tasks like writing text articles, generating and debugging codes, querying databases, and translating languages. It discusses the challenges and proposes C 2V2 desiderata (Control, Consistency, Value, Veracity) to meet responsible AI requirements for future general-purpose AI systems.",28.72,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13142v1_TVWorld Foundations for Remote-Control TV Agents.pdf,TVWorld: Foundations for Remote-Control TV Agents,"Zhantao Ma1*, Quanfeng Lu1*, Shuai Zhong1, Dahai Yu3, Ping Luo1, Michael K. Ng 2†",,,"Large Vision-Language Models, Remote-Control Interaction, TV Navigation, Graph-Based Abstraction, Focus-Awareness, Topology-Awareness","Recent large vision-language models have demonstrated strong potential for device control. However, existing research has primarily focused on point-and-click interaction, while remote-control interaction commonly encountered in everyday TV usage remains largely underexplored. To fill this gap, we introduce TVWorld, an offline graph-based abstraction of real-world TV navigation that enables reproducible and deployment-free evaluation. On this basis, we derive two complementary benchmarks that comprehensively assess TV-use capabilities: TVWorld-N for topology-aware navigation and TVWorld-G for focus-aware grounding. These benchmarks expose a key limitation of existing agents: insufficient topology awareness for focus-based, long-horizon TV navigation. Motivated by this finding, we propose a Topology-Aware Training framework that injects topology awareness into LVLMs. Using this framework, we develop TVTheseus, a foundation model specialized for TV navigation. TVTheseus achieves a success rate of 68.3% on TVWorld-N, surpassing strong closed-source baselines such as Gemini 3 Flash and establishing state-of-the-art (SOTA) performance.",29.08,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13160v1_Training instability in deep learning follows low-.pdf,Training instability in deep learning follows low-dimensional dynamical principles,"Zhipeng Zhang, Zhenjie Yao, Kai Li, Lei Yang",10.13160/v1.2601.13160,2601.13160,"training instability, deep learning, reinforcement learning, large language models, dynamical systems","Deep learning systems achieve remarkable empirical performance, yet the stability of the training process itself remains poorly understood. Training unfolds as a high-dimensional dynamical system in which small perturbations can induce abrupt and irreversible collapse, undermining reproducibility and scalability.",28.72,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13166v1_From 100000 images to winning the first brain MRI .pdf,"From 100,000+ images to winning the first brain MRI foundation model challenges","Pedro  M.  Gordaliza, Jaume  Banus, Benoît  Gérin, Maxence  Wynen, Nataliia  Molchanova, Jonas  Richiardi, Meritxell  Bach  Cuadra",,,"medical image analysis, foundation models, 3D brain MRI, self-supervised learning, brain pathology, neuroimaging","Developing Foundation Models for medical image analysis is essential to overcome the unique challenges of radiological tasks. Our solution ranked first in tracks of both contests, relying on a U-Net CNN architecture combined with strategies leveraging anatomical priors and neuroimaging domain knowledge. Notably, our models trained 1-2 orders of magnitude faster and were 10× smaller than competing transformer-based approaches. Models are available here.",26.78,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13186v1_Prompt Injection Mitigation with Agentic AI Nested.pdf,"Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching","Diego Gosmar, Deborah A. Dahl",10.48550/arXiv.2601.13186,2601.13186,"Prompt Injection, Large Language Models, Multi-Agent Systems, Semantic Caching, Nested Learning, AI Sustainability","This paper extends the evaluation framework for prompt injection mitigation with semantic similarity-based caching, a dedicated fourth-agent rule-based evaluator, and a fifth metric (Observability Score Ratio) to yield TIVS-O, investigating how defence effectiveness interacts with transparency in a HOPE-inspired Nested Learning architecture.",28.51,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13187v1_Scientific production in the era of Large Language.pdf,Scientific production in the era of Large Language Models,"Keigo Kusumegi, Xinyu Yang, Paul Ginsparg, Mathijs de Vaan, Toby Stuart, Yian Yin",10.1126/science.adw3000,,"Large Language Models, Scientific research, Paper production, Writing complexity, Scientific evaluation","Large Language Models are reshaping scientific research, leading to increased paper production and changes in manuscript quality and citation patterns.",28.82,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13197v1_Diffusion-Driven Synthetic Tabular Data Generation.pdf,Diffusion-Driven Synthetic Tabular Data Generation for Enhanced DoS/DDoS Attack Classification,"Aravind B, Anirud R.S., Sai Surya Teja N, Bala Subrahmanya Sriranga Navaneeth A, Karthika R, Mohankumar N",,,"Diffusion models, Tabular data, Synthetic data generation, Class imbalance, DoS/DDoS attacks, Network intrusion detection","This paper addresses class imbalance in network intrusion detection using Tabular Denoising Diffusion Probability Models (TabDDPM) for data augmentation. It synthesizes high-fidelity minority-class samples from the CIC-IDS2017 dataset through iterative denoising processes, enabling an ANN classifier to achieve near-perfect recall on previously underrepresented attack classes.",27.52,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13206v1_Real-Time Deadlines Reveal Temporal Awareness Fail.pdf,Real-Time Deadlines Reveal Temporal Awareness Failures in LLM Strategic Dialogues,"Neil Sehgal, Sharath Chandra Guntuku, Lyle Ungar",,,"Large Language Models, Temporal Awareness, Strategic Reasoning, Negotiation, Real-Time Constraints","Large Language Models struggle to track elapsed time under real-time deadlines, leading to suboptimal behavior in strategic dialogues. This study investigates how LLMs adjust their behavior in time-sensitive settings, revealing a systematic lack of temporal awareness that will constrain LLM deployment in many time-sensitive applications.",26.53,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13217v1_Beyond Single-shot Writing Deep Research Agents ar.pdf,Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn Report Revision,"Bingsen Chen1,2,∗, Boyan Li3,†, Ping Nie5, Yuyu Zhang6, Xi Ye3,4, Chen Zhao1,2,‡",10.48550/arxiv.2601.13217,2601.13217,"Deep Research Agents, Multi-turn Report Revision, Human Research Practice, Report Generation, Self-reflection, Peer Feedback","This paper introduces MRDRE, an evaluation suite for Deep Research Agents (DRAs) that establishes multi-turn report revision as a new evaluation axis. It analyzes five diverse DRAs and reveals a critical limitation: while agents can address most user feedback, they also regress on 16–27% of previously covered content and citation quality. Over multiple revision turns, even the best-performing agents leave significant headroom, as they continue to disrupt content outside the feedback's scope and fail to preserve earlier edits.",28.08,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13222v1_Incorporating QA Nuggets into Retrieval-Augmented .pdf,Incorporating Q&A Nuggets into Retrieval-Augmented Generation,"Laura Dietz1, Bryan Li2, Gabrielle Liu 3, Jia-Huei Ju 4, Eugene Yang5, Dawn Lawrie5, William Walden5, James Mayfield 5",Not found,2601.13222,"RAG, LLM judge, nugget-based evaluation","This paper presents Crucible, a Nugget-Augmented Generation System that integrates ideas from automatic evaluation into Retrieval-augmented Generation. Crucible constructs a bank of Q&A nuggets from retrieved documents to guide extraction, selection, and report generation, avoiding repeated information through clear and interpretable Q&A semantics and maintaining citation provenance throughout the generation process. Evaluated on the TREC Neu-CLIR 2024 collection, Crucible substantially outperforms Ginger, a recent nugget-based RAG system, in nugget recall, density, and citation grounding.",28.68,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13227v1_Insider Knowledge How Much Can RAG Systems Gain fr.pdf,Insider Knowledge: How Much Can RAG Systems Gain from Evaluation Secrets?,"Laura Dietz1 , Bryan Li 2 , Eugene Yang3 , Dawn Lawrie3 , William Walden3 , James Mayfield 3",,2601.13227v1,"Retrieval-augmented generation, LLM judge, Nugget evaluation","This paper investigates the risk of faulty measurements in RAG systems due to the integration of LLM judges in evaluation frameworks. By comparing nugget-based RAG systems like Ginger and Crucible against strong baselines such as GptResearcher, the authors demonstrate that near-perfect evaluation scores can be achieved when elements of the evaluation are leaked or can be predicted. The results highlight the importance of blind evaluation settings and methodological diversity to guard against mistaking metric overfitting for genuine system progress.",29.0,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13228v1_Autoregressive Models Rival Diffusion Models at AN.pdf,AUTOREGRESSIVEMODELSRIVAL DIFFUSIONMODELS ATANY-ORDER GENERATION,"Tianqi Du, Lizhe Fang, Weijie Yang, Chenheng Zhang, Zeming Wei, Yifei Wang, Yisen Wang",Not found,Not found,"Diffusion models, Autoregressive models, Language models, Text generation, Any-order generation, Bidirectional conditioning, Infilling, Rewriting, Self-correction",This paper discusses the limitations of diffusion language models and proposes Any-order Any-subset Autoregressive (A3) modeling as a generalized framework that extends the standard AR factorization to arbitrary token groups and generation orders. A3 preserves the probabilistic rigor and multi-layer dependency modeling of AR while inheriting diffusion models' flexibility for parallel and bidirectional generation. Experiments demonstrate that A3 outperforms diffusion-based models while maintaining flexible decoding.,28.15,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13233v1_RAG A Random-Forest-Based Generative Design Framew.pdf,RAG: A Random-Forest-Based Generative Design Framework for Uncertainty-Aware Design of Metamaterials with Complex Functional Requirements,"Bolin Chen, Dex Doksoo Lee, Wei “Wayne” Chen, Wei Chen",Not found,2601.13233,"Random forest, Generative design, Functional response, Uncertainty quantification","This paper introduces a RAndom-forest-based Generative approach (RAG) to address the challenges of inverse design for metamaterials with complex functional requirements, which are often high-dimensional and involve non-existence or non-uniqueness of feasible solutions. RAG leverages the small-data compatibility of random forests and reformulates the forward mapping in a discretization-invariant way to enable data-efficient predictions of high-dimensional functional responses during inverse design.",28.12,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13235v1_RubRIX Rubric-Driven Risk Mitigation in Caregiver-.pdf,RubRIX: Rubric-Driven Risk Mitigation in Caregiver-AI Interactions,"Drishti Goel1, Jeongah Lee2, Qiuyue Joy Zhong2, Violeta J. Rodriguez1, Daniel S. Brown3, Ravi Karkar 2, Dong Whi Yoo 4, Koustuv Saha 1",,,"Caregiving, AI, Risk Mitigation, Ethics of Care, Large Language Models, Rubric-based Risk Index","This paper introduces RubRIX, a theory-driven, clinician-validated framework for evaluating risks in Large Language Model (LLM) caregiving responses. Grounded in the Elements of an Ethic of Care, RubRIX operationalizes five empirically-derived risk dimensions: Attention, Bias & Stigma, Information Inaccuracy, Uncritical Affirmation, and Epistemic Arrogance. The authors evaluate six state-of-the-art LLMs on over 20,000 caregiver queries from Reddit and ALZConnected, demonstrating that Rubric-guided refinement consistently reduces risk-components by 45-98% after one iteration across models. This work contributes a methodological approach for developing domain-sensitive, user-centered evaluation frameworks for high-burden contexts, highlighting the importance of domain-sensitive, interactional risk evaluation for the responsible deployment of LLMs in caregiving support contexts.",28.75,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13236v1_Pixelwise Uncertainty Quantification of Accelerate.pdf,Pixelwise Uncertainty Quantiﬁcation of Accelerated MRI Reconstruction,"Ilias I. Giannakopoulos, Lokesh B Gautham Muthukumar, Yvonne W. Lui, Riccardo Lattanzi",Not found,Not found,"Conformal Prediction, Magnetic Resonance Imaging, Parallel Imaging, Quantile Regression, Uncertainty Quantification","This work introduces a general framework for pixel-wise uncertainty quantification in parallel MRI reconstructions, enabling automatic identification of unreliable regions without access to any ground-truth reference image. The method integrates conformal quantile regression with image reconstruction methods to estimate statistically rigorous pixel-wise uncertainty intervals. Quantitative experiments demonstrate strong agreement between predicted uncertainty maps and true reconstruction error. The proposed framework enables evaluation of reconstruction quality without access to fully-sampled ground-truth reference images, representing a step toward adaptive MRI acquisition protocols.",27.64,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13238v1_A Semantic Decoupling-Based Two-Stage Rainy-Day At.pdf,A Semantic Decoupling–Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision–Language Models,"Chengyin Hu, Xiang Chen, Zhe Jia, Weiwen Shi, Fengyu Zhang, Jiujiang Guo, Yiwei Wei",,,"Vision-Language Models, Rainy-Day Attack, Weather Robustness, Semantic Decoupling, Adversarial Framework","This paper introduces a novel adversarial framework to analyze the robustness of Vision-Language Models (VLMs) under real-world weather conditions, specifically focusing on rainy scenarios. The framework consists of two stages: Stage 1 models the global effects of rainfall by applying a low-dimensional global modulation to condition the embedding space and gradually weaken the original semantic decision boundaries. Stage 2 introduces structured rain variations by explicitly modeling multi-scale raindrop appearance and rainfall-induced illumination changes, optimizing the resulting non-differentiable weather space to induce stable semantic shifts. Experiments across multiple tasks demonstrate that even physically plausible, highly constrained weather perturbations can induce substantial semantic misalignment in mainstream VLMs, posing potential safety and reliability risks in real-world deployment.",28.11,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13240v1_KOCO-BENCH Can Large Language Models Leverage Doma.pdf,KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?,"Xue Jiang, Jiaru Qian, Xianjie Shi, Chenjie Li, Hao Zhu, Ziyu Wang, Jielun Zhang, Zheyu Zhao, Kechi Zhang, Jia Li, Wenpin Jiao, Zhi Jin, Ge Li, Yihong Dong",,,"Large Language Models, Domain Knowledge, Software Development, Benchmark, Domain Specialization","This paper presents KOCO-BENCH, a novel benchmark designed to evaluate domain specialization methods in real-world software development. KOCO-BENCH includes 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora and multi-granularity evaluation tasks. The authors highlight the significant challenges posed by KOCO-BENCH to state-of-the-art LLMs and the urgent need for more effective domain specialization methods.",27.92,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13247v1_Aligning Agentic World Models via Knowledgeable Ex.pdf,Aligning Agentic World Models via Knowledgeable Experience Learning,"Baochang Ren, Yunzhi Yao, Rui Sun, Shuofei Qiao, Ningyu Zhang, Huajun Chen",Not found,2601.13247,"Large Language Models, World Models, Embodied Intelligence, Physical Grounding, Process Experience, Goal Experience","Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack procedural grounding to respect the immutable laws of the physical world. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which struggle to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. WorldMind introduces a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback, unifying Process Experience and Goal Experience to achieve superior performance and cross-model, cross-environment transferability.",28.19,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13260v1_Stop Taking Tokenizers for Granted They Are Core D.pdf,Stop Taking Tokenizers for Granted: They Are Core Design Decisions in Large Language Models,"Sawsan Alqahtani†, Mir Tafseer Nayeem♣*, Md Tahmid Rahman Laskar♦, Tasnim Mohiuddin♢, M Saiful Bari♥",Not found,Not found,"Large Language Models, Tokenization, Subword Tokenization, Byte Pair Encoding, Model Design, Fairness, Efficiency, Adaptability","This paper reframes tokenization as a core modeling decision rather than a pre-processing step, arguing for a context-aware framework that integrates tokenizer and model co-design, guided by linguistic, domain, and deployment considerations. It highlights the limitations of common subword approaches such as Byte Pair Encoding and calls for standardized evaluation and transparent reporting to make tokenization choices accountable and comparable.",27.6,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13262v1_CURE-Med Curriculum-Informed Reinforcement Learnin.pdf,CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning,"Eric Onyame∗, Akash Ghosh∗, Subhadip Baidya, Sriparna Saha, Xiuying Chen, Chirag Agarwal",Not found,Not found,"Curriculum-Informed Reinforcement Learning, Multilingual Medical Reasoning, Code-Switching, Group Relative Policy Optimization, Language Consistency, Logical Correctness","This work introduces CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries. It proposes CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to improve logical correctness and language stability across thirteen languages.",28.04,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13268v1_Improving the Safety and Trustworthiness of Medica.pdf,Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation Loops,"Zainab Ghafoor, Md Shafiqul Islam, Koushik Howlader, Md Rasel Khondokar, Tanusree Bhattacharjee, Sayantan Chakraborty, Adrito Roy, Ushashi Bhattacharjee, Tirtho Roy",Not found,Not found,"Medical AI, Large Language Models, Multi-Agent Systems, Ethical Compliance, Safety Assessment","This work introduces a multi-agent refinement framework designed to enhance the safety and reliability of medical Large Language Models (LLMs) through structured, iterative alignment. The system combines two generative models—DeepSeek R1 and Med-PaLM—with two evaluation agents, LLaMA 3.1 and Phi-4, which assess responses using the American Medical Association’s (AMA) Principles of Medical Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol. The study evaluates performance across 900 clinically diverse queries spanning nine ethical domains, measuring convergence efficiency, ethical violation reduction, and domain-specific risk behavior. Results demonstrate that DeepSeek R1 achieves faster convergence (mean 2.34 vs. 2.67 iterations), while Med-PaLM shows superior handling of privacy-sensitive scenarios. The iterative multi-agent loop achieved an 89% reduction in ethical violations and a 92% risk downgrade rate, underscoring the effectiveness of the approach.",28.93,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13295v1_CooperBench Why Coding Agents Cannot be Your Teamm.pdf,CooperBench: Why Coding Agents Cannot be Your Teammates Yet,"CooperBench, Arpandeep Khatua, Hao Zhu, Peter Tran, Arya Prabhudesai, Frederic Sadrieh, Johann K. Lieberwirth, Xinkai Yu, Yicheng Fu, Michael J. Ryan, Jiaxin Pei, Diyi Yang",2026-1-21,CooperBench,"CooperBench, coding agents, teamwork, coordination, conflicts, compatibility, open-source, benchmark","CooperBench evaluates over 600 collaborative coding tasks across 12 libraries in 4 programming languages, revealing that AI agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. The study identifies three key issues: communication jams, deviations from commitments, and incorrect expectations about others' plans, observations, and communication.",26.92,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13317v1_Paid Voices vs. Public Feeds Interpretable Cross-P.pdf,Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme Modeling of Climate Discourse,"Samantha Sudhoff*, Pranav Perumal, Zhaoqing Wu, Tunazzina Islam*",,,"Climate communication, Paid advertising, Public social media, Thematic analysis, Large language models","Climate discourse online plays a crucial role in shaping public understanding of climate change and influencing political and policy outcomes. This work presents a comparative analysis of climate discourse across paid advertisements on Meta (previously known as Facebook) and public posts on Bluesky from July 2024 to September 2025, using an interpretable, end-to-end thematic discovery and assignment framework. The framework clusters texts by semantic similarity and leverages large language models to generate concise, human-interpretable theme labels. The quality of induced themes is evaluated using both human judgments and an LLM-based evaluator, and their semantic coherence is validated through downstream stance prediction and theme-guided retrieval tasks. The findings show that platform-level incentives are reflected in the thematic structure, stance alignment, and temporal responsiveness of climate narratives.",28.6,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13327v1_PepEDiff Zero-Shot Peptide Binder Design via Prote.pdf,PepEDiﬀ: Zero-Shot Peptide Binder Design via Protein Embedding Diffusion,"Po-Yu Liang, Tibo Duran, Jun Bai",,2601.13327,"Deep Learning, Drug Discovery, Protein Design","A novel peptide binder generator that designs binding sequences given a target receptor protein sequence and its pocket residues, improving structural and sequence diversity by generating sequences directly in a continuous latent space derived from a pretrained protein embedding model.",28.49,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13348v1_The AI Genie Phenomenon and Three Types of AI Chat.pdf,"The AI Genie Phenomenon and Three Types of AI Chatbot Addiction: Escapist Roleplays, Pseudosocial Companions, and Epistemic Rabbit Holes","M. Karen Shen, Jessica Huang, Olivia Liang, IG-JAE KIM, DONGWOOK YOON",Not found,Not found,"AI chatbot, addiction, addictive use, technology addiction, escapist roleplay, pseudosocial companion, epistemic rabbit hole","Recent reports on generative AI chatbot use raise concerns about its addictive potential. This study examines how to characterize AI chatbot addiction—why users become addicted, the symptoms commonly reported, and the distinct types it comprises.",27.77,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13352v1_LLM-as-RNN A Recurrent Language Model for Memory U.pdf,LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction,"Yuxing Lu*12, J. Ben Tamo*1, Weichen Zhao3, Nan Sun4, Yishan Zhong1, Wenqi Shi5, Jinzhuo Wang†2, May D. Wang†1",,,"Large language models, Recurrent Neural Networks, Long Short-Term Memory, Transformer architectures, In-Context Learning, Memory updates, Sequence prediction","This paper proposes LLM-as-RNN, an inference-only framework that turns a frozen large language model into a recurrent predictor by representing its hidden state as natural-language memory. This state is updated at each timestep via feedback-driven text rewrites, enabling learning without parameter updates. The method significantly outperforms zero-shot, full-history, and MemPrompt baselines on sequential benchmarks in healthcare, meteorology, and finance across different model families.",27.9,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13358v1_The Geometry of Thought How Scale Restructures Rea.pdf,The Geometry of Thought: How Scale Restructures Reasoning in Large Language Models,Samuel Cyrenius Anderson,Not found,2601.13358,"Large Language Models, Neural Scaling Laws, Reasoning Tasks, Chain-of-Thought, Model Parameters, Parameter Increase, Geometric Reorganizations, Domain-Specific Effects","This paper investigates how scale impacts reasoning in large language models, revealing that scale restructures reasoning rather than uniformly improving it. It analyzes 25,000+ chain-of-thought trajectories across four domains (Law, Science, Code, Math) and two scales (8B, 70B parameters), discovering that neural scaling laws trigger domain-specific phase transitions rather than uniform capability gains. The findings suggest that the geometry of thought determines the cost of thought, offering a blueprint for inference acceleration.",28.2,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13376v1_Bounded Minds Generative Machines Envisioning Conv.pdf,"Bounded Minds, Generative Machines",JIQUN LIU,XXXXXXX,,"Bounded Rationality, Heuristics, Conversational AI, GenAI, Evaluation","This article outlines a research pathway grounded in bounded rationality, arguing that conversational AI should be designed to work with human heuristics rather than against them, and identifies key directions for detecting cognitive vulnerability, supporting judgment under uncertainty, and evaluating conversational systems beyond factual accuracy, toward decision quality and cognitive robustness.",26.24,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13383v1_A Lightweight Modular Framework for Constructing A.pdf,"A LIGHTWEIGHTMODULARFRAMEWORK FORCONSTRUCTING AUTONOMOUSAGENTSDRIVEN BYLARGELANGUAGE MODELS: DESIGN, IMPLEMENTATION,ANDAPPLICATIONS IN AGENTFORGE","A. A. Jafari, C. Ozcinar, G. Anbarjafari",Not found,2601.13383,"autonomous agents, large language models, modular architecture, natural language processing, software framework, task automation, artificial intelligence, open-source software","The paper presents AgentForge, a lightweight, open-source Python framework designed to democratize the construction of LLM-driven autonomous agents through a principled modular architecture. It introduces three key innovations: a composable skill abstraction, a unified LLM backend interface, and a declarative YAML-based configuration system. Comprehensive experimental evaluation demonstrates competitive task completion rates and reduced development time compared to existing frameworks.",28.35,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13385v1_Organ-Aware Attention Improves CT Triage and Class.pdf,Organ-Aware Attention Improves CT Triage and Classification,"Lavsen Dahal, Yubraj Bhandari, Geoffrey D. Rubin, Joseph Y. Lo",,,"computed tomography, CT triage, classification, organ-aware attention, Vision-Language Models, 3D anatomy, radiologist burnout","This study presents ORACLE-CT, an encoder-agnostic, organ-aware head that pairs Organ-Masked Attention with Organ-Scalar Fusion. It achieves state-of-the-art supervised classification performance across chest and abdomen CT under a unified evaluation protocol.",26.0,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13392v1_Beyond Memorization Testing LLM Reasoning on Unsee.pdf,Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation Tasks,"Shlok Shelat, Jay Raval, Souvik Roy, Manas Gaur",,,"Large language models, Formal language tasks, Deterministic finite automata, Pattern matching, Symbolic reasoning, Theory of Computation","Large language models have demonstrated strong performance on formal language tasks, yet whether this reflects genuine symbolic reasoning or pattern matching on familiar constructions remains unclear. We introduce a benchmark for DFA construction from regular languages, comprising factual knowledge questions, seen construction problems, and unseen problems. Models achieve perfect accuracy on factual questions and 84-90% on seen tasks, but accuracy drops sharply on unseen problems, with failures stemming from systematic misinterpretation of language constraints, incorrect handling of Kleene-star semantics, and a failure to preserve global consistency. We evaluate a three-stage hint protocol that enables correction of shallow errors but does not reliably resolve globally inconsistent or structurally flawed automata. Our analysis across multiple prompting strategies reveals that errors persist regardless of prompting approach, exposing a fundamental gap between LLMs' ability to generate syntactically plausible DFAs and their capacity for semantically correct formal reasoning.",28.2,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13398v1_Can LLMs Compress and Decompress Evaluating Code U.pdf,Evaluating Code Understanding and Execution via Invertibility,"Nickil Maveli, Antonio Vergari, Shay B. Cohen",,,"Code-LLMs, round-trip consistency, inversion, LLMs performance, code reasoning","Recent progress in Code-LLMs has demonstrated remarkable performance across various software engineering applications. However, evaluating the reasoning ability of Code-LLMs requires going beyond isolated input–output predictions. This paper presents ROUNDTRIPCODEEVAL (RTCE), a comprehensive benchmark consisting of four distinct code execution reasoning tasks designed to rigorously test round-trip consistency. RTCE provides an execution-free, exact-match evaluation of bijection fidelity, assessing whether models preserve a consistent one-to-one mapping between encoding and decoding operations across various algorithms and directions. The authors systematically evaluate state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms. Each method yields modest improvements, but none closes the gap, indicating that current LLMs struggle with true round-trip consistency, which demonstrates that they lack the internal coherence required for trustworthy code reasoning. RTCE surfaces several new and previously unmeasured insights that are not captured by existing benchmarks.",28.35,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13400v1_Deep Image Prior with L0 Gradient Regularizer for .pdf,DEEP IMAGE PRIOR WITH L0 GRADIENT REGULARIZER FOR IMAGE SMOOTHING,"Nhat Thanh Tran, Kevin Bui, Jack Xin",Not found,Not found,"image smoothing, optimization, ADMM, deep image prior, ℓ 0 gradient","Proposes DIP-ℓ 0, a deep image prior framework that incorporates the ℓ 0 gradient regularizer for high-quality image smoothing without requiring a training dataset. Utilizes an alternating direction method of multipliers algorithm for nonconvex, nonsmooth ℓ 0 minimization. Demonstrates superior performance in edge-preserving image smoothing and JPEG artifact removal compared to many image smoothing algorithms.",27.42,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13401v1_Reasoning with Pixel-level Precision QVLM Architec.pdf,Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset,"Peter A. Massih1,2 & Eric Cosatto1",,,"Vision-Language Models, Quantitative Spatial Reasoning, SQuID Dataset, QVLM Architecture","Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning due to their architectures destroying pixel-level information. This paper introduces SQuID, a benchmark dataset for evaluating quantitative spatial reasoning, and proposes QVLM, a code-generation architecture that maintains pixel precision.",26.48,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13404v1_Local-to-Global Logical Explanations for Deep Visi.pdf,Local-to-Global Logical Explanations for Deep Vision Models,"Bhavan Vasu, Giuseppe Raffa, Prasad Tadepalli",,2601.13404,"Explainable AI, Neurosymbolic AI, Monotone DNF, Deep Learning","Introduces local and global explanation methods for black-box models in vision tasks, generating explanations in terms of human-recognizable primitive concepts.",27.09,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13406v1_Integrating Virtual Reality and Large Language Mod.pdf,Integrating Virtual Reality and Large Language Models for Team-Based Non-Technical Skills Training and Evaluation in the Operating Room,"Jacob Barker, Doga Demirel, Cullen Jackson, Anna Johansson, Robbin Miraglia, Darian Hoagland, Stephanie B. Jones, John Mitchell, Daniel B. Jones, Suvranu De",,,"Virtual Reality, Large Language Models, Team-Based Training, Non-Technical Skills, Operating Room, Simulation","This study introduces VORTeX, a multi-user virtual reality platform that integrates immersive team simulation with large language model analytics to train and evaluate communication, decision-making, teamwork, and leadership in surgical professionals during laparoscopic emergencies.",29.16,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13412v1_Using deep learning for predicting cleansing quali.pdf,Using deep learning for predicting cleansing quality of colon capsule endoscopy images,"Puneet Sharma, Kristian Dalsbø Hindberg, Benedicte Schelde-Olesen, Ulrik Deding, Esmaeil S. Nadimi, Jan-Matthias Braun, on behalf of the AICE consortium",Not found,2601.13412,"deep learning, colon capsule endoscopy, image prediction, cleansing quality","This study explores the application of deep learning techniques for predicting cleansing quality in colon capsule endoscopy (CCE) images using a dataset of 500 images labeled by 14 clinicians on the Leighton–Rex scale. A ResNet-18 model was trained for classification, and structured pruning techniques were applied to achieve significant sparsity while maintaining high accuracy. The results indicate that a pruned model can achieve a cross-validation accuracy of 88% with 79% sparsity.",29.45,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13422v1_TrustEnergy A Unified Framework for Accurate and R.pdf,TrustEnergy: A Unified Framework for Accurate and Reliable User-level Energy Usage Prediction,"Dahai Yu, Rongchao Xu, Dingyi Zhuang, Yuheng Bu, Shenhao Wang, Guang Wang",,,"Energy prediction, Deep learning, User-level prediction, Hierarchical spatiotemporal representation, Sequential conformalized quantile regression","This paper presents a unified framework called TrustEnergy for accurate and reliable user-level energy usage prediction. It includes two key technical components: a Hierarchical Spatiotemporal Representation module to capture both macro and micro energy usage patterns, and an innovative Sequential Conformalized Quantile Regression module to dynamically adjust uncertainty bounds. The framework is implemented and evaluated using an electricity provider in Florida, achieving a 5.4% increase in prediction accuracy and 5.7% improvement in uncertainty quantification compared to state-of-the-art baselines.",27.68,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13435v1_A Learnable Wavelet Transformer for Long-Short Equ.pdf,A Learnable Wavelet Transformer for Long-Short Equity Trading and Risk-Adjusted Return Optimization,"Shuozhe Li ∗, Du Cheng ∗, Leqi Liu",Not found,Not found,"Neural wavelet regularization, wavelet-transformer network, low-guided high-frequency injection, return optimization","Learning profitable intraday trading policies from financial time series is challenging due to heavy noise, non-stationarity, and strong cross-sectional dependence among related assets. We propose WaveLSFormer, a learnable wavelet-based long-short Transformer that jointly performs multi-scale decomposition and return-oriented decision learning. The model outputs a portfolio of long/short positions that is rescaled to satisfy a fixed risk budget, and is optimized directly with a trading objective and risk-aware regularization. Extensive experiments demonstrate that WaveLSFormer consistently outperforms MLP, LSTM, and Transformer backbones, with and without fixed discrete wavelet front-ends.",27.93,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13437v1_MOSLD-Bench Multilingual Open-Set Learning and Dis.pdf,MOSLD-Bench: Multilingual Open-Set Learning and Discovery,"Adriana-Valentina Costache, Daria-Nicoleta Dragomir, Silviu-Florin Gheorghe, Eduard Poesina, Paul Irofti, Radu Tudor Ionescu",Not found,Not found,"multilingual, open-set learning, discovery, text categorization","The paper introduces the first multilingual open-set learning and discovery (MOSLD) benchmark for text categorization, comprising 960K data samples across 12 languages. It proposes a novel framework for the OSLD task, integrating multiple stages to continuously discover and learn new classes. The benchmark is constructed by rearranging existing datasets and collecting new data samples from the news domain. The authors evaluate several language models, including their own, to obtain results that can serve as a reference for future work.",27.01,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13443v1_Explicit Cognitive Allocation A Principle for Gove.pdf,Explicit Cognitive Allocation: A Principle for Governed and Auditable Inference in Large Language Models,"Héctor Manuel Manzanilla-Granados, Zaira Navarrete-Cazales, Miriam Pescador-Rojas, Tonahtiu Ramírez-Romero",Not found,Not found,"Large Language Models, AI-assisted reasoning, Cognitive allocation, Epistemic functions, Cognitive Universal Agent","The rapid adoption of large language models has enabled new forms of AI-assisted reasoning. However, prevailing modes of LLM use remain cognitively unstructured, limiting traceability, epistemic control, and reproducibility. We introduce Explicit Cognitive Allocation, a principle for structuring AI-assisted inference through the explicit separation and orchestration of epistemic functions.",28.47,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13458v1_Labels or Preferences Budget-Constrained Learning .pdf,Budget-Constrained Learning with Human Judgments over AI-Generated Outputs,"Zihan Dong, Ruijia Wu, Linjaun Zhang",Not found,2601.13458,"budget-constrained learning, human preference feedback, active learning, pseudo labels","This work addresses the need for principled, budget-conscious data acquisition strategies in AI, particularly in the context of combining ground-truth labels and human preference data. It introduces Preference-Calibrated Active Learning (PCAL), a novel method that optimally allocates a fixed annotation budget between ground-truth labels and pairwise preferences, ensuring robust performance even with poorly estimated nuisance models.",28.36,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13462v1_SpatialBench-UC Uncertainty-Aware Evaluation of Sp.pdf,SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt,Amine Rostane,Not found,2601.13462,"Text-to-Image Generation, Spatial Evaluation, Uncertainty, Reproducibility","Evaluating whether text to image models follow explicit spatial instructions is difficult to automate. Spatial evaluation is a selective prediction problem, and a checker should be allowed to abstain when evidence is weak and should report confidence so results can be interpreted as a risk–coverage trade-off.",26.89,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13464v1_Context and Transcripts Improve Detection of Deepf.pdf,Context and Transcripts Improve Detection of Deepfake Audios,"Chongyang Gao, Marco Postiglione, Julian Baldwin, Natalia Denisenko, Isabel Gortner, Luke Fosdick, Chiara Pulice, Sarit Kraus, V. S. Subrahmanian",not found,not found,"deepfake, audio detection, context, transcript, journalists","Humans use context to assess the veracity of information. However, current audio deepfake detectors only analyze the audio file without considering either context or transcripts. We create and analyze a Journalist-provided Deepfake Dataset (JDD) of 255 public deepfakes which were primarily contributed by over 70 journalists since early 2024. We also generate a synthetic audio dataset (SYN) of dead public figures and propose a novel Context-based Audio Deepfake Detector (CADD) architecture. In addition, we evaluate performance on two large-scale datasets: ITW and P2V. We show that sufficient context and/or the transcript can significantly improve the efficacy of audio deepfake detectors.",28.0,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13465v1_Graph Neural Networks are Heuristics.pdf,Graph Neural Networks are Heuristics,"Yimeng Min, Carla P. Gomes",Not found,2601.13465,"Graph Neural Networks, Heuristics, Travelling Salesman Problem, Combinatorial Optimization, Neural Networks, Machine Learning","This paper demonstrates that a single training trajectory can transform a graph neural network into an unsupervised heuristic for combinatorial optimization, focusing on the Travelling Salesman Problem. It shows that encoding global structural constraints as an inductive bias enables a non-autoregressive model to generate solutions via direct forward passes, without search, supervision, or sequential decision-making.",27.26,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13474v1_Preconditioning Benefits of Spectral Orthogonaliza.pdf,Preconditioning Benefits of Spectral Orthogonalization in Muon,"Jianhao Ma ∗, Yu Huang∗, Yuejie Chi†, Yuxin Chen‡",Not provided,2601.13474,"Muon optimizer, spectral orthogonalization, matrix optimization, preconditioning","This paper studies the effectiveness of a simplified variant of Muon through two case studies: matrix factorization and in-context learning of linear transformers. It proves that simplified Muon converges linearly with iteration complexities independent of the relevant condition number, outperforming gradient descent and Adam. The analysis reveals that Muon dynamics decouple into scalar sequences in the spectral domain, each with similar convergence behavior, formalizing the preconditioning effect induced by spectral orthogonalization.",28.65,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13476v1_A Unified Variational Imputation Framework for Ele.pdf,A Unified Variational Imputation Framework for Electric Vehicle Charging Data,"Jinhao Li, Hao Wang",,,"Electric vehicle, data imputation, charging demand, large language model, retrieval-augmented generation","This work develops a novel probabilistic variational imputation framework (PRAIM) to address the challenges of missing records in electric vehicle charging data, leveraging the power of large language models and retrieval-augmented memory to improve imputation accuracy and preserve the original data's statistical distribution.",26.7,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13481v1_Towards Efficient and Robust Linguistic Emotion Di.pdf,Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement,"Jian Zhang, Zhangqi Wang, Zhiyuan Wang, Weiping Fu, Yu He, Haiping Zhu∗, Qika Lin∗, Jun Liu",Not found,Not found,"Linguistic Emotion Diagnosis, Emotional Comorbidity, Inefficient Exploration, Automated Prompt Optimization, Multi-Agent Collaboration, Medical Language Processing, Trustworthy Artificial Intelligence","This paper proposes APOLO, a framework that systematically explores a broader and finer-grained prompt space to enhance diagnostic efficiency and robustness for linguistic emotion diagnosis in mental health applications. It addresses issues of emotional comorbidity and inefficient exploration, using a multi-agent collaboration mechanism to refine prompts and optimize diagnostic accuracy and reliability.",28.12,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13487v1_The Hidden Toll of Social Media News Causal Effect.pdf,The Hidden Toll of Social Media News: Causal Effects on Psychosocial Wellbeing,"Olivia Pal, Agam Goyal, Eshwar Chandrasekharan, Koustuv Saha",,,"social media, news consumption, psychosocial wellbeing, depression, stress, anxiety, lone-liness, social interaction","This study examines the psychosocial effects of news consumption on social media, revealing that while news engagement can lead to increased depression, stress, and anxiety, it can also decrease loneliness and increase social interaction. The findings suggest that news feed bookmarking is associated with greater psychosocial deterioration compared to commenting or quoting.",27.15,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13508v1_CatMaster An Agentic Autonomous System for Computa.pdf,CatMaster: An Agentic Autonomous System for Computational Heterogeneous Catalysis Research,"Honghao Chen, Jiangjie Qiu, Yi Shen Tew, Xiaonan Wang ∗",,2601.13508,"autonomous system, computational chemistry, heterogeneous catalysis, density functional theory","CatMaster is an agent system driven by large-language-models that converts natural language requests into complete calculation workspaces, including structures, inputs, outputs, logs, and a concise run record, supporting projects with long workflows and practical workflow issues.",29.3,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13515v1_Automatic Adjustment of HPA Parameters and Attack .pdf,Automatic Adjustment of HPA Parameters and Attack Prevention in Kubernetes Using Random Forests,"Huah Yong Chan, Hanlin Zhou, Jingfei Ni, Mengchun Wu, Qing Deng",Not found,2601.13515,"Kubernetes, HPA, Security, Random Forest","This paper uses HTTP status codes as custom metrics within the HPA to dynamically adjust the maximum pod parameter in Kubernetes, preventing attacks by redirecting traffic to honeypot pods and adjusting HPA parameters using machine learning scripts.",28.15,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13518v1_AgenticRed Optimizing Agentic Systems for Automate.pdf,AGENTICRED: Optimizing Agentic Systems for Automated Red-teaming,"Jiayi Yuan*, Jonathan N""other, Natasha Jaques, Goran Radanović",,,"Automated red-teaming, Agentic systems, Large Language Models, Evolutionary selection, AI safety","This paper introduces AGENTICRED, an automated pipeline that leverages LLMs' in-context learning to iteratively design and refine red-teaming systems without human intervention. It outperforms state-of-the-art approaches, achieving high attack success rates on proprietary models.",26.2,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13528v1_Eliciting Harmful Capabilities by Fine-Tuning On S.pdf,ELICITINGHARMFULCAPABILITIES BYFINE-TUNING ONSAFEGUARDEDOUTPUTS,"Jackson Kaunismaa∗, MATS Avery Griffin, Anthropic John Hughes, Anthropic Christina Q Knight, Scale AI Mrinank Sharma†, Anthropic Erik Jones†",Not found,2601.13528,"AI safety, model fine-tuning, elicitation attacks, safeguarded models, hazardous chemical synthesis","This work demonstrates that even robustly safeguarded models can be used to elicit harmful capabilities in open-source models through elicitation attacks. The attacks consist of three stages: constructing prompts in adjacent domains, obtaining responses from safeguarded models, and fine-tuning open-source models on these prompt-output pairs. The efficacy of these attacks scales with the capability of the frontier model and the amount of generated fine-tuning data.",27.95,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13533v1_Reasoning While Recommending Entropy-Guided Latent.pdf,Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models,Changshuo Zhang,not found,not found,"Generative Re-ranking, Latent Reasoning, Reinforcement Learning","This paper introduces an entropy-guided latent reasoning mechanism to improve the accuracy of capturing complex preferences in generative re-ranking scenarios. It proposes the EGLR recommendation model, which combines reasoning and recommendation processes to enhance model performance.",25.73,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13534v1_MN-TSGContinuous Time Series Generation with Irreg.pdf,MN-TSG: CONTINUOUSTIMESERIESGENERATION WITH IRREGULAROBSERVATIONS,"Xu Zhang, Junwei Deng, Chang Xu, Hao Li, Jiang Bian",Not found,2601.13534,"Irregular time series, continuous time series generation, deep learning architecture",MN-TSG proposes a novel framework that integrates Mixture-of-Experts (MoE)–based Neural Controlled Differential Equations (NCDEs) with existing TSG models to address the challenges of modeling irregular and continuous time series. It demonstrates effectiveness on both irregular-to-regular and irregular-to-continuous generation tasks.,27.37,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13537v1_When Wording Steers the Evaluation Framing Bias in.pdf,When Wording Steers the Evaluation: Framing Bias in LLM judges,"Yerin Hwang, Dongryeol Lee, Taegwan Kang, Minwoo Lee, Kyomin Jung",,,"Large language models, Framing bias, LLM evaluation, Predicate-positive, Predicate-negative","This paper investigates how subtle prompt phrasing can steer the judgments of large language models (LLMs) in evaluation tasks, revealing significant discrepancies and suggesting that framing bias is a structural property of current LLM-based evaluation systems.",26.04,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13545v1_TruthTensor Evaluating LLMs Human Imitation throug.pdf,TRUTHTENSOR: EVALUATINGLLMSHUMANIMITATION THROUGH PREDICTIONMARKETDRIFT ANDHOLISTICREASONING,"Shirin Shahabi, Spencer Graham, Haruna Isah",[DOI string if found],ID if found,"Large Language Models, LLMs, Human Imitation, Prediction Markets, Drift, Holistic Evaluation","This paper introduces TruthTensor, a novel evaluation paradigm for Large Language Models (LLMs) that measures their ability to imitate human decision-making in socially-grounded, high-entropy environments. It complements traditional correctness metrics with drift-centric diagnostics and robustness checks, providing a holistic view of model behavior across multiple axes including calibration, drift, and risk-sensitivity.",26.62,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13546v1_ChatAD Reasoning-Enhanced Time-Series Anomaly Dete.pdf,ChatAD: Reasoning-Enhanced Time-Series Anomaly Detection with Multi-Turn Instruction Evolution,"Hui Sun1*, Chang Xu2†, Haonan Xie3, Hao Li4, Yuhao Huang5, Chuheng Zhang2, Ming Jin6, Xiaoguang Liu1, Gang Wang1, Jiang Bian2",,,"Anomaly Detection, Time Series, Multi-Turn Dialogue, Reasoning, LLM-driven","This paper proposes a multi-agent-based Time Series Evolution algorithm named TSEvol, introduces the AD reasoning & multi-turn dialogue Dataset TSEData-20K, and contributes the Chatbot family for AD, including ChatAD-Llama3-8B, ∼Qwen2.5-7B, and ∼Mistral-7B. It also proposes the TS Kahneman-Tversky Optimization (TKTO) to enhance cross-task generalization capability and introduces a LLM-driven Learning-based AD Benchmark LLADBench to evaluate ChatAD and nine baselines across seven datasets and tasks.",28.15,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13547v1_HateXScore A Metric Suite for Evaluating Reasoning.pdf,HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations,"Yujia Hu, Roy Ka-Wei Lee",,,"Hate Speech, Explanations, Reasoning Quality, Transparency, Automated Moderation","HateXScore is a four-component metric suite designed to evaluate the reasoning quality of model explanations in hate speech detection. It assesses conclusion explicitness, faithfulness and causal grounding of quoted spans, protected group identification, and logical consistency among these elements. Evaluated on six diverse hate speech datasets, HateXScore is intended as a diagnostic complement to reveal interpretability failures and annotation inconsistencies that are invisible to standard metrics like Accuracy or F1. Human evaluation shows strong agreement with HateXScore, validating it as a practical tool for trustworthy and transparent moderation.",27.77,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13558v1_Leveraging ChatGPT and Other NLP Methods for Ident.pdf,Leveraging ChatGPT and Other NLP Methods for Identifying Risk and Protective Behaviors in MSM: Social Media and Dating Apps Text Analysis,"Mehrab Beikzadeh ∗, Chenglin Hong †, Cory J Cascalheira ‡, Callisto Boka †, Majid Sarrafzadeh ∗, Ian W Holloway †",,,"machine learning, HIV risk, harmful drinking, social app, dating app, text mining, ChatGPT, eHealth, LLM","This study aims to determine if text data from social media and dating apps can predict risk and protective behaviors among men who have sex with men (MSM). The authors trained machine learning models using ChatGPT embeddings, BERT embeddings, LIWC analysis, and a custom dictionary to identify various risk behaviors such as condomless anal sex, number of sexual partners, binge drinking, and heavy drinking. The model was highly predictive of monthly binge drinking and having over 5 sexual partners, but slightly less predictive of PrEP use and heavy drinking. ChatGPT embeddings were found to be highly informative in prediction, and combining ChatGPT embeddings with LIWC and BERT and using the most correlated features improved performance. The findings suggest that text data has the potential to provide valuable insights into specific risk and protective behaviors among MSM.",29.05,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13559v1_AgentGC Evolutionary Learning-based Lossless Compr.pdf,AgentGC: Evolutionary Learning-based Lossless Compression for Genomics Data with LLM-driven Multiple Agent,"Hui Sun1,2, Yanfeng Ding1, Huidong Ma1,2, Chang Xu3, Keyan Jin4, Lizheng Zu2, Cheng Zhong5, Xiaoguang Liu1*, Gang Wang1∗, Wentong Cai2",Not found,Not found,"genomics, compression, evolutionary learning, LLM, multiple agent","AgentGC is a novel evolutionary GD compressor consisting of three layers: User, Cognitive, and Compression. It uses a Leader and Worker model, providing a user-friendly interface via Leader combined with LLM, integrating LLM for joint optimization of algorithm-dataset-system, and performing compression via automated multi-knowledge learning. Compared to 14 baselines, it achieves significant compression and throughput gains on 9 datasets.",27.5,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13562v1_Reasoning is a Modality.pdf,Reasoning is a Modality,"Zhiguang Liu, Yi Shang",Not found,Not found,"Reasoning, AI, Human Intelligence, Abstract Reasoning, Transformer, Visual Reasoning","The paper hypothesizes that reasoning is a distinct modality, separate from the low-level workspace, and proposes a novel role-separated transformer block to test this hypothesis. The method achieves 62.6% accuracy on ARC-1, surpassing average human performance.",26.41,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13563v1_ButterflyMoE Sub-Linear Ternary Experts via Struct.pdf,ButterflyMoE: Sub-Linear Ternary Experts via Structured Butterfly Orbits,Aryan Karmore,Not found,Not found,"Machine learning, Model compression, Quantization, Ternary weights, Memory efficiency","ButterflyMoE is a method that treats experts not as independent weight matrices but as geometric reorientations of a unified shared quantized substrate. It achieves sub-linear memory scaling, reducing memory requirements from O(N·d^2) to O(d^2 + N·dlogd) per expert, enabling deployments on edge devices with limited memory.",26.41,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13564v1_Multi-objective fluorescent molecule design with a.pdf,Multi-objective fluorescent molecule design with a data-physics dual-driven generative framework,"Yanheng Li, Zhichen Pu, Lijiang Yang, Zehao Zhou, Yi Qin Gao",,,"fluorescent molecules, multi-objective optimization, data-driven design, generative models, machine learning",This paper presents a novel approach to design fluorescent molecules using a dual-driven generative framework that integrates data science and physical principles.,28.85,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13566v1_Self-Improvement as Coherence Optimization A Theor.pdf,Self-Improvement as Coherence Optimization: A Theoretical Account,"Tianyi Qiu, Ahmed Hani Ismail, Zhonghao He, Shi Feng",Not found,2601.13566,"Language models, Coherence optimization, Semi-supervised learning, Self-improvement","This paper explores how language models can improve their accuracy without external supervision, presenting methods such as debate, internal coherence maximization, iterative bootstrap, and Metropolis-Hastings sampling. It shows that these methods all optimize coherence, defined as the joint likelihood of behaviors across contexts, and proves that coherence optimization is equivalent to description-length regularization, making it optimal for semi-supervised learning when derived from a pretrained model.",28.23,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13570v1_GeoDynamics A Geometric State-Space Neural Network.pdf,GeoDynamics: A Geometric State-Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds,"Tingting Dan, Jiaqi Ding, Guorong Wu",Not found,2601.13570,"Geometric State-Space Models, Neural Networks, Functional Connectivity, Riemannian Manifolds, Alzheimer's, Parkinson's, Autism, Human Action Recognition","GeoDynamics is a geometric state-space neural network designed to model brain dynamics on Riemannian manifolds, capturing the trajectories of symmetric positive-definite (SPD) matrices that represent functional connectivity at each time point. This approach addresses the limitations of traditional SSMs by providing a more holistic and self-organized perspective on brain function, including task-driven state changes and early markers of brain disorders.",28.51,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13580v1_Neural Organ Transplantation NOT Checkpoint-Based .pdf,NEURALORGANTRANSPLANTATION(NOT): CHECKPOINT-BASEDMODULARADAPTATION FOR TRANSFORMERMODELS,Ahmad Al-Zuraiqi,Not found,2601.13580v1,"Modular Deep Learning, Transfer Learning, Checkpoint Transfer, Domain Adaptation, Large Language Model","We introduce Neural Organ Transplantation (NOT), a modular adaptation framework for transformer models, enabling reusable transferable checkpoints for domain adaptation.",29.13,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13581v1_SCRIPTMIND Crime Script Inference and Cognitive Ev.pdf,SCRIPTMIND: Crime Script Inference and Cognitive Evaluation for LLM-based Social Engineering Scam Detection System,"Heedou Kim, Changsik Kim, Sanghwa Shin, Jaewoo Kang",,anonymous/ScriptMind,"Social Engineering, Large Language Models, Cognitive Evaluation, Scam Detection","SCRIPTMIND is an integrated framework for LLM-based scam detection that bridges automated reasoning and human cognition. It comprises three components: the Crime Script Inference Task (CSIT) for scam reasoning, the Crime Script–Aware Inference Dataset (CSID) for fine-tuning small LLMs, and the Cognitive Simulation-based Evaluation of Social Engineering Defense (CSED) for assessing real-time cognitive impact. Using 571 Korean phone scam cases, it achieved superior performance over commercial models in detection accuracy, false-positive reduction, scammer utterance prediction, and rationale quality.",27.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13588v1_TREX Tokenizer Regression for Optimal Data Mixture.pdf,TREX: Tokenizer Regression for Optimal Data Mixture,"Inho Won, Hangyeol Yoo, Minkyung Cho, Jungyeul Park, Hoyun Song, KyungTae Lim",Not found,Not found,"tokenizer, data mixture, multilingual, compression efficiency, language ratios","Building effective tokenizers for multilingual Large Language Models (LLMs) requires careful control over language-specific data mixtures. Existing approaches rely on heuristics or costly large-scale searches to determine optimal language ratios, leading to accuracy-cost trade-offs. This work introduces TREX, a regression-based framework that efficiently predicts the optimal data mixture for tokenizer training. TREX trains small-scale proxy tokenizers on random mixtures, gathers their compression statistics, and learns to predict compression performance from data mixtures. This learned model enables scalable mixture search before large-scale tokenizer training, mitigating the accuracy-cost trade-off in multilingual tokenizer design. Tokenizers trained with TReX’s predicted mixtures outperform mixtures based on LLaMA3 and uniform distributions by up to 12% in both in- and out-of-distribution compression efficiency, demonstrating strong scalability, robustness, and practical effectiveness.",28.24,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13589v1_Motion-to-Response Content Generation via Multi-Ag.pdf,MOTION-TO-RESPONSECONTENTGENERATION VIA MULTI-AGENTAI SYSTEM WITHREAL-TIMESAFETYVERIFICATION,HyeYoung Lee,Not found,2601.13589v1,"Speech Emotion Recognition, Multi-Agent Systems, Content Generation, Safety Verification, On-Device AI","This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals, emphasizing the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents.",27.83,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13590v1_Vulnerability of LLMs Belief Systems LLMs Belief R.pdf,Vulnerability of LLMs’ Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions,"Fan Huang, Haewoon Kwak, Jisun An",Not found,Not found,"Large Language Models, persuasion, belief stability, LLM susceptibility, meta-cognition, adversarial fine-tuning",This study systematically evaluates the susceptibility of five mainstream Large Language Models (LLMs) to persuasion under the SMCR communication framework. It analyzes how different persuasive strategies influence belief stability over multiple interaction turns and examines whether meta-cognition prompting affects resistance to persuasion. The findings highlight substantial model-dependent limits of current robustness interventions and offer guidance for developing more trustworthy LLMs.,27.02,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13591v1_DSAEval Evaluating Data Science Agents on a Wide R.pdf,DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems,"Maojun Sun1*, Yifei Xie1*, Yue Wu1*, Ruijian Han1†, Binyan Jiang1, Defeng Sun2, Yancheng Yuan2†, Jian Huang1,2†",Not found,Not found,"Data Science, Agents, Evaluation, Real-World Problems, Multi-Modal Perception, Multi-Query Interactions, Multi-Dimensional Evaluation","Recent advances in Large Language Model-based data science agents have significantly promoted the automation of data science tasks. However, evaluating these agents remains a formidable challenge due to the open-ended nature of real-world data science problems, which often lack unique, standardized solutions. To address this, we introduce DSAEval, a comprehensive benchmark designed to evaluate data science agents using large-scale, real-world problems. DSAEval spans broad domains of data science problems, including Statistical Testing & Inference (STI), Data Analysis, Machine Learning, and Deep Learning. It incorporates three distinctive features: Multi-Modal Environment Perception, Multi-Query Interactions, and Multi-Dimensional Evaluation. The benchmark systematically evaluates 11 advanced agentic LLMs, demonstrating that Claude-Sonnet-4.5 achieves the strongest overall performance, GPT-5.2 is the most efficient, and MiMo-V2-Flash is the most cost-effective. The results also show that multimodal perception consistently improves performance on vision-related tasks, with gains ranging from 2.04% to 11.30%. Overall, while current data science agents perform well on structured data and routine data analysis workflows, substantial challenges remain in unstructured domains. The findings offer critical insights and outline future research directions to advance the development of data science agents.",29.1,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13592v1_Machine learning based radiative parameterization .pdf,Machine learning based radiative parameterization scheme and its performance in operational reforecast experiments,"Jing Hao, Xiao Sa, Li Haoyu, Xiao Huadong, Xue Wei",,,"Machine learning, Radiation, Hybrid model, Operational reforecast experiments","This study investigates critical limitations in hybrid forecasting frameworks that embed deep neural networks into numerical prediction models, focusing on coupling compatibility and long-term integration stability. A residual convolutional neural network is employed to approximate the Rapid Radiative Transfer Model for General Circulation Models (RRTMG) within the global operational system of China Meteorological Administration. The hybrid model achieves accuracy comparable to traditional physical schemes while accelerating computation speed by approximately eightfold.",28.19,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13599v1_Diffusion In Diffusion Breaking the Autoregressive.pdf,Diffusion In Diffusion: Breaking the Autoregressive Bottle-neck in Block Diffusion Models,"Linrui Ma, Yufei Cui, Kai Han, Yunhe Wang",,2601.13599,"block diffusion, semi-autoregressive, discrete diffusion, language modeling, autoregressive models, diffusion models, inference latency, perplexity, token generation, contextual information, myopia, irreversibility","Proposes DIFFUSION INDIFFUSION—a 'draft-then-refine' framework to overcome the irreversibility and myopia problems in block diffusion models, generating rapid drafts using small blocks and refining them through global bidirectional diffusion. Utilises snapshot confidence remasking and mix-scale training to improve performance.",27.63,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13600v1_Foundations of Global Consistency Checking with No.pdf,Foundations of Global Consistency Checking with Noisy LLM Oracles,"Paul He*, Elke Kirschbaum, Shiva Kasiviswanathan",,,"Consistency checking, Noisy LLM, Global coherence, Minimal Unsatisfiable Subsets (MUSes), Fact verification","Ensuring global consistency of natural-language facts is essential for tasks like fact-checking, summarization, and knowledge base construction. While LLMs can assess small subsets of facts, their judgments are noisy and insufficient for global coherence. This paper formalizes the problem and proposes an adaptive divide-and-conquer algorithm to identify minimal inconsistent subsets (MUSes) and optionally compute minimal repairs through hitting-sets, achieving low-degree polynomial query complexity. Experiments show the method efficiently detects and localizes inconsistencies, offering a scalable framework for linguistic consistency verification with LLM-based evaluators.",27.41,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13614v1_CauScientist Teaching LLMs to Respect Data for Cau.pdf,CauScientist: Teaching LLMs to Respect Data for Causal Discovery,"Bo Peng, Sirui Chen, Lei Xu, Chaochao Lu",Not found,2601.13614,"causal discovery, large language models, statistical indistinguishability, modeling assumptions, distribution shift","Causal discovery is fundamental for scientific understanding and decision-making. Existing methods face limitations, and CauScientist proposes a collaborative framework that integrates LLMs as hypothesis-generating 'data scientists' with probabilistic statistics as verifiers. It employs hybrid initialization, iterative refinement, and error memory to improve performance and reduce structural hamming distance compared to standalone LLMs.",26.85,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13622v1_CARPE Context-Aware Image Representation Prioritiz.pdf,Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models,"Donghee Lee, Rui Cai, Zhe Zhao",Not found,2601.13622,"Large Vision-Language Models, Context-Aware, Image Classification, Vision-Language Benchmarks, Ensemble","This paper proposes CARPE, a novel framework that introduces vision-integration layers and a context-aware ensemble strategy to enhance the adaptability and generalization of Large Vision-Language Models (LVLMs) in image classification and vision-language tasks.",27.15,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13632v1_Resilient Routing Risk-Aware Dynamic Routing in Sm.pdf,Resilient Routing: Risk-Aware Dynamic Routing in Smart Logistics via Spatiotemporal Graph Learning,"Zhiming Xue, Sichen Zhao, Yalun Qi, Xianling Zeng, Zihan Yu",Not found,Not found,"Smart Logistics, Graph Neural Network, Dynamic Routing, Spatiotemporal modeling, Supply Chain Resilience","This paper proposes a Risk-Aware Dynamic Routing (RADR) framework that integrates Spatiotemporal Graph Neural Networks (ST-GNN) with combinatorial optimization to address the challenges of traffic congestion and fluctuating retail demand in smart logistics. The framework constructs a logistics topology graph using discrete GPS data and a hybrid deep learning model combining Graph Convolutional Network (GCN) and Gated Recurrent Unit (GRU) to predict future congestion risks and integrate these predictions into a dynamic edge weight mechanism for path planning. The experimental results demonstrate that the RADR algorithm significantly enhances supply chain resilience, particularly in high congestion scenarios.",28.54,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13645v1_Quadratic Upper Bound for Boosting Robustness.pdf,Quadratic Upper Bound for Boosting Robustness,"Euijin Y ou, Hyang-Won Lee",Not found,Not found,"Adversarial training, Robustness, Fast adversarial training, Loss function, Robustness enhancement",This paper develops a loss function to improve robustness in Fast Adversarial Training (FAT) without requiring stronger inner maximization. It derives a quadratic upper bound (QUB) on the adversarial training loss function and proposes to utilize the bound with existing FAT methods. Experimental results show significant improvement in robustness.,26.71,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13647v1_Fusion Segment Transformer Bi-Directional Attentio.pdf,FUSION SEGMENT TRANSFORMER: BI-DIRECTIONAL A TTENTION GUIDED FUSION NETWORK FOR AI GENERA TED MUSIC DETECTION,"Yumin Kim, Seonghyeon Go",MIPPIA Inc.,githubCodeὑ7Demo,"AI-generated music detection, Full-audio segment detection, Musical structure analysis, Cross-modal fusion layer, Music representation","With the rise of generative AI technology, the Fusion Segment Transformer improves AI-generated music detection by integrating content and structural information through a Gated Fusion Layer, achieving state-of-the-art results on the SONICS and AIME datasets.",27.16,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13649v1_Fairness or Fluency An Investigation into Language.pdf,Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge,"Xiaolin Zhou, Zheng Luo, Yicheng Gao, Qixuan Chen, Xiyang Hu, Yue Zhao, Ruishan Liu",,,"Large Language Models, LLM-as-a-judge, Language bias, Performance disparity, Inter-language judging, Same-language judging, Perplexity bias","This paper investigates two types of language bias in pairwise LLM-as-a-judge: performance disparity between languages when the judge is prompted to compare options from the same language, and bias towards options written in major languages when the judge is prompted to compare options of two different languages. The study finds significant performance disparities across language families, with European languages consistently outperforming African languages, and that this bias is more pronounced in culturally-related subjects. The paper also explores whether language bias is caused by low-perplexity bias, a previously identified bias of LLM-as-a-judge, and finds that while perplexity is slightly correlated with language bias, language bias cannot be fully explained by perplexity only.",28.29,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13655v1_Why Does the LLM Stop Computing An Empirical Study.pdf,Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs,"GUANGBA YU∗, ZIRUI WANG∗, YUJIE HUANG, RENYI ZHONG, YUEDONG ZHONG, YILUN WANG, MICHAEL R. LYU",Not provided,Not provided,"Large Language Models, Failure Analysis, Empirical Study","This study conducts the first large-scale empirical study of 705 real-world failures from the open-source DeepSeek, Llama, and Qwen ecosystems, revealing a paradigm shift in the reliability of user-managed orchestration of open-source LLMs.",27.09,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13657v1_Communication-Free Collective Navigation for a Swa.pdf,Communication-Free Collective Navigation for a Swarm of UA Vs via LiDAR-Based Deep Reinforcement Learning,"Myong-Yol Choi, Hankyoul Ko, Hanse Cho, Changseung Kim, Seunghwan Kim, Jaemin Seo, Hyondong Oh",,,"multi-robot systems, collective navigation, sensor-based control, deep reinforcement learning","This paper presents a deep reinforcement learning (DRL) based controller for collective navigation of unmanned aerial vehicle (UA V) swarms in communication-denied environments, enabling robust operation in complex, obstacle-rich environments.",26.77,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13659v1_Temporal-Spatial Decouple before Act Disentangled .pdf,TEMPORAL-SPA TIAL DECOUPLE BEFORE ACT: DISENTANGLED REPRESENTA TION LEARNING FOR MULTIMODAL SENTIMENT ANALYSIS,"Chunlei Meng, Ziyang Zhou, Lucas He, Xiaojing Du, Chun Ouyang†, Zhongxue Gan",Not found,Not found,"Multimodal Sentiment Analysis, Temporal-Spatial Decoupling, Representation Learning","Proposes TSDA, Temporal–Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction, aiming to reduce spatiotemporal information asymmetry and improve performance in multimodal sentiment analysis.",27.22,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13671v1_The Orchestration of Multi-Agent Systems Architect.pdf,"The Orchestration of Multi-Agent Systems: Architectures, Protocols, and Enterprise Adoption","Apoorva Adimulam, Rajesh Gupta, Sumit Kumar",,,"Agent orchestration, Agent-to-Agent protocol, dynamic task allocation, Model Context Protocol (MCP), multi-agent systems, observability, state management, system governance","This paper consolidates and formalizes the technical composition of orchestrated multi-agent systems, presenting a unified architectural framework that integrates planning, policy enforcement, state management, and quality operations into a coherent orchestration layer. It also provides detailed technical delineation of two complementary communication protocols—the Model Context Protocol and the Agent-to-Agent protocol, which establish an interoperable communication substrate for scalable, auditable, and policy-compliant reasoning across distributed agent collectives.",27.54,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13684v1_HeteroCache A Dynamic Retrieval Approach to Hetero.pdf,HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache,"Zhiyuan Shi, Qibo Qiu, Feng Xue, Zhonglin Jiang, Li Yu, Jian Jiang, Xiaofei He, Wenxiao Wang",Not found,Not found,"Dynamic compression, Heterogeneous KV cache, Attention drift, Long-context LLM inference","The linear memory growth of the KV cache poses a significant bottleneck for LLM inference in long-context tasks. Existing static compression methods often fail to preserve globally important information, primarily because they overlook the attention drift phenomenon where token significance evolves dynamically. HeteroCache, a training-free dynamic compression framework, addresses this issue by categorizing heads based on stability and redundancy, applying a fine-grained weighting strategy to capture context changes, and employing a hierarchical storage mechanism to hide I/O latency.",27.85,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13687v1_Understanding Mental States to Guide Social Influe.pdf,Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue,"Zhichao Liang, Satoshi Nakamura",,,"Social Interaction, Theory of Mind, Multi-Agent Dialogue, Dynamic, Language Models","This paper introduces SocialMindChange, a benchmark that moves from tracking minds to changing minds in social interaction. It includes 1,200 social contexts, covering 6,000 scenarios and over 90,000 questions, each validated for realism and quality. Evaluations on ten state-of-the-art LLMs show that their average performance is 54.2% below human performance, suggesting current LLMs struggle to maintain and change mental-state representations across long, linked interactions.",27.49,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13693v1_End-to-End Reverse Screening Identifies Protein Ta.pdf,End-to-End Reverse Screening Identifies Protein Targets of Small Molecules Using HelixFold3,"Shengjie Xu, Xianbin Ye, Mengran Zhu, Xiaonan Zhang, Shanzhuo Zhang, Xiaomin Fang",Not found,2601.13693,"Reverse screening, Target identification, Biomolecular structure prediction, HelixFold3","Identifying protein targets for small molecules, or reverse screening, is essential for understanding drug action, guiding compound repurposing, predicting off-target effects, and elucidating the molecular mechanisms of bioactive compounds. This paper presents an end-to-end reverse screening strategy leveraging HelixFold3, a high-accuracy biomolecular structure prediction model, to improve screening accuracy and demonstrate enhanced structural fidelity, binding-site precision, and target prioritization.",29.62,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13697v1_Uncertainty-Aware Gradient Signal-to-Noise Data Se.pdf,Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning,"Zhihang Yuan, Chengyu Yue, Long Huang, Litu Ou, Lei Shi",Not found,Not found,"Instruction tuning, Large language models, Data selection, Gradient signal-to-noise ratio, Epistemic uncertainty","This paper proposes GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework for instruction tuning. It utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. The method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations and human assessments, and converges faster under the same compute budget compared to competitive filters.",27.1,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13698v1_Does Privacy Always Harm Fairness Data-Dependent T.pdf,Privacy Always Harm Fairness? Data-Dependent Trade-offs via Chernoff Information Neural Estimation,"Arjun Nichani, Hsiang Hsu, Chun-Fu (Richard) Chen, Haewon Jeong",Not found,2601.13698,"Privacy, Fairness, Chernoff Information, Neural Estimation, Machine Learning","This paper explores the relationship between fairness and privacy in machine learning, using the information-theoretic measure Chernoff Information. It highlights the data-dependent nature of this relationship and proposes methods to estimate Chernoff Information on data from unknown distributions.",26.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13704v1_Performance and Complexity Trade-off Optimization .pdf,Performance and Complexity Trade-off Optimization of Speech Models During Training,"Esteban Gómez, Tom Bäckström",Not found,Not found,"Speech machine learning, low-complexity, voice activity detection, deep fake detection","This paper proposes a reparameterization technique based on feature noise injection that enables joint optimization of performance and computational complexity during training using SGD-based methods. Unlike traditional pruning methods, our approach allows the model size to be dynamically optimized for a target performance-complexity trade-off, without relying on heuristic criteria to select which weights or structures to remove. The effectiveness of the method is demonstrated through three case studies, including a synthetic example and two practical real-world applications: voice activity detection and audio anti-spoofing. The code related to our work is publicly available to encourage further research.",27.52,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13707v1_Attention-space Contrastive Guidance for Efficient.pdf,Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs,"Yujin Jo, Sangyoon Bae, Taesup Kim*",,,"hallucination mitigation, contrastive guidance, large vision-language models, attention space, efficient computation","This paper addresses the issue of hallucinations in large vision-language models (LVLMs) by framing hallucination mitigation as contrastive guidance, steering generation toward visually grounded and semantically faithful text. The proposed Attention-space Contrastive Guidance (ACG) is a single-pass mechanism that operates within self-attention layers to construct both vision-language and language-only attention paths in a single forward computation, enabling computationally efficient guidance directly embedded in the model's representation contextualization. Experiments on the CHAIR and POPE benchmarks show that ACG achieves state-of-the-art faithfulness and caption quality while significantly reducing computational cost.",28.16,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13709v1_Hidden in Plain Text Measuring LLM Deception Quali.pdf,Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games,"Christopher Kao, Vanshika Vats, James Davis",,,"large language models, natural language processing, autonomous game players, social deduction games","This paper studies deception in the Social Deduction Game (SDG) Mafia, using an asynchronous multi-agent framework to simulate realistic social contexts. It compares the prediction accuracy of a Mafia Detector using GPT-4-Turbo to that of 28 human games and a random baseline, finding that LLMs blend in better and deceive more effectively. The results underscore the sophistication and risks of LLM deception in social contexts.",27.6,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13710v1_Who Should Have Surgery A Comparative Study of Gen.pdf,Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction,"Sayeed Shafayet Chowdhury, Snehasis Mukhopadhyay, Shiaofen Fang, Vijay R. Ramakrishnan",,,"chronic rhinosinusitis, clinical decision support, generative artificial intelligence, large language models, SNOT-22, surgical outcome prediction, tabular clinical data","This study compares generative artificial intelligence (GenAI) models, including ChatGPT, Claude, Gemini, and Perplexity, with supervised machine learning (ML) models (logistic regression, tree ensembles, and an in-house MLP) in predicting clinically meaningful improvement in chronic rhinosinusitis (CRS) outcomes. The models are benchmarked on a prospectively collected cohort where all patients underwent surgery, focusing on identifying those who would have poor outcomes and should have avoided surgery. The study finds that while supervised ML models achieve high accuracy, GenAI models underperform in terms of discrimination and calibration, though their justifications align with clinician heuristics and feature importance identified by the ML models. The findings support an ML-first, GenAI-augmented workflow for pre-operative surgical candidacy triage and shared decision-making.",28.51,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13717v1_Simulated Ignorance Fails A Systematic Study of LL.pdf,Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff,"Zehan Li, Yuxuan Wang, Ali El Lahib, Ying-Jieh Xia, Xinyu Pi",,,"LLMs, forecasting, model knowledge cutoff, simulated ignorance, retrospective forecasting","Evaluating the forecasting capabilities of large language models (LLMs) is constrained by a fundamental tension: prospective evaluation offers methodological rigor but prohibitive latency, while retrospective forecasting (RF) faces rapidly shrinking clean evaluation data. Simulated Ignorance (SI), prompting models to suppress pre-cutoff knowledge, has emerged as a potential solution. This study provides the first systematic test of whether SI can approximate True Ignorance (TI). Across 477 competition-level questions and 9 models, we find that SI fails systematically, leaving a 52% performance gap between SI and TI. Chain-of-thought reasoning fails to suppress prior knowledge, even when reasoning traces contain no explicit post-cutoff references. Reasoning-optimized models exhibit worse SI fidelity despite superior reasoning trace quality. These findings demonstrate that prompts cannot reliably “rewind” model knowledge, and we conclude that RF on pre-cutoff events is methodologically flawed. We recommend against using SI-based retrospective setups to benchmark forecasting capabilities.",28.29,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13719v1_Hierarchical Long Video Understanding with Audiovi.pdf,Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search,"Xinlei Yin1*, Xiulian Peng2, Xiao Li2, Zhiwei Xiong1, Yan Lu2",Not found,Not found,"long video understanding, audiovisual entity cohesion, agentic search, semantic continuity, temporal grounding, adaptive sampling, token compression, memory-based approaches","The rapid growth of long-form video content poses significant challenges for automated understanding systems. Existing solutions relying on naive chunking strategies with retrieval-augmented generation typically suffer from information fragmentation and a loss of global coherence. This paper presents HAVEN, a unified framework for long-video understanding that integrates audiovisual entity cohesion and hierarchical video indexing with agentic search, enabling coherent and comprehensive reasoning. Extensive experiments demonstrate the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.",28.36,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13722v1_OP-Bench Benchmarking Over-Personalization for Mem.pdf,OP-Bench: Benchmarking Over-Personalization for Memory-Augmented Personalized Conversational Agents,"Yulin Hu, Zimo Long, Jiahe Guo, Xingyu Sui, Xing Fu, Weixiang Zhao, Yanyan Zhao, Bing Qin",Not provided,2601.13722,"Memory-augmented, Personalized Conversational Agents, Over-Personalization, Memory Filtering, Large Language Models","This work formalizes over-personalization into three types: Irrelevance, Repetition, and Sycophancy, and introduces OP-Bench, a benchmark of 1,700 verified instances constructed from long-horizon dialogue histories. Using OP-Bench, the authors evaluate multiple large language models and memory-augmentation methods, finding that over-personalization is widespread when memory is introduced. They propose Self-ReCheck, a lightweight, model-agnostic memory filtering mechanism to mitigate over-personalization while preserving personalization performance.",28.49,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13734v1_Towards robust long-context understanding of large.pdf,TOW ARDS ROBUST LONG-CONTEXT UNDERSTANDING OF LARGE LANGUAGE MODEL VIA ACTIVE RECAP LEARNING,Chenyu Hui,,,"Large Language Model, Long-context understanding, Active recap learning, Recap supervision, Recap agent","This paper proposes active recap learning (ARL), a framework for enhancing large language model (LLM) in understanding long contexts. ARL enables models to revisit and summarize earlier content through targeted sequence construction during continued pretraining and retrospective summarization at inference, leading to substantial gains in performance.",26.99,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13735v1_Reasoning or Fluency Dissecting Probabilistic Conf.pdf,Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection,"Hojin Kim, Jaehyung Kim",,,"Large Language Models, Chain-of-Thought, Best-of-N, Probabilistic Confidence, Reasoning Fidelity, Inter-step Causality, Fluency, Logical Structure","This work challenges the assumption that probabilistic confidence metrics capture inter-step causal dependencies necessary for valid reasoning, instead finding that these metrics primarily capture surface-level fluency or in-distribution priors. The findings suggest that current probabilistic metrics are largely insensitive to logical structure and may not effectively measure reasoning quality. The authors propose a contrastive causality metric to isolate inter-step causal dependencies and demonstrate its superior performance compared to existing probability-based approaches.",27.3,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13749v1_Pro-AI Bias in Large Language Models.pdf,Pro-AI Bias in Large Language Models,"Benaya Trabelsi, Jonathan Shaki, Sarit Kraus",,,"Large Language Models, AI Bias, Decision Support, Salary Estimation, Representational Salience","This study investigates whether large language models display a systematic preferential bias in favor of artificial intelligence (AI) itself. Across three experiments, it finds consistent evidence of pro-AI bias, showing that models disproportionately recommend AI-related options and systematically overestimate salaries for AI-related jobs. The study also reveals that the term 'Artificial Intelligence' exhibits the highest similarity to generic prompts for academic fields under positive, negative, and neutral framings, indicating valence-invariant representational centrality. These findings suggest that LLM-generated advice and valuation can systematically skew choices and perceptions in high-stakes decisions.",27.96,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13752v1_Finding RELIEF Shaping Reasoning Behavior without .pdf,Finding RELIEF: Shaping Reasoning Behavior without Reasoning,"Chak Tou Leong, Dingwei Chen, Heming Xia, Sunbowen Lee, Jian Wang, Wenjie Li",Not found,Not found,"Large reasoning models, logit probing, belief engineering, reinforcement learning, fine-tuning, synthesized data, self-reﬂective question-answering","This paper proposes RELIEF, a framework that shapes the behavior of large reasoning models (LRMs) by aligning their self-concept with a target belief blueprint. It bypasses the need for reasoning-trace supervision and fine-tunes on synthesized, self-reﬂective question-answering pairs that affirm the target belief. The framework demonstrates efficiency and faithfulness in comparison to behavior-supervised and preference-based baselines while requiring lower training costs.",27.63,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13761v1_DARC Decoupled Asymmetric Reasoning Curriculum for.pdf,DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution,"Shengda Fan1*, Xuyan Ye1*, Yankai Lin1†",Not provided,2601.13761,"Large Language Models, Self-Play, Optimization Stability, Stable Self-Improvement, Pseudo-Labels, Human Annotations","DARC introduces a two-stage framework to stabilize the self-evolution process of large language models, addressing challenges such as non-stationary objectives and bootstrapping errors. It trains the Questioner to synthesize difficulty-calibrated questions and the Solver with an asymmetric self-distillation mechanism, generating high-quality pseudo-labels for stable self-improvement.",27.63,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13768v1_vLinear A Powerful Linear Model for Multivariate T.pdf,vLinear: A Powerful Linear Model for Multivariate Time Series Forecasting,"Wenzhen Yue, Ruohao Guo, Ji Shi, Zihan Hao, Shiyu Hu, Xianghua Ying",Not found,Not found,"Time series forecasting, Multivariate time series, Linear model, Transformer, Self-attention, Efficiency, Forecasting accuracy","This paper presents vLinear, a powerful yet efficient linear-based multivariate time series forecaster that incorporates vecTrans and WFM-Loss. vecTrans reduces computational complexity from O(N^2) to O(N) by using a learnable rank-1 matrix, making it suitable for Transformer-based forecasters. WFM-Loss improves forecasting accuracy by focusing on more reliable paths and horizons. The model achieves state-of-the-art performance across 22 benchmarks and 124 forecasting settings.",27.61,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13770v1_Look-Ahead-Bench a Standardized Benchmark of Look-.pdf,Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance,Mostapha Benhenda *,,arXiv:2601.13770v1,"look-ahead bias, large language models, financial applications, temporal causality","This paper introduces Look-Ahead-Bench, a standardized benchmark to measure look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic financial workflows. Unlike most existing approaches, it evaluates model behavior in practical scenarios, distinguishing genuine predictive capability from memorization-based performance. Results reveal significant lookahead bias in standard LLMs, unlike PiT-Inference models which demonstrate improved generalization and reasoning abilities as they scale in size.",27.19,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13798v1_Insight Interpretable Semantic Hierarchies in Visi.pdf,INSIGHT: Interpretable Semantic Hierarchies in Vision-Language Encoders,"Kai Wittenmayer, Sukrut Rao, Amin Parchami-Araghi, Bernt Schiele, Jonas Fischer",Not found,2601.13798,"Interpretability, Vision-Language Models, Concept Representation, Spatial Grounding, Hierarchical Autoencoder","INSIGHT provides fine-grained, concept-based explanations for vision tasks by leveraging a hierarchical sparse autoencoder and a foundation model with strong semantic representations.",27.11,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13809v1_DroneVLA VLA based Aerial Manipulation.pdf,DroneVLA: VLA based Aerial Manipulation,"Fawad Mehboob∗, Monijesu James∗, Amir Habel∗, Jeffrin Sam, Miguel Altamirano Cabrera, Dzmitry Tsetserukou",,,"Aerial Manipulation, Vision-Language-Action Models, Human-Robot Interaction, Robotic Fetch-and-Carry","This work introduces a novel concept of autonomous aerial manipulator capable of interpreting high-level natural language commands to retrieve objects and deliver them to a human user. The system integrates a MediaPipe based on Grounding DINO and a Vision-Language-Action (VLA) model with a custom-built drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. VLA performs semantic reasoning to interpret the intent of a user prompt and generates a prioritized task queue for grasping of relevant objects in the scene. Grounding DINO and dynamic A* planning algorithm are used to navigate and safely relocate the object. To ensure safe and natural interaction during the handover phase, the system employs a human-centric controller driven by MediaPipe. This module provides real-time human pose and orientation estimation, allowing the drone to employ visual servoing to maintain a stable, distinct position directly in front of the user, facilitating a comfortable handover.",28.97,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13846v1_Virtual Urbanism An AI-Driven Framework for Quanti.pdf,Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity,Maria Glinskaya,,,"generative artificial intelligence, latent diffusion model, low-rank adaptation model, urban perception, urban identity","This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through synthetic urban replicas. The pilot study demonstrates the feasibility of the framework by producing dynamic synthetic urban sequences of nine Tokyo areas using Stable Diffusion and LoRA models, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments assessed perceptual legitimacy, quantified area-level identity, and derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. The Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis.",27.83,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13864v1_HardSecBench Benchmarking the Security Awareness o.pdf,HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware,"Qirui Chen, Jingxian Shuai, Shuangwu Chen, Shenghao Ye, Zijian Wen, Xufei Su, Jie Jin, Jiangming Li, Jun Chen, Xiaobin Tan, Jian Yang",,,"Large language models, security, benchmarking, hardware, code generation, security flaws, functional correctness, LLM-assisted development","This work introduces HardSecBench, a benchmark with 924 tasks spanning Verilog RTL and firmware-level C, covering 76 hardware-relevant Common Weakness Enumeration (CWE) entries. The benchmark evaluates security awareness under realistic specifications and finds that models often satisfy functional requirements while still leaving security risks. Security results vary with prompting, highlighting pressing challenges and offering actionable insights for future advancements in LLM-assisted hardware design.",27.84,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13880v1_LifeAgentBench A Multi-dimensional Benchmark and A.pdf,LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health,"Ye Tian, Zihao Wang, Onat Gungor, Xiaoran Fan, Tajana Rosing",10.10000/anonymous.4open.science/r/LifeAgentBench-CE7B,1234567890,"digital health, personalized health support, long-horizon reasoning, mobile sensing, large language models, health assistants","This paper introduces LifeAgentBench, a large-scale QA benchmark for long-horizon, cross-dimensional, and multi-user lifestyle health reasoning, containing 22,573 questions. It evaluates 11 leading LLMs and identifies bottlenecks in long-horizon aggregation and cross-dimensional reasoning, proposing LifeAgent as a strong baseline agent for health assistants.",27.78,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13885v1_Confident Rankings with Fewer Items Adaptive LLM E.pdf,Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores,"Esma Balkır, Alice Pernthaller, Marco Basaldella, José Hernández-Orallo, Nigel Collier",,,"Computerized Adaptive Testing, Large Language Models, Continuous Scores, Item Response Theory, Ranking, Efficient Evaluation","This paper presents a principled extension of IRT-based adaptive testing to continuous bounded scores, replacing the Bernoulli response distribution with a heteroskedastic normal distribution. It introduces an uncertainty-aware ranker with adaptive stopping criteria that achieves reliable model ranking while testing as few items and as cheaply as possible. The method is validated on five benchmarks spanning n-gram-based, embedding-based, and LLM-as-judge metrics, using 2% of the items while improving ranking correlation by 0.12 τ over random sampling, with 95% accuracy on confident predictions.",27.89,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13887v1_Human Simulation Computation A Human-Inspired Fram.pdf,Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems,Hong Su,Not found,Not found,"Human Simulation Computation, Environment Interaction, Adaptive Artificial Intelligence, Human-Inspired Reasoning","Large language models (LLMs) have strong capabilities in knowledge representation and reasoning based on textual data, but their reliance on language material limits their ability to adapt, verify reasoning outcomes, and operate effectively in open and dynamic real-world environments. This paper proposes Human Simulation Computation (HSC), a human-inspired computational framework that models intelligence as a continuous, closed-loop process involving thinking, action, learning, reflection, and activity scheduling. HSC emphasizes active participation within the internal reasoning process and in interactions with the environment, where actions are used not only to achieve goals but also to automatically refine and improve internal reasoning mechanisms without external intervention. HSC incorporates human thinking strategies across all stages of the internal reasoning process, such as main-feature-oriented reasoning, scope expansion through action, and on-time learning driven by environmental feedback.",28.89,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13895v1_OmniOVCD Streamlining Open-Vocabulary Change Detec.pdf,OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3,"Xu Zhang, Danyang Li, Yingjie Xia, Xiaohang Dong, Hualong Yu, Jianye Wang, Qicheng Li",,,"Change Detection, Open-Vocabulary, SAM 3, Segment Anything Model, Synergistic Fusion, Instance Decoupling","This paper proposes OmniOVCD, a new framework for Open-Vocabulary Change Detection (OVCD) that utilizes the Segment Anything Model 3 (SAM 3) to streamline the process. By leveraging the decoupled output heads of SAM 3, the authors introduce a Synergistic Fusion to Instance Decoupling (SFID) strategy, which fuses semantic, instance, and presence outputs to construct land-cover masks and decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images, resulting in accurate change masks. Experiments on four public benchmarks demonstrate SOTA performance.",27.75,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13897v1_TractRLFusion A GPT-Based Multi-Critic Policy Fusi.pdf,TRACTRLFUSION: A GPT-BASED MULTI-CRITIC POLICY FUSION FRAMEWORK FOR FIBER TRACTOGRAPHY,"Ankita Joshi, Ashutosh Sharma, Anoushkrit Goel, Ranjeet Ranjan Jha, Chirag Ahuja, Arnav Bhavsar, Aditya Nigam",,,"Diffusion MRI, Tractography, Reinforcement Learning, Transformers","Tractography plays a pivotal role in the non-invasive reconstruction of white matter fiber pathways, providing vital information on brain connectivity and supporting precise neurosurgical planning. TractRLFusion, a novel GPT-based policy fusion framework, integrates multiple RL policies through a data-driven fusion strategy, demonstrating improved accuracy and anatomical reliability compared to individual RL policies and state-of-the-art classical and DRL methods.",28.09,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13904v1_PREFAB PREFerence-based Affective Modeling for Low.pdf,PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation,"Jaeyoung Moon, Youjin Choi, Yucheon Park, David Melhart, Georgios N. Yannakakis, Kyung-Joong Kim",10.1145/3675094.3678379,2601.13904,"Affective Computing, Preference Learning, Self-Annotation, User Modeling, Ordinal Representation, Peak-End Rule","PREFAB is a low-budget retrospective self-annotation method that targets affective inflection regions, employing a preference-learning model to detect relative affective changes and directing annotators to label only selected segments while interpolating the remainder of the stimulus.",27.37,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13920v1_Asymmetric regularization mechanism for GAN traini.pdf,Asymmetric regularization mechanism for GAN training with Variational Inequalities,"Spyridon C. Giagtzoglou, Mark H.M. Winands, Barbara Franci",,,"GANs, Variational Inequalities, Nash Equilibrium, Regularization, Extrapolation-from-the-Past (EFTP)",We propose an asymmetric regularization mechanism for stabilizing the training of generative adversarial networks (GANs) by formulating the training as a Nash equilibrium seeking problem and using variational inequalities and monotone operator theory. This approach ensures last-iterate linear convergence of a single-call Extrapolation-from-the-Past (EFTP) method and stabilizes the trajectory even when strong monotonicity cannot be achieved.,27.55,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13938v1_IF-GEO Conflict-Aware Instruction Fusion for Multi.pdf,IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization,"Heyang Zhou, JiaJia Chen, Xiaolu Chen, Jie Bao, Zhen Chen*, Yong Liao",,,"Generative Search Engines, Conflict-Aware Instruction Fusion, Multi-Query Optimization, Large Language Models, Source Visibility","Proposes IF-GEO, a framework for improving content visibility in generated responses through targeted document revisions, addressing the challenge of optimizing a document for diverse queries under a limited content budget by mining distinct optimization preferences from representative latent queries and synthesizing a Global Revision Blueprint for guided editing via conflict-aware instruction fusion.",27.09,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13942v1_Glance-or-Gaze Incentivizing LMMs to Adaptively Fo.pdf,Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning,"Hongbo Bai, Yujin Zhou, Yile Wu, Chi-Min Chan, Pengcheng Wen, Kunhao Pan, Sirui Han, Yike Guo",,not found,"Large Multimodal Models, Reinforcement Learning, Visual Question Answering, Knowledge Intensive Queries, Dynamic Information, Static Parametric Knowledge","This paper proposes Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. The dual-stage training strategy includes Reflective GoG Behavior Alignment via supervised fine-tuning and Complexity-Adaptive Reinforcement Learning, enhancing the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance.",27.86,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13948v1_Stream-Voice-Anon Enhancing Utility of Real-Time S.pdf,Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models,"Nikita Kuzmin, Songting Liu, Kong Aik Lee, Eng Siong Chng",Not provided,Not provided,"speaker anonymization, neural audio codec, language models, streaming, privacy preservation, disentanglement","This paper presents Stream-Voice-Anon, a system that adapts modern causal LM-based neural audio codec architectures specifically for streaming speaker anonymization. It incorporates techniques to prevent speaker information leakage and compares dynamic and fixed delay configurations to explore latency-privacy trade-offs in real-time scenarios.",26.88,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13964v1_RL-BioAug Label-Efficient Reinforcement Learning f.pdf,RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning,"Cheol-Hui Lee, Hwa-Yeon Lee, Dong-Joo Kim",,,"Reinforcement Learning, Self-Supervised Learning, EEG Representation Learning, Data Augmentation, Contrastive Learning","This paper proposes RL-BioAug, a framework that uses a label-efficient reinforcement learning agent to autonomously determine optimal augmentation policies for self-supervised EEG representation learning, significantly outperforming random selection strategies in terms of performance metrics.",26.58,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13969v1_Autonomous Knowledge Graph Exploration with Adapti.pdf,Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval,"Joaquín Polonuer, Lucas Vittor, Iñaki Arango, Ayush Noori, David A. Clifton, Luciano Del Corro, Marinka Zitnik",Not found,Not found,"Knowledge Graphs, Knowledge Retrieval, Language Models, Adaptive Retrieval, Graph Exploration","ARK: Adaptive Retriever of Knowledge is an agent-based KG retriever that balances breadth and depth in search, using global lexical search and one-hop neighborhood exploration. It adapts tool use to query type, improving retrieval performance over retrieval-based and agent-free methods.",26.17,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13992v1_The Whole Is Greater Than the Sum of Its Parts A C.pdf,The Whole Is Greater Than the Sum of Its Parts: A Compatibility-Aware Multi-Teacher CoT Distillation Framework,"Jin Cui, Jiaqi Guo, Jiepeng Zhou, Ruixuan Yang, Jiayi Lu, Jiajun Xu, Jiangcheng Song, Boran Zhao, Pengju Ren",,,"Chain-of-Thought, Multi-Teacher, CoT Distillation, Compact Student Models, Compatibility, Catastrophic Forgetting, Reasoning Path Diversity, Answer Consistency, Decoupled LoRA","This paper introduces COMPACT, a framework that adaptively fuses supervisions from different teachers to transfer reasoning prowess into compact student models (SLMs). It addresses the challenges of teacher-student incompatibility and passive supervision, ensuring genuine logic internalization and preventing catastrophic forgetting. The framework uses a multi-dimensional metric to dynamically weight teacher gradients based on the student's real-time compatibility, integrating diverse reasoning capabilities without damaging the model's original knowledge structure.",27.95,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13994v1_torch-sla Differentiable Sparse Linear Algebra wit.pdf,torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch,Mingyuan Chi,10.48550/arxiv.2601.13994,2601.13994,"PyTorch, sparse linear algebra, differentiable programming, adjoint solvers, multi-GPU scaling","An open-source PyTorch library for GPU-accelerated, scalable, and differentiable sparse linear algebra, addressing challenges in sparse matrix operations, multi-GPU scaling, and PyTorch integration for end-to-end differentiable simulations.",27.16,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.13999v1_DAME Duration-Aware Matryoshka Embedding for Durat.pdf,Duration-Aware Matryoshka Embedding for Short-Speaker Verification,"Youngmoon Jung*, Joon-Young Yang*, Ju-ho Kim, Jaeyoung Roh, Chang Woo Han, Hoon-Young Cho",Not provided,Not provided,"Speaker Verification, Short Utterance, Duration-Aware, Matryoshka Embedding, Multi-Scale Aggregation","Short-utterance speaker verification remains challenging due to limited speaker-discriminative cues in short speech segments. The proposed Duration-Aware Matryoshka Embedding (DAME) builds a nested hierarchy of sub-embeddings aligned to utterance durations, capturing compact speaker traits from short utterances and richer details from longer speech. DAME supports both training from scratch and fine-tuning, improving performance across durations.",28.04,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14012v1_MATE Matryoshka Audio-Text Embeddings for Open-Voc.pdf,MATE: MATRYOSHKA AUDIO–TEXT EMBEDDINGS FOR OPEN-VOCABULARY KEYWORD SPOTTING,"Youngmoon Jung, Myunghun Jung, Joon-Young Yang, Yong-Hyeok Lee, Jaeyoung Roh, Hoon-Young Cho",,,"Keyword spotting, open-vocabulary, text enrollment, audio–text embedding, deep metric learning","This paper proposes Matryoshka Audio–Text Embeddings (MATE), a dual-encoder framework that encodes multiple embedding granularities within a single vector via nested sub-embeddings. MATE is trained with standard deep metric learning objectives for audio–text keyword spotting, achieving state-of-the-art results on WSJ and LibriPhrase without any inference overhead.",27.66,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14022v1_Credible CO2 Comparisons A Machine Learning Approa.pdf,Credible CO2 Comparisons: A Machine Learning Approach to Vehicle Powertrain Assessment,"Rodrigo Pereira David, David1, Luciano Araujo Dourado Filho, Daniel Marques da Silva, João Alfredo Cal-Braz",xxx/xxxx,,"machine learning, vehicle emissions, electric vehicles","This paper proposes a machine learning-based framework for like-for-like operational assessment of internal combustion engine vehicles (ICEVs) and electric vehicles (EVs) under identical, real-world driving conditions, enabling fair and reproducible evaluation of powertrain technologies.",26.79,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14027v1_Numina-Lean-Agent An Open and General Agentic Reas.pdf,Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics,"Junqi Liu, Zihao Zhou, Zekai Zhu, Marco Dos Santos, Weikun He, Jiawei Liu, Ran Wang, Lihong Zhi, Qiufeng Wang, Wenda Li",,,"formal mathematics, agentic reasoning, theorem proving, Neural theorem proving, Lean","This paper introduces Numina-Lean-Agent, a general coding agent designed for formal theorem proving in mathematical systems like Lean. It combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, theorem retrieval, informal proving, and auxiliary reasoning. The system demonstrates strong performance, solving all problems in Putnam 2025 and successfully formalizing the Brascamp–Lieb theorem. It is released at https://github.com/project-numina/numina-lean-agent.",28.03,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14039v1_Generalizing Abstention for Noise-Robust Learning .pdf,Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation,"Wesam Moustafa, Hossam Elsafty, Helen Schneider, Lorenz Sparrenberg, Rafet Sifa",Not found,2601.14039,"Abstention, Medical Image Segmentation, Label Noise, Noise-Robust Learning, Loss Functions","This paper addresses the problem of label noise in medical image segmentation, a critical issue arising from the inherent difficulty of manual annotation. It introduces a universal and modular abstention framework capable of enhancing the noise-robustness of a diverse range of loss functions, demonstrating its versatility and effectiveness through experiments on the CaDIS and DSAD medical datasets.",27.65,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14041v1_Top 10 Open Challenges Steering the Future of Diff.pdf,Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants,"Yunhe Wang, Kai Han, Huiling Zhen, Yuchuan Tian, Hanting Chen, Yongbing Huang, Yufei Cui, Yingte Shu, Shan Gao, Ismail Elezi, Roy Vaughan Miles, Songcen Xu, Feng Wen, Chao Xu, Sinan Zeng, Dacheng Tao",,2601.14041,"Large Language Models, Diffusion Models, Transformers","This Perspective identifies ten fundamental challenges for Diffusion Language Models (DLMs), including architectural inertia and gradient sparsity, and proposes a strategic roadmap to overcome these limitations and enable next-generation AI capable of complex structural reasoning and dynamic self-correction.",29.14,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14047v1_Collective intelligence in science direct elicitat.pdf,COLLECTIVE INTELLIGENCE IN SCIENCE: DIRECT ELICITATION OF DIVERSE INFORMATION FROM EXPERTS WITH UNKNOWN INFORMATION STRUCTURE,"ALEXEY V. OSIPOV, NIKOLAY N. OSIPOV",Not found,2601.14047,"interpretability, wisdom of crowd, play money, prediction market, information pooling, information elicitation, rational expectation equilibrium, direct communication, large language models, scientific collaboration","Proposes a simple mechanism based on a self-resolving play-money prediction market entangled with a chat to efficiently aggregate relevant information from a large group of experts with private information on a complex scientific hypothesis, even if the ground truth cannot be established and experts initially know nothing about each other.",28.25,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14051v1_Kakugo Distillation of Low-Resource Languages into.pdf,Kakugo: Distillation of Low-Resource Languages into Small Language Models,"Peter Devine, Mardhiyah Sanni, Farid Adilazuarda, Julieta Gil Loizaga, Barry Haddow",Not found,Not found,"Small Language Models, Low-Resource Languages, Model Distillation, Synthetic Data, Teacher-Student Model, Natural Language Processing","We present Kakugo, a novel and cost-effective pipeline designed to train general-purpose Small Language Models (SLMs) for low-resource languages using only the language name as input. By using a large teacher model to generate synthetic prompts and translate instruction datasets, we produced training data and SLMs for 54 low-resource languages. Evaluations across a diverse set of general natural language processing tasks demonstrate that our pipeline consistently improves performance over base models.",26.89,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14053v1_LLMOrbit A Circular Taxonomy of Large Language Mod.pdf,LLMOrbit: A Circular Taxonomy of Large Language Models,"Badri N. Patro, Vijay S. Agneeswaran",,2601.14053,"large language models, artificial intelligence, generative AI, agentic systems, scaling wall, post-training gains, efficiency revolution, democratization","This paper presents LLMOrbit, a comprehensive circular taxonomy of large language models spanning 2019-2025, examining over 50 major models across 15 organizations through eight interconnected orbital dimensions. It identifies three critical crises threatening AI progress and six different paradigms to break the scaling wall, including test-time compute, quantization, distributed edge computing, model merging, efficient training, and small specialized models. The paper also discusses three fundamental paradigm shifts: post-training gains, efficiency revolution, and democratization, and provides insights into key techniques and future directions.",27.67,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14055v1_Decoder-Free Supervoxel GNN for Accurate Brain-Tum.pdf,Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI,"Andrea Protani, Marc Molina Van Den Bosch, Lorenzo Giusti, Heloisa Barbosa Da Silva, Paolo Cacace, Albert Sund Aillet, Miguel Angel Gonzalez Ballester, Friedhelm Hummel, Luigi Serio",Not found,2601.14055,"BrainTumorLocalization, GraphNeuralNetworks, Multi-modal MRI, Supervoxel, Regression","Our approach introduces SVGFormer, a decoder-free pipeline that partitions the volume into a semantic graph of super-voxels. Its hierarchical encoder learns rich node representations by combining patch-level Transformer with supervoxel-level GraphAttention Network, achieving strong performance in node-level classification and tumor proportion regression.",28.33,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14056v1_POCI-Diff Position Objects Consistently and Intera.pdf,POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion,"Andrea Rigo, Luca Stornaiuolo, Weijie Wang, Mauro Martino, Bruno Lepri, Nicu Sebe",Not found,2601.14056,"Diffusion, Image Generation, 3D Layout","We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes, supporting object insertion, removal, and transformation via regeneration rather than pixel deformation. It preserves object identity and consistency across edits using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence.",28.55,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14063v1_XCR-Bench A Multi-Task Benchmark for Evaluating Cu.pdf,XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs,"Mohsinul Kabir, Tasnim Ahmed, Md Mezbaur Rahman, Shaoxiong Ji, Hassan Alhuzali, Sophia Ananiadou",,,"large language models, cross-cultural competence, cultural reasoning, cross-cultural benchmark, Culture-Specific Items (CSIs), social etiquette, cultural reference, social norms, beliefs, values, linguistic bias, ethno-religious biases","XCR-Bench is a benchmark for evaluating cross-cultural reasoning in large language models (LLMs), consisting of 4.9k parallel sentences and 1,098 unique Culture-Specific Items (CSIs). It integrates Newmark’s CSI framework with Hall’s Triad of Culture, enabling systematic analysis of cultural reasoning beyond surface-level artifacts and into semi-visible and invisible cultural elements. The findings show that state-of-the-art LLMs exhibit consistent weaknesses in identifying and adapting CSIs related to social etiquette and cultural reference, and evidence of regional and ethno-religious biases during cultural adaptation.",28.08,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14069v1_Unsupervised Video Class-Incremental Learning via .pdf,Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management,"Nattapong Kurpukdee, Adrian G. Bors",Not provided,Not provided,"Unsupervised, Video Class-Incremental, Video Continual Learning, Deep Embedded Clustering","This paper proposes a simple yet effective approach to address unsupervised video class incremental learning, focusing on providing a set of representative video features during each task without assuming any class or task information. It progressively builds a series of deep clusters from the extracted features and uses the model updated from the previous task as an initial state for the current learning task. The approach significantly outperforms other baselines on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, by ignoring the labels from the supervised setting.",27.94,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14084v1_DermaBench A Clinician-Annotated Benchmark Dataset.pdf,DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning,"Abdurrahim Yilmaz, Ozan Erdem, Ece Gokyayla, Ayda Acar, Burc Bugra Dagtas, Dilara Ilhan Erdil, Gulsum Gencoglan, Burak Temelkuran",,,"Dermatology, Visual Question Answering, Multimodal Learning, Clinician Annotation, Benchmark Dataset","DermaBench is a clinician-annotated dermatology Visual Question Answering (VQA) benchmark dataset built on the Diverse Dermatology Images (DDI) dataset. It evaluates models' ability to interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions.",27.98,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14086v1_Two-Stream temporal transformer for video action c.pdf,TWO-STREAM TEMPORAL TRANSFORMER FOR VIDEO ACTION CLASSIFICATION,"Nattapong Kurpukdee, Adrian G. Bors",Not provided,Not provided,"Video Transformer, Optical Flow, Two-Stream video processing, Video Action Classification",This study introduces a new two-stream transformer video classifier that extracts spatio-temporal information from content and optical flow representing movement information. The proposed model identifies self-attention features across the joint optical flow and temporal frame domain and represents their relationships within the transformer encoder mechanism. Experimental results show excellent classification results on three well-known video datasets of human activities.,27.44,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14087v1_1-bit Count-based Sorting Unit to Reduce Link Powe.pdf,1-bit Count-based Sorting Unit to Reduce Link Power in DNN Accelerators,"Ruichi Han, Yizhi Chen, Tong Lei, Jordi Altayo Gonzalez, Ahmed Hemani",Not provided,Not provided,"1-bit count, approximate computing, bit transition reduction, link power","This work proposes a hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks (CNN) to reduce link power in DNN accelerators. By leveraging approximate computing to group population counts into coarse-grained buckets, our design achieves hardware area reductions while preserving the link power benefits of data reordering. The approximate sorting unit achieves up to 35.4% area reduction compared to a precise implementation.",27.33,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14091v1_Zero-shot adaptable task planning for autonomous c.pdf,Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems,"Hossein Naderi1, Alireza Shojaei2, Lifu Huang3, Philip Agee4, Kereshmeh Afsari5, Abiola Akanmu6",,,"Construction robotics, quadruped robots, robot task planning, multi-AI agent, LLMs, VLMs, GPT4o","This study explores the potential of foundation models to enhance the adaptability and generalizability of task planning in construction robots, proposing four models using lightweight, open-source large language models (LLMs) and vision language models (VLMs). These models include one single agent and three multi-agent teams that collaborate to create robot action plans. The models are evaluated across three construction roles: Painter, Safety Inspector, and Floor Tiling. Results show that the four-agent team outperforms the state-of-the-art GPT-4o in most metrics while being ten times more cost-effective. Additionally, teams with three and four agents demonstrate improved generalizability. By discussing how agent behaviors influence outputs, this study enhances the understanding of AI teams and supports future research in diverse unstructured environments beyond construction.",28.74,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14096v1_Remapping and navigation of an embedding space via.pdf,Remapping and navigation of an embedding space via error minimization: a fundamental organizational principle of cognition in natural and artificial systems,"Benedikt Hartl, Léo Pio-Lopez, Chris Fields, Michael Levin",Not found,2601.14096,"Evolution, Development, Intelligence, Active Inference, Navigation Policy, Nested Embedding Spaces",A fundamental organizational principle of cognition in natural and artificial systems is the remapping and navigation of an embedding space via error minimization.,30.43,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14099v1_Causal feature selection framework for stable soft.pdf,Causal Feature Selection Framework for Stable Soft Sensor Modeling based on Time-Delayed Cross Mapping,"Shi-Shun Chen, Xiao-Yang Li, Enrico Zio",,,"Causal Feature Selection, Soft Sensor Modeling, Time-Delayed Cross Mapping, Stability",This paper presents a causal feature selection framework for developing stable soft sensors using time-delayed cross mapping.,29.22,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14115v1_Riemannian Liquid Spatio-Temporal Graph Network.pdf,Riemannian Liquid Spatio-Temporal Graph Network,"Liangsi Lu, Jingchao Wang, Zhaorong Dai, Hanqian Liu, Yang Shi",10.1145/3774904.3792090,,"Riemannian Manifolds, Neural ODEs, Spatio-Temporal Graphs","This paper introduces the Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), a framework that combines continuous-time liquid dynamics with the geometric inductive biases of Riemannian manifolds to model graph evolution on curved spaces, overcoming the limitations of Euclidean space. Extensive experiments demonstrate superior performance on graphs with complex structures.",27.35,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14124v1_Style Transfer as Bias Mitigation Diffusion Models.pdf,Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic,"Saad Mankarious, Ayah Zirikly",Not found,2601.14124,"Bias mitigation, Synthetic data, Style transfer, Diffusion models, Mental health, Arabic language","This work proposes a pretraining-free diffusion-based approach for synthetic text generation that frames bias mitigation as a style transfer problem. Using the CARMA Arabic mental health corpus, it focuses on male-to-female style transfer to augment underrepresented female-authored content. Quantitative evaluations show high semantic fidelity and meaningful stylistic divergence, while qualitative analysis confirms plausible gender transformations.",27.82,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14152v1_Lost in the Prompt Order Revealing the Limitations.pdf,Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models,"Hyunjong Ok, Jaeho Lee",,,"language models, prompt sensitivity, causal attention, multiple-choice question answering","Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. This work investigates a striking case in multiple-choice question answering, where placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%. Through systematic architectural analysis, causal attention is identified as the core mechanism responsible for this phenomenon.",26.81,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14154v1_LLM Augmented Intervenable Multimodal Adaptor for .pdf,LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery,"Shubham Pandey†, Bhavin Jawade†, Srirangaraj Setlur†, Venu Govindaraju†, Kenneth Seastedt⋆",Not found,Not found,"deep learning, lung cancer surgery, postoperative complications, interventional deep learning, hyperspherical embedding, radiomics, machine learning","We present MIRA-CLE, a deep learning architecture for predicting postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRA-CLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. An interventional deep learning module enhances transparency and clinical utility, providing interpretable and actionable insights for domain experts. Validation on a real-world dataset shows MIRA-CLE outperforms traditional models and contemporary LLMs for personalized and explainable postoperative risk management.",28.49,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14157v1_ConceptCaps -- a Distilled Concept Dataset for Int.pdf,ConceptCaps - a Distilled Concept Dataset for Interpretability in Music Models,"Bruno Sienkiewicz, Łukasz Neumann, Mateusz Modrzejewski",,,"Music, Interpretability, Concept Activation Vectors, Variational Autoencoder, Language Model, Music Generation, Audio Captioning","A dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy, designed to improve coherence and controllability in concept-based interpretability methods like TCA V.",26.76,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14160v1_Domain-Adaptation through Synthetic Data Fine-Tuni.pdf,Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law,"Ali Hamza Bashir, Muhammad Rehan Khalid, Kostadin Cvejoski, Jana Birr, Armin Berger, Sandra Halscheidt, Christian Temath, Rafet Sifa, David Berghaus",,,"Domain Adaptation, Large Language Models, German Law, Synthetic Data, Parameter-Efficient Fine-Tuning","This paper presents an effective method for adapting advanced Large Language Models (LLMs) to German legal question answering through a novel synthetic data generation approach. The approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes, using rigorous automated filtering methods and parameter-efficient fine-tuning techniques. The results demonstrate significant outperformance of LLMs adapted with the synthetic dataset on German legal question answering tasks, highlighting the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.",28.4,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14171v1_Paper2Rebuttal A Multi-Agent Framework for Transpa.pdf,Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance,"Qianli Ma*, Chang Guo*, Zhiheng Tian*, Siyu Wang, Jipeng Xiao, Yuanhao Yue, Zhipeng Zhang†",,,"peer review, rebuttal generation, multi-agent systems, transparent assistance","This paper introduces REBUTTALAGENT, a multi-agent framework designed to assist authors in generating transparent and evidence-backed rebuttals to peer reviews. It reframes rebuttal generation as an evidence-centric planning task, decomposing complex feedback into atomic concerns and dynamically constructing hybrid contexts. The system integrates an autonomous external search module to resolve concerns requiring outside literature, ensuring every argument is anchored in verifiable manuscript details. The authors validate their approach on the proposed REBUTTALBENCH, demonstrating superior performance in coverage, faithfulness, and strategic coherence compared to strong baselines. The code will be released.",27.87,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14172v1_Human Values in a Single Sentence Moral Presence H.pdf,"Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum","Víctor Yeste, Paolo Rosso",,2601.14172,"human values, text sentiment, Schwartz motivational continuum, transformer ensembles, sentence-level detection","This study focuses on identifying the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. It addresses the challenges posed by sparse moral cues and class imbalance in out-of-context sentences from news and political manifestos, using a binary moral presence task and various models including DeBERTa-base with lightweight signals. The findings suggest that lightweight signals and small ensembles yield the most reliable improvements, while hierarchical gating offers limited benefit under the constraints of an 8GB single-GPU setup.",28.91,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14175v1_A model of errors in transformers.pdf,A model of errors in transformers,"Suvrat Raju, Praneeth Netrapalli",,,"transformers, language models, LLMs, errors, attention mechanism, quantitative model, error rate, deterministic tasks, arithmetic, dynamic programming, tower of Hanoi, list reversal, linear transformations","We study the error rate of LLMs on tasks like arithmetic that require a deterministic output and repetitive processing of tokens. We derive a quantitative two-parameter relationship between accuracy and task complexity, inspired by an effective field theory perspective. Our model provides an alternative to suggestions that errors indicate the collapse of reasoning or an inability to express compositional functions.",26.86,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14192v1_Toward Efficient Agents Memory Tool learning and P.pdf,"Toward Efficient Agents: A Survey of Memory, Tool learning, and Planning","Xiaofang Yang1,2,†, Lijun Li1,2,†, Heng Zhou1,3,†, Tong Zhu1,†, Xiaoye Qu1, Yuchen Fan1, Qianshan Wei5, Rui Ye4, Li Kang1,4, Yiran Qin6, Zhiqiang Kou7, Daizong Liu8, Qi Li5, Ning Ding9, Siheng Chen4, Jing Shao1",,2601.14192v1,"Agents, Efficiency, Agent Memory, Tool Learning, Planning","This paper investigates the efficiency of agents in three core components: memory, tool learning, and planning, considering costs such as latency, tokens, and steps. It reviews recent approaches that differ in implementation yet converge on shared high-level principles, including bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency. The trade-off between effectiveness and cost is also discussed through the Pareto frontier.",29.17,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14209v1_InT Self-Proposed Interventions Enable Credit Assi.pdf,InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning,"Matthew Y. R. Yang, Hao Bai, Ian Wu, Gene Yang, Amrith Setlur, Aviral Kumar",,arXiv:2309.16617,"Reinforcement Learning, Large Language Models, Credit Assignment, Outcome-Based Reinforcement Learning, Intervention Training","This paper introduces Intervention Training (InT), a method for improving the reasoning capabilities of large language models (LLMs) by fine-grained credit assignment on their own reasoning traces. InT proposes short, targeted corrections to steer trajectories toward higher reward, enabling effective credit assignment by upweighting the likelihood of the interventions in place of mistakes. The approach uses reference solutions and exploits the ease of verifying a model-generated solution compared to generating a correct one from scratch, identifying the first error and proposing a single-step intervention to redirect the trajectory toward the correct solution. The model then applies supervised fine-tuning to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. The results show a significant improvement in accuracy over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models.",28.95,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14230v1_MASCOT Towards Multi-Agent Socio-Collaborative Com.pdf,MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems,"Yiyang Wang, Yiqiao Jin, Alex Cabral, Josiah Hester",,,"Multi-agent systems, Socio-collaborative companions, Persona collapse, Social sycophancy, Persona-aware behavioral alignment, Collaborative dialogue optimization","Proposes MASCOT, a framework for multi-perspective socio-collaborative companions to address persona collapse and social sycophancy in multi-agent systems, improving persona consistency and social contribution.",26.59,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14232v1_KAGE-Bench Fast Known-Axis Visual Generalization E.pdf,KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning,"Egor Cherepannov, Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov",,2601.06099,"Reinforcement Learning, Pixel-based RL, Visual Generalization, Known-Axis Evaluation, JAX, 2D Platformer","This paper introduces KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. It defines KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, the authors observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. The fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors.",28.05,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14234v1_Q-learning with Adjoint Matching.pdf,Q-learning with Adjoint Matching,"Qiyang Li, Sergey Levine",Not found,2601.14234,"Reinforcement Learning, Q-learning, Adjoint Matching, Flow Policy, Diffusion Policy, Continuous Action","Q-learning with Adjoint Matching (QAM) is a novel TD-based reinforcement learning algorithm that tackles the challenge of optimizing expressive diffusion or flow-matching policies with respect to a parameterized Q-function. It sidesteps the numerical instability of direct gradient-based optimization through a multi-step denoising process by leveraging adjoint matching, a technique from generative modeling. QAM provides an unbiased, expressive policy at the optimum and consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.",27.18,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14235v1_Opportunities in AIML for the Rubin LSST Dark Ener.pdf,Opportunities in AI/ML for the Rubin LSST,"Dark Energy Science Collaboration, Eric Aubourg, Camille Avestruz, Matthew R. Becker, Biswajit Biswas, Rahul Biswas, Boris Bolliet, Adam S. Bolton, Clecio R. Bom, Raphaël Bonnet-Guerrini, Alexandre Boucaud, Jean-Eric Campagne, Chihway Chang, Aleksandra Ciprijanović, Johann Cohen-Tanugi, Michael W. Coughlin, John Franklin Crenshaw, Juan C. Cuevas-Tello, Juan de Vicente, Seth W. Digel, Steven Dillmann, Alex Drlica-Wagner, Sydney Erickson, Alexander T. Gagliano, Christos Georgiou, Aritra Ghosh, Matthew Grayling, Kirill A. Grishin, Alan Heavens, Lindsay R. House, Mustapha Ishak, Wassim Kabalan, Arun Kannawadi, Francis Lanusse, C. Danielle Leonard, Pierre-Franc François Léget, Michelle Lochner, Yao-Yuan Mao, Peter Melchior, Grant Merz, Martin Millon, Anais Moller, Gautham Narayan, Yuuki Omori, Hiranya Peiris, Andrés A. Plazas Malagón, Nesar Ramachandra, Benjamin Remy, Cécile Roucelle, Jaime Ruiz-Zapatero, Stefan Schuldt, Ignacio Sevilla-Noarbe, Ved G. Shah, Tjitske Starkenburg, Stephen Thorp, Laura Toribio San Cipriano, Tilman Tröster, Roberto Trotta, Padma Venkatraman, Amanda Wasserman, Tim White, Justine Zeghal, Tianqing Zhang, Yuanyuan Zhang",,,"AI, Machine Learning, Dark Energy, LSST",This paper discusses the opportunities and applications of AI/ML techniques in the context of the Rubin LSST for dark energy science.,27.61,Qwen2.5-3B,AMD RX 6800 (Vulkan)
2601.14255v1_VideoMaMa Mask-Guided Video Matting via Generative.pdf,VideoMaMa: Mask-Guided Video Matting via Generative Prior,"Sangbeom Lim, Seoung Wug Oh, Jiahui Huang, Heeji Yoon, Seungryong Kim, Joon-Young Lee",https://doi.org/10.1101/2601.14255v1,2601.14255v1,"video matting, mask-guided, generative prior, zero-shot generalization, pseudo-labeling","VideoMaMa is a diffusion-based model that generates high-quality alpha mattes from input binary segmentation masks, demonstrating strong zero-shot generalization to real-world footage.",28.55,Qwen2.5-3B,AMD RX 6800 (Vulkan)
