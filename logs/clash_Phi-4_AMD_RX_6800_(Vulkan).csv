filename,title,authors,doi,arxiv_id,keywords,summary,tps,model,platform
2601.07192v1_Relink Constructing Query-Driven Evidence Graph On.pdf,Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG,"Manzong Huang, Chenyang Bu, Yi He, Xingrui Zhuo, Xindong Wu",,,"GraphRAG, Knowledge Graph, Large Language Models, Retrieval-Augmented Generation, Open-Domain Question Answering, Evidence Graph, Latent Relations, Hallucinations","Graph-based Retrieval-Augmented Generation (GraphRAG) mitigates hallucinations in Large Language Models (LLMs) by grounding them in structured knowledge. However, current GraphRAG methods are limited by a static, pre-constructed Knowledge Graph (KG), which faces challenges such as incomplete reasoning paths and the presence of distractor facts. To address these issues, the paper proposes Relink, a framework that dynamically constructs a query-specific evidence graph. Relink instantiates required facts from a latent relation pool derived from the original text corpus to repair broken paths and employs a query-aware evaluation strategy to select the most useful candidates for answering queries. This approach allows Relink to discard distractor facts and construct precise evidence paths, achieving significant improvements in performance on Open-Domain Question Answering benchmarks.",18.33,Phi-4,AMD RX 6800 (Vulkan)
2601.07197v1_Beyond Variance Knowledge-Aware LLM Compression vi.pdf,Beyond Variance: Knowledge-Aware LLM Compression via Fisher-Aligned Subspace Diagnostics,"Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma",,,"Large Language Models, activation compression, Fisher Information Matrix, Singular Value Decomposition, knowledge preservation, Fisher-Aligned Subspace Compression, Dependence Violation Score, transformer architectures","This paper introduces Fisher-Aligned Subspace Compression (FASC), a novel framework for compressing Large Language Models (LLMs) that prioritizes factual knowledge preservation. Unlike traditional methods such as Singular Value Decomposition (SVD), which focus on high-variance dimensions, FASC uses the Fisher Information Matrix to identify and preserve dimensions critical for factual knowledge, often found in low-variance but high-gradient-sensitivity subspaces. The paper proposes the Dependence Violation Score (ρ) as a diagnostic metric to quantify activation-gradient coupling, revealing where factual knowledge is stored within transformer architectures. Experiments on models like Mistral-7B and Llama-3-8B show that FASC maintains 6-8% more accuracy on knowledge-intensive benchmarks compared to variance-based methods at 50% rank reduction. This enables a 7B model to match the factual recall of a 13B uncompressed model, highlighting ρ as a key indicator of stored knowledge.",19.95,Phi-4,AMD RX 6800 (Vulkan)
2601.07199v1_Forward versus Backward Comparing Reasoning Object.pdf,Forward versus Backward: Comparing Reasoning Objectives in Direct Preference Optimization,"Murtaza Nikazad, Raghuram Ramanujan",,,"Direct Preference Optimization, reasoning, large language models, hallucination, GSM8K, Low-Rank Adaptation","This paper explores the impact of different training objectives on the reasoning reliability of large language models using Direct Preference Optimization. It compares forward chain-of-thought generation, which trains models to produce correct reasoning traces, with backward verification, which trains models to verify and acknowledge errors in candidate solutions. Experiments on the GSM8K dataset reveal a trade-off: forward-only training significantly improves accuracy, while backward-only training reduces false positives but offers minimal accuracy gains. Both methods decrease the model's error acknowledgment rate, suggesting increased confidence in outputs. The study concludes that forward and backward reasoning objectives provide distinct and complementary learning signals, enhancing problem-solving and verification calibration, respectively. The research pipeline, implemented via Low-Rank Adaptation, is made available for further study.",20.67,Phi-4,AMD RX 6800 (Vulkan)
2601.07200v1_Safeguarding LLM Fine-tuning via Push-Pull Distrib.pdf,Safeguarding LLM Fine-tuning via Push-Pull Distributional Alignment,"Haozhong Wang, Zhuo Li, Yibo Yang, He Zhao, Hongyuan Zha, Dandan Guo",,,"Large Language Models, fine-tuning, safety alignment, Optimal Transport, distribution-level alignment, data purification","The paper introduces Safety Optimal Transport (SOT), a novel framework for safe fine-tuning of Large Language Models (LLMs). SOT addresses the erosion of safety alignment during fine-tuning by reframing the challenge from an instance-level filtering task to a distribution-level alignment task. It employs a dual-reference 'push-pull' weight-learning mechanism that optimizes sample importance by pulling the downstream distribution towards a trusted safe anchor and pushing it away from a harmful reference. This approach establishes a robust geometric safety boundary, effectively purifying the training data. Extensive experiments demonstrate that SOT enhances model safety while maintaining competitive performance, achieving a superior safety-utility trade-off compared to existing methods.",18.86,Phi-4,AMD RX 6800 (Vulkan)
2601.07201v1_CalPro Prior-Aware Evidential--Conformal Predictio.pdf,CalPro: Prior-Aware Evidential–Conformal Prediction with Structure-Aware Guarantees for Protein Structures,"Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma",,,"protein structure prediction, uncertainty quantification, conformal prediction, evidential learning, distribution shift, calibration, ligand docking","CalPro introduces a novel framework for robust uncertainty quantification in protein structure prediction, addressing the limitations of existing confidence estimates like pLDDT. It combines a geometric evidential head, a differentiable conformal layer, and domain priors to provide shift-robust uncertainty quantification. The framework maintains near-nominal coverage and achieves tighter intervals in regions where priors are informative. Empirical results show significant improvements in coverage degradation, calibration error, and ligand-docking success compared to baselines. The approach is validated on both biological and non-biological benchmarks, demonstrating its applicability to structured regression tasks where local reliability is encoded as priors.",18.72,Phi-4,AMD RX 6800 (Vulkan)
